{
  "topics": {
    "data": {
      "0": {
        "name": "0_speech_ASR_model_models",
        "keywords": [
          [
            "speech",
            0.024665049211856967
          ],
          [
            "ASR",
            0.017933849407870697
          ],
          [
            "model",
            0.016080309372167235
          ],
          [
            "models",
            0.014066962690344149
          ],
          [
            "Speech",
            0.01261686504881634
          ],
          [
            "language",
            0.012288277040345382
          ],
          [
            "training",
            0.011871201279461513
          ],
          [
            "data",
            0.011780985690274536
          ],
          [
            "TTS",
            0.011676440287705106
          ],
          [
            "text",
            0.011451181430462587
          ]
        ],
        "count": 3710
      },
      "1": {
        "name": "1_music_generation_musical_audio",
        "keywords": [
          [
            "music",
            0.04381413738878939
          ],
          [
            "generation",
            0.019898128698315604
          ],
          [
            "musical",
            0.018105421145693627
          ],
          [
            "audio",
            0.01750203334472416
          ],
          [
            "Music",
            0.016662085408757788
          ],
          [
            "model",
            0.013032922308033965
          ],
          [
            "models",
            0.011563103381899491
          ],
          [
            "text",
            0.009158183044818058
          ],
          [
            "music generation",
            0.008639264506386435
          ],
          [
            "symbolic",
            0.008258744732023936
          ]
        ],
        "count": 1396
      },
      "2": {
        "name": "2_speaker_Speaker_diarization_speech",
        "keywords": [
          [
            "speaker",
            0.04315876204517221
          ],
          [
            "Speaker",
            0.01898820438436267
          ],
          [
            "diarization",
            0.01709308840464635
          ],
          [
            "speech",
            0.014355974407014315
          ],
          [
            "verification",
            0.012327751612150323
          ],
          [
            "model",
            0.011685724522850124
          ],
          [
            "visual",
            0.011042980458493867
          ],
          [
            "performance",
            0.010945645168372847
          ],
          [
            "recognition",
            0.01093948401406091
          ],
          [
            "speaker verification",
            0.010820597259320207
          ]
        ],
        "count": 1117
      },
      "3": {
        "name": "3_attacks_audio_detection_adversarial",
        "keywords": [
          [
            "attacks",
            0.022289710277884865
          ],
          [
            "audio",
            0.019997649159432248
          ],
          [
            "detection",
            0.018221893469072832
          ],
          [
            "adversarial",
            0.0172274534995553
          ],
          [
            "deepfake",
            0.01582506927391507
          ],
          [
            "speech",
            0.015713369035760115
          ],
          [
            "spoofing",
            0.013838405405593328
          ],
          [
            "attack",
            0.013356836403436552
          ],
          [
            "privacy",
            0.012423955399290552
          ],
          [
            "speaker",
            0.011853833959845444
          ]
        ],
        "count": 888
      },
      "4": {
        "name": "4_enhancement_speech_speech enhancement_noise",
        "keywords": [
          [
            "enhancement",
            0.0294152882587389
          ],
          [
            "speech",
            0.02874768509255403
          ],
          [
            "speech enhancement",
            0.0248646728212655
          ],
          [
            "noise",
            0.014997729009352674
          ],
          [
            "channel",
            0.013974889260040336
          ],
          [
            "network",
            0.013771512891002879
          ],
          [
            "Speech",
            0.012974842550604554
          ],
          [
            "Enhancement",
            0.012828164985070223
          ],
          [
            "SE",
            0.012754659318578292
          ],
          [
            "performance",
            0.012523337039795747
          ]
        ],
        "count": 751
      },
      "5": {
        "name": "5_sound_acoustic_room_source",
        "keywords": [
          [
            "sound",
            0.029488555623634446
          ],
          [
            "acoustic",
            0.019336088125001608
          ],
          [
            "room",
            0.016760905628220284
          ],
          [
            "source",
            0.01514129045460834
          ],
          [
            "method",
            0.015099805226619571
          ],
          [
            "microphone",
            0.013724536447193299
          ],
          [
            "field",
            0.013500438468072373
          ],
          [
            "spatial",
            0.013095481492011436
          ],
          [
            "estimation",
            0.011981144480088768
          ],
          [
            "localization",
            0.011908043634237463
          ]
        ],
        "count": 681
      },
      "6": {
        "name": "6_event_audio_sound_classification",
        "keywords": [
          [
            "event",
            0.020407033855806726
          ],
          [
            "audio",
            0.020390727354704895
          ],
          [
            "sound",
            0.019961428737572513
          ],
          [
            "classification",
            0.015825987445190395
          ],
          [
            "events",
            0.012918496258919132
          ],
          [
            "data",
            0.012569830719937647
          ],
          [
            "SED",
            0.012531382206027866
          ],
          [
            "sound event",
            0.012482068962519821
          ],
          [
            "Sound",
            0.011782974473378992
          ],
          [
            "learning",
            0.011529076955600664
          ]
        ],
        "count": 618
      },
      "7": {
        "name": "7_emotion_emotion recognition_Emotion_recognition",
        "keywords": [
          [
            "emotion",
            0.04086556897674479
          ],
          [
            "emotion recognition",
            0.02615030313231896
          ],
          [
            "Emotion",
            0.025239501617280034
          ],
          [
            "recognition",
            0.02023959424359984
          ],
          [
            "Recognition",
            0.01615925835196395
          ],
          [
            "emotional",
            0.014328251925144813
          ],
          [
            "speech",
            0.01423231194207246
          ],
          [
            "speech emotion",
            0.013928970609938977
          ],
          [
            "emotions",
            0.01387158459116965
          ],
          [
            "Speech",
            0.013350498742488802
          ]
        ],
        "count": 542
      },
      "8": {
        "name": "8_speech_EEG_features_dysarthric",
        "keywords": [
          [
            "speech",
            0.03043990663076137
          ],
          [
            "EEG",
            0.017924884676701038
          ],
          [
            "features",
            0.015344777305415676
          ],
          [
            "dysarthric",
            0.013089496502582431
          ],
          [
            "articulatory",
            0.012393811514299293
          ],
          [
            "data",
            0.011136671565559562
          ],
          [
            "Speech",
            0.0111010122693329
          ],
          [
            "model",
            0.010412955242358012
          ],
          [
            "signals",
            0.008865394331012114
          ],
          [
            "classification",
            0.008679502563590126
          ]
        ],
        "count": 535
      },
      "9": {
        "name": "9_audio_visual_Audio_text",
        "keywords": [
          [
            "audio",
            0.048272932677527525
          ],
          [
            "visual",
            0.023851511963272582
          ],
          [
            "Audio",
            0.02363182815998132
          ],
          [
            "text",
            0.01583374640672776
          ],
          [
            "captioning",
            0.01353781626859514
          ],
          [
            "language",
            0.013468007856808108
          ],
          [
            "video",
            0.012307207586256953
          ],
          [
            "models",
            0.012217115026127046
          ],
          [
            "modal",
            0.011352475317794047
          ],
          [
            "sound",
            0.011086720936446817
          ]
        ],
        "count": 517
      },
      "10": {
        "name": "10_speech_quality_audio_neural",
        "keywords": [
          [
            "speech",
            0.022183051722476452
          ],
          [
            "quality",
            0.018234790370444447
          ],
          [
            "audio",
            0.015626255025198734
          ],
          [
            "neural",
            0.01521515342104825
          ],
          [
            "model",
            0.014256844870849798
          ],
          [
            "high",
            0.01345122752835521
          ],
          [
            "models",
            0.013390439063525541
          ],
          [
            "generative",
            0.012450447115774633
          ],
          [
            "codecs",
            0.012196884691688089
          ],
          [
            "vocoder",
            0.012191894594915787
          ]
        ],
        "count": 349
      },
      "11": {
        "name": "11_COVID_respiratory_cough_heart",
        "keywords": [
          [
            "COVID",
            0.04037034915291991
          ],
          [
            "respiratory",
            0.02900955355631456
          ],
          [
            "cough",
            0.02617290401728421
          ],
          [
            "heart",
            0.019646741896204037
          ],
          [
            "sounds",
            0.01865996903879566
          ],
          [
            "sound",
            0.017212819359609503
          ],
          [
            "detection",
            0.014581542460775216
          ],
          [
            "classification",
            0.014232348142575539
          ],
          [
            "data",
            0.013176120604777236
          ],
          [
            "learning",
            0.01233244369035914
          ]
        ],
        "count": 331
      },
      "12": {
        "name": "12_separation_source_source separation_audio",
        "keywords": [
          [
            "separation",
            0.027683539295826346
          ],
          [
            "source",
            0.022334438779493874
          ],
          [
            "source separation",
            0.02224719986497567
          ],
          [
            "audio",
            0.021187737197867273
          ],
          [
            "music",
            0.015677625456539283
          ],
          [
            "model",
            0.013589232577095071
          ],
          [
            "effects",
            0.01298296195450178
          ],
          [
            "neural",
            0.01210102513016368
          ],
          [
            "time",
            0.010959957712895837
          ],
          [
            "sound",
            0.010887281247034987
          ]
        ],
        "count": 303
      },
      "13": {
        "name": "13_singing_singing voice_voice_Singing",
        "keywords": [
          [
            "singing",
            0.08268055888992547
          ],
          [
            "singing voice",
            0.04643518274556669
          ],
          [
            "voice",
            0.042884125276273395
          ],
          [
            "Singing",
            0.03327005324837384
          ],
          [
            "SVS",
            0.02672028817565196
          ],
          [
            "singer",
            0.02150504739791059
          ],
          [
            "Voice",
            0.018727199572302417
          ],
          [
            "voice synthesis",
            0.018682562619222853
          ],
          [
            "pitch",
            0.01829213874977256
          ],
          [
            "singing voice synthesis",
            0.01625341923074023
          ]
        ],
        "count": 190
      },
      "14": {
        "name": "14_species_bird_classification_data",
        "keywords": [
          [
            "species",
            0.036053799561024814
          ],
          [
            "bird",
            0.025968135084434386
          ],
          [
            "classification",
            0.021554930272881738
          ],
          [
            "data",
            0.019962391296567284
          ],
          [
            "monitoring",
            0.019564833983012966
          ],
          [
            "learning",
            0.018385009193117356
          ],
          [
            "bioacoustic",
            0.015689039195972965
          ],
          [
            "animal",
            0.014048260324997276
          ],
          [
            "vocalizations",
            0.013904391414981423
          ],
          [
            "acoustic",
            0.013897285365491067
          ]
        ],
        "count": 172
      },
      "15": {
        "name": "15_KWS_keyword_spotting_Keyword",
        "keywords": [
          [
            "KWS",
            0.05840247818462445
          ],
          [
            "keyword",
            0.04745709118563305
          ],
          [
            "spotting",
            0.03378947645389047
          ],
          [
            "Keyword",
            0.03168313416189051
          ],
          [
            "keyword spotting",
            0.02826884049998163
          ],
          [
            "Spotting",
            0.023568647066597583
          ],
          [
            "keywords",
            0.018675650699328705
          ],
          [
            "model",
            0.016224860711893246
          ],
          [
            "devices",
            0.011970910910662574
          ],
          [
            "false",
            0.011204341500538884
          ]
        ],
        "count": 171
      },
      "16": {
        "name": "16_face_talking_lip_speech",
        "keywords": [
          [
            "face",
            0.034098063775325954
          ],
          [
            "talking",
            0.030615816226344596
          ],
          [
            "lip",
            0.02794589972778246
          ],
          [
            "speech",
            0.024914750462236807
          ],
          [
            "facial",
            0.024089875178023353
          ],
          [
            "video",
            0.023353838059136465
          ],
          [
            "audio",
            0.018653872600096735
          ],
          [
            "head",
            0.015188438195456006
          ],
          [
            "3D",
            0.014692195773950057
          ],
          [
            "visual",
            0.01360693678316667
          ]
        ],
        "count": 139
      },
      "17": {
        "name": "17_anomalous_detection_machine_ASD",
        "keywords": [
          [
            "anomalous",
            0.036448999633393986
          ],
          [
            "detection",
            0.03043177223757324
          ],
          [
            "machine",
            0.028858192968544676
          ],
          [
            "ASD",
            0.024427751735183034
          ],
          [
            "sound",
            0.022642854550497648
          ],
          [
            "anomaly",
            0.020078714608235203
          ],
          [
            "Anomalous",
            0.01981008999931344
          ],
          [
            "anomalous sound",
            0.019603667361157595
          ],
          [
            "sounds",
            0.01930884629488213
          ],
          [
            "sound detection",
            0.01906924948482998
          ]
        ],
        "count": 131
      },
      "18": {
        "name": "18_MOS_quality_speech_prediction",
        "keywords": [
          [
            "MOS",
            0.0464717413612549
          ],
          [
            "quality",
            0.03244813178041213
          ],
          [
            "speech",
            0.028214220801855823
          ],
          [
            "prediction",
            0.02436733725975708
          ],
          [
            "speech quality",
            0.022654819562253417
          ],
          [
            "assessment",
            0.01759462017366565
          ],
          [
            "MOS prediction",
            0.017023214024334255
          ],
          [
            "Speech",
            0.01449133518877433
          ],
          [
            "models",
            0.014490244302865954
          ],
          [
            "model",
            0.014250835408449574
          ]
        ],
        "count": 128
      }
    },
    "correlations": [
      [
        1.0,
        -0.7206337354543855,
        -0.6443190640799122,
        -0.6655121708453224,
        -0.6437869035749536,
        -0.7341607340443421,
        -0.7373444360264374,
        -0.6784008685565523,
        -0.5581393639513818,
        -0.697734133910114,
        -0.6225179284326837,
        -0.7501787702639686,
        -0.7237742329563414,
        -0.7336116681716972,
        -0.6906876428597172,
        -0.7518777316558318,
        -0.6552465520819799,
        -0.7457231522902348,
        -0.6341041159893805
      ],
      [
        -0.7206337354543855,
        1.0,
        -0.7562920511557432,
        -0.6912665802246325,
        -0.7463694823385443,
        -0.7432746980580954,
        -0.6919193657015887,
        -0.7384317977643022,
        -0.7331512457485205,
        -0.6366130691163416,
        -0.7021099341534436,
        -0.7632543380700312,
        -0.6676597075607298,
        -0.7423145802646081,
        -0.7224991610937415,
        -0.7632678816449369,
        -0.7368739341870549,
        -0.7406320163925142,
        -0.7346104259812927
      ],
      [
        -0.6443190640799122,
        -0.7562920511557432,
        1.0,
        -0.6823707368812908,
        -0.7032258344573503,
        -0.7478216645574782,
        -0.7536166540944358,
        -0.6875644135009953,
        -0.6465739266469027,
        -0.723807402763488,
        -0.6994153539904218,
        -0.7556412727620003,
        -0.7200886167279137,
        -0.6943110578501248,
        -0.7239642838244622,
        -0.756698851919055,
        -0.6989557933158215,
        -0.7482256625413579,
        -0.7065346427465866
      ],
      [
        -0.6655121708453224,
        -0.6912665802246325,
        -0.6823707368812908,
        1.0,
        -0.6619781476415787,
        -0.7473061616546589,
        -0.6431465015031084,
        -0.7339497700624549,
        -0.5941744404089462,
        -0.5923979506254637,
        -0.6270642475769015,
        -0.7473316599843224,
        -0.6510051967866123,
        -0.7332496602048846,
        -0.7208049106042205,
        -0.753937091671961,
        -0.600941756068746,
        -0.6156252642375908,
        -0.6545475995631624
      ],
      [
        -0.6437869035749536,
        -0.7463694823385443,
        -0.7032258344573503,
        -0.6619781476415787,
        1.0,
        -0.7294995000657964,
        -0.7456543692096216,
        -0.7416695567404388,
        -0.574105906875277,
        -0.7274198854544598,
        -0.4734534301462657,
        -0.7582564017004746,
        -0.6854472781587462,
        -0.7524123978156116,
        -0.7328527669623952,
        -0.7570073956463004,
        -0.6005551171999509,
        -0.7525197883130315,
        -0.5697105564863671
      ],
      [
        -0.7341607340443421,
        -0.7432746980580954,
        -0.7478216645574782,
        -0.7473061616546589,
        -0.7294995000657964,
        1.0,
        -0.6422511049445607,
        -0.758131081534763,
        -0.7389224419400819,
        -0.7094405284050369,
        -0.7380248248491668,
        -0.7432272539855528,
        -0.7142271790521713,
        -0.7534205475921208,
        -0.7304479367527371,
        -0.7644676033321247,
        -0.7542553791652509,
        -0.657413197629952,
        -0.7495109192273439
      ],
      [
        -0.7373444360264374,
        -0.6919193657015887,
        -0.7536166540944358,
        -0.6431465015031084,
        -0.7456543692096216,
        -0.6422511049445607,
        1.0,
        -0.7466727448445141,
        -0.743433545118704,
        -0.5549952076416478,
        -0.7434820467300964,
        -0.7345063856784437,
        -0.6396557785782936,
        -0.7600121532810181,
        -0.6530907043197909,
        -0.7572784731428649,
        -0.7358314312411861,
        -0.6452500709088584,
        -0.7458793755436242
      ],
      [
        -0.6784008685565523,
        -0.7384317977643022,
        -0.6875644135009953,
        -0.7339497700624549,
        -0.7416695567404388,
        -0.758131081534763,
        -0.7466727448445141,
        1.0,
        -0.6502706500035953,
        -0.7265183273249423,
        -0.734885280843389,
        -0.7601720062511279,
        -0.7522634758476844,
        -0.7510772259803007,
        -0.744599873033283,
        -0.7583631647703131,
        -0.7237979788107824,
        -0.7541735217299652,
        -0.7325879012666465
      ],
      [
        -0.5581393639513818,
        -0.7331512457485205,
        -0.6465739266469027,
        -0.5941744404089462,
        -0.574105906875277,
        -0.7389224419400819,
        -0.743433545118704,
        -0.6502706500035953,
        1.0,
        -0.7202907433384624,
        -0.5355178936524722,
        -0.7483801408315736,
        -0.7344151248864995,
        -0.7384411110265259,
        -0.7297938495717049,
        -0.7554659096724898,
        -0.5148814847721336,
        -0.7386887257431458,
        -0.5824274982040694
      ],
      [
        -0.697734133910114,
        -0.6366130691163416,
        -0.723807402763488,
        -0.5923979506254637,
        -0.7274198854544598,
        -0.7094405284050369,
        -0.5549952076416478,
        -0.7265183273249423,
        -0.7202907433384624,
        1.0,
        -0.7010544633555671,
        -0.7526542329010397,
        -0.6098973613512679,
        -0.7510523171507559,
        -0.7034265776772792,
        -0.7527521921272822,
        -0.6413523293260194,
        -0.7288469738940471,
        -0.7245603916375729
      ],
      [
        -0.6225179284326837,
        -0.7021099341534436,
        -0.6994153539904218,
        -0.6270642475769015,
        -0.4734534301462657,
        -0.7380248248491668,
        -0.7434820467300964,
        -0.734885280843389,
        -0.5355178936524722,
        -0.7010544633555671,
        1.0,
        -0.7564967277262553,
        -0.7229061333602246,
        -0.7179425194183886,
        -0.7305204687280693,
        -0.7599703494590071,
        -0.5543649457447182,
        -0.7510724312896628,
        -0.3018677550893569
      ],
      [
        -0.7501787702639686,
        -0.7632543380700312,
        -0.7556412727620003,
        -0.7473316599843224,
        -0.7582564017004746,
        -0.7432272539855528,
        -0.7345063856784437,
        -0.7601720062511279,
        -0.7483801408315736,
        -0.7526542329010397,
        -0.7564967277262553,
        1.0,
        -0.7581238806313186,
        -0.7597730967917805,
        -0.7326798895519813,
        -0.7584732276341741,
        -0.7599882222076524,
        -0.7167583174222252,
        -0.7600439198711159
      ],
      [
        -0.7237742329563414,
        -0.6676597075607298,
        -0.7200886167279137,
        -0.6510051967866123,
        -0.6854472781587462,
        -0.7142271790521713,
        -0.6396557785782936,
        -0.7522634758476844,
        -0.7344151248864995,
        -0.6098973613512679,
        -0.7229061333602246,
        -0.7581238806313186,
        1.0,
        -0.7325258314866612,
        -0.72754498028935,
        -0.7622317952353669,
        -0.7312261818170936,
        -0.7446993689171331,
        -0.7359844735170198
      ],
      [
        -0.7336116681716972,
        -0.7423145802646081,
        -0.6943110578501248,
        -0.7332496602048846,
        -0.7524123978156116,
        -0.7534205475921208,
        -0.7600121532810181,
        -0.7510772259803007,
        -0.7384411110265259,
        -0.7510523171507559,
        -0.7179425194183886,
        -0.7597730967917805,
        -0.7325258314866612,
        1.0,
        -0.7457053175879031,
        -0.7633148832508443,
        -0.7341728493072014,
        -0.755045135591262,
        -0.720460434590408
      ],
      [
        -0.6906876428597172,
        -0.7224991610937415,
        -0.7239642838244622,
        -0.7208049106042205,
        -0.7328527669623952,
        -0.7304479367527371,
        -0.6530907043197909,
        -0.744599873033283,
        -0.7297938495717049,
        -0.7034265776772792,
        -0.7305204687280693,
        -0.7326798895519813,
        -0.72754498028935,
        -0.7457053175879031,
        1.0,
        -0.7534758965712729,
        -0.7355508890569687,
        -0.6422303442421771,
        -0.7244207257862756
      ],
      [
        -0.7518777316558318,
        -0.7632678816449369,
        -0.756698851919055,
        -0.753937091671961,
        -0.7570073956463004,
        -0.7644676033321247,
        -0.7572784731428649,
        -0.7583631647703131,
        -0.7554659096724898,
        -0.7527521921272822,
        -0.7599703494590071,
        -0.7584732276341741,
        -0.7622317952353669,
        -0.7633148832508443,
        -0.7534758965712729,
        1.0,
        -0.7588099228889422,
        -0.7573221924279414,
        -0.7616821174208851
      ],
      [
        -0.6552465520819799,
        -0.7368739341870549,
        -0.6989557933158215,
        -0.600941756068746,
        -0.6005551171999509,
        -0.7542553791652509,
        -0.7358314312411861,
        -0.7237979788107824,
        -0.5148814847721336,
        -0.6413523293260194,
        -0.5543649457447182,
        -0.7599882222076524,
        -0.7312261818170936,
        -0.7341728493072014,
        -0.7355508890569687,
        -0.7588099228889422,
        1.0,
        -0.7411689145174278,
        -0.5975882609440424
      ],
      [
        -0.7457231522902348,
        -0.7406320163925142,
        -0.7482256625413579,
        -0.6156252642375908,
        -0.7525197883130315,
        -0.657413197629952,
        -0.6452500709088584,
        -0.7541735217299652,
        -0.7386887257431458,
        -0.7288469738940471,
        -0.7510724312896628,
        -0.7167583174222252,
        -0.7446993689171331,
        -0.755045135591262,
        -0.6422303442421771,
        -0.7573221924279414,
        -0.7411689145174278,
        1.0,
        -0.7481119115421881
      ],
      [
        -0.6341041159893805,
        -0.7346104259812927,
        -0.7065346427465866,
        -0.6545475995631624,
        -0.5697105564863671,
        -0.7495109192273439,
        -0.7458793755436242,
        -0.7325879012666465,
        -0.5824274982040694,
        -0.7245603916375729,
        -0.3018677550893569,
        -0.7600439198711159,
        -0.7359844735170198,
        -0.720460434590408,
        -0.7244207257862756,
        -0.7616821174208851,
        -0.5975882609440424,
        -0.7481119115421881,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        37,
        4,
        11,
        2,
        7,
        2,
        5,
        5,
        2,
        6,
        0,
        3,
        2,
        2,
        3,
        3,
        0,
        0,
        0
      ],
      "2020-02": [
        36,
        5,
        20,
        3,
        15,
        3,
        8,
        8,
        4,
        9,
        0,
        0,
        8,
        4,
        6,
        1,
        3,
        3,
        4
      ],
      "2020-03": [
        32,
        3,
        12,
        3,
        8,
        3,
        3,
        7,
        2,
        4,
        0,
        1,
        5,
        3,
        2,
        0,
        2,
        1,
        6
      ],
      "2020-04": [
        42,
        14,
        23,
        1,
        6,
        1,
        2,
        1,
        4,
        7,
        0,
        5,
        5,
        4,
        1,
        1,
        1,
        2,
        2
      ],
      "2020-05": [
        89,
        5,
        22,
        2,
        18,
        4,
        4,
        7,
        10,
        15,
        1,
        6,
        5,
        6,
        10,
        7,
        2,
        5,
        12
      ],
      "2020-06": [
        49,
        15,
        13,
        2,
        14,
        5,
        3,
        3,
        9,
        8,
        0,
        4,
        5,
        6,
        7,
        1,
        2,
        4,
        3
      ],
      "2020-07": [
        44,
        12,
        21,
        3,
        9,
        9,
        15,
        7,
        3,
        19,
        0,
        0,
        6,
        2,
        4,
        2,
        0,
        1,
        9
      ],
      "2020-08": [
        51,
        24,
        37,
        2,
        11,
        3,
        1,
        10,
        6,
        11,
        1,
        3,
        10,
        9,
        1,
        4,
        3,
        1,
        11
      ],
      "2020-09": [
        24,
        5,
        8,
        0,
        3,
        1,
        2,
        7,
        4,
        3,
        0,
        2,
        4,
        5,
        3,
        4,
        0,
        2,
        7
      ],
      "2020-10": [
        100,
        25,
        42,
        6,
        17,
        9,
        8,
        12,
        6,
        22,
        1,
        6,
        15,
        6,
        7,
        4,
        4,
        1,
        13
      ],
      "2020-11": [
        58,
        12,
        28,
        2,
        20,
        7,
        5,
        6,
        5,
        10,
        1,
        5,
        9,
        5,
        2,
        3,
        1,
        1,
        7
      ],
      "2020-12": [
        36,
        7,
        17,
        2,
        7,
        8,
        1,
        1,
        3,
        6,
        2,
        8,
        3,
        2,
        4,
        1,
        0,
        3,
        8
      ],
      "2021-01": [
        26,
        9,
        7,
        2,
        5,
        2,
        3,
        3,
        1,
        7,
        1,
        5,
        7,
        2,
        1,
        1,
        0,
        1,
        4
      ],
      "2021-02": [
        50,
        11,
        17,
        2,
        13,
        2,
        13,
        9,
        5,
        8,
        1,
        5,
        9,
        7,
        6,
        3,
        2,
        7,
        8
      ],
      "2021-03": [
        52,
        11,
        13,
        7,
        3,
        10,
        6,
        6,
        4,
        8,
        1,
        5,
        7,
        6,
        4,
        3,
        0,
        2,
        10
      ],
      "2021-04": [
        84,
        11,
        24,
        6,
        14,
        6,
        6,
        9,
        8,
        13,
        2,
        5,
        6,
        3,
        3,
        4,
        3,
        1,
        16
      ],
      "2021-05": [
        46,
        11,
        15,
        0,
        13,
        3,
        9,
        7,
        3,
        7,
        1,
        4,
        8,
        5,
        8,
        1,
        3,
        2,
        9
      ],
      "2021-06": [
        72,
        11,
        30,
        2,
        12,
        8,
        11,
        14,
        13,
        11,
        1,
        11,
        10,
        4,
        9,
        5,
        2,
        3,
        11
      ],
      "2021-07": [
        62,
        16,
        23,
        6,
        9,
        6,
        13,
        8,
        5,
        15,
        0,
        10,
        6,
        8,
        10,
        2,
        4,
        3,
        5
      ],
      "2021-08": [
        51,
        15,
        10,
        3,
        5,
        2,
        2,
        1,
        7,
        12,
        2,
        5,
        5,
        3,
        5,
        4,
        1,
        4,
        5
      ],
      "2021-09": [
        45,
        11,
        30,
        6,
        7,
        9,
        2,
        5,
        5,
        11,
        4,
        5,
        7,
        2,
        5,
        2,
        1,
        0,
        6
      ],
      "2021-10": [
        112,
        11,
        31,
        4,
        21,
        6,
        15,
        13,
        7,
        23,
        0,
        8,
        19,
        16,
        16,
        8,
        2,
        1,
        9
      ],
      "2021-11": [
        54,
        16,
        15,
        3,
        20,
        6,
        7,
        9,
        6,
        8,
        2,
        5,
        10,
        6,
        0,
        3,
        1,
        5,
        8
      ],
      "2021-12": [
        33,
        7,
        13,
        4,
        12,
        3,
        2,
        10,
        6,
        11,
        1,
        2,
        7,
        2,
        4,
        2,
        1,
        3,
        4
      ],
      "2022-01": [
        43,
        5,
        10,
        5,
        7,
        2,
        2,
        11,
        6,
        11,
        2,
        10,
        6,
        5,
        7,
        4,
        0,
        5,
        4
      ],
      "2022-02": [
        61,
        15,
        28,
        11,
        22,
        8,
        11,
        6,
        7,
        17,
        3,
        3,
        6,
        5,
        4,
        1,
        0,
        6,
        11
      ],
      "2022-03": [
        92,
        13,
        46,
        10,
        20,
        11,
        10,
        23,
        7,
        15,
        4,
        6,
        12,
        14,
        6,
        4,
        6,
        5,
        17
      ],
      "2022-04": [
        105,
        10,
        31,
        8,
        18,
        5,
        7,
        11,
        5,
        21,
        3,
        5,
        8,
        11,
        6,
        4,
        6,
        4,
        19
      ],
      "2022-05": [
        57,
        9,
        10,
        3,
        13,
        3,
        2,
        3,
        5,
        16,
        2,
        3,
        2,
        4,
        9,
        3,
        1,
        3,
        6
      ],
      "2022-06": [
        71,
        9,
        19,
        6,
        21,
        3,
        8,
        11,
        6,
        20,
        1,
        6,
        4,
        6,
        7,
        4,
        6,
        4,
        11
      ],
      "2022-07": [
        61,
        12,
        22,
        3,
        7,
        9,
        6,
        14,
        1,
        20,
        3,
        2,
        8,
        6,
        3,
        5,
        2,
        2,
        4
      ],
      "2022-08": [
        40,
        16,
        15,
        6,
        4,
        6,
        2,
        8,
        1,
        10,
        1,
        5,
        4,
        4,
        2,
        4,
        3,
        9,
        2
      ],
      "2022-09": [
        48,
        16,
        17,
        4,
        5,
        4,
        5,
        3,
        3,
        11,
        1,
        5,
        4,
        4,
        4,
        4,
        2,
        1,
        7
      ],
      "2022-10": [
        150,
        19,
        42,
        9,
        18,
        9,
        10,
        18,
        12,
        29,
        6,
        6,
        8,
        10,
        8,
        6,
        2,
        4,
        9
      ],
      "2022-11": [
        126,
        18,
        33,
        8,
        28,
        9,
        4,
        18,
        7,
        23,
        4,
        6,
        12,
        9,
        5,
        7,
        1,
        2,
        16
      ],
      "2022-12": [
        45,
        10,
        8,
        5,
        7,
        2,
        1,
        8,
        2,
        15,
        1,
        3,
        3,
        3,
        8,
        1,
        5,
        1,
        7
      ],
      "2023-01": [
        27,
        19,
        4,
        2,
        5,
        6,
        2,
        4,
        2,
        15,
        3,
        2,
        1,
        3,
        2,
        0,
        1,
        2,
        3
      ],
      "2023-02": [
        52,
        7,
        18,
        0,
        22,
        4,
        7,
        13,
        9,
        13,
        2,
        0,
        6,
        3,
        6,
        1,
        5,
        1,
        5
      ],
      "2023-03": [
        73,
        11,
        27,
        4,
        11,
        7,
        9,
        13,
        8,
        23,
        8,
        3,
        10,
        4,
        1,
        3,
        6,
        4,
        6
      ],
      "2023-04": [
        40,
        15,
        5,
        3,
        4,
        5,
        1,
        7,
        5,
        13,
        1,
        2,
        3,
        5,
        2,
        4,
        3,
        3,
        2
      ],
      "2023-05": [
        132,
        16,
        29,
        8,
        13,
        8,
        8,
        20,
        13,
        33,
        6,
        5,
        10,
        9,
        8,
        8,
        4,
        9,
        9
      ],
      "2023-06": [
        109,
        26,
        20,
        6,
        15,
        13,
        9,
        18,
        8,
        26,
        4,
        3,
        10,
        6,
        7,
        2,
        4,
        2,
        8
      ],
      "2023-07": [
        61,
        27,
        12,
        6,
        10,
        9,
        0,
        8,
        4,
        18,
        3,
        3,
        6,
        2,
        7,
        2,
        3,
        2,
        5
      ],
      "2023-08": [
        57,
        20,
        18,
        7,
        9,
        8,
        5,
        13,
        9,
        18,
        2,
        4,
        5,
        8,
        10,
        5,
        4,
        3,
        10
      ],
      "2023-09": [
        167,
        27,
        35,
        13,
        31,
        9,
        16,
        17,
        11,
        47,
        8,
        10,
        7,
        14,
        10,
        6,
        6,
        5,
        17
      ],
      "2023-10": [
        75,
        20,
        22,
        6,
        17,
        10,
        4,
        12,
        6,
        20,
        3,
        3,
        4,
        10,
        4,
        2,
        4,
        6,
        10
      ],
      "2023-11": [
        46,
        17,
        11,
        1,
        3,
        8,
        2,
        2,
        1,
        13,
        5,
        5,
        3,
        6,
        2,
        2,
        2,
        5,
        5
      ],
      "2023-12": [
        68,
        17,
        22,
        4,
        9,
        7,
        7,
        19,
        3,
        19,
        5,
        3,
        5,
        4,
        8,
        4,
        6,
        8,
        3
      ],
      "2024-01": [
        84,
        17,
        20,
        4,
        13,
        12,
        6,
        11,
        7,
        29,
        5,
        2,
        5,
        11,
        2,
        3,
        4,
        2,
        15
      ],
      "2024-02": [
        58,
        25,
        13,
        4,
        10,
        12,
        6,
        10,
        10,
        19,
        3,
        2,
        1,
        7,
        7,
        0,
        1,
        6,
        4
      ],
      "2024-03": [
        49,
        10,
        10,
        4,
        7,
        12,
        9,
        15,
        4,
        23,
        5,
        1,
        4,
        6,
        6,
        4,
        1,
        3,
        4
      ],
      "2024-04": [
        49,
        23,
        13,
        7,
        5,
        2,
        3,
        8,
        2,
        14,
        1,
        2,
        3,
        7,
        5,
        2,
        1,
        0,
        5
      ],
      "2024-05": [
        55,
        22,
        14,
        7,
        8,
        6,
        1,
        12,
        1,
        23,
        2,
        7,
        2,
        7,
        3,
        2,
        1,
        3,
        4
      ],
      "2024-06": [
        157,
        24,
        32,
        14,
        18,
        12,
        13,
        22,
        13,
        38,
        8,
        7,
        6,
        16,
        14,
        10,
        3,
        5,
        19
      ],
      "2024-07": [
        91,
        32,
        20,
        8,
        10,
        7,
        5,
        17,
        7,
        29,
        6,
        6,
        5,
        5,
        12,
        3,
        1,
        1,
        4
      ],
      "2024-08": [
        71,
        21,
        24,
        13,
        9,
        12,
        5,
        8,
        3,
        28,
        4,
        4,
        7,
        7,
        5,
        6,
        6,
        3,
        9
      ],
      "2024-09": [
        153,
        44,
        50,
        15,
        25,
        17,
        11,
        25,
        6,
        49,
        9,
        4,
        11,
        20,
        13,
        4,
        8,
        17,
        20
      ],
      "2024-10": [
        97,
        40,
        20,
        9,
        11,
        6,
        8,
        14,
        9,
        28,
        8,
        2,
        6,
        12,
        12,
        2,
        1,
        8,
        11
      ],
      "2024-11": [
        54,
        25,
        12,
        10,
        2,
        10,
        4,
        8,
        7,
        14,
        0,
        1,
        6,
        4,
        2,
        2,
        5,
        4,
        6
      ],
      "2024-12": [
        69,
        25,
        23,
        5,
        11,
        6,
        1,
        14,
        6,
        38,
        5,
        4,
        0,
        4,
        9,
        3,
        7,
        1,
        3
      ],
      "2025-01": [
        78,
        28,
        18,
        9,
        10,
        7,
        3,
        17,
        11,
        26,
        5,
        4,
        6,
        11,
        4,
        2,
        2,
        4,
        10
      ],
      "2025-02": [
        57,
        26,
        6,
        9,
        15,
        4,
        7,
        9,
        4,
        31,
        2,
        4,
        5,
        6,
        5,
        1,
        4,
        2,
        8
      ],
      "2025-03": [
        64,
        24,
        7,
        3,
        6,
        5,
        6,
        9,
        3,
        30,
        2,
        2,
        3,
        3,
        5,
        0,
        7,
        6,
        4
      ],
      "2025-04": [
        49,
        18,
        6,
        4,
        1,
        10,
        5,
        4,
        3,
        16,
        6,
        4,
        6,
        10,
        6,
        2,
        2,
        1,
        2
      ],
      "2025-05": [
        142,
        35,
        31,
        22,
        16,
        12,
        5,
        26,
        16,
        58,
        6,
        6,
        10,
        12,
        9,
        8,
        7,
        7,
        26
      ],
      "2025-06": [
        141,
        50,
        30,
        7,
        18,
        10,
        7,
        16,
        7,
        41,
        3,
        3,
        10,
        7,
        11,
        9,
        6,
        5,
        24
      ],
      "2025-07": [
        109,
        36,
        23,
        13,
        14,
        9,
        11,
        13,
        7,
        29,
        6,
        4,
        7,
        5,
        5,
        4,
        3,
        5,
        11
      ],
      "2025-08": [
        99,
        34,
        20,
        12,
        19,
        8,
        4,
        22,
        8,
        33,
        7,
        5,
        2,
        5,
        6,
        5,
        7,
        4,
        12
      ],
      "2025-09": [
        54,
        18,
        5,
        13,
        6,
        6,
        3,
        6,
        3,
        13,
        2,
        4,
        3,
        1,
        4,
        1,
        4,
        4,
        7
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages",
          "year": "2024-11",
          "abstract": "This paper presents a novel multistage fine-tuning strategy designed to\nenhance automatic speech recognition (ASR) performance in low-resource\nlanguages using OpenAI's Whisper model. In this approach we aim to build ASR\nmodel for languages with limited digital resources by sequentially adapting the\nmodel across linguistically similar languages. We experimented this on the\nMalasar language, a Dravidian language spoken by approximately ten thousand\npeople in the Western Ghats of South India. Malasar language faces critical\nchallenges for technological intervention due to its lack of a native script\nand absence of digital or spoken data resources. Working in collaboration with\nWycliffe India and Malasar community members, we created a spoken Malasar\ncorpus paired with transcription in Tamil script, a closely related major\nlanguage. In our approach to build ASR model for Malasar, we first build an\nintermediate Tamil ASR, leveraging higher data availability for Tamil annotated\nspeech. This intermediate model is subsequently fine-tuned on Malasar data,\nallowing for more effective ASR adaptation despite limited resources. The\nmultistage fine-tuning strategy demonstrated significant improvements over\ndirect fine-tuning on Malasar data alone, achieving a word error rate (WER) of\n51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning\nmethod. Further a WER reduction to 47.3% was achieved through punctuation\nremoval in post-processing, which addresses formatting inconsistencies that\nimpact evaluation. Our results underscore the effectiveness of sequential\nmultistage fine-tuning combined with targeted post-processing as a scalable\nstrategy for ASR system development in low-resource languages, especially where\nlinguistic similarities can be leveraged to bridge gaps in training data.",
          "arxiv_id": "2411.04573v1"
        },
        {
          "title": "Multi-task Language Modeling for Improving Speech Recognition of Rare Words",
          "year": "2020-11",
          "abstract": "End-to-end automatic speech recognition (ASR) systems are increasingly\npopular due to their relative architectural simplicity and competitive\nperformance. However, even though the average accuracy of these systems may be\nhigh, the performance on rare content words often lags behind hybrid ASR\nsystems. To address this problem, second-pass rescoring is often applied\nleveraging upon language modeling. In this paper, we propose a second-pass\nsystem with multi-task learning, utilizing semantic targets (such as intent and\nslot prediction) to improve speech recognition performance. We show that our\nrescoring model trained with these additional tasks outperforms the baseline\nrescoring model, trained with only the language modeling task, by 1.4% on a\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\nrelative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR\ndeduction compared with RNN Transducer only ASR baseline for rare words\nrecognition.",
          "arxiv_id": "2011.11715v4"
        },
        {
          "title": "Dual Script E2E framework for Multilingual and Code-Switching ASR",
          "year": "2021-06",
          "abstract": "India is home to multiple languages, and training automatic speech\nrecognition (ASR) systems for languages is challenging. Over time, each\nlanguage has adopted words from other languages, such as English, leading to\ncode-mixing. Most Indian languages also have their own unique scripts, which\nposes a major limitation in training multilingual and code-switching ASR\nsystems.\n  Inspired by results in text-to-speech synthesis, in this work, we use an\nin-house rule-based phoneme-level common label set (CLS) representation to\ntrain multilingual and code-switching ASR for Indian languages. We propose two\nend-to-end (E2E) ASR systems. In the first system, the E2E model is trained on\nthe CLS representation, and we use a novel data-driven back-end to recover the\nnative language script. In the second system, we propose a modification to the\nE2E model, wherein the CLS representation and the native language characters\nare used simultaneously for training. We show our results on the multilingual\nand code-switching tasks of the Indic ASR Challenge 2021. Our best results\nachieve 6% and 5% improvement (approx) in word error rate over the baseline\nsystem for the multilingual and code-switching tasks, respectively, on the\nchallenge development data.",
          "arxiv_id": "2106.01400v1"
        }
      ],
      "1": [
        {
          "title": "Editing Music with Melody and Text: Using ControlNet for Diffusion Transformer",
          "year": "2024-10",
          "abstract": "Despite the significant progress in controllable music generation and\nediting, challenges remain in the quality and length of generated music due to\nthe use of Mel-spectrogram representations and UNet-based model structures. To\naddress these limitations, we propose a novel approach using a Diffusion\nTransformer (DiT) augmented with an additional control branch using ControlNet.\nThis allows for long-form and variable-length music generation and editing\ncontrolled by text and melody prompts. For more precise and fine-grained melody\ncontrol, we introduce a novel top-$k$ constant-Q Transform representation as\nthe melody prompt, reducing ambiguity compared to previous representations\n(e.g., chroma), particularly for music with multiple tracks or a wide range of\npitch values. To effectively balance the control signals from text and melody\nprompts, we adopt a curriculum learning strategy that progressively masks the\nmelody prompt, resulting in a more stable training process. Experiments have\nbeen performed on text-to-music generation and music-style transfer tasks using\nopen-source instrumental recording data. The results demonstrate that by\nextending StableAudio, a pre-trained text-controlled DiT model, our approach\nenables superior melody-controlled editing while retaining good text-to-music\ngeneration performance. These results outperform a strong MusicGen baseline in\nterms of both text-based generation and melody preservation for editing. Audio\nexamples can be found at https://stable-audio-control.github.io.",
          "arxiv_id": "2410.05151v2"
        },
        {
          "title": "A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions",
          "year": "2020-11",
          "abstract": "The utilization of deep learning techniques in generating various contents\n(such as image, text, etc.) has become a trend. Especially music, the topic of\nthis paper, has attracted widespread attention of countless researchers.The\nwhole process of producing music can be divided into three stages,\ncorresponding to the three levels of music generation: score generation\nproduces scores, performance generation adds performance characteristics to the\nscores, and audio generation converts scores with performance characteristics\ninto audio by assigning timbre or generates music in audio format directly.\nPrevious surveys have explored the network models employed in the field of\nautomatic music generation. However, the development history, the model\nevolution, as well as the pros and cons of same music generation task have not\nbeen clearly illustrated. This paper attempts to provide an overview of various\ncomposition tasks under different music generation levels, covering most of the\ncurrently popular music generation tasks using deep learning. In addition, we\nsummarize the datasets suitable for diverse tasks, discuss the music\nrepresentations, the evaluation methods as well as the challenges under\ndifferent levels, and finally point out several future directions.",
          "arxiv_id": "2011.06801v1"
        },
        {
          "title": "MusFlow: Multimodal Music Generation via Conditional Flow Matching",
          "year": "2025-04",
          "abstract": "Music generation aims to create music segments that align with human\naesthetics based on diverse conditional information. Despite advancements in\ngenerating music from specific textual descriptions (e.g., style, genre,\ninstruments), the practical application is still hindered by ordinary users'\nlimited expertise or time to write accurate prompts. To bridge this application\ngap, this paper introduces MusFlow, a novel multimodal music generation model\nusing Conditional Flow Matching. We employ multiple Multi-Layer Perceptrons\n(MLPs) to align multimodal conditional information into the audio's CLAP\nembedding space. Conditional flow matching is trained to reconstruct the\ncompressed Mel-spectrogram in the pretrained VAE latent space guided by aligned\nfeature embedding. MusFlow can generate music from images, story texts, and\nmusic captions. To collect data for model training, inspired by multi-agent\ncollaboration, we construct an intelligent data annotation workflow centered\naround a fine-tuned Qwen2-VL model. Using this workflow, we build a new\nmultimodal music dataset, MMusSet, with each sample containing a quadruple of\nimage, story text, music caption, and music piece. We conduct four sets of\nexperiments: image-to-music, story-to-music, caption-to-music, and multimodal\nmusic generation. Experimental results demonstrate that MusFlow can generate\nhigh-quality music pieces whether the input conditions are unimodal or\nmultimodal. We hope this work can advance the application of music generation\nin multimedia field, making music creation more accessible. Our generated\nsamples, code and dataset are available at musflow.github.io.",
          "arxiv_id": "2504.13535v1"
        }
      ],
      "2": [
        {
          "title": "Attention-based Encoder-Decoder End-to-End Neural Diarization with Embedding Enhancer",
          "year": "2023-09",
          "abstract": "Deep neural network-based systems have significantly improved the performance\nof speaker diarization tasks. However, end-to-end neural diarization (EEND)\nsystems often struggle to generalize to scenarios with an unseen number of\nspeakers, while target speaker voice activity detection (TS-VAD) systems tend\nto be overly complex. In this paper, we propose a simple attention-based\nencoder-decoder network for end-to-end neural diarization (AED-EEND). In our\ntraining process, we introduce a teacher-forcing strategy to address the\nspeaker permutation problem, leading to faster model convergence. For\nevaluation, we propose an iterative decoding method that outputs diarization\nresults for each speaker sequentially. Additionally, we propose an Enhancer\nmodule to enhance the frame-level speaker embeddings, enabling the model to\nhandle scenarios with an unseen number of speakers. We also explore replacing\nthe transformer encoder with a Conformer architecture, which better models\nlocal information. Furthermore, we discovered that commonly used simulation\ndatasets for speaker diarization have a much higher overlap ratio compared to\nreal data. We found that using simulated training data that is more consistent\nwith real data can achieve an improvement in consistency. Extensive\nexperimental validation demonstrates the effectiveness of our proposed\nmethodologies. Our best system achieved a new state-of-the-art diarization\nerror rate (DER) performance on all the CALLHOME (10.08%), DIHARD II (24.64%),\nand AMI (13.00%) evaluation benchmarks, when no oracle voice activity detection\n(VAD) is used. Beyond speaker diarization, our AED-EEND system also shows\nremarkable competitiveness as a speech type detection model.",
          "arxiv_id": "2309.06672v1"
        },
        {
          "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
          "year": "2022-07",
          "abstract": "This paper proposes an online target speaker voice activity detection system\nfor speaker diarization tasks, which does not require a priori knowledge from\nthe clustering-based diarization system to obtain the target speaker\nembeddings. First, we employ a ResNet-based front-end model to extract the\nframe-level speaker embeddings for each coming block of a signal. Next, we\npredict the detection state of each speaker based on these frame-level speaker\nembeddings and the previously estimated target speaker embedding. Then, the\ntarget speaker embeddings are updated by aggregating these frame-level speaker\nembeddings according to the predictions in the current block. We iteratively\nextract the results for each block and update the target speaker embedding\nuntil reaching the end of the signal. Experimental results show that the\nproposed method is better than the offline clustering-based diarization system\non the AliMeeting dataset.",
          "arxiv_id": "2207.05920v1"
        },
        {
          "title": "End-to-End Neural Diarization: Reformulating Speaker Diarization as Simple Multi-label Classification",
          "year": "2020-02",
          "abstract": "The most common approach to speaker diarization is clustering of speaker\nembeddings. However, the clustering-based approach has a number of problems;\ni.e., (i) it is not optimized to minimize diarization errors directly, (ii) it\ncannot handle speaker overlaps correctly, and (iii) it has trouble adapting\ntheir speaker embedding models to real audio recordings with speaker overlaps.\nTo solve these problems, we propose the End-to-End Neural Diarization (EEND),\nin which a neural network directly outputs speaker diarization results given a\nmulti-speaker recording. To realize such an end-to-end model, we formulate the\nspeaker diarization problem as a multi-label classification problem and\nintroduce a permutation-free objective function to directly minimize\ndiarization errors. Besides its end-to-end simplicity, the EEND method can\nexplicitly handle speaker overlaps during training and inference. Just by\nfeeding multi-speaker recordings with corresponding speaker segment labels, our\nmodel can be easily adapted to real conversations. We evaluated our method on\nsimulated speech mixtures and real conversation datasets. The results showed\nthat the EEND method outperformed the state-of-the-art x-vector\nclustering-based method, while it correctly handled speaker overlaps. We\nexplored the neural network architecture for the EEND method, and found that\nthe self-attention-based neural network was the key to achieving excellent\nperformance. In contrast to conditioning the network only on its previous and\nnext hidden states, as is done using bidirectional long short-term memory\n(BLSTM), self-attention is directly conditioned on all the frames. By\nvisualizing the attention weights, we show that self-attention captures global\nspeaker characteristics in addition to local speech activity dynamics, making\nit especially suitable for dealing with the speaker diarization problem.",
          "arxiv_id": "2003.02966v1"
        }
      ],
      "3": [
        {
          "title": "Voice Spoofing Countermeasures: Taxonomy, State-of-the-art, experimental analysis of generalizability, open challenges, and the way forward",
          "year": "2022-10",
          "abstract": "Malicious actors may seek to use different voice-spoofing attacks to fool ASV\nsystems and even use them for spreading misinformation. Various countermeasures\nhave been proposed to detect these spoofing attacks. Due to the extensive work\ndone on spoofing detection in automated speaker verification (ASV) systems in\nthe last 6-7 years, there is a need to classify the research and perform\nqualitative and quantitative comparisons on state-of-the-art countermeasures.\nAdditionally, no existing survey paper has reviewed integrated solutions to\nvoice spoofing evaluation and speaker verification, adversarial/antiforensics\nattacks on spoofing countermeasures, and ASV itself, or unified solutions to\ndetect multiple attacks using a single model. Further, no work has been done to\nprovide an apples-to-apples comparison of published countermeasures in order to\nassess their generalizability by evaluating them across corpora. In this work,\nwe conduct a review of the literature on spoofing detection using hand-crafted\nfeatures, deep learning, end-to-end, and universal spoofing countermeasure\nsolutions to detect speech synthesis (SS), voice conversion (VC), and replay\nattacks. Additionally, we also review integrated solutions to voice spoofing\nevaluation and speaker verification, adversarial and anti-forensics attacks on\nvoice countermeasures, and ASV. The limitations and challenges of the existing\nspoofing countermeasures are also presented. We report the performance of these\ncountermeasures on several datasets and evaluate them across corpora. For the\nexperiments, we employ the ASVspoof2019 and VSDC datasets along with GMM, SVM,\nCNN, and CNN-GRU classifiers. (For reproduceability of the results, the code of\nthe test bed can be found in our GitHub Repository.",
          "arxiv_id": "2210.00417v2"
        },
        {
          "title": "Defense against adversarial attacks on spoofing countermeasures of ASV",
          "year": "2020-03",
          "abstract": "Various forefront countermeasure methods for automatic speaker verification\n(ASV) with considerable performance in anti-spoofing are proposed in the\nASVspoof 2019 challenge. However, previous work has shown that countermeasure\nmodels are vulnerable to adversarial examples indistinguishable from natural\ndata. A good countermeasure model should not only be robust against spoofing\naudio, including synthetic, converted, and replayed audios; but counteract\ndeliberately generated examples by malicious adversaries. In this work, we\nintroduce a passive defense method, spatial smoothing, and a proactive defense\nmethod, adversarial training, to mitigate the vulnerability of ASV spoofing\ncountermeasure models against adversarial examples. This paper is among the\nfirst to use defense methods to improve the robustness of ASV spoofing\ncountermeasure models under adversarial attacks. The experimental results show\nthat these two defense methods positively help spoofing countermeasure models\ncounter adversarial examples.",
          "arxiv_id": "2003.03065v1"
        },
        {
          "title": "One-class Learning Towards Synthetic Voice Spoofing Detection",
          "year": "2020-10",
          "abstract": "Human voices can be used to authenticate the identity of the speaker, but the\nautomatic speaker verification (ASV) systems are vulnerable to voice spoofing\nattacks, such as impersonation, replay, text-to-speech, and voice conversion.\nRecently, researchers developed anti-spoofing techniques to improve the\nreliability of ASV systems against spoofing attacks. However, most methods\nencounter difficulties in detecting unknown attacks in practical use, which\noften have different statistical distributions from known attacks. Especially,\nthe fast development of synthetic voice spoofing algorithms is generating\nincreasingly powerful attacks, putting the ASV systems at risk of unseen\nattacks. In this work, we propose an anti-spoofing system to detect unknown\nsynthetic voice spoofing attacks (i.e., text-to-speech or voice conversion)\nusing one-class learning. The key idea is to compact the bona fide speech\nrepresentation and inject an angular margin to separate the spoofing attacks in\nthe embedding space. Without resorting to any data augmentation methods, our\nproposed system achieves an equal error rate (EER) of 2.19% on the evaluation\nset of ASVspoof 2019 Challenge logical access scenario, outperforming all\nexisting single systems (i.e., those without model ensemble).",
          "arxiv_id": "2010.13995v2"
        }
      ],
      "4": [
        {
          "title": "SRIB-LEAP submission to Far-field Multi-Channel Speech Enhancement Challenge for Video Conferencing",
          "year": "2021-06",
          "abstract": "This paper presents the details of the SRIB-LEAP submission to the\nConferencingSpeech challenge 2021. The challenge involved the task of\nmulti-channel speech enhancement to improve the quality of far field speech\nfrom microphone arrays in a video conferencing room. We propose a two stage\nmethod involving a beamformer followed by single channel enhancement. For the\nbeamformer, we incorporated self-attention mechanism as inter-channel\nprocessing layer in the filter-and-sum network (FaSNet), an end-to-end\ntime-domain beamforming system. The single channel speech enhancement is done\nin log spectral domain using convolution neural network (CNN)-long short term\nmemory (LSTM) based architecture. We achieved improvements in objective quality\nmetrics - perceptual evaluation of speech quality (PESQ) of 0.5 on the noisy\ndata. On subjective quality evaluation, the proposed approach improved the mean\nopinion score (MOS) by an absolute measure of 0.9 over the noisy audio.",
          "arxiv_id": "2106.12763v1"
        },
        {
          "title": "Consistency-aware multi-channel speech enhancement using deep neural networks",
          "year": "2020-02",
          "abstract": "This paper proposes a deep neural network (DNN)-based multi-channel speech\nenhancement system in which a DNN is trained to maximize the quality of the\nenhanced time-domain signal. DNN-based multi-channel speech enhancement is\noften conducted in the time-frequency (T-F) domain because spatial filtering\ncan be efficiently implemented in the T-F domain. In such a case, ordinary\nobjective functions are computed on the estimated T-F mask or spectrogram.\nHowever, the estimated spectrogram is often inconsistent, and its amplitude and\nphase may change when the spectrogram is converted back to the time-domain.\nThat is, the objective function does not evaluate the enhanced time-domain\nsignal properly. To address this problem, we propose to use an objective\nfunction defined on the reconstructed time-domain signal. Specifically, speech\nenhancement is conducted by multi-channel Wiener filtering in the T-F domain,\nand its result is converted back to the time-domain. We propose two objective\nfunctions computed on the reconstructed signal where the first one is defined\nin the time-domain, and the other one is defined in the T-F domain. Our\nexperiment demonstrates the effectiveness of the proposed system comparing to\nT-F masking and mask-based beamforming.",
          "arxiv_id": "2002.05831v1"
        },
        {
          "title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition",
          "year": "2022-10",
          "abstract": "It has been shown that the intelligibility of noisy speech can be improved by\nspeech enhancement algorithms. However, speech enhancement has not been\nestablished as an effective frontend for robust automatic speech recognition\n(ASR) in noisy conditions compared to an ASR model trained on noisy speech\ndirectly. The divide between speech enhancement and ASR impedes the progress of\nrobust ASR systems especially as speech enhancement has made big strides in\nrecent years. In this work, we focus on eliminating this divide with an ARN\n(attentive recurrent network) based time-domain enhancement model. The proposed\nsystem fully decouples speech enhancement and an acoustic model trained only on\nclean speech. Results on the CHiME-2 corpus show that ARN enhanced speech\ntranslates to improved ASR results. The proposed system achieves $6.28\\%$\naverage word error rate, outperforming the previous best by $19.3\\%$\nrelatively.",
          "arxiv_id": "2210.13318v3"
        }
      ],
      "5": [
        {
          "title": "In situ sound absorption estimation with the discrete complex image source method",
          "year": "2024-04",
          "abstract": "Estimating the sound absorption in situ relies on accurately describing the\nmeasured sound field. Evidence suggests that modeling the reflection of\nimpinging spherical waves is important, especially for compact measurement\nsystems. This article proposes a method for estimating the sound absorption\ncoefficient of a material sample by mapping the sound pressure, measured by a\nmicrophone array, to a distribution of monopoles along a line in the complex\nplane. The proposed method is compared to modeling the sound field as a\nsuperposition of two sources (a monopole and an image source). The obtained\ninverse problems are solved with Tikhonov regularization, with automatic choice\nof the regularization parameter by the L-curve criterion. The sound absorption\nmeasurement is tested with simulations of the sound field above infinite and\nfinite porous absorbers. The approaches are compared to the plane-wave\nabsorption coefficient and the one obtained by spherical wave incidence.\nExperimental analysis of two porous samples and one resonant absorber is also\ncarried out in situ. Four arrays were tested with an increasing aperture and\nnumber of sensors. It was demonstrated that measurements are feasible even with\nan array with only a few microphones. The discretization of the integral\nequation led to a more accurate reconstruction of the sound pressure and\nparticle velocity at the sample's surface. The resulting absorption coefficient\nagrees with the one obtained for spherical wave incidence, indicating that\nincluding more monopoles along the complex line is an essential feature of the\nsound field.",
          "arxiv_id": "2404.11399v1"
        },
        {
          "title": "Deep Sound Field Reconstruction in Real Rooms: Introducing the ISOBEL Sound Field Dataset",
          "year": "2021-02",
          "abstract": "Knowledge of loudspeaker responses are useful in a number of applications,\nwhere a sound system is located inside a room that alters the listening\nexperience depending on position within the room. Acquisition of sound fields\nfor sound sources located in reverberant rooms can be achieved through labor\nintensive measurements of impulse response functions covering the room, or\nalternatively by means of reconstruction methods which can potentially require\nsignificantly fewer measurements. This paper extends evaluations of sound field\nreconstruction at low frequencies by introducing a dataset with measurements\nfrom four real rooms. The ISOBEL Sound Field dataset is publicly available, and\naims to bridge the gap between synthetic and real-world sound fields in\nrectangular rooms. Moreover, the paper advances on a recent deep learning-based\nmethod for sound field reconstruction using a very low number of microphones,\nand proposes an approach for modeling both magnitude and phase response in a\nU-Net-like neural network architecture. The complex-valued sound field\nreconstruction demonstrates that the estimated room transfer functions are of\nhigh enough accuracy to allow for personalized sound zones with contrast ratios\ncomparable to ideal room transfer functions using 15 microphones below 150 Hz.",
          "arxiv_id": "2102.06455v1"
        },
        {
          "title": "Room geometry blind inference based on the localization of real sound source and first order reflections",
          "year": "2022-07",
          "abstract": "The conventional room geometry blind inference techniques with acoustic\nsignals are conducted based on the prior knowledge of the environment, such as\nthe room impulse response (RIR) or the sound source position, which will limit\nits application under unknown scenarios. To solve this problem, we have\nproposed a room geometry reconstruction method in this paper by using the\ngeometric relation between the direct signal and first-order reflections. In\naddition to the information of the compact microphone array itself, this method\ndoes not need any precognition of the environmental parameters. Besides, the\nlearning-based DNN models are designed and used to improve the accuracy and\nintegrity of the localization results of the direct source and first-order\nreflections. The direction of arrival (DOA) and time difference of arrival\n(TDOA) information of the direct and reflected signals are firstly estimated\nusing the proposed DCNN and TD-CNN models, which have higher sensitivity and\naccuracy than the conventional methods. Then the position of the sound source\nis inferred by integrating the DOA, TDOA and array height using the proposed\nDNN model. After that, the positions of image sources and corresponding\nboundaries are derived based on the geometric relation. Experimental results of\nboth simulations and real measurements verify the effectiveness and accuracy of\nthe proposed techniques compared with the conventional methods under different\nreverberant environments.",
          "arxiv_id": "2207.10478v2"
        }
      ],
      "6": [
        {
          "title": "Cross-Referencing Self-Training Network for Sound Event Detection in Audio Mixtures",
          "year": "2021-05",
          "abstract": "Sound event detection is an important facet of audio tagging that aims to\nidentify sounds of interest and define both the sound category and time\nboundaries for each sound event in a continuous recording. With advances in\ndeep neural networks, there has been tremendous improvement in the performance\nof sound event detection systems, although at the expense of costly data\ncollection and labeling efforts. In fact, current state-of-the-art methods\nemploy supervised training methods that leverage large amounts of data samples\nand corresponding labels in order to facilitate identification of sound\ncategory and time stamps of events. As an alternative, the current study\nproposes a semi-supervised method for generating pseudo-labels from\nunsupervised data using a student-teacher scheme that balances self-training\nand cross-training. Additionally, this paper explores post-processing which\nextracts sound intervals from network prediction, for further improvement in\nsound event detection performance. The proposed approach is evaluated on sound\nevent detection task for the DCASE2020 challenge. The results of these methods\non both \"validation\" and \"public evaluation\" sets of DESED database show\nsignificant improvement compared to the state-of-the art systems in\nsemi-supervised learning.",
          "arxiv_id": "2105.13392v1"
        },
        {
          "title": "Sound Event Detection Using Duration Robust Loss Function",
          "year": "2020-06",
          "abstract": "Many methods of sound event detection (SED) based on machine learning regard\na segmented time frame as one data sample to model training. However, the sound\ndurations of sound events vary greatly depending on the sound event class,\ne.g., the sound event ``fan'' has a long time duration, while the sound event\n``mouse clicking'' is instantaneous. The difference in the time duration\nbetween sound event classes thus causes a serious data imbalance problem in\nSED. In this paper, we propose a method for SED using a duration robust loss\nfunction, which can focus model training on sound events of short duration. In\nthe proposed method, we focus on a relationship between the duration of the\nsound event and the ease/difficulty of model training. In particular, many\nsound events of long duration (e.g., sound event ``fan'') are stationary\nsounds, which have less variation in their acoustic features and their model\ntraining is easy. Meanwhile, some sound events of short duration (e.g., sound\nevent ``object impact'') have more than one audio pattern, such as attack,\ndecay, and release parts. We thus apply a class-wise reweighting to the\nbinary-cross entropy loss function depending on the ease/difficulty of model\ntraining. Evaluation experiments conducted using TUT Sound Events 2016/2017 and\nTUT Acoustic Scenes 2016 datasets show that the proposed method respectively\nimproves the detection performance of sound events by 3.15 and 4.37 percentage\npoints in macro- and micro-Fscores compared with a conventional method using\nthe binary-cross entropy loss function.",
          "arxiv_id": "2006.15253v1"
        },
        {
          "title": "Event-Independent Network for Polyphonic Sound Event Localization and Detection",
          "year": "2020-09",
          "abstract": "Polyphonic sound event localization and detection is not only detecting what\nsound events are happening but localizing corresponding sound sources. This\nseries of tasks was first introduced in DCASE 2019 Task 3. In 2020, the sound\nevent localization and detection task introduces additional challenges in\nmoving sound sources and overlapping-event cases, which include two events of\nthe same type with two different direction-of-arrival (DoA) angles. In this\npaper, a novel event-independent network for polyphonic sound event\nlocalization and detection is proposed. Unlike the two-stage method we proposed\nin DCASE 2019 Task 3, this new network is fully end-to-end. Inputs to the\nnetwork are first-order Ambisonics (FOA) time-domain signals, which are then\nfed into a 1-D convolutional layer to extract acoustic features. The network is\nthen split into two parallel branches. The first branch is for sound event\ndetection (SED), and the second branch is for DoA estimation. There are three\ntypes of predictions from the network, SED predictions, DoA predictions, and\nevent activity detection (EAD) predictions that are used to combine the SED and\nDoA features for on-set and off-set estimation. All of these predictions have\nthe format of two tracks indicating that there are at most two overlapping\nevents. Within each track, there could be at most one event happening. This\narchitecture introduces a problem of track permutation. To address this\nproblem, a frame-level permutation invariant training method is used.\nExperimental results show that the proposed method can detect polyphonic sound\nevents and their corresponding DoAs. Its performance on the Task 3 dataset is\ngreatly increased as compared with that of the baseline method.",
          "arxiv_id": "2010.00140v1"
        }
      ],
      "7": [
        {
          "title": "Enhancing Speech Emotion Recognition with Multi-Task Learning and Dynamic Feature Fusion",
          "year": "2025-08",
          "abstract": "This study investigates fine-tuning self-supervised learn ing (SSL) models\nusing multi-task learning (MTL) to enhance\n  speech emotion recognition (SER). The framework simultane ously handles four\nrelated tasks: emotion recognition, gender\n  recognition, speaker verification, and automatic speech recog nition. An\ninnovative co-attention module is introduced to dy namically capture the\ninteractions between features from the\n  primary emotion classification task and auxiliary tasks, en abling\ncontext-aware fusion. Moreover, We introduce the Sam ple Weighted Focal\nContrastive (SWFC) loss function to ad dress class imbalance and semantic\nconfusion by adjusting sam ple weights for difficult and minority samples. The\nmethod is\n  validated on the Categorical Emotion Recognition task of the\n  Speech Emotion Recognition in Naturalistic Conditions Chal lenge, showing\nsignificant performance improvements.",
          "arxiv_id": "2508.17878v1"
        },
        {
          "title": "Improving Speech Emotion Recognition Through Cross Modal Attention Alignment and Balanced Stacking Model",
          "year": "2025-05",
          "abstract": "Emotion plays a fundamental role in human interaction, and therefore systems\ncapable of identifying emotions in speech are crucial in the context of\nhuman-computer interaction. Speech emotion recognition (SER) is a challenging\nproblem, particularly in natural speech and when the available data is\nimbalanced across emotions. This paper presents our proposed system in the\ncontext of the 2025 Speech Emotion Recognition in Naturalistic Conditions\nChallenge. Our proposed architecture leverages cross-modality, utilizing\ncross-modal attention to fuse representations from different modalities. To\naddress class imbalance, we employed two training designs: (i) weighted\ncrossentropy loss (WCE); and (ii) WCE with an additional neutralexpressive soft\nmargin loss and balancing. We trained a total of 12 multimodal models, which\nwere ensembled using a balanced stacking model. Our proposed system achieves a\nMacroF1 score of 0.4094 and an accuracy of 0.4128 on 8-class speech emotion\nrecognition.",
          "arxiv_id": "2505.20007v2"
        },
        {
          "title": "Emotion Neural Transducer for Fine-Grained Speech Emotion Recognition",
          "year": "2024-03",
          "abstract": "The mainstream paradigm of speech emotion recognition (SER) is identifying\nthe single emotion label of the entire utterance. This line of works neglect\nthe emotion dynamics at fine temporal granularity and mostly fail to leverage\nlinguistic information of speech signal explicitly. In this paper, we propose\nEmotion Neural Transducer for fine-grained speech emotion recognition with\nautomatic speech recognition (ASR) joint training. We first extend typical\nneural transducer with emotion joint network to construct emotion lattice for\nfine-grained SER. Then we propose lattice max pooling on the alignment lattice\nto facilitate distinguishing emotional and non-emotional frames. To adapt\nfine-grained SER to transducer inference manner, we further make blank, the\nspecial symbol of ASR, serve as underlying emotion indicator as well, yielding\nFactorized Emotion Neural Transducer. For typical utterance-level SER, our ENT\nmodels outperform state-of-the-art methods on IEMOCAP in low word error rate.\nExperiments on IEMOCAP and the latest speech emotion diarization dataset ZED\nalso demonstrate the superiority of fine-grained emotion modeling. Our code is\navailable at https://github.com/ECNU-Cross-Innovation-Lab/ENT.",
          "arxiv_id": "2403.19224v1"
        }
      ],
      "8": [
        {
          "title": "Recent Progress in the CUHK Dysarthric Speech Recognition System",
          "year": "2022-01",
          "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\nin the past few decades, recognition of disordered speech remains a highly\nchallenging task to date. Disordered speech presents a wide spectrum of\nchallenges to current data intensive deep neural networks (DNNs) based ASR\ntechnologies that predominantly target normal speech. This paper presents\nrecent research efforts at the Chinese University of Hong Kong (CUHK) to\nimprove the performance of disordered speech recognition systems on the largest\npublicly available UASpeech dysarthric speech corpus. A set of novel modelling\ntechniques including neural architectural search, data augmentation using\nspectra-temporal perturbation, model based speaker adaptation and cross-domain\ngeneration of visual features within an audio-visual speech recognition (AVSR)\nsystem framework were employed to address the above challenges. The combination\nof these techniques produced the lowest published word error rate (WER) of\n25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER\nreduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric\nspeech recognition system featuring a 6-way DNN system combination and cross\nadaptation of out-of-domain normal speech data trained systems. Bayesian model\nadaptation further allows rapid adaptation to individual dysarthric speakers to\nbe performed using as little as 3.06 seconds of speech. The efficacy of these\ntechniques were further demonstrated on a CUDYS Cantonese dysarthric speech\nrecognition task.",
          "arxiv_id": "2201.05845v2"
        },
        {
          "title": "Relating EEG to continuous speech using deep neural networks: a review",
          "year": "2023-02",
          "abstract": "Objective. When a person listens to continuous speech, a corresponding\nresponse is elicited in the brain and can be recorded using\nelectroencephalography (EEG). Linear models are presently used to relate the\nEEG recording to the corresponding speech signal. The ability of linear models\nto find a mapping between these two signals is used as a measure of neural\ntracking of speech. Such models are limited as they assume linearity in the\nEEG-speech relationship, which omits the nonlinear dynamics of the brain. As an\nalternative, deep learning models have recently been used to relate EEG to\ncontinuous speech. Approach. This paper reviews and comments on\ndeep-learning-based studies that relate EEG to continuous speech in single- or\nmultiple-speakers paradigms. We point out recurrent methodological pitfalls and\nthe need for a standard benchmark of model analysis. Main results. We gathered\n29 studies. The main methodological issues we found are biased\ncross-validations, data leakage leading to over-fitted models, or\ndisproportionate data size compared to the model's complexity. In addition, we\naddress requirements for a standard benchmark model analysis, such as public\ndatasets, common evaluation metrics, and good practices for the match-mismatch\ntask. Significance. We present a review paper summarizing the main\ndeep-learning-based studies that relate EEG to speech while addressing\nmethodological pitfalls and important considerations for this newly expanding\nfield. Our study is particularly relevant given the growing application of deep\nlearning in EEG-speech decoding.",
          "arxiv_id": "2302.01736v4"
        },
        {
          "title": "Accurate synthesis of Dysarthric Speech for ASR data augmentation",
          "year": "2023-08",
          "abstract": "Dysarthria is a motor speech disorder often characterized by reduced speech\nintelligibility through slow, uncoordinated control of speech production\nmuscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers\ncommunicate more effectively. However, robust dysarthria-specific ASR requires\na significant amount of training speech, which is not readily available for\ndysarthric talkers. This paper presents a new dysarthric speech synthesis\nmethod for the purpose of ASR training data augmentation. Differences in\nprosodic and acoustic characteristics of dysarthric spontaneous speech at\nvarying severity levels are important components for dysarthric speech\nmodeling, synthesis, and augmentation. For dysarthric speech synthesis, a\nmodified neural multi-talker TTS is implemented by adding a dysarthria severity\nlevel coefficient and a pause insertion model to synthesize dysarthric speech\nfor varying severity levels. To evaluate the effectiveness for synthesis of\ntraining data for ASR, dysarthria-specific speech recognition was used. Results\nshow that a DNN-HMM model trained on additional synthetic dysarthric speech\nachieves WER improvement of 12.2% compared to the baseline, and that the\naddition of the severity level and pause insertion controls decrease WER by\n6.5%, showing the effectiveness of adding these parameters. Overall results on\nthe TORGO database demonstrate that using dysarthric synthetic speech to\nincrease the amount of dysarthric-patterned speech for training has significant\nimpact on the dysarthric ASR systems. In addition, we have conducted a\nsubjective evaluation to evaluate the dysarthric-ness and similarity of\nsynthesized speech. Our subjective evaluation shows that the perceived\ndysartrhic-ness of synthesized speech is similar to that of true dysarthric\nspeech, especially for higher levels of dysarthria",
          "arxiv_id": "2308.08438v1"
        }
      ],
      "9": [
        {
          "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
          "year": "2025-04",
          "abstract": "Audio is essential for multimodal video understanding. On the one hand, video\ninherently contains audio, which supplies complementary information to vision.\nBesides, video large language models (Video-LLMs) can encounter many\naudio-centric settings. However, existing Video-LLMs and Audio-Visual Large\nLanguage Models (AV-LLMs) exhibit deficiencies in exploiting audio information,\nleading to weak understanding and hallucinations. To solve the issues, we delve\ninto the model architecture and dataset. (1) From the architectural\nperspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent\nalignment of audio and visual modalities in both temporal and spatial\ndimensions ensures a comprehensive and accurate understanding of videos.\nSpecifically, we devise an audio-visual multi-scale adapter for multi-scale\ninformation aggregation, which achieves spatial alignment. For temporal\nalignment, we propose audio-visual interleaved merging. (2) From the dataset\nperspective, we curate an audio-visual caption and instruction-tuning dataset,\ncalled AVU. It comprises 5.2 million diverse, open-ended data tuples (video,\naudio, question, answer) and introduces a novel data partitioning strategy.\nExtensive experiments show our model not only achieves remarkable performance\nin audio-visual understanding, but also mitigates potential hallucinations.",
          "arxiv_id": "2504.02061v1"
        },
        {
          "title": "Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues",
          "year": "2024-02",
          "abstract": "How to effectively interact audio with vision has garnered considerable\ninterest within the multi-modality research field. Recently, a novel\naudio-visual segmentation (AVS) task has been proposed, aiming to segment the\nsounding objects in video frames under the guidance of audio cues. However,\nmost existing AVS methods are hindered by a modality imbalance where the visual\nfeatures tend to dominate those of the audio modality, due to a unidirectional\nand insufficient integration of audio cues. This imbalance skews the feature\nrepresentation towards the visual aspect, impeding the learning of joint\naudio-visual representations and potentially causing segmentation inaccuracies.\nTo address this issue, we propose AVSAC. Our approach features a Bidirectional\nAudio-Visual Decoder (BAVD) with integrated bidirectional bridges, enhancing\naudio cues and fostering continuous interplay between audio and visual\nmodalities. This bidirectional interaction narrows the modality imbalance,\nfacilitating more effective learning of integrated audio-visual\nrepresentations. Additionally, we present a strategy for audio-visual\nframe-wise synchrony as fine-grained guidance of BAVD. This strategy enhances\nthe share of auditory components in visual features, contributing to a more\nbalanced audio-visual representation learning. Extensive experiments show that\nour method attains new benchmarks in AVS performance.",
          "arxiv_id": "2402.02327v2"
        },
        {
          "title": "CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model for Multimodal Processing",
          "year": "2024-01",
          "abstract": "There has been a long-standing quest for a unified audio-visual-text model to\nenable various multimodal understanding tasks, which mimics the listening,\nseeing and reading process of human beings. Humans tends to represent knowledge\nusing two separate systems: one for representing verbal (textual) information\nand one for representing non-verbal (visual and auditory) information. These\ntwo systems can operate independently but can also interact with each other.\nMotivated by this understanding of human cognition, in this paper, we introduce\nCoAVT -- a novel cognition-inspired Correlated Audio-Visual-Text pre-training\nmodel to connect the three modalities. It contains a joint audio-visual encoder\nthat learns to encode audio-visual synchronization information together with\nthe audio and visual content for non-verbal information, and a text encoder to\nhandle textual input for verbal information. To bridge the gap between\nmodalities, CoAVT employs a query encoder, which contains a set of learnable\nquery embeddings, and extracts the most informative audiovisual features of the\ncorresponding text. Additionally, to leverage the correspondences between audio\nand vision with language respectively, we also establish the audio-text and\nvisual-text bi-modal alignments upon the foundational audiovisual-text\ntri-modal alignment to enhance the multimodal representation learning. Finally,\nwe jointly optimize CoAVT model with three multimodal objectives: contrastive\nloss, matching loss and language modeling loss. Extensive experiments show that\nCoAVT can learn strong multimodal correlations and be generalized to various\ndownstream tasks. CoAVT establishes new state-of-the-art performance on\ntext-video retrieval task on AudioCaps for both zero-shot and fine-tuning\nsettings, audio-visual event classification and audio-visual retrieval tasks on\nAudioSet and VGGSound.",
          "arxiv_id": "2401.12264v2"
        }
      ],
      "10": [
        {
          "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
          "year": "2022-07",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved\nleading performances in many generative tasks. However, the inherited iterative\nsampling process costs hinder their applications to text-to-speech deployment.\nThrough the preliminary study on diffusion model parameterization, we find that\nprevious gradient-based TTS models require hundreds or thousands of iterations\nto guarantee high sample quality, which poses a challenge for accelerating\nsampling. In this work, we propose ProDiff, on progressive fast diffusion model\nfor high-quality text-to-speech. Unlike previous work estimating the gradient\nfor data density, ProDiff parameterizes the denoising model by directly\npredicting clean data to avoid distinct quality degradation in accelerating\nsampling. To tackle the model convergence challenge with decreased diffusion\niterations, ProDiff reduces the data variance in the target site via knowledge\ndistillation. Specifically, the denoising model uses the generated\nmel-spectrogram from an N-step DDIM teacher as the training target and distills\nthe behavior into a new model with N/2 steps. As such, it allows the TTS model\nto make sharp predictions and further reduces the sampling time by orders of\nmagnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to\nsynthesize high-fidelity mel-spectrograms, while it maintains sample quality\nand diversity competitive with state-of-the-art models using hundreds of steps.\nProDiff enables a sampling speed of 24x faster than real-time on a single\nNVIDIA 2080Ti GPU, making diffusion models practically applicable to\ntext-to-speech synthesis deployment for the first time. Our extensive ablation\nstudies demonstrate that each design in ProDiff is effective, and we further\nshow that ProDiff can be easily extended to the multi-speaker setting. Audio\nsamples are available at \\url{https://ProDiff.github.io/.}",
          "arxiv_id": "2207.06389v1"
        },
        {
          "title": "CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model",
          "year": "2023-05",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have shown promising\nperformance for speech synthesis. However, a large number of iterative steps\nare required to achieve high sample quality, which restricts the inference\nspeed. Maintaining sample quality while increasing sampling speed has become a\nchallenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based\n\"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through a\nsingle diffusion sampling step while achieving high audio quality. The\nconsistency constraint is applied to distill a consistency model from a\nwell-designed diffusion-based teacher model, which ultimately yields superior\nperformances in the distilled CoMoSpeech. Our experiments show that by\ngenerating audio recordings by a single sampling step, the CoMoSpeech achieves\nan inference speed more than 150 times faster than real-time on a single NVIDIA\nA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based\nspeech synthesis truly practical. Meanwhile, objective and subjective\nevaluations on text-to-speech and singing voice synthesis show that the\nproposed teacher models yield the best audio quality, and the one-step sampling\nbased CoMoSpeech achieves the best inference speed with better or comparable\naudio quality to other conventional multi-step diffusion model baselines. Audio\nsamples are available at https://comospeech.github.io/.",
          "arxiv_id": "2305.06908v4"
        },
        {
          "title": "FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis",
          "year": "2022-04",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved\nleading performances in many generative tasks. However, the inherited iterative\nsampling process costs hindered their applications to speech synthesis. This\npaper proposes FastDiff, a fast conditional diffusion model for high-quality\nspeech synthesis. FastDiff employs a stack of time-aware location-variable\nconvolutions of diverse receptive field patterns to efficiently model long-term\ntime dependencies with adaptive conditions. A noise schedule predictor is also\nadopted to reduce the sampling steps without sacrificing the generation\nquality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer,\nFastDiff-TTS, which generates high-fidelity speech waveforms without any\nintermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff\ndemonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech\nsamples. Also, FastDiff enables a sampling speed of 58x faster than real-time\non a V100 GPU, making diffusion models practically applicable to speech\nsynthesis deployment for the first time. We further show that FastDiff\ngeneralized well to the mel-spectrogram inversion of unseen speakers, and\nFastDiff-TTS outperformed other competing methods in end-to-end text-to-speech\nsynthesis. Audio samples are available at \\url{https://FastDiff.github.io/}.",
          "arxiv_id": "2204.09934v1"
        }
      ],
      "11": [
        {
          "title": "COVID-19 Cough Classification using Machine Learning and Global Smartphone Recordings",
          "year": "2020-12",
          "abstract": "We present a machine learning based COVID-19 cough classifier which can\ndiscriminate COVID-19 positive coughs from both COVID-19 negative and healthy\ncoughs recorded on a smartphone. This type of screening is non-contact, easy to\napply, and can reduce the workload in testing centres as well as limit\ntransmission by recommending early self-isolation to those who have a cough\nsuggestive of COVID-19. The datasets used in this study include subjects from\nall six continents and contain both forced and natural coughs, indicating that\nthe approach is widely applicable. The publicly available Coswara dataset\ncontains 92 COVID-19 positive and 1079 healthy subjects, while the second\nsmaller dataset was collected mostly in South Africa and contains 18 COVID-19\npositive and 26 COVID-19 negative subjects who have undergone a SARS-CoV\nlaboratory test. Both datasets indicate that COVID-19 positive coughs are\n15\\%-20\\% shorter than non-COVID coughs. Dataset skew was addressed by applying\nthe synthetic minority oversampling technique (SMOTE). A leave-$p$-out\ncross-validation scheme was used to train and evaluate seven machine learning\nclassifiers: LR, KNN, SVM, MLP, CNN, LSTM and Resnet50. Our results show that\nalthough all classifiers were able to identify COVID-19 coughs, the best\nperformance was exhibited by the Resnet50 classifier, which was best able to\ndiscriminate between the COVID-19 positive and the healthy coughs with an area\nunder the ROC curve (AUC) of 0.98. An LSTM classifier was best able to\ndiscriminate between the COVID-19 positive and COVID-19 negative coughs, with\nan AUC of 0.94 after selecting the best 13 features from a sequential forward\nselection (SFS). Since this type of cough audio classification is\ncost-effective and easy to deploy, it is potentially a useful and viable means\nof non-contact COVID-19 screening.",
          "arxiv_id": "2012.01926v2"
        },
        {
          "title": "QUCoughScope: An Artificially Intelligent Mobile Application to Detect Asymptomatic COVID-19 Patients using Cough and Breathing Sounds",
          "year": "2021-03",
          "abstract": "In the break of COVID-19 pandemic, mass testing has become essential to\nreduce the spread of the virus. Several recent studies suggest that a\nsignificant number of COVID-19 patients display no physical symptoms\nwhatsoever. Therefore, it is unlikely that these patients will undergo COVID-19\ntest, which increases their chances of unintentionally spreading the virus.\nCurrently, the primary diagnostic tool to detect COVID-19 is RT-PCR test on\ncollected respiratory specimens from the suspected case. This requires patients\nto travel to a laboratory facility to be tested, thereby potentially infecting\nothers along the way.It is evident from recent researches that asymptomatic\nCOVID-19 patients cough and breath in a different way than the healthy people.\nSeveral research groups have created mobile and web-platform for crowdsourcing\nthe symptoms, cough and breathing sounds from healthy, COVID-19 and Non-COVID\npatients. Some of these data repositories were made public. We have received\nsuch a repository from Cambridge University team under data-sharing agreement,\nwhere we have cough and breathing sound samples for 582 and 141 healthy and\nCOVID-19 patients, respectively. 87 COVID-19 patients were asymptomatic, while\nrest of them have cough. We have developed an Android application to\nautomatically screen COVID-19 from the comfort of people homes. Test subjects\ncan simply download a mobile application, enter their symptoms, record an audio\nclip of their cough and breath, and upload the data anonymously to our servers.\nOur backend server converts the audio clip to spectrogram and then apply our\nstate-of-the-art machine learning model to classify between cough sounds\nproduced by COVID-19 patients, as opposed to healthy subjects or those with\nother respiratory conditions. The system can detect asymptomatic COVID-19\npatients with a sensitivity more than 91%.",
          "arxiv_id": "2103.12063v1"
        },
        {
          "title": "Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory Sound Data",
          "year": "2020-06",
          "abstract": "Audio signals generated by the human body (e.g., sighs, breathing, heart,\ndigestion, vibration sounds) have routinely been used by clinicians as\nindicators to diagnose disease or assess disease progression. Until recently,\nsuch signals were usually collected through manual auscultation at scheduled\nvisits. Research has now started to use digital technology to gather bodily\nsounds (e.g., from digital stethoscopes) for cardiovascular or respiratory\nexamination, which could then be used for automatic analysis. Some initial work\nshows promise in detecting diagnostic signals of COVID-19 from voice and\ncoughs. In this paper we describe our data analysis over a large-scale\ncrowdsourced dataset of respiratory sounds collected to aid diagnosis of\nCOVID-19. We use coughs and breathing to understand how discernible COVID-19\nsounds are from those in asthma or healthy controls. Our results show that even\na simple binary machine learning classifier is able to classify correctly\nhealthy and COVID-19 sounds. We also show how we distinguish a user who tested\npositive for COVID-19 and has a cough from a healthy user with a cough, and\nusers who tested positive for COVID-19 and have a cough from users with asthma\nand a cough. Our models achieve an AUC of above 80% across all tasks. These\nresults are preliminary and only scratch the surface of the potential of this\ntype of data and audio-based machine learning. This work opens the door to\nfurther investigation of how automatically analysed respiratory patterns could\nbe used as pre-screening signals to aid COVID-19 diagnosis.",
          "arxiv_id": "2006.05919v3"
        }
      ],
      "12": [
        {
          "title": "Unsupervised Music Source Separation Using Differentiable Parametric Source Models",
          "year": "2022-01",
          "abstract": "Supervised deep learning approaches to underdetermined audio source\nseparation achieve state-of-the-art performance but require a dataset of\nmixtures along with their corresponding isolated source signals. Such datasets\ncan be extremely costly to obtain for musical mixtures. This raises a need for\nunsupervised methods. We propose a novel unsupervised model-based deep learning\napproach to musical source separation. Each source is modelled with a\ndifferentiable parametric source-filter model. A neural network is trained to\nreconstruct the observed mixture as a sum of the sources by estimating the\nsource models' parameters given their fundamental frequencies. At test time,\nsoft masks are obtained from the synthesized source signals. The experimental\nevaluation on a vocal ensemble separation task shows that the proposed method\noutperforms learning-free methods based on nonnegative matrix factorization and\na supervised deep learning baseline. Integrating domain knowledge in the form\nof source models into a data-driven method leads to high data efficiency: the\nproposed approach achieves good separation quality even when trained on less\nthan three minutes of audio. This work makes powerful deep learning based\nseparation usable in scenarios where training data with ground truth is\nexpensive or nonexistent.",
          "arxiv_id": "2201.09592v2"
        },
        {
          "title": "Multi-Task Audio Source Separation",
          "year": "2021-07",
          "abstract": "The audio source separation tasks, such as speech enhancement, speech\nseparation, and music source separation, have achieved impressive performance\nin recent studies. The powerful modeling capabilities of deep neural networks\ngive us hope for more challenging tasks. This paper launches a new multi-task\naudio source separation (MTASS) challenge to separate the speech, music, and\nnoise signals from the monaural mixture. First, we introduce the details of\nthis task and generate a dataset of mixtures containing speech, music, and\nbackground noises. Then, we propose an MTASS model in the complex domain to\nfully utilize the differences in spectral characteristics of the three audio\nsignals. In detail, the proposed model follows a two-stage pipeline, which\nseparates the three types of audio signals and then performs signal\ncompensation separately. After comparing different training targets, the\ncomplex ratio mask is selected as a more suitable target for the MTASS. The\nexperimental results also indicate that the residual signal compensation module\nhelps to recover the signals further. The proposed model shows significant\nadvantages in separation performance over several well-known separation models.",
          "arxiv_id": "2107.06467v1"
        },
        {
          "title": "Separate This, and All of these Things Around It: Music Source Separation via Hyperellipsoidal Queries",
          "year": "2025-01",
          "abstract": "Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.",
          "arxiv_id": "2501.16171v1"
        }
      ],
      "13": [
        {
          "title": "JVS-MuSiC: Japanese multispeaker singing-voice corpus",
          "year": "2020-01",
          "abstract": "Thanks to developments in machine learning techniques, it has become possible\nto synthesize high-quality singing voices of a single singer. An open\nmultispeaker singing-voice corpus would further accelerate the research in\nsinging-voice synthesis. However, conventional singing-voice corpora only\nconsist of the singing voices of a single singer. We designed a Japanese\nmultispeaker singing-voice corpus called \"JVS-MuSiC\" with the aim to analyze\nand synthesize a variety of voices. The corpus consists of 100 singers'\nrecordings of the same song, Katatsumuri, which is a Japanese children's song.\nIt also includes another song that is different for each singer. In this paper,\nwe describe the design of the corpus and experimental analyses using JVS-MuSiC.\nWe investigated the relationship between 1) the similarity of singing voices\nand perceptual oneness of unison singing voices and between 2) the similarity\nof singing voices and that of speech. The results suggest that 1) there is a\npositive and moderate correlation between singing-voice similarity and the\noneness of unison and that 2) the correlation between singing-voice similarity\nand speech similarity is weak. This corpus is freely available online.",
          "arxiv_id": "2001.07044v1"
        },
        {
          "title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System",
          "year": "2021-08",
          "abstract": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.",
          "arxiv_id": "2108.02776v1"
        },
        {
          "title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis",
          "year": "2023-12",
          "abstract": "Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://aaronz345.github.io/StyleSingerDemo/.",
          "arxiv_id": "2312.10741v5"
        }
      ],
      "14": [
        {
          "title": "A Bird Song Detector for improving bird identification through Deep Learning: a case study from Doana",
          "year": "2025-03",
          "abstract": "Passive Acoustic Monitoring is a key tool for biodiversity conservation, but\nthe large volumes of unsupervised audio it generates present major challenges\nfor extracting meaningful information. Deep Learning offers promising\nsolutions. BirdNET, a widely used bird identification model, has shown success\nin many study systems but is limited at local scale due to biases in its\ntraining data, which focus on specific locations and target sounds rather than\nentire soundscapes. A key challenge in bird species identification is that many\nrecordings either lack target species or contain overlapping vocalizations,\ncomplicating automatic identification. To address these problems, we developed\na multi-stage pipeline for automatic bird vocalization identification in\nDo\\~nana National Park (SW Spain), a wetland of high conservation concern. We\ndeployed AudioMoth recorders in three main habitats across nine locations and\nmanually annotated 461 minutes of audio, resulting in 3749 labeled segments\nspanning 34 classes. We first applied a Bird Song Detector to isolate bird\nvocalizations using spectrogram-based image processing. Then, species were\nclassified using custom models trained at the local scale. Applying the Bird\nSong Detector before classification improved species identification, as all\nmodels performed better when analyzing only the segments where birds were\ndetected. Specifically, the combination of detector and fine-tuned BirdNET\noutperformed the baseline without detection. This approach demonstrates the\neffectiveness of integrating a Bird Song Detector with local classification\nmodels. These findings highlight the need to adapt general-purpose tools to\nspecific ecological challenges. Automatically detecting bird species helps\ntrack the health of this threatened ecosystem, given birds sensitivity to\nenvironmental change, and supports conservation planning to reduce biodiversity\nloss.",
          "arxiv_id": "2503.15576v2"
        },
        {
          "title": "Global birdsong embeddings enable superior transfer learning for bioacoustic classification",
          "year": "2023-07",
          "abstract": "Automated bioacoustic analysis aids understanding and protection of both\nmarine and terrestrial animals and their habitats across extensive\nspatiotemporal scales, and typically involves analyzing vast collections of\nacoustic data. With the advent of deep learning models, classification of\nimportant signals from these datasets has markedly improved. These models power\ncritical data analyses for research and decision-making in biodiversity\nmonitoring, animal behaviour studies, and natural resource management. However,\ndeep learning models are often data-hungry and require a significant amount of\nlabeled training data to perform well. While sufficient training data is\navailable for certain taxonomic groups (e.g., common bird species), many\nclasses (such as rare and endangered species, many non-bird taxa, and\ncall-type) lack enough data to train a robust model from scratch. This study\ninvestigates the utility of feature embeddings extracted from audio\nclassification models to identify bioacoustic classes other than the ones these\nmodels were originally trained on. We evaluate models on diverse datasets,\nincluding different bird calls and dialect types, bat calls, marine mammals\ncalls, and amphibians calls. The embeddings extracted from the models trained\non bird vocalization data consistently allowed higher quality classification\nthan the embeddings trained on general audio datasets. The results of this\nstudy indicate that high-quality feature embeddings from large-scale acoustic\nbird classifiers can be harnessed for few-shot transfer learning, enabling the\nlearning of new classes from a limited quantity of training data. Our findings\nreveal the potential for efficient analyses of novel bioacoustic tasks, even in\nscenarios where available training data is limited to a few samples.",
          "arxiv_id": "2307.06292v2"
        },
        {
          "title": "Unsupervised outlier detection to improve bird audio dataset labels",
          "year": "2025-04",
          "abstract": "The Xeno-Canto bird audio repository is an invaluable resource for those\ninterested in vocalizations and other sounds made by birds around the world.\nThis is particularly the case for machine learning researchers attempting to\nimprove on the bird species recognition accuracy of classification models.\nHowever, the task of extracting labeled datasets from the recordings found in\nthis crowd-sourced repository faces several challenges. One challenge of\nparticular significance to machine learning practitioners is that one bird\nspecies label is applied to each audio recording, but frequently other sounds\nare also captured including other bird species, other animal sounds,\nanthropogenic and other ambient sounds. These non-target bird species sounds\ncan result in dataset labeling discrepancies referred to as label noise. In\nthis work we present a cleaning process consisting of audio preprocessing\nfollowed by dimensionality reduction and unsupervised outlier detection (UOD)\nto reduce the label noise in a dataset derived from Xeno-Canto recordings. We\ninvestigate three neural network dimensionality reduction techniques: two\nflavors of convolutional autoencoders and variational deep embedding (VaDE\n(Jiang, 2017)). While both methods show some degree of effectiveness at\ndetecting outliers for most bird species datasets, we found significant\nvariation in the performance of the methods from one species to the next. We\nbelieve that the results of this investigation demonstrate that the application\nof our cleaning process can meaningfully reduce the label noise of bird species\ndatasets derived from Xeno-Canto audio repository but results vary across\nspecies.",
          "arxiv_id": "2504.18650v1"
        }
      ],
      "15": [
        {
          "title": "CaTT-KWS: A Multi-stage Customized Keyword Spotting Framework based on Cascaded Transducer-Transformer",
          "year": "2022-07",
          "abstract": "Customized keyword spotting (KWS) has great potential to be deployed on edge\ndevices to achieve hands-free user experience. However, in real applications,\nfalse alarm (FA) would be a serious problem for spotting dozens or even\nhundreds of keywords, which drastically affects user experience. To solve this\nproblem, in this paper, we leverage the recent advances in transducer and\ntransformer based acoustic models and propose a new multi-stage customized KWS\nframework named Cascaded Transducer-Transformer KWS (CaTT-KWS), which includes\na transducer based keyword detector, a frame-level phone predictor based force\nalignment module and a transformer based decoder. Specifically, the streaming\ntransducer module is used to spot keyword candidates in audio stream. Then\nforce alignment is implemented using the phone posteriors predicted by the\nphone predictor to finish the first stage keyword verification and refine the\ntime boundaries of keyword. Finally, the transformer decoder further verifies\nthe triggered keyword. Our proposed CaTT-KWS framework reduces FA rate\neffectively without obviously hurting keyword recognition accuracy.\nSpecifically, we can get impressively 0.13 FA per hour on a challenging\ndataset, with over 90% relative reduction on FA comparing to the transducer\nbased detection model, while keyword recognition accuracy only drops less than\n2%.",
          "arxiv_id": "2207.01267v1"
        },
        {
          "title": "Exploring Sequence-to-Sequence Transformer-Transducer Models for Keyword Spotting",
          "year": "2022-11",
          "abstract": "In this paper, we present a novel approach to adapt a sequence-to-sequence\nTransformer-Transducer ASR system to the keyword spotting (KWS) task. We\nachieve this by replacing the keyword in the text transcription with a special\ntoken <kw> and training the system to detect the <kw> token in an audio stream.\nAt inference time, we create a decision function inspired by conventional KWS\napproaches, to make our approach more suitable for the KWS task. Furthermore,\nwe introduce a specific keyword spotting loss by adapting the\nsequence-discriminative Minimum Bayes-Risk training technique. We find that our\napproach significantly outperforms ASR based KWS systems. When compared with a\nconventional keyword spotting system, our proposal has similar performance\nwhile bringing the advantages and flexibility of sequence-to-sequence training.\nAdditionally, when combined with the conventional KWS system, our approach can\nimprove the performance at any operation point.",
          "arxiv_id": "2211.06478v1"
        },
        {
          "title": "Progressive Continual Learning for Spoken Keyword Spotting",
          "year": "2022-01",
          "abstract": "Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. To tackle such challenges, we propose a\nprogressive continual learning strategy for small-footprint spoken keyword\nspotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a\nnetwork instantiator to generate the task-specific sub-networks for remembering\npreviously learned keywords. As a result, the PCL-KWS approach incrementally\nlearns new keywords without forgetting prior knowledge. Besides, the\nkeyword-aware network scaling mechanism of PCL-KWS constrains the growth of\nmodel parameters while achieving high performance. Experimental results show\nthat after learning five new tasks sequentially, our proposed PCL-KWS approach\narchives the new state-of-the-art performance of 92.8% average accuracy for all\nthe tasks on Google Speech Command dataset compared with other baselines.",
          "arxiv_id": "2201.12546v2"
        }
      ],
      "16": [
        {
          "title": "StableFace: Analyzing and Improving Motion Stability for Talking Face Generation",
          "year": "2022-08",
          "abstract": "While previous speech-driven talking face generation methods have made\nsignificant progress in improving the visual quality and lip-sync quality of\nthe synthesized videos, they pay less attention to lip motion jitters which\ngreatly undermine the realness of talking face videos. What causes motion\njitters, and how to mitigate the problem? In this paper, we conduct systematic\nanalyses on the motion jittering problem based on a state-of-the-art pipeline\nthat uses 3D face representations to bridge the input audio and output video,\nand improve the motion stability with a series of effective designs. We find\nthat several issues can lead to jitters in synthesized talking face video: 1)\njitters from the input 3D face representations; 2) training-inference mismatch;\n3) lack of dependency modeling among video frames. Accordingly, we propose\nthree effective solutions to address this issue: 1) we propose a gaussian-based\nadaptive smoothing module to smooth the 3D face representations to eliminate\njitters in the input; 2) we add augmented erosions on the input data of the\nneural renderer in training to simulate the distortion in inference to reduce\nmismatch; 3) we develop an audio-fused transformer generator to model\ndependency among video frames. Besides, considering there is no off-the-shelf\nmetric for measuring motion jitters in talking face video, we devise an\nobjective metric (Motion Stability Index, MSI), to quantitatively measure the\nmotion jitters by calculating the reciprocal of variance acceleration.\nExtensive experimental results show the superiority of our method on\nmotion-stable face video generation, with better quality than previous systems.",
          "arxiv_id": "2208.13717v1"
        },
        {
          "title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation",
          "year": "2023-03",
          "abstract": "Speech-driven 3D face animation aims to generate realistic facial expressions\nthat match the speech content and emotion. However, existing methods often\nneglect emotional facial expressions or fail to disentangle them from speech\ncontent. To address this issue, this paper proposes an end-to-end neural\nnetwork to disentangle different emotions in speech so as to generate rich 3D\nfacial expressions. Specifically, we introduce the emotion disentangling\nencoder (EDE) to disentangle the emotion and content in the speech by\ncross-reconstructed speech signals with different emotion labels. Then an\nemotion-guided feature fusion decoder is employed to generate a 3D talking face\nwith enhanced emotion. The decoder is driven by the disentangled identity,\nemotional, and content embeddings so as to generate controllable personal and\nemotional styles. Finally, considering the scarcity of the 3D emotional talking\nface data, we resort to the supervision of facial blendshapes, which enables\nthe reconstruction of plausible 3D faces from 2D emotional data, and contribute\na large-scale 3D emotional talking face dataset (3D-ETF) to train the network.\nOur experiments and user studies demonstrate that our approach outperforms\nstate-of-the-art methods and exhibits more diverse facial movements. We\nrecommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/emotalk",
          "arxiv_id": "2303.11089v2"
        },
        {
          "title": "AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person",
          "year": "2021-08",
          "abstract": "Automatically generating videos in which synthesized speech is synchronized\nwith lip movements in a talking head has great potential in many human-computer\ninteraction scenarios. In this paper, we present an automatic method to\ngenerate synchronized speech and talking-head videos on the basis of text and a\nsingle face image of an arbitrary person as input. In contrast to previous\ntext-driven talking head generation methods, which can only synthesize the\nvoice of a specific person, the proposed method is capable of synthesizing\nspeech for any person that is inaccessible in the training stage. Specifically,\nthe proposed method decomposes the generation of synchronized speech and\ntalking head videos into two stages, i.e., a text-to-speech (TTS) stage and a\nspeech-driven talking head generation stage. The proposed TTS module is a\nface-conditioned multi-speaker TTS model that gets the speaker identity\ninformation from face images instead of speech, which allows us to synthesize a\npersonalized voice on the basis of the input face image. To generate the\ntalking head videos from the face images, a facial landmark-based method that\ncan predict both lip movements and head rotations is proposed. Extensive\nexperiments demonstrate that the proposed method is able to generate\nsynchronized speech and talking head videos for arbitrary persons and\nnon-persons. Synthesized speech shows consistency with the given face regarding\nto the synthesized voice's timbre and one's appearance in the image, and the\nproposed landmark-based talking head method outperforms the state-of-the-art\nlandmark-based method on generating natural talking head videos.",
          "arxiv_id": "2108.04325v2"
        }
      ],
      "17": [
        {
          "title": "Distributed collaborative anomalous sound detection by embedding sharing",
          "year": "2024-03",
          "abstract": "To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.",
          "arxiv_id": "2403.16610v1"
        },
        {
          "title": "Transformer-based Autoencoder with ID Constraint for Unsupervised Anomalous Sound Detection",
          "year": "2023-10",
          "abstract": "Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous\nsounds of devices when only normal sound data is available. The autoencoder\n(AE) and self-supervised learning based methods are two mainstream methods.\nHowever, the AE-based methods could be limited as the feature learned from\nnormal sounds can also fit with anomalous sounds, reducing the ability of the\nmodel in detecting anomalies from sound. The self-supervised methods are not\nalways stable and perform differently, even for machines of the same type. In\naddition, the anomalous sound may be short-lived, making it even harder to\ndistinguish from normal sound. This paper proposes an ID constrained\nTransformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly\nscore computation for unsupervised ASD. Machine ID is employed to constrain the\nlatent space of the Transformer-based autoencoder (TransAE) by introducing a\nsimple ID classifier to learn the difference in the distribution for the same\nmachine type and enhance the ability of the model in distinguishing anomalous\nsound. Moreover, weighted anomaly score computation is introduced to highlight\nthe anomaly scores of anomalous events that only appear for a short time.\nExperiments performed on DCASE 2020 Challenge Task2 development dataset\ndemonstrate the effectiveness and superiority of our proposed method.",
          "arxiv_id": "2310.08950v1"
        },
        {
          "title": "SSDPT: Self-Supervised Dual-Path Transformer for Anomalous Sound Detection in Machine Condition Monitoring",
          "year": "2022-08",
          "abstract": "Anomalous sound detection for machine condition monitoring has great\npotential in the development of Industry 4.0. However, these anomalous sounds\nof machines are usually unavailable in normal conditions. Therefore, the models\nemployed have to learn acoustic representations with normal sounds for\ntraining, and detect anomalous sounds while testing. In this article, we\npropose a self-supervised dual-path Transformer (SSDPT) network to detect\nanomalous sounds in machine monitoring. The SSDPT network splits the acoustic\nfeatures into segments and employs several DPT blocks for time and frequency\nmodeling. DPT blocks use attention modules to alternately model the interactive\ninformation about the frequency and temporal components of the segmented\nacoustic features. To address the problem of lack of anomalous sound, we adopt\na self-supervised learning approach to train the network with normal sound.\nSpecifically, this approach randomly masks and reconstructs the acoustic\nfeatures, and jointly classifies machine identity information to improve the\nperformance of anomalous sound detection. We evaluated our method on the\nDCASE2021 task2 dataset. The experimental results show that the SSDPT network\nachieves a significant increase in the harmonic mean AUC score, in comparison\nto present state-of-the-art methods of anomalous sound detection.",
          "arxiv_id": "2208.03421v1"
        }
      ],
      "18": [
        {
          "title": "Learning to Maximize Speech Quality Directly Using MOS Prediction for Neural Text-to-Speech",
          "year": "2020-11",
          "abstract": "Although recent neural text-to-speech (TTS) systems have achieved\nhigh-quality speech synthesis, there are cases where a TTS system generates\nlow-quality speech, mainly caused by limited training data or information loss\nduring knowledge distillation. Therefore, we propose a novel method to improve\nspeech quality by training a TTS model under the supervision of perceptual\nloss, which measures the distance between the maximum possible speech quality\nscore and the predicted one. We first pre-train a mean opinion score (MOS)\nprediction model and then train a TTS model to maximize the MOS of synthesized\nspeech using the pre-trained MOS prediction model. The proposed method can be\napplied independently regardless of the TTS model architecture or the cause of\nspeech quality degradation and efficiently without increasing the inference\ntime or model complexity. The evaluation results for the MOS and phone error\nrate demonstrate that our proposed approach improves previous models in terms\nof both naturalness and intelligibility.",
          "arxiv_id": "2011.01174v5"
        },
        {
          "title": "SAMOS: A Neural MOS Prediction Model Leveraging Semantic Representations and Acoustic Features",
          "year": "2024-11",
          "abstract": "Assessing the naturalness of speech using mean opinion score (MOS) prediction\nmodels has positive implications for the automatic evaluation of speech\nsynthesis systems. Early MOS prediction models took the raw waveform or\namplitude spectrum of speech as input, whereas more advanced methods employed\nself-supervised-learning (SSL) based models to extract semantic representations\nfrom speech for MOS prediction. These methods utilized limited aspects of\nspeech information for MOS prediction, resulting in restricted prediction\naccuracy. Therefore, in this paper, we propose SAMOS, a MOS prediction model\nthat leverages both Semantic and Acoustic information of speech to be assessed.\nSpecifically, the proposed SAMOS leverages a pretrained wav2vec2 to extract\nsemantic representations and uses the feature extractor of a pretrained\nBiVocoder to extract acoustic features. These two types of features are then\nfed into the prediction network, which includes multi-task heads and an\naggregation layer, to obtain the final MOS score. Experimental results\ndemonstrate that the proposed SAMOS outperforms current state-of-the-art MOS\nprediction models on the BVCC dataset and performs comparable performance on\nthe BC2019 dataset, according to the results of system-level evaluation\nmetrics.",
          "arxiv_id": "2411.11232v1"
        },
        {
          "title": "Selecting N-lowest scores for training MOS prediction models",
          "year": "2025-06",
          "abstract": "The automatic speech quality assessment (SQA) has been extensively studied to\npredict the speech quality without time-consuming questionnaires. Recently,\nneural-based SQA models have been actively developed for speech samples\nproduced by text-to-speech or voice conversion, with a primary focus on\ntraining mean opinion score (MOS) prediction models. The quality of each speech\nsample may not be consistent across the entire duration, and it remains unclear\nwhich segments of the speech receive the primary focus from humans when\nassigning subjective evaluation for MOS calculation. We hypothesize that when\nhumans rate speech, they tend to assign more weight to low-quality speech\nsegments, and the variance in ratings for each sample is mainly due to\naccidental assignment of higher scores when overlooking the poor quality speech\nsegments. Motivated by the hypothesis, we analyze the VCC2018 and BVCC\ndatasets. Based on the hypothesis, we propose the more reliable representative\nvalue N_low-MOS, the mean of the $N$-lowest opinion scores. Our experiments\nshow that LCC and SRCC improve compared to regular MOS when employing N_low-MOS\nto MOSNet training. This result suggests that N_low-MOS is a more intrinsic\nrepresentative value of subjective speech quality and makes MOSNet a better\ncomparator of VC models.",
          "arxiv_id": "2506.18326v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:21:39Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}