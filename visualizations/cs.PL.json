{
  "topics": {
    "data": {
      "0": {
        "name": "0_code_language_programming_program",
        "keywords": [
          [
            "code",
            0.03810130025852554
          ],
          [
            "language",
            0.03455731519540369
          ],
          [
            "programming",
            0.029950850510218913
          ],
          [
            "program",
            0.0290271505321058
          ],
          [
            "programs",
            0.028535906327487873
          ],
          [
            "data",
            0.025586952583346916
          ],
          [
            "type",
            0.02319117664226687
          ],
          [
            "model",
            0.021788987140023853
          ],
          [
            "paper",
            0.02160755822259952
          ],
          [
            "approach",
            0.020556590818176346
          ]
        ],
        "count": 4322
      },
      "1": {
        "name": "1_quantum_Quantum_circuit_classical",
        "keywords": [
          [
            "quantum",
            0.22266881956163911
          ],
          [
            "Quantum",
            0.061282769013532246
          ],
          [
            "circuit",
            0.04568301250468016
          ],
          [
            "classical",
            0.03880962389878631
          ],
          [
            "circuits",
            0.03850735818805607
          ],
          [
            "quantum programming",
            0.03393215930880466
          ],
          [
            "programming",
            0.03225724165539966
          ],
          [
            "programs",
            0.029837116267544456
          ],
          [
            "language",
            0.023897145573908554
          ],
          [
            "computing",
            0.02360898393894562
          ]
        ],
        "count": 174
      }
    },
    "correlations": [
      [
        1.0,
        -0.7459421460446303
      ],
      [
        -0.7459421460446303,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        47,
        0
      ],
      "2020-02": [
        63,
        0
      ],
      "2020-03": [
        35,
        4
      ],
      "2020-04": [
        65,
        3
      ],
      "2020-05": [
        57,
        5
      ],
      "2020-06": [
        46,
        0
      ],
      "2020-07": [
        56,
        1
      ],
      "2020-08": [
        60,
        0
      ],
      "2020-09": [
        52,
        1
      ],
      "2020-10": [
        88,
        2
      ],
      "2020-11": [
        48,
        0
      ],
      "2020-12": [
        42,
        3
      ],
      "2021-01": [
        62,
        5
      ],
      "2021-02": [
        50,
        0
      ],
      "2021-03": [
        61,
        1
      ],
      "2021-04": [
        59,
        1
      ],
      "2021-05": [
        63,
        1
      ],
      "2021-06": [
        56,
        1
      ],
      "2021-07": [
        69,
        3
      ],
      "2021-08": [
        73,
        1
      ],
      "2021-09": [
        55,
        5
      ],
      "2021-10": [
        52,
        3
      ],
      "2021-11": [
        54,
        5
      ],
      "2021-12": [
        55,
        4
      ],
      "2022-01": [
        50,
        2
      ],
      "2022-02": [
        52,
        2
      ],
      "2022-03": [
        64,
        0
      ],
      "2022-04": [
        30,
        6
      ],
      "2022-05": [
        72,
        7
      ],
      "2022-06": [
        48,
        2
      ],
      "2022-07": [
        74,
        1
      ],
      "2022-08": [
        61,
        0
      ],
      "2022-09": [
        64,
        0
      ],
      "2022-10": [
        51,
        4
      ],
      "2022-11": [
        59,
        4
      ],
      "2022-12": [
        60,
        2
      ],
      "2023-01": [
        49,
        0
      ],
      "2023-02": [
        55,
        3
      ],
      "2023-03": [
        62,
        3
      ],
      "2023-04": [
        54,
        4
      ],
      "2023-05": [
        83,
        1
      ],
      "2023-06": [
        46,
        2
      ],
      "2023-07": [
        77,
        1
      ],
      "2023-08": [
        56,
        1
      ],
      "2023-09": [
        77,
        1
      ],
      "2023-10": [
        72,
        5
      ],
      "2023-11": [
        75,
        10
      ],
      "2023-12": [
        70,
        0
      ],
      "2024-01": [
        62,
        2
      ],
      "2024-02": [
        58,
        3
      ],
      "2024-03": [
        83,
        1
      ],
      "2024-04": [
        88,
        4
      ],
      "2024-05": [
        79,
        1
      ],
      "2024-06": [
        54,
        4
      ],
      "2024-07": [
        60,
        2
      ],
      "2024-08": [
        63,
        4
      ],
      "2024-09": [
        49,
        6
      ],
      "2024-10": [
        74,
        3
      ],
      "2024-11": [
        74,
        3
      ],
      "2024-12": [
        64,
        4
      ],
      "2025-01": [
        68,
        5
      ],
      "2025-02": [
        80,
        3
      ],
      "2025-03": [
        77,
        2
      ],
      "2025-04": [
        84,
        9
      ],
      "2025-05": [
        84,
        2
      ],
      "2025-06": [
        59,
        4
      ],
      "2025-07": [
        93,
        4
      ],
      "2025-08": [
        89,
        3
      ],
      "2025-09": [
        45,
        1
      ]
    },
    "papers": {
      "0": [
        {
          "title": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation",
          "year": "2022-08",
          "abstract": "Large language models have demonstrated the ability to generate both natural\nlanguage and programming language text. Such models open up the possibility of\nmulti-language code generation: could code generation models generalize\nknowledge from one language to another? Although contemporary code generation\nmodels can generate semantically correct Python code, little is known about\ntheir abilities with other languages. We propose MultiPL-E, a system for\ntranslating unit test-driven code generation benchmarks to new languages. We\ncreate the first massively multilingual code generation benchmark by using\nMultiPL-E to translate two popular Python code generation benchmarks to 18\nadditional programming languages.\n  We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18\nlanguages that encompass a range of programming paradigms and popularity. Using\nthese new parallel benchmarks, we evaluate the multi-language performance of\nthree state-of-the-art code generation models: Codex, CodeGen, and InCoder. We\nfind that Codex matches or even exceeds its performance on Python for several\nother languages. The range of programming languages represented in MultiPL-E\nallow us to explore the impact of language frequency and language features on\nmodel performance. Finally, the MultiPL-E approach of compiling code generation\nbenchmarks to new programming languages is both scalable and extensible, making\nit straightforward to evaluate new models, benchmarks, and languages.",
          "arxiv_id": "2208.08227v4"
        },
        {
          "title": "An Exploratory Study on Code Attention in BERT",
          "year": "2022-04",
          "abstract": "Many recent models in software engineering introduced deep neural models\nbased on the Transformer architecture or use transformer-based Pre-trained\nLanguage Models (PLM) trained on code. Although these models achieve the state\nof the arts results in many downstream tasks such as code summarization and bug\ndetection, they are based on Transformer and PLM, which are mainly studied in\nthe Natural Language Processing (NLP) field. The current studies rely on the\nreasoning and practices from NLP for these models in code, despite the\ndifferences between natural languages and programming languages. There is also\nlimited literature on explaining how code is modeled.\n  Here, we investigate the attention behavior of PLM on code and compare it\nwith natural language. We pre-trained BERT, a Transformer based PLM, on code\nand explored what kind of information it learns, both semantic and syntactic.\nWe run several experiments to analyze the attention values of code constructs\non each other and what BERT learns in each layer. Our analyses show that BERT\npays more attention to syntactic entities, specifically identifiers and\nseparators, in contrast to the most attended token [CLS] in NLP. This\nobservation motivated us to leverage identifiers to represent the code sequence\ninstead of the [CLS] token when used for code clone detection. Our results show\nthat employing embeddings from identifiers increases the performance of BERT by\n605% and 4% F1-score in its lower layers and the upper layers, respectively.\nWhen identifiers' embeddings are used in CodeBERT, a code-based PLM, the\nperformance is improved by 21-24% in the F1-score of clone detection. The\nfindings can benefit the research community by using code-specific\nrepresentations instead of applying the common embeddings used in NLP, and open\nnew directions for developing smaller models with similar performance.",
          "arxiv_id": "2204.10200v1"
        },
        {
          "title": "Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models",
          "year": "2024-10",
          "abstract": "High-level synthesis (HLS) allows hardware designers to create hardware\ndesigns with high-level programming languages like C/C++/OpenCL, which greatly\nimproves hardware design productivity. However, existing HLS flows require\nprogrammers' hardware design expertise and rely on programmers' manual code\ntransformations and directive annotations to guide compiler optimizations.\nOptimizing HLS designs requires non-trivial HLS expertise and tedious iterative\nprocess in HLS code optimization. Automating HLS code optimizations has become\na burning need. Recently, large language models (LLMs) trained on massive code\nand programming tasks have demonstrated remarkable proficiency in comprehending\ncode, showing the ability to handle domain-specific programming queries\ndirectly without labor-intensive fine-tuning. In this work, we propose a novel\nretrieval-augmented LLM-based approach to effectively optimize high-level\nsynthesis (HLS) programs. Our proposed method leverages few-shot learning,\nenabling large language models to adopt domain-specific knowledge through\nnatural language prompts. We propose a unique framework, Retrieve Augmented\nLarge Language Model Aided Design (RALAD), designed to enhance LLMs'\nperformance in HLS code optimization tasks. RALAD employs advanced embedding\ntechniques and top-\\emph{k} search algorithms to dynamically source relevant\nknowledge from extensive databases, thereby providing contextually appropriate\nresponses to complex programming queries. Our implementation of RALAD on two\nspecialized domains, utilizing comparatively smaller language models, achieves\nan impressive 80\\% success rate in compilation tasks and outperforms general\nLLMs by 3.7 -- 19$\\times$ in latency improvement.",
          "arxiv_id": "2410.07356v1"
        }
      ],
      "1": [
        {
          "title": "A Case for Synthesis of Recursive Quantum Unitary Programs",
          "year": "2023-11",
          "abstract": "Quantum programs are notoriously difficult to code and verify due to\nunintuitive quantum knowledge associated with quantum programming. Automated\ntools relieving the tedium and errors associated with low-level quantum details\nwould hence be highly desirable. In this paper, we initiate the study of\nprogram synthesis for quantum unitary programs that recursively define a family\nof unitary circuits for different input sizes, which are widely used in\nexisting quantum programming languages. Specifically, we present QSynth, the\nfirst quantum program synthesis framework, including a new inductive quantum\nprogramming language, its specification, a sound logic for reasoning, and an\nencoding of the reasoning procedure into SMT instances. By leveraging existing\nSMT solvers, QSynth successfully synthesizes ten quantum unitary programs\nincluding quantum adder circuits, quantum eigenvalue inversion circuits and\nQuantum Fourier Transformation, which can be readily transpiled to executable\nprograms on major quantum platforms, e.g., Q#, IBM Qiskit, and AWS Braket.",
          "arxiv_id": "2311.11503v2"
        },
        {
          "title": "QPanda: high-performance quantum computing framework for multiple application scenarios",
          "year": "2022-12",
          "abstract": "With the birth of Noisy Intermediate Scale Quantum (NISQ) devices and the\nverification of \"quantum supremacy\" in random number sampling and boson\nsampling, more and more fields hope to use quantum computers to solve specific\nproblems, such as aerodynamic design, route allocation, financial option\nprediction, quantum chemical simulation to find new materials, and the\nchallenge of quantum cryptography to automotive industry security. However,\nthese fields still need to constantly explore quantum algorithms that adapt to\nthe current NISQ machine, so a quantum programming framework that can face\nmulti-scenarios and application needs is required. Therefore, this paper\nproposes QPanda, an application scenario-oriented quantum programming framework\nwith high-performance simulation. Such as designing quantum chemical simulation\nalgorithms based on it to explore new materials, building a quantum machine\nlearning framework to serve finance, etc. This framework implements\nhigh-performance simulation of quantum circuits, a configuration of the fusion\nprocessing backend of quantum computers and supercomputers, and compilation and\noptimization methods of quantum programs for NISQ machines. Finally, the\nexperiment shows that quantum jobs can be executed with high fidelity on the\nquantum processor using quantum circuit compile and optimized interface and\nhave better simulation performance.",
          "arxiv_id": "2212.14201v1"
        },
        {
          "title": "TensorFlow Quantum: A Software Framework for Quantum Machine Learning",
          "year": "2020-03",
          "abstract": "We introduce TensorFlow Quantum (TFQ), an open source library for the rapid\nprototyping of hybrid quantum-classical models for classical or quantum data.\nThis framework offers high-level abstractions for the design and training of\nboth discriminative and generative quantum models under TensorFlow and supports\nhigh-performance quantum circuit simulators. We provide an overview of the\nsoftware architecture and building blocks through several examples and review\nthe theory of hybrid quantum-classical neural networks. We illustrate TFQ\nfunctionalities via several basic applications including supervised learning\nfor quantum classification, quantum control, simulating noisy quantum circuits,\nand quantum approximate optimization. Moreover, we demonstrate how one can\napply TFQ to tackle advanced quantum learning tasks including meta-learning,\nlayerwise learning, Hamiltonian learning, sampling thermal states, variational\nquantum eigensolvers, classification of quantum phase transitions, generative\nadversarial networks, and reinforcement learning. We hope this framework\nprovides the necessary tools for the quantum computing and machine learning\nresearch communities to explore models of both natural and artificial quantum\nsystems, and ultimately discover new quantum algorithms which could potentially\nyield a quantum advantage.",
          "arxiv_id": "2003.02989v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:20:08Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}