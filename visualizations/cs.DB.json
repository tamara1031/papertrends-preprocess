{
  "topics": {
    "data": {
      "0": {
        "name": "0_data_query_queries_performance",
        "keywords": [
          [
            "data",
            0.038771506405109755
          ],
          [
            "query",
            0.020930786053126954
          ],
          [
            "queries",
            0.01633946968874498
          ],
          [
            "performance",
            0.014428643061446231
          ],
          [
            "model",
            0.013921257837769735
          ],
          [
            "paper",
            0.013603587782747723
          ],
          [
            "database",
            0.01310968969156097
          ],
          [
            "time",
            0.012339117554862463
          ],
          [
            "models",
            0.01223808694751019
          ],
          [
            "systems",
            0.01111562897556599
          ]
        ],
        "count": 4796
      },
      "1": {
        "name": "1_privacy_data_private_Privacy",
        "keywords": [
          [
            "privacy",
            0.0644484746879489
          ],
          [
            "data",
            0.05080345847669289
          ],
          [
            "private",
            0.017929523810878818
          ],
          [
            "Privacy",
            0.017606615437439008
          ],
          [
            "differential",
            0.015181738569284835
          ],
          [
            "differential privacy",
            0.015030381099223465
          ],
          [
            "sensitive",
            0.014996798217643676
          ],
          [
            "synthetic",
            0.014386253867037208
          ],
          [
            "queries",
            0.014146030176419092
          ],
          [
            "utility",
            0.013491285838161785
          ]
        ],
        "count": 350
      },
      "2": {
        "name": "2_series_time_time series_mining",
        "keywords": [
          [
            "series",
            0.04890583116703527
          ],
          [
            "time",
            0.039891390986875344
          ],
          [
            "time series",
            0.03782989718132888
          ],
          [
            "mining",
            0.036680529741527516
          ],
          [
            "data",
            0.03393736422450294
          ],
          [
            "patterns",
            0.028382725154093052
          ],
          [
            "pattern",
            0.024419187292189955
          ],
          [
            "algorithm",
            0.021658829586024664
          ],
          [
            "detection",
            0.017584261433681296
          ],
          [
            "algorithms",
            0.017084013802440297
          ]
        ],
        "count": 255
      },
      "3": {
        "name": "3_process_event_mining_process mining",
        "keywords": [
          [
            "process",
            0.11014759667195909
          ],
          [
            "event",
            0.07925187765767916
          ],
          [
            "mining",
            0.05633382368868739
          ],
          [
            "process mining",
            0.05599042198270805
          ],
          [
            "Process",
            0.041813611993846034
          ],
          [
            "logs",
            0.038671285619883235
          ],
          [
            "processes",
            0.0376307603953621
          ],
          [
            "event logs",
            0.03321883724537466
          ],
          [
            "data",
            0.030007900121989863
          ],
          [
            "log",
            0.029749464327298344
          ]
        ],
        "count": 156
      },
      "4": {
        "name": "4_blockchain_blockchains_consensus_Blockchain",
        "keywords": [
          [
            "blockchain",
            0.0782259296806847
          ],
          [
            "blockchains",
            0.035218204969950144
          ],
          [
            "consensus",
            0.03491273435248229
          ],
          [
            "Blockchain",
            0.028473124088407038
          ],
          [
            "transactions",
            0.02546013695429784
          ],
          [
            "data",
            0.02485717529636487
          ],
          [
            "protocols",
            0.02117977450643097
          ],
          [
            "systems",
            0.020010564061149563
          ],
          [
            "transaction",
            0.018551134276472658
          ],
          [
            "shard",
            0.01742724139814615
          ]
        ],
        "count": 114
      }
    },
    "correlations": [
      [
        1.0,
        -0.3951048123611756,
        -0.446823081330459,
        -0.6497578128817865,
        -0.7276628938948753
      ],
      [
        -0.3951048123611756,
        1.0,
        -0.3937030914326215,
        -0.6638161879635329,
        -0.7259353821110844
      ],
      [
        -0.446823081330459,
        -0.3937030914326215,
        1.0,
        -0.6038395379189354,
        -0.7365482675566429
      ],
      [
        -0.6497578128817865,
        -0.6638161879635329,
        -0.6038395379189354,
        1.0,
        -0.7400548389980893
      ],
      [
        -0.7276628938948753,
        -0.7259353821110844,
        -0.7365482675566429,
        -0.7400548389980893,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        38,
        10,
        8,
        6,
        2
      ],
      "2020-02": [
        35,
        7,
        9,
        16,
        2
      ],
      "2020-03": [
        70,
        10,
        4,
        7,
        5
      ],
      "2020-04": [
        55,
        14,
        13,
        10,
        1
      ],
      "2020-05": [
        30,
        15,
        13,
        8,
        3
      ],
      "2020-06": [
        35,
        13,
        15,
        10,
        1
      ],
      "2020-07": [
        36,
        11,
        6,
        12,
        0
      ],
      "2020-08": [
        32,
        13,
        10,
        10,
        1
      ],
      "2020-09": [
        45,
        10,
        15,
        8,
        1
      ],
      "2020-10": [
        50,
        13,
        9,
        15,
        1
      ],
      "2020-11": [
        27,
        9,
        10,
        7,
        2
      ],
      "2020-12": [
        34,
        12,
        9,
        11,
        0
      ],
      "2021-01": [
        36,
        8,
        15,
        7,
        3
      ],
      "2021-02": [
        31,
        6,
        4,
        9,
        3
      ],
      "2021-03": [
        54,
        17,
        9,
        14,
        2
      ],
      "2021-04": [
        47,
        2,
        10,
        7,
        0
      ],
      "2021-05": [
        36,
        11,
        5,
        13,
        1
      ],
      "2021-06": [
        39,
        8,
        8,
        6,
        0
      ],
      "2021-07": [
        42,
        13,
        9,
        17,
        7
      ],
      "2021-08": [
        26,
        8,
        8,
        5,
        2
      ],
      "2021-09": [
        28,
        11,
        6,
        9,
        0
      ],
      "2021-10": [
        40,
        9,
        14,
        12,
        0
      ],
      "2021-11": [
        35,
        10,
        10,
        6,
        0
      ],
      "2021-12": [
        33,
        10,
        12,
        11,
        2
      ],
      "2022-01": [
        54,
        9,
        15,
        7,
        0
      ],
      "2022-02": [
        39,
        10,
        8,
        12,
        1
      ],
      "2022-03": [
        42,
        9,
        7,
        13,
        0
      ],
      "2022-04": [
        34,
        11,
        5,
        11,
        2
      ],
      "2022-05": [
        35,
        11,
        8,
        7,
        3
      ],
      "2022-06": [
        36,
        12,
        14,
        13,
        3
      ],
      "2022-07": [
        39,
        16,
        9,
        6,
        4
      ],
      "2022-08": [
        45,
        13,
        7,
        5,
        2
      ],
      "2022-09": [
        39,
        7,
        11,
        11,
        1
      ],
      "2022-10": [
        40,
        10,
        4,
        8,
        2
      ],
      "2022-11": [
        30,
        11,
        16,
        12,
        3
      ],
      "2022-12": [
        26,
        10,
        10,
        12,
        3
      ],
      "2023-01": [
        30,
        10,
        10,
        8,
        0
      ],
      "2023-02": [
        28,
        11,
        8,
        9,
        3
      ],
      "2023-03": [
        44,
        17,
        15,
        6,
        2
      ],
      "2023-04": [
        44,
        6,
        15,
        15,
        1
      ],
      "2023-05": [
        43,
        17,
        5,
        11,
        1
      ],
      "2023-06": [
        53,
        12,
        10,
        10,
        4
      ],
      "2023-07": [
        56,
        17,
        14,
        13,
        2
      ],
      "2023-08": [
        49,
        11,
        11,
        12,
        2
      ],
      "2023-09": [
        34,
        7,
        7,
        13,
        3
      ],
      "2023-10": [
        55,
        15,
        15,
        6,
        1
      ],
      "2023-11": [
        46,
        10,
        15,
        10,
        2
      ],
      "2023-12": [
        59,
        20,
        12,
        8,
        2
      ],
      "2024-01": [
        39,
        8,
        5,
        10,
        0
      ],
      "2024-02": [
        37,
        12,
        9,
        9,
        2
      ],
      "2024-03": [
        74,
        18,
        13,
        20,
        2
      ],
      "2024-04": [
        42,
        10,
        11,
        15,
        1
      ],
      "2024-05": [
        51,
        13,
        14,
        16,
        0
      ],
      "2024-06": [
        61,
        14,
        15,
        27,
        1
      ],
      "2024-07": [
        58,
        14,
        6,
        17,
        2
      ],
      "2024-08": [
        46,
        19,
        8,
        15,
        1
      ],
      "2024-09": [
        49,
        11,
        9,
        17,
        2
      ],
      "2024-10": [
        48,
        18,
        11,
        21,
        1
      ],
      "2024-11": [
        69,
        13,
        16,
        14,
        2
      ],
      "2024-12": [
        76,
        18,
        16,
        14,
        2
      ],
      "2025-01": [
        55,
        12,
        10,
        13,
        3
      ],
      "2025-02": [
        71,
        22,
        12,
        13,
        1
      ],
      "2025-03": [
        63,
        21,
        14,
        20,
        3
      ],
      "2025-04": [
        83,
        21,
        15,
        20,
        5
      ],
      "2025-05": [
        90,
        16,
        10,
        21,
        0
      ],
      "2025-06": [
        67,
        13,
        10,
        13,
        0
      ],
      "2025-07": [
        77,
        18,
        13,
        19,
        3
      ],
      "2025-08": [
        101,
        14,
        12,
        15,
        5
      ],
      "2025-09": [
        38,
        7,
        4,
        9,
        1
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Resource Utilization Monitoring for Raw Data Query Processing",
          "year": "2022-12",
          "abstract": "Scientific experiments, simulations, and modern applications generate large\namounts of data. Data is stored in raw format to avoid the high loading time of\ntraditional database management systems. Researchers have proposed many\ntechniques to improve query execution time for raw data and reduce data loading\ntime for traditional systems. The core of all the proposed techniques is\nefficient utilization of resources by processing only required data or reducing\noperations on data. The processed data caching in the main memory or disk can\nresolve this issue and avoid repeated processing of data. However, limitations\nof resources like main memory space, storage IO speeds, and additional storage\nspace requirements on disk need to be considered to provide reliable and\nscalable solutions for cloud or in-house deployments. This paper presents\nimprovements to the raw data query processing framework by integrating a\nresource monitoring module. The experiments were performed using a scientific\ndataset known Sloan Digital Sky Survey (SDSS). Analysis of monitored resources\nrevealed that sampling queries had the lowest resource utilization. The\nPostgresRAW can answer simple 0-JOIN queries faster than PostgreSQL. While one\nor more JOIN complex queries need to be answered using PostgreSQL to reduce\nworkload execution time (WET). The results section discusses resource\nrequirements of simple, complex, and sampling type queries. The result analysis\nof query types and resource utilization patterns assisted in proposing Query\nComplexity Aware (QCA) and Resource Utilization Aware (RUA) data partitioning\ntechniques for raw engines and DBMS to reduce cost or data to result time.",
          "arxiv_id": "2212.10793v1"
        },
        {
          "title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows",
          "year": "2024-11",
          "abstract": "Real-world enterprise text-to-SQL workflows often involve complex cloud or\nlocal data across various database systems, multiple SQL queries in various\ndialects, and diverse operations from data transformation to analytics. We\nintroduce Spider 2.0, an evaluation framework comprising 632 real-world\ntext-to-SQL workflow problems derived from enterprise-level database use cases.\nThe databases in Spider 2.0 are sourced from real data applications, often\ncontaining over 1,000 columns and stored in local or cloud database systems\nsuch as BigQuery and Snowflake. We show that solving problems in Spider 2.0\nfrequently requires understanding and searching through database metadata,\ndialect documentation, and even project-level codebases. This challenge calls\nfor models to interact with complex SQL workflow environments, process\nextremely long contexts, perform intricate reasoning, and generate multiple SQL\nqueries with diverse operations, often exceeding 100 lines, which goes far\nbeyond traditional text-to-SQL challenges. Our evaluations indicate that based\non o1-preview, our code agent framework successfully solves only 21.3% of the\ntasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on\nSpider 2.0 show that while language models have demonstrated remarkable\nperformance in code generation -- especially in prior text-to-SQL benchmarks --\nthey require significant improvement in order to achieve adequate performance\nfor real-world enterprise usage. Progress on Spider 2.0 represents crucial\nsteps towards developing intelligent, autonomous, code agents for real-world\nenterprise settings. Our code, baseline models, and data are available at\nhttps://spider2-sql.github.io",
          "arxiv_id": "2411.07763v2"
        },
        {
          "title": "NL2KQL: From Natural Language to Kusto Query",
          "year": "2024-04",
          "abstract": "Data is growing rapidly in volume and complexity. Proficiency in database\nquery languages is pivotal for crafting effective queries. As coding assistants\nbecome more prevalent, there is significant opportunity to enhance database\nquery languages. The Kusto Query Language (KQL) is a widely used query language\nfor large semi-structured data such as logs, telemetries, and time-series for\nbig data analytics platforms. This paper introduces NL2KQL an innovative\nframework that uses large language models (LLMs) to convert natural language\nqueries (NLQs) to KQL queries. The proposed NL2KQL framework includes several\nkey components: Schema Refiner which narrows down the schema to its most\npertinent elements; the Few-shot Selector which dynamically selects relevant\nexamples from a few-shot dataset; and the Query Refiner which repairs syntactic\nand semantic errors in KQL queries. Additionally, this study outlines a method\nfor generating large datasets of synthetic NLQ-KQL pairs which are valid within\na specific database contexts. To validate NL2KQL's performance, we utilize an\narray of online (based on query execution) and offline (based on query parsing)\nmetrics. Through ablation studies, the significance of each framework component\nis examined, and the datasets used for benchmarking are made publicly\navailable. This work is the first of its kind and is compared with available\nbaselines to demonstrate its effectiveness.",
          "arxiv_id": "2404.02933v4"
        }
      ],
      "1": [
        {
          "title": "Differentially Private Streaming Data Release under Temporal Correlations via Post-processing",
          "year": "2023-06",
          "abstract": "The release of differentially private streaming data has been extensively\nstudied, yet striking a good balance between privacy and utility on temporally\ncorrelated data in the stream remains an open problem. Existing works focus on\nenhancing privacy when applying differential privacy to correlated data,\nhighlighting that differential privacy may suffer from additional privacy\nleakage under correlations; consequently, a small privacy budget has to be used\nwhich worsens the utility. In this work, we propose a post-processing framework\nto improve the utility of differential privacy data release under temporal\ncorrelations. We model the problem as a maximum posterior estimation given the\nreleased differentially private data and correlation model and transform it\ninto nonlinear constrained programming. Our experiments on synthetic datasets\nshow that the proposed approach significantly improves the utility and accuracy\nof differentially private data by nearly a hundred times in terms of mean\nsquare error when a strict privacy budget is given.",
          "arxiv_id": "2306.13293v2"
        },
        {
          "title": "GeoPointGAN: Synthetic Spatial Data with Local Label Differential Privacy",
          "year": "2022-05",
          "abstract": "Synthetic data generation is a fundamental task for many data management and\ndata science applications. Spatial data is of particular interest, and its\nsensitive nature often leads to privacy concerns. We introduce GeoPointGAN, a\nnovel GAN-based solution for generating synthetic spatial point datasets with\nhigh utility and strong individual level privacy guarantees. GeoPointGAN's\narchitecture includes a novel point transformation generator that learns to\nproject randomly generated point co-ordinates into meaningful synthetic\nco-ordinates that capture both microscopic (e.g., junctions, squares) and\nmacroscopic (e.g., parks, lakes) geographic features. We provide our privacy\nguarantees through label local differential privacy, which is more practical\nthan traditional local differential privacy. We seamlessly integrate this level\nof privacy into GeoPointGAN by augmenting the discriminator to the point level\nand implementing a randomized response-based mechanism that flips the labels\nassociated with the 'real' and 'fake' points used in training. Extensive\nexperiments show that GeoPointGAN significantly outperforms recent solutions,\nimproving by up to 10 times compared to the most competitive baseline. We also\nevaluate GeoPointGAN using range, hotspot, and facility location queries, which\nconfirm the practical effectiveness of GeoPointGAN for privacy-preserving\nquerying. The results illustrate that a strong level of privacy is achieved\nwith little-to-no adverse utility cost, which we explain through the\ngeneralization and regularization effects that are realized by flipping the\nlabels of the data during training.",
          "arxiv_id": "2205.08886v1"
        },
        {
          "title": "Infinite Stream Estimation under Personalized $w$-Event Privacy",
          "year": "2025-09",
          "abstract": "Streaming data collection is indispensable for stream data analysis, such as\nevent monitoring. However, publishing these data directly leads to privacy\nleaks. $w$-event privacy is a valuable tool to protect individual privacy\nwithin a given time window while maintaining high accuracy in data collection.\nMost existing $w$-event privacy studies on infinite data stream only focus on\nhomogeneous privacy requirements for all users. In this paper, we propose\npersonalized $w$-event privacy protection that allows different users to have\ndifferent privacy requirements in private data stream estimation. Specifically,\nwe design a mechanism that allows users to maintain constant privacy\nrequirements at each time slot, namely Personalized Window Size Mechanism\n(PWSM). Then, we propose two solutions to accurately estimate stream data\nstatistics while achieving $w$-event level $\\epsilon$ personalized differential\nprivacy ( ($w$, $\\epsilon$)-EPDP), namely Personalized Budget Distribution\n(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the\nsame privacy budget for the next time step as the amount consumed in the\nprevious release. PBA fully absorbs the privacy budget from the previous $k$\ntime slots, while also borrowing from the privacy budget of the next $k$ time\nslots, to increase the privacy budget for the current time slot. We prove that\nboth PBD and PBA outperform the state-of-the-art private stream estimation\nmethods while satisfying the privacy requirements of all users. We demonstrate\nthe efficiency and effectiveness of our PBD and PBA on both real and synthetic\ndata sets, compared with the recent uniformity $w$-event approaches, Budget\nDistribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error\nthan BD on average on real data sets. Besides, our PBA achieves 24.9% less\nerror than BA on average on synthetic data sets.",
          "arxiv_id": "2509.08387v1"
        }
      ],
      "2": [
        {
          "title": "Efficient Temporal Pattern Mining in Big Time Series Using Mutual Information -- Full Version",
          "year": "2020-10",
          "abstract": "Very large time series are increasingly available from an ever wider range of\nIoT-enabled sensors deployed in different environments. Significant insights\ncan be gained by mining temporal patterns from these time series. Unlike\ntraditional pattern mining, temporal pattern mining (TPM) adds event time\nintervals into extracted patterns, making them more expressive at the expense\nof increased mining time complexity. Existing TPM methods either cannot scale\nto large datasets, or work only on pre-processed temporal events rather than on\ntime series. This paper presents our Frequent Temporal Pattern Mining from Time\nSeries (FTPMf TS) approach which provides: (1) The end-to-end FTPMf TS process\ntaking time series as input and producing frequent temporal patterns as output.\n(2) The efficient Hierarchical Temporal Pattern Graph Mining (HTPGM) algorithm\nthat uses efficient data structures for fast support and confidence\ncomputation, and employs effective pruning techniques for significantly faster\nmining. (3) An approximate version of HTPGM that uses mutual information, a\nmeasure of data correlation known from information theory, to prune unpromising\ntime series from the search space. (4) An extensive experimental evaluation\nshowing that HTPGM outperforms the baselines in runtime and memory consumption,\nand can scale to big datasets. The approximate HTPGM is up to two orders of\nmagnitude faster and less memory consuming than the baselines, while retaining\nhigh accuracy.",
          "arxiv_id": "2010.03653v8"
        },
        {
          "title": "Top-k contrast order-preserving pattern mining",
          "year": "2023-10",
          "abstract": "Recently, order-preserving pattern (OPP) mining, a new sequential pattern\nmining method, has been proposed to mine frequent relative orders in a time\nseries. Although frequent relative orders can be used as features to classify a\ntime series, the mined patterns do not reflect the differences between two\nclasses of time series well. To effectively discover the differences between\ntime series, this paper addresses the top-k contrast OPP (COPP) mining and\nproposes a COPP-Miner algorithm to discover the top-k contrast patterns as\nfeatures for time series classification, avoiding the problem of improper\nparameter setting. COPP-Miner is composed of three parts: extreme point\nextraction to reduce the length of the original time series, forward mining,\nand reverse mining to discover COPPs. Forward mining contains three steps:\ngroup pattern fusion strategy to generate candidate patterns, the support rate\ncalculation method to efficiently calculate the support of a pattern, and two\npruning strategies to further prune candidate patterns. Reverse mining uses one\npruning strategy to prune candidate patterns and consists of applying the same\nprocess as forward mining. Experimental results validate the efficiency of the\nproposed algorithm and show that top-k COPPs can be used as features to obtain\na better classification performance.",
          "arxiv_id": "2310.02612v2"
        },
        {
          "title": "OPP-Miner: Order-preserving sequential pattern mining",
          "year": "2022-01",
          "abstract": "A time series is a collection of measurements in chronological order.\nDiscovering patterns from time series is useful in many domains, such as stock\nanalysis, disease detection, and weather forecast. To discover patterns,\nexisting methods often convert time series data into another form, such as\nnominal/symbolic format, to reduce dimensionality, which inevitably deviates\nthe data values. Moreover, existing methods mainly neglect the order\nrelationships between time series values. To tackle these issues, inspired by\norder-preserving matching, this paper proposes an Order-Preserving sequential\nPattern (OPP) mining method, which represents patterns based on the order\nrelationships of the time series data. An inherent advantage of such\nrepresentation is that the trend of a time series can be represented by the\nrelative order of the values underneath the time series data. To obtain\nfrequent trends in time series, we propose the OPP-Miner algorithm to mine\npatterns with the same trend (sub-sequences with the same relative order).\nOPP-Miner employs the filtration and verification strategies to calculate the\nsupport and uses pattern fusion strategy to generate candidate patterns. To\ncompress the result set, we also study finding the maximal OPPs. Experiments\nvalidate that OPP-Miner is not only efficient and scalable but can also\ndiscover similar sub-sequences in time series. In addition, case studies show\nthat our algorithms have high utility in analyzing the COVID-19 epidemic by\nidentifying critical trends and improve the clustering performance.",
          "arxiv_id": "2202.03140v2"
        }
      ],
      "3": [
        {
          "title": "An Open-Source Integration of Process Mining Features into the Camunda Workflow Engine: Data Extraction and Challenges",
          "year": "2020-09",
          "abstract": "Process mining provides techniques to improve the performance and compliance\nof operational processes. Although sometimes the term \"workflow mining\" is\nused, the application in the context of Workflow Management (WFM) and Business\nProcess Management (BPM) systems is limited. The main reason is that WFM/BPM\nsystems control the process, leaving less room for flexibility and the\ncorresponding deviations. However, as this paper shows, it is easy to extract\nevent data from systems like Camunda, one of the leading open-source WFM/BPM\nsystems. Moreover, although the respective process engines control the process\nflow, process mining is still able to provide valuable insights, such as the\nanalysis of the performance of the paths and the mining of the decision rules.\nThis demo paper presents a process mining connector to Camunda that extracts\nevent logs and process models, allowing for the application of existing process\nmining tools. We also analyzed the added value of different process mining\ntechniques in the context of Camunda. We discuss a subset of process mining\ntechniques that nicely complements the process intelligence capabilities of\nCamunda. Through this demo paper, we hope to boost the use of process mining\namong Camunda users.",
          "arxiv_id": "2009.06209v1"
        },
        {
          "title": "A Python Tool for Object-Centric Process Mining Comparison",
          "year": "2022-02",
          "abstract": "Object-centric process mining provides a more holistic view of processes\nwhere we analyze processes with multiple case notions. However, most\nobject-centric process mining techniques consider the whole event log rather\nthan the comparison of existing behaviors in the log. In this paper, we\nintroduce a stand-alone object-centric process cube tool built on the PM4PY-MDL\nprocess mining framework. Our infrastructure uses both object and event\nattributes to build the process cube which leads to different types of\nmaterialization. Furthermore, our tool is equipped with the state of the art\nobject-centric process mining techniques. Through our tool the user can\nvisualize the extracted object-centric event log from process cube operations,\nexport the object-centric event log, discover the state-of-the-art\nobject-centric process model for the extracted log, and compare the process\nmodels side-by-side.",
          "arxiv_id": "2202.05709v1"
        },
        {
          "title": "Extended Event Log: Towards a Unified Standard for Process Mining",
          "year": "2024-12",
          "abstract": "Process mining has grown popular today given their ability to provide\nmanagers with insights into the actual business process as executed by\nemployees. Process mining depends on event logs found in process aware\ninformation systems to model business processes. This has raised the need to\ndevelop event log standards given that event logs are the entry point to any\nprocess mining project. One of the main challenges of event logs and process\nmining in general as was mentioned by the IEEE task force on process mining\ndeals with the finding, merging and cleaning event data.This resulted in having\nmultiple event log standards with different features. This paper attempts to\npropose a new unified standard for event logs that enriches the results of\nprocess mining without the need to tailor event logs for each process mining\nproject.",
          "arxiv_id": "2412.18012v1"
        }
      ],
      "4": [
        {
          "title": "Novel Architecture to Create and Maintain Personal Blockchains",
          "year": "2022-12",
          "abstract": "Blockchain has been touted as a revolutionary technology. However, despite\nthe excitement, blockchain has not been adopted in many fields. Many are\nhesitant to adopt blockchain technology due to privacy concerns, barriers to\nuse, or lack of practical use cases. In this work, we outline a potential\nblockchain use case for tracking financial transactions across multiple\nfinancial institutions. We show the downsides of traditional centralized\napproaches and that blockchain approaches fail to give all the privacy and\naccessibility required for this use case. Thus we propose a novel blockchain\narchitecture to support our use case. This novel architecture combines the ease\nof use of public blockchains with the privacy of private blockchains by\nallowing users to create personal blockchains. We believe this novel personal\nblockchain architecture will lead to more blockchain adoption, particularly in\nuse cases handling private data.",
          "arxiv_id": "2212.14671v1"
        },
        {
          "title": "Efficient Forkless Blockchain Databases",
          "year": "2025-08",
          "abstract": "Operating nodes in an L1 blockchain remains costly despite recent advances in\nblockchain technology. One of the most resource-intensive components of a node\nis the blockchain database, also known as StateDB, that manages balances,\nnonce, code, and the persistent storage of accounts/smart contracts. Although\nthe blockchain industry has transitioned from forking to forkless chains due to\nimproved consensus protocols, forkless blockchains still rely on legacy forking\ndatabases that are suboptimal for their purposes. In this paper, we propose a\nforkless blockchain database, showing a 100x improvement in storage and a 10x\nimprovement in throughput compared to the geth-based Fantom Blockchain client.",
          "arxiv_id": "2508.20686v1"
        },
        {
          "title": "Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain Transactions",
          "year": "2020-01",
          "abstract": "The interoperability across multiple blockchains would play a critical role\nin future blockchain-based data management paradigm. Existing techniques either\nwork only for two blockchains or requires a centralized component to govern the\ncross-blockchain transaction execution, neither of which would meet the\nscalability requirement. This paper proposes a new distributed commit protocol,\nnamely \\textit{cross-blockchain transaction} (CBT), for conducting transactions\nacross an arbitrary number of blockchains without any centralized component.\nThe key idea of CBT is to extend the two-phase commit protocol with a heartbeat\nmechanism to ensure the liveness of CBT without introducing additional nodes or\nblockchains. We have implemented CBT and compared it to the state-of-the-art\nprotocols, demonstrating CBT's low overhead (3.6\\% between two blockchains,\nless than $1\\%$ among 32 or more blockchains) and high scalability (linear\nscalability on up to 64-blockchain transactions). In addition, we developed a\ngraphic user interface for users to virtually monitor the status of the\ncross-blockchain transactions.",
          "arxiv_id": "2001.01174v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:40:09Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}