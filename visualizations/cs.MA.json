{
  "topics": {
    "data": {
      "0": {
        "name": "0_LLM_AI_agent_LLMs",
        "keywords": [
          [
            "LLM",
            0.020466633857795846
          ],
          [
            "AI",
            0.018839019513838002
          ],
          [
            "agent",
            0.01878802920596838
          ],
          [
            "LLMs",
            0.01805613969045924
          ],
          [
            "agents",
            0.01775103468314888
          ],
          [
            "language",
            0.01629308720097204
          ],
          [
            "systems",
            0.0139132925071329
          ],
          [
            "models",
            0.013449511672275025
          ],
          [
            "multi",
            0.012545567349694708
          ],
          [
            "human",
            0.01202033861123828
          ]
        ],
        "count": 1011
      },
      "1": {
        "name": "1_robots_robot_MAPF_problem",
        "keywords": [
          [
            "robots",
            0.01756501680924708
          ],
          [
            "robot",
            0.016380413868509844
          ],
          [
            "MAPF",
            0.015604681928440554
          ],
          [
            "problem",
            0.01404034486586817
          ],
          [
            "Multi",
            0.013651333940728035
          ],
          [
            "multi",
            0.013475238831880334
          ],
          [
            "agents",
            0.012499595517323495
          ],
          [
            "planning",
            0.012482832231310758
          ],
          [
            "time",
            0.011566680384426136
          ],
          [
            "agent",
            0.01101645446566787
          ]
        ],
        "count": 839
      },
      "2": {
        "name": "2_agent_learning_MARL_multi agent",
        "keywords": [
          [
            "agent",
            0.028737629567999644
          ],
          [
            "learning",
            0.026361408092166593
          ],
          [
            "MARL",
            0.023169315894686144
          ],
          [
            "multi agent",
            0.02140938568674501
          ],
          [
            "multi",
            0.02063132103824784
          ],
          [
            "agents",
            0.02044869537430319
          ],
          [
            "Multi",
            0.017141014856419133
          ],
          [
            "reinforcement",
            0.01660024633826718
          ],
          [
            "reinforcement learning",
            0.016453650360090125
          ],
          [
            "Learning",
            0.016260402052585644
          ]
        ],
        "count": 640
      },
      "3": {
        "name": "3_distributed_consensus_network_algorithm",
        "keywords": [
          [
            "distributed",
            0.027743790040264393
          ],
          [
            "consensus",
            0.020492936206926885
          ],
          [
            "network",
            0.01914364616968043
          ],
          [
            "algorithm",
            0.01848049426517765
          ],
          [
            "convergence",
            0.016637556950965235
          ],
          [
            "data",
            0.015968400347549715
          ],
          [
            "communication",
            0.015880349933540504
          ],
          [
            "optimization",
            0.014420475113892429
          ],
          [
            "agents",
            0.014129833571754083
          ],
          [
            "local",
            0.012826826336117468
          ]
        ],
        "count": 421
      },
      "4": {
        "name": "4_traffic_vehicles_driving_vehicle",
        "keywords": [
          [
            "traffic",
            0.046785156811225005
          ],
          [
            "vehicles",
            0.02360524536564032
          ],
          [
            "driving",
            0.023131656715684946
          ],
          [
            "vehicle",
            0.017356075366555906
          ],
          [
            "control",
            0.015440378595623167
          ],
          [
            "autonomous",
            0.014098836116978512
          ],
          [
            "safety",
            0.01224713996591065
          ],
          [
            "Traffic",
            0.012074836509966037
          ],
          [
            "learning",
            0.011395086470643372
          ],
          [
            "scenarios",
            0.010638351546205222
          ]
        ],
        "count": 316
      },
      "5": {
        "name": "5_social_cooperation_agents_learning",
        "keywords": [
          [
            "social",
            0.023811608386231455
          ],
          [
            "cooperation",
            0.02196483652196638
          ],
          [
            "agents",
            0.02112435857247315
          ],
          [
            "learning",
            0.01912162111938306
          ],
          [
            "collective",
            0.015137666718425953
          ],
          [
            "agent",
            0.014615578367196068
          ],
          [
            "dynamics",
            0.011619657973218286
          ],
          [
            "behavior",
            0.011152336941556156
          ],
          [
            "behaviors",
            0.011145962354188962
          ],
          [
            "systems",
            0.011100004007123167
          ]
        ],
        "count": 258
      },
      "6": {
        "name": "6_games_equilibrium_Nash_game",
        "keywords": [
          [
            "games",
            0.046736728557727175
          ],
          [
            "equilibrium",
            0.02730844926135099
          ],
          [
            "Nash",
            0.026855960234329616
          ],
          [
            "game",
            0.021971570411367023
          ],
          [
            "Markov",
            0.021555499621199025
          ],
          [
            "learning",
            0.020185670155408265
          ],
          [
            "sum",
            0.018408825259176927
          ],
          [
            "algorithm",
            0.01721061463555335
          ],
          [
            "convergence",
            0.0164072869158975
          ],
          [
            "equilibria",
            0.016401026774621182
          ]
        ],
        "count": 195
      },
      "7": {
        "name": "7_model_COVID_simulation_models",
        "keywords": [
          [
            "model",
            0.027789196679167103
          ],
          [
            "COVID",
            0.02098566784594603
          ],
          [
            "simulation",
            0.019985089498930752
          ],
          [
            "models",
            0.018737969331696712
          ],
          [
            "population",
            0.01847111280928967
          ],
          [
            "pandemic",
            0.018032142826561504
          ],
          [
            "epidemic",
            0.017597613299174426
          ],
          [
            "disease",
            0.015623738643744658
          ],
          [
            "spread",
            0.015082381110542884
          ],
          [
            "agent",
            0.014602661500478694
          ]
        ],
        "count": 174
      },
      "8": {
        "name": "8_logic_model_checking_systems",
        "keywords": [
          [
            "logic",
            0.034845294840899745
          ],
          [
            "model",
            0.022785105563469493
          ],
          [
            "checking",
            0.022348136150000017
          ],
          [
            "systems",
            0.02174592132478738
          ],
          [
            "verification",
            0.02066824447420598
          ],
          [
            "epistemic",
            0.020127234329641933
          ],
          [
            "knowledge",
            0.01842088006205496
          ],
          [
            "Logic",
            0.01790954060734063
          ],
          [
            "agents",
            0.016219930806378357
          ],
          [
            "agent",
            0.0157782042036203
          ]
        ],
        "count": 144
      },
      "9": {
        "name": "9_matching_fairness_agents_problem",
        "keywords": [
          [
            "matching",
            0.026613852721898878
          ],
          [
            "fairness",
            0.0239375125257523
          ],
          [
            "agents",
            0.02270064656775199
          ],
          [
            "problem",
            0.021990492157195316
          ],
          [
            "envy",
            0.021688750432028305
          ],
          [
            "mechanism",
            0.019391227052339113
          ],
          [
            "allocation",
            0.01884920774656803
          ],
          [
            "preferences",
            0.018280437364764476
          ],
          [
            "mechanisms",
            0.01819066081210688
          ],
          [
            "stable",
            0.016960635298725133
          ]
        ],
        "count": 122
      },
      "10": {
        "name": "10_network_networks_resource_learning",
        "keywords": [
          [
            "network",
            0.02245274435411376
          ],
          [
            "networks",
            0.016792545155815387
          ],
          [
            "resource",
            0.015835093001719532
          ],
          [
            "learning",
            0.01558666489695353
          ],
          [
            "Learning",
            0.013790454542221916
          ],
          [
            "edge",
            0.01276527612018049
          ],
          [
            "wireless",
            0.012246289392158084
          ],
          [
            "multi",
            0.011706335287188827
          ],
          [
            "agent",
            0.011651949847635219
          ],
          [
            "access",
            0.011518597149102012
          ]
        ],
        "count": 119
      },
      "11": {
        "name": "11_energy_grid_power_electricity",
        "keywords": [
          [
            "energy",
            0.06684382485853885
          ],
          [
            "grid",
            0.022813744020346582
          ],
          [
            "power",
            0.021727181580232124
          ],
          [
            "electricity",
            0.019849904389176595
          ],
          [
            "renewable",
            0.017818670127384372
          ],
          [
            "demand",
            0.01724004625387984
          ],
          [
            "Energy",
            0.01480793323311661
          ],
          [
            "control",
            0.014681405362342876
          ],
          [
            "renewable energy",
            0.014253518919204514
          ],
          [
            "market",
            0.013890573596572042
          ]
        ],
        "count": 115
      },
      "12": {
        "name": "12_voting_voters_candidates_elections",
        "keywords": [
          [
            "voting",
            0.074386559530391
          ],
          [
            "voters",
            0.045836942535505396
          ],
          [
            "candidates",
            0.038171094348632036
          ],
          [
            "elections",
            0.03619922924807525
          ],
          [
            "election",
            0.03143426893209946
          ],
          [
            "rules",
            0.0256734805666345
          ],
          [
            "projects",
            0.02467987899767886
          ],
          [
            "voter",
            0.023759112799713804
          ],
          [
            "approval",
            0.023454121039704724
          ],
          [
            "vote",
            0.021081765844310218
          ]
        ],
        "count": 106
      },
      "13": {
        "name": "13_market_trading_financial_markets",
        "keywords": [
          [
            "market",
            0.06523394264905785
          ],
          [
            "trading",
            0.034120568209099394
          ],
          [
            "financial",
            0.026274479592800742
          ],
          [
            "markets",
            0.023243990436183387
          ],
          [
            "agent",
            0.022423752647851906
          ],
          [
            "economic",
            0.019683496898947505
          ],
          [
            "learning",
            0.016484710861139562
          ],
          [
            "strategies",
            0.01621267474771018
          ],
          [
            "agents",
            0.015094655406551843
          ],
          [
            "model",
            0.014101997893142863
          ]
        ],
        "count": 97
      },
      "14": {
        "name": "14_opinion_opinions_dynamics_social",
        "keywords": [
          [
            "opinion",
            0.07936890404834325
          ],
          [
            "opinions",
            0.04714674911153836
          ],
          [
            "dynamics",
            0.030078896250957915
          ],
          [
            "social",
            0.03004844632534891
          ],
          [
            "model",
            0.02942155924449778
          ],
          [
            "opinion dynamics",
            0.024769772372531054
          ],
          [
            "consensus",
            0.023831286620102633
          ],
          [
            "influence",
            0.023393945531688706
          ],
          [
            "polarization",
            0.022715552407908717
          ],
          [
            "agents",
            0.019751086383234667
          ]
        ],
        "count": 95
      },
      "15": {
        "name": "15_traffic_ride_demand_congestion",
        "keywords": [
          [
            "traffic",
            0.023373966912892338
          ],
          [
            "ride",
            0.02209694002265651
          ],
          [
            "demand",
            0.01980834081567358
          ],
          [
            "congestion",
            0.019347249756231533
          ],
          [
            "mobility",
            0.017030721532323413
          ],
          [
            "vehicle",
            0.016886526850226017
          ],
          [
            "drivers",
            0.016569905261999297
          ],
          [
            "time",
            0.015568626855774452
          ],
          [
            "travel",
            0.015520199353267322
          ],
          [
            "road",
            0.015418013812406242
          ]
        ],
        "count": 86
      }
    },
    "correlations": [
      [
        1.0,
        -0.665395781520626,
        -0.2991427532425881,
        -0.699851198118893,
        -0.7397063232757142,
        -0.35276015731551236,
        -0.688931981871022,
        -0.6737999444016196,
        -0.47668816136667386,
        -0.4548388906439039,
        -0.6085578058846273,
        -0.7454985102107783,
        -0.6978469711783272,
        -0.5081583651541158,
        -0.6983667210438242,
        -0.7562601554566832
      ],
      [
        -0.665395781520626,
        1.0,
        -0.6087639416659574,
        -0.6980742842991863,
        -0.7393617069280691,
        -0.6483677648650903,
        -0.7454448650211087,
        -0.735781588284613,
        -0.6476584971196835,
        -0.6745043251420402,
        -0.6576893922206302,
        -0.7314812697223736,
        -0.7360449706052139,
        -0.6719642955934269,
        -0.7301980250304404,
        -0.7601193737614049
      ],
      [
        -0.2991427532425881,
        -0.6087639416659574,
        1.0,
        -0.6668610355370276,
        -0.6817365698760076,
        -0.030636685225678917,
        -0.555964919585851,
        -0.6747013964558906,
        -0.4654514056121313,
        -0.4307928100814107,
        0.01065191519402251,
        -0.709155302664312,
        -0.6954709466592965,
        -0.3660901210682779,
        -0.6963792566784741,
        -0.7453557252048104
      ],
      [
        -0.699851198118893,
        -0.6980742842991863,
        -0.6668610355370276,
        1.0,
        -0.7302958935090231,
        -0.6822351313280364,
        -0.69423687446627,
        -0.7123471996728662,
        -0.6498355043718687,
        -0.6579390577624712,
        -0.5132581681441309,
        -0.723466978285609,
        -0.7082511769179202,
        -0.7026818871110587,
        -0.7240520546862312,
        -0.7588007318430756
      ],
      [
        -0.7397063232757142,
        -0.7393617069280691,
        -0.6817365698760076,
        -0.7302958935090231,
        1.0,
        -0.7095784509331566,
        -0.7361976989733356,
        -0.716039548509291,
        -0.7011130403664667,
        -0.7231430747656604,
        -0.6841850909261428,
        -0.7325808147534565,
        -0.7422220698417804,
        -0.7331641739380557,
        -0.7321285525747259,
        -0.59657359001816
      ],
      [
        -0.35276015731551236,
        -0.6483677648650903,
        -0.030636685225678917,
        -0.6822351313280364,
        -0.7095784509331566,
        1.0,
        -0.5761828926296242,
        -0.6824517540589105,
        -0.556891785360778,
        -0.20926604886354797,
        -0.31974300172141573,
        -0.7277535936546837,
        -0.6766799364770961,
        -0.48508182079898776,
        -0.6611708539300145,
        -0.7561069978436008
      ],
      [
        -0.688931981871022,
        -0.7454448650211087,
        -0.555964919585851,
        -0.69423687446627,
        -0.7361976989733356,
        -0.5761828926296242,
        1.0,
        -0.7278889423838065,
        -0.6515529015350461,
        -0.6364542361702801,
        -0.6046608324248455,
        -0.7485260578669131,
        -0.7066807532445998,
        -0.6414905870558851,
        -0.7166448708031157,
        -0.7563317349235372
      ],
      [
        -0.6737999444016196,
        -0.735781588284613,
        -0.6747013964558906,
        -0.7123471996728662,
        -0.716039548509291,
        -0.6824517540589105,
        -0.7278889423838065,
        1.0,
        -0.5643900691538579,
        -0.6863545422166202,
        -0.7085068645839825,
        -0.725454056964923,
        -0.7034149866212668,
        -0.5756569836245392,
        -0.4924619865485532,
        -0.7312130479136436
      ],
      [
        -0.47668816136667386,
        -0.6476584971196835,
        -0.4654514056121313,
        -0.6498355043718687,
        -0.7011130403664667,
        -0.556891785360778,
        -0.6515529015350461,
        -0.5643900691538579,
        1.0,
        -0.4271670322800125,
        -0.6112091376241782,
        -0.72579420646085,
        -0.5057475727218751,
        -0.6202449226130058,
        -0.5776741914559764,
        -0.7416464198160961
      ],
      [
        -0.4548388906439039,
        -0.6745043251420402,
        -0.4307928100814107,
        -0.6579390577624712,
        -0.7231430747656604,
        -0.20926604886354797,
        -0.6364542361702801,
        -0.6863545422166202,
        -0.4271670322800125,
        1.0,
        -0.6381236909569754,
        -0.7358914462618993,
        -0.3664070818752635,
        -0.6169565995878589,
        -0.679523870200173,
        -0.7294252126415264
      ],
      [
        -0.6085578058846273,
        -0.6576893922206302,
        0.01065191519402251,
        -0.5132581681441309,
        -0.6841850909261428,
        -0.31974300172141573,
        -0.6046608324248455,
        -0.7085068645839825,
        -0.6112091376241782,
        -0.6381236909569754,
        1.0,
        -0.7002854527243864,
        -0.7297823371130949,
        -0.5028524281591557,
        -0.7164606087151832,
        -0.7406635048993291
      ],
      [
        -0.7454985102107783,
        -0.7314812697223736,
        -0.709155302664312,
        -0.723466978285609,
        -0.7325808147534565,
        -0.7277535936546837,
        -0.7485260578669131,
        -0.725454056964923,
        -0.72579420646085,
        -0.7358914462618993,
        -0.7002854527243864,
        1.0,
        -0.7496489168465553,
        -0.6993496254440538,
        -0.7405705855897436,
        -0.7401428063733774
      ],
      [
        -0.6978469711783272,
        -0.7360449706052139,
        -0.6954709466592965,
        -0.7082511769179202,
        -0.7422220698417804,
        -0.6766799364770961,
        -0.7066807532445998,
        -0.7034149866212668,
        -0.5057475727218751,
        -0.3664070818752635,
        -0.7297823371130949,
        -0.7496489168465553,
        1.0,
        -0.7105443493351637,
        -0.6823675612821423,
        -0.7550812199356841
      ],
      [
        -0.5081583651541158,
        -0.6719642955934269,
        -0.3660901210682779,
        -0.7026818871110587,
        -0.7331641739380557,
        -0.48508182079898776,
        -0.6414905870558851,
        -0.5756569836245392,
        -0.6202449226130058,
        -0.6169565995878589,
        -0.5028524281591557,
        -0.6993496254440538,
        -0.7105443493351637,
        1.0,
        -0.6990769588208863,
        -0.7445733716982665
      ],
      [
        -0.6983667210438242,
        -0.7301980250304404,
        -0.6963792566784741,
        -0.7240520546862312,
        -0.7321285525747259,
        -0.6611708539300145,
        -0.7166448708031157,
        -0.4924619865485532,
        -0.5776741914559764,
        -0.679523870200173,
        -0.7164606087151832,
        -0.7405705855897436,
        -0.6823675612821423,
        -0.6990769588208863,
        1.0,
        -0.7523933125009359
      ],
      [
        -0.7562601554566832,
        -0.7601193737614049,
        -0.7453557252048104,
        -0.7588007318430756,
        -0.59657359001816,
        -0.7561069978436008,
        -0.7563317349235372,
        -0.7312130479136436,
        -0.7416464198160961,
        -0.7294252126415264,
        -0.7406635048993291,
        -0.7401428063733774,
        -0.7550812199356841,
        -0.7445733716982665,
        -0.7523933125009359,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        6,
        3,
        15,
        1,
        7,
        3,
        3,
        4,
        6,
        4,
        1,
        4,
        2,
        2,
        0,
        0
      ],
      "2020-02": [
        7,
        2,
        21,
        7,
        9,
        2,
        8,
        4,
        5,
        2,
        3,
        3,
        1,
        1,
        2,
        0
      ],
      "2020-03": [
        11,
        3,
        24,
        7,
        7,
        2,
        7,
        4,
        4,
        3,
        4,
        4,
        1,
        4,
        3,
        0
      ],
      "2020-04": [
        12,
        4,
        19,
        1,
        5,
        5,
        2,
        11,
        2,
        3,
        1,
        2,
        1,
        0,
        6,
        0
      ],
      "2020-05": [
        9,
        5,
        11,
        1,
        6,
        0,
        11,
        5,
        1,
        7,
        0,
        0,
        1,
        4,
        3,
        0
      ],
      "2020-06": [
        14,
        3,
        38,
        3,
        2,
        3,
        5,
        12,
        5,
        4,
        0,
        3,
        1,
        2,
        3,
        0
      ],
      "2020-07": [
        9,
        2,
        17,
        2,
        4,
        6,
        5,
        6,
        2,
        2,
        2,
        5,
        3,
        4,
        2,
        0
      ],
      "2020-08": [
        6,
        2,
        14,
        5,
        3,
        1,
        8,
        8,
        5,
        5,
        2,
        2,
        1,
        2,
        2,
        2
      ],
      "2020-09": [
        6,
        6,
        20,
        5,
        6,
        2,
        4,
        11,
        3,
        6,
        0,
        8,
        3,
        6,
        2,
        1
      ],
      "2020-10": [
        15,
        2,
        27,
        2,
        7,
        6,
        4,
        4,
        9,
        5,
        0,
        3,
        1,
        4,
        0,
        1
      ],
      "2020-11": [
        10,
        7,
        19,
        2,
        4,
        4,
        4,
        9,
        3,
        5,
        0,
        1,
        0,
        2,
        1,
        0
      ],
      "2020-12": [
        8,
        7,
        22,
        3,
        5,
        4,
        5,
        9,
        4,
        5,
        1,
        2,
        2,
        2,
        1,
        0
      ],
      "2021-01": [
        11,
        4,
        9,
        4,
        3,
        6,
        5,
        7,
        2,
        2,
        3,
        2,
        3,
        1,
        1,
        0
      ],
      "2021-02": [
        8,
        4,
        26,
        2,
        5,
        6,
        11,
        9,
        6,
        6,
        4,
        1,
        1,
        4,
        1,
        0
      ],
      "2021-03": [
        9,
        4,
        30,
        3,
        6,
        6,
        7,
        9,
        7,
        3,
        2,
        6,
        2,
        4,
        2,
        0
      ],
      "2021-04": [
        12,
        6,
        17,
        4,
        2,
        2,
        3,
        5,
        5,
        3,
        2,
        1,
        3,
        1,
        0,
        2
      ],
      "2021-05": [
        6,
        4,
        27,
        6,
        6,
        4,
        6,
        9,
        6,
        3,
        4,
        1,
        4,
        1,
        1,
        1
      ],
      "2021-06": [
        4,
        5,
        28,
        3,
        2,
        7,
        14,
        10,
        7,
        10,
        0,
        2,
        2,
        3,
        2,
        1
      ],
      "2021-07": [
        5,
        1,
        21,
        2,
        4,
        3,
        5,
        6,
        3,
        0,
        1,
        3,
        3,
        2,
        5,
        0
      ],
      "2021-08": [
        9,
        3,
        14,
        0,
        2,
        2,
        3,
        4,
        3,
        5,
        1,
        5,
        2,
        4,
        2,
        0
      ],
      "2021-09": [
        5,
        6,
        28,
        3,
        10,
        1,
        4,
        6,
        5,
        4,
        0,
        1,
        1,
        2,
        2,
        1
      ],
      "2021-10": [
        9,
        3,
        36,
        2,
        5,
        4,
        10,
        7,
        4,
        4,
        0,
        4,
        0,
        5,
        0,
        0
      ],
      "2021-11": [
        6,
        3,
        22,
        3,
        2,
        1,
        10,
        3,
        5,
        3,
        0,
        3,
        3,
        0,
        1,
        0
      ],
      "2021-12": [
        5,
        3,
        28,
        3,
        10,
        7,
        3,
        4,
        3,
        5,
        0,
        1,
        0,
        3,
        2,
        1
      ],
      "2022-01": [
        8,
        2,
        26,
        3,
        3,
        4,
        9,
        2,
        7,
        3,
        2,
        2,
        4,
        2,
        2,
        1
      ],
      "2022-02": [
        14,
        7,
        22,
        8,
        5,
        2,
        4,
        6,
        4,
        3,
        1,
        3,
        2,
        4,
        3,
        0
      ],
      "2022-03": [
        14,
        4,
        37,
        4,
        2,
        4,
        4,
        6,
        5,
        7,
        1,
        3,
        1,
        1,
        4,
        0
      ],
      "2022-04": [
        8,
        2,
        18,
        2,
        7,
        3,
        4,
        5,
        5,
        3,
        1,
        0,
        2,
        2,
        3,
        0
      ],
      "2022-05": [
        9,
        5,
        23,
        5,
        3,
        3,
        4,
        10,
        8,
        4,
        2,
        4,
        3,
        3,
        1,
        2
      ],
      "2022-06": [
        12,
        6,
        23,
        4,
        6,
        4,
        13,
        10,
        7,
        0,
        1,
        2,
        4,
        3,
        6,
        0
      ],
      "2022-07": [
        8,
        3,
        19,
        5,
        10,
        3,
        7,
        4,
        9,
        5,
        0,
        1,
        4,
        2,
        0,
        1
      ],
      "2022-08": [
        5,
        4,
        25,
        2,
        3,
        6,
        3,
        3,
        6,
        7,
        0,
        4,
        0,
        4,
        2,
        1
      ],
      "2022-09": [
        8,
        6,
        39,
        2,
        1,
        2,
        6,
        3,
        6,
        1,
        1,
        5,
        2,
        2,
        0,
        0
      ],
      "2022-10": [
        8,
        7,
        34,
        3,
        9,
        5,
        6,
        7,
        10,
        2,
        1,
        5,
        3,
        5,
        3,
        1
      ],
      "2022-11": [
        11,
        3,
        33,
        3,
        5,
        3,
        6,
        3,
        6,
        2,
        3,
        5,
        2,
        1,
        5,
        0
      ],
      "2022-12": [
        10,
        2,
        27,
        1,
        2,
        6,
        7,
        6,
        3,
        4,
        1,
        3,
        1,
        1,
        4,
        0
      ],
      "2023-01": [
        9,
        4,
        24,
        6,
        3,
        4,
        11,
        2,
        4,
        9,
        3,
        2,
        1,
        0,
        0,
        0
      ],
      "2023-02": [
        11,
        2,
        43,
        2,
        6,
        4,
        8,
        7,
        5,
        5,
        1,
        3,
        1,
        3,
        1,
        1
      ],
      "2023-03": [
        18,
        10,
        34,
        5,
        7,
        3,
        5,
        5,
        7,
        10,
        0,
        2,
        2,
        5,
        1,
        0
      ],
      "2023-04": [
        17,
        5,
        23,
        2,
        4,
        6,
        4,
        6,
        2,
        6,
        0,
        1,
        0,
        0,
        2,
        0
      ],
      "2023-05": [
        20,
        6,
        43,
        6,
        8,
        8,
        8,
        5,
        2,
        8,
        2,
        6,
        4,
        1,
        6,
        0
      ],
      "2023-06": [
        8,
        5,
        37,
        2,
        6,
        2,
        8,
        4,
        3,
        3,
        1,
        2,
        0,
        5,
        4,
        0
      ],
      "2023-07": [
        13,
        6,
        27,
        2,
        5,
        5,
        6,
        7,
        12,
        3,
        1,
        3,
        5,
        3,
        4,
        0
      ],
      "2023-08": [
        14,
        4,
        24,
        3,
        5,
        5,
        4,
        6,
        5,
        3,
        1,
        9,
        1,
        1,
        1,
        0
      ],
      "2023-09": [
        13,
        5,
        24,
        4,
        3,
        0,
        4,
        3,
        5,
        2,
        1,
        4,
        1,
        1,
        4,
        0
      ],
      "2023-10": [
        26,
        9,
        30,
        7,
        7,
        4,
        9,
        10,
        15,
        3,
        1,
        1,
        3,
        1,
        2,
        1
      ],
      "2023-11": [
        13,
        3,
        32,
        1,
        7,
        4,
        6,
        5,
        5,
        4,
        3,
        4,
        0,
        2,
        2,
        0
      ],
      "2023-12": [
        18,
        5,
        42,
        4,
        6,
        3,
        4,
        6,
        5,
        4,
        2,
        3,
        3,
        2,
        3,
        0
      ],
      "2024-01": [
        17,
        4,
        33,
        4,
        3,
        3,
        6,
        13,
        8,
        2,
        1,
        8,
        4,
        4,
        0,
        2
      ],
      "2024-02": [
        29,
        5,
        30,
        5,
        5,
        1,
        13,
        9,
        8,
        6,
        1,
        2,
        2,
        2,
        5,
        0
      ],
      "2024-03": [
        35,
        8,
        35,
        4,
        10,
        4,
        9,
        4,
        7,
        5,
        0,
        4,
        1,
        2,
        4,
        2
      ],
      "2024-04": [
        25,
        2,
        32,
        3,
        9,
        5,
        2,
        7,
        6,
        0,
        5,
        5,
        1,
        1,
        3,
        0
      ],
      "2024-05": [
        27,
        5,
        30,
        0,
        8,
        4,
        11,
        8,
        8,
        3,
        0,
        4,
        4,
        5,
        0,
        0
      ],
      "2024-06": [
        34,
        0,
        33,
        0,
        7,
        2,
        8,
        6,
        5,
        7,
        2,
        3,
        1,
        3,
        3,
        0
      ],
      "2024-07": [
        23,
        2,
        22,
        5,
        4,
        5,
        6,
        3,
        3,
        2,
        1,
        2,
        3,
        1,
        2,
        0
      ],
      "2024-08": [
        19,
        4,
        27,
        3,
        5,
        6,
        8,
        7,
        4,
        0,
        0,
        5,
        2,
        1,
        4,
        0
      ],
      "2024-09": [
        32,
        7,
        29,
        3,
        13,
        6,
        2,
        5,
        5,
        4,
        1,
        2,
        1,
        3,
        2,
        0
      ],
      "2024-10": [
        54,
        10,
        41,
        4,
        5,
        2,
        10,
        6,
        3,
        4,
        1,
        4,
        4,
        3,
        3,
        0
      ],
      "2024-11": [
        36,
        7,
        30,
        0,
        6,
        2,
        4,
        4,
        4,
        5,
        1,
        8,
        1,
        4,
        4,
        1
      ],
      "2024-12": [
        32,
        8,
        33,
        1,
        6,
        5,
        8,
        2,
        7,
        2,
        1,
        1,
        1,
        3,
        3,
        0
      ],
      "2025-01": [
        45,
        5,
        43,
        3,
        5,
        2,
        9,
        5,
        4,
        1,
        3,
        1,
        2,
        3,
        1,
        0
      ],
      "2025-02": [
        69,
        4,
        57,
        4,
        11,
        0,
        10,
        8,
        5,
        7,
        0,
        3,
        4,
        5,
        4,
        0
      ],
      "2025-03": [
        75,
        7,
        44,
        3,
        11,
        4,
        10,
        7,
        9,
        3,
        0,
        6,
        0,
        3,
        0,
        1
      ],
      "2025-04": [
        52,
        4,
        33,
        3,
        6,
        4,
        4,
        7,
        9,
        3,
        0,
        3,
        1,
        1,
        6,
        0
      ],
      "2025-05": [
        115,
        5,
        60,
        3,
        12,
        2,
        10,
        5,
        7,
        3,
        0,
        8,
        1,
        5,
        3,
        0
      ],
      "2025-06": [
        73,
        7,
        37,
        1,
        9,
        2,
        7,
        9,
        7,
        1,
        1,
        6,
        3,
        3,
        4,
        1
      ],
      "2025-07": [
        97,
        4,
        37,
        2,
        6,
        5,
        6,
        13,
        5,
        5,
        6,
        5,
        0,
        5,
        4,
        0
      ],
      "2025-08": [
        112,
        5,
        39,
        2,
        3,
        6,
        10,
        11,
        8,
        7,
        1,
        8,
        2,
        4,
        3,
        0
      ],
      "2025-09": [
        42,
        4,
        18,
        2,
        3,
        2,
        5,
        3,
        2,
        4,
        1,
        1,
        2,
        4,
        3,
        0
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures",
          "year": "2023-10",
          "abstract": "Large language models (LLMs) have revolutionized the field of artificial\nintelligence, endowing it with sophisticated language understanding and\ngeneration capabilities. However, when faced with more complex and\ninterconnected tasks that demand a profound and iterative thought process, LLMs\nreveal their inherent limitations. Autonomous LLM-powered multi-agent systems\nrepresent a strategic response to these challenges. Such systems strive for\nautonomously tackling user-prompted goals by decomposing them into manageable\ntasks and orchestrating their execution and result synthesis through a\ncollective of specialized intelligent agents. Equipped with LLM-powered\nreasoning capabilities, these agents harness the cognitive synergy of\ncollaborating with their peers, enhanced by leveraging contextual resources\nsuch as tools and datasets. While these architectures hold promising potential\nin amplifying AI capabilities, striking the right balance between different\nlevels of autonomy and alignment remains the crucial challenge for their\neffective operation. This paper proposes a comprehensive multi-dimensional\ntaxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems\nbalance the dynamic interplay between autonomy and alignment across various\naspects inherent to architectural viewpoints such as goal-driven task\nmanagement, agent composition, multi-agent collaboration, and context\ninteraction. It also includes a domain-ontology model specifying fundamental\narchitectural concepts. Our taxonomy aims to empower researchers, engineers,\nand AI practitioners to systematically analyze the architectural dynamics and\nbalancing strategies employed by these increasingly prevalent AI systems. The\nexploratory taxonomic classification of selected representative LLM-powered\nmulti-agent systems illustrates its practical utility and reveals potential for\nfuture research and development.",
          "arxiv_id": "2310.03659v1"
        },
        {
          "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges",
          "year": "2024-01",
          "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide\narray of tasks. Due to the impressive planning and reasoning abilities of LLMs,\nthey have been used as autonomous agents to do many tasks automatically.\nRecently, based on the development of using one LLM as a single planning or\ndecision-making agent, LLM-based multi-agent systems have achieved considerable\nprogress in complex problem-solving and world simulation. To provide the\ncommunity with an overview of this dynamic field, we present this survey to\noffer an in-depth discussion on the essential aspects of multi-agent systems\nbased on LLMs, as well as the challenges. Our goal is for readers to gain\nsubstantial insights on the following questions: What domains and environments\ndo LLM-based multi-agents simulate? How are these agents profiled and how do\nthey communicate? What mechanisms contribute to the growth of agents'\ncapacities? For those interested in delving into this field of study, we also\nsummarize the commonly used datasets or benchmarks for them to have convenient\naccess. To keep researchers updated on the latest studies, we maintain an\nopen-source GitHub repository, dedicated to outlining the research on LLM-based\nmulti-agent systems.",
          "arxiv_id": "2402.01680v2"
        },
        {
          "title": "Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations",
          "year": "2024-03",
          "abstract": "This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.",
          "arxiv_id": "2403.07769v3"
        }
      ],
      "1": [
        {
          "title": "Multi-Agent Path Finding with Real Robot Dynamics and Interdependent Tasks for Automated Warehouses",
          "year": "2024-08",
          "abstract": "Multi-Agent Path Finding (MAPF) is an important optimization problem\nunderlying the deployment of robots in automated warehouses and factories.\nDespite the large body of work on this topic, most approaches make heavy\nsimplifications, both on the environment and the agents, which make the\nresulting algorithms impractical for real-life scenarios. In this paper, we\nconsider a realistic problem of online order delivery in a warehouse, where a\nfleet of robots bring the products belonging to each order from shelves to\nworkstations. This creates a stream of inter-dependent pickup and delivery\ntasks and the associated MAPF problem consists of computing realistic\ncollision-free robot trajectories fulfilling these tasks. To solve this MAPF\nproblem, we propose an extension of the standard Prioritized Planning algorithm\nto deal with the inter-dependent tasks (Interleaved Prioritized Planning) and a\nnovel Via-Point Star (VP*) algorithm to compute an optimal dynamics-compliant\nrobot trajectory to visit a sequence of goal locations while avoiding moving\nobstacles. We prove the completeness of our approach and evaluate it in\nsimulation as well as in a real warehouse.",
          "arxiv_id": "2408.14527v1"
        },
        {
          "title": "LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control",
          "year": "2025-07",
          "abstract": "We propose a multi-robot control paradigm to solve point-to-point navigation\ntasks for a team of holonomic robots with access to the full environment\ninformation. The framework invokes two processes asynchronously at high\nfrequency: (i) a centralized, discrete, and full-horizon planner for computing\ncollision- and deadlock-free paths rapidly, leveraging recent advances in\nmulti-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal\ntrajectory controllers that ensure all robots independently follow their\nassigned paths reliably. This hierarchical shift in planning representation\nfrom (i) discrete and coupled to (ii) continuous and decoupled domains enables\nthe framework to maintain long-term scalable motion synthesis. As an\ninstantiation of this idea, we present LF, which combines a fast\nstate-of-the-art MAPF solver (LaCAM), and a robust feedback control stack\n(Freyja) for executing agile robot maneuvers. LF provides a robust and\nversatile mechanism for lifelong multi-robot navigation even under asynchronous\nand partial goal updates, and adapts to dynamic workspaces simply by quick\nreplanning. We present various multirotor and ground robot demonstrations,\nincluding the deployment of 15 real multirotors with random, consecutive target\nupdates while a person walks through the operational workspace.",
          "arxiv_id": "2507.11464v1"
        },
        {
          "title": "Graph-Based Multi-Robot Path Finding and Planning",
          "year": "2022-06",
          "abstract": "Purpose of Review\n  Planning collision-free paths for multiple robots is important for real-world\nmulti-robot systems and has been studied as an optimization problem on graphs,\ncalled Multi-Agent Path Finding (MAPF). This review surveys different\ncategories of classic and state-of-the-art MAPF algorithms and different\nresearch attempts to tackle the challenges of generalizing MAPF techniques to\nreal-world scenarios.\n  Recent Findings\n  Solving MAPF problems optimally is computationally challenging. Recent\nadvances have resulted in MAPF algorithms that can compute collision-free paths\nfor hundreds of robots and thousands of navigation tasks in seconds of runtime.\nMany variants of MAPF have been formalized to adapt MAPF techniques to\ndifferent real-world requirements, such as considerations of robot kinematics,\nonline optimization for real-time systems, and the integration of task\nassignment and path planning.\n  Summary\n  Algorithmic techniques for MAPF problems have addressed important aspects of\nseveral multi-robot applications, including automated warehouse fulfillment and\nsortation, automated train scheduling, and navigation of non-holonomic robots\nand quadcopters. This showcases their potential for real-world applications of\nlarge-scale multi-robot systems.",
          "arxiv_id": "2206.11319v1"
        }
      ],
      "2": [
        {
          "title": "ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization",
          "year": "2024-10",
          "abstract": "Offline reinforcement learning (RL) has garnered significant attention for\nits ability to learn effective policies from pre-collected datasets without the\nneed for further environmental interactions. While promising results have been\ndemonstrated in single-agent settings, offline multi-agent reinforcement\nlearning (MARL) presents additional challenges due to the large joint\nstate-action space and the complexity of multi-agent behaviors. A key issue in\noffline RL is the distributional shift, which arises when the target policy\nbeing optimized deviates from the behavior policy that generated the data. This\nproblem is exacerbated in MARL due to the interdependence between agents' local\npolicies and the expansive joint state-action space. Prior approaches have\nprimarily addressed this challenge by incorporating regularization in the space\nof either Q-functions or policies. In this work, we introduce a regularizer in\nthe space of stationary distributions to better handle distributional shift.\nOur algorithm, ComaDICE, offers a principled framework for offline cooperative\nMARL by incorporating stationary distribution regularization for the global\nlearning policy, complemented by a carefully structured multi-agent value\ndecomposition strategy to facilitate multi-agent training. Through extensive\nexperiments on the multi-agent MuJoCo and StarCraft II benchmarks, we\ndemonstrate that ComaDICE achieves superior performance compared to\nstate-of-the-art offline MARL methods across nearly all tasks.",
          "arxiv_id": "2410.01954v1"
        },
        {
          "title": "PAC: Assisted Value Factorisation with Counterfactual Predictions in Multi-Agent Reinforcement Learning",
          "year": "2022-06",
          "abstract": "Multi-agent reinforcement learning (MARL) has witnessed significant progress\nwith the development of value function factorization methods. It allows\noptimizing a joint action-value function through the maximization of factorized\nper-agent utilities due to monotonicity. In this paper, we show that in\npartially observable MARL problems, an agent's ordering over its own actions\ncould impose concurrent constraints (across different states) on the\nrepresentable function class, causing significant estimation error during\ntraining. We tackle this limitation and propose PAC, a new framework leveraging\nAssistive information generated from Counterfactual Predictions of optimal\njoint action selection, which enable explicit assistance to value function\nfactorization through a novel counterfactual loss. A variational\ninference-based information encoding method is developed to collect and encode\nthe counterfactual predictions from an estimated baseline. To enable\ndecentralized execution, we also derive factorized per-agent policies inspired\nby a maximum-entropy MARL framework. We evaluate the proposed PAC on\nmulti-agent predator-prey and a set of StarCraft II micromanagement tasks.\nEmpirical results demonstrate improved results of PAC over state-of-the-art\nvalue-based and policy-based multi-agent reinforcement learning algorithms on\nall benchmarks.",
          "arxiv_id": "2206.11420v3"
        },
        {
          "title": "Consensus Learning for Cooperative Multi-Agent Reinforcement Learning",
          "year": "2022-06",
          "abstract": "Almost all multi-agent reinforcement learning algorithms without\ncommunication follow the principle of centralized training with decentralized\nexecution. During centralized training, agents can be guided by the same\nsignals, such as the global state. During decentralized execution, however,\nagents lack the shared signal. Inspired by viewpoint invariance and contrastive\nlearning, we propose consensus learning for cooperative multi-agent\nreinforcement learning in this paper. Although based on local observations,\ndifferent agents can infer the same consensus in discrete space. During\ndecentralized execution, we feed the inferred consensus as an explicit input to\nthe network of agents, thereby developing their spirit of cooperation. Our\nproposed method can be extended to various multi-agent reinforcement learning\nalgorithms with small model changes. Moreover, we carry out them on some fully\ncooperative tasks and get convincing results.",
          "arxiv_id": "2206.02583v3"
        }
      ],
      "3": [
        {
          "title": "The fastest linearly converging discrete-time average consensus using buffered information",
          "year": "2022-06",
          "abstract": "In this letter, we study the problem of accelerating reaching average\nconsensus over connected graphs in a discrete-time communication setting.\nLiterature has shown that consensus algorithms can be accelerated by increasing\nthe graph connectivity or optimizing the weights agents place on the\ninformation received from their neighbors. In this letter instead of altering\nthe communication graph, we investigate two methods that use buffered states to\naccelerate reaching average consensus over a given graph. In the first method,\nwe study how convergence rate of the well-known first-order Laplacian average\nconsensus algorithm changes with delayed feedback and obtain a sufficient\ncondition on the ranges of delay that leads to faster convergence. In the\nsecond proposed method, we show how average consensus problem can be cast as a\nconvex optimization problem and solved by first-order accelerated optimization\nalgorithms for strongly-convex cost functions. We construct the fastest\nconverging average consensus algorithm using the so-called Triple Momentum\noptimization algorithm. We demonstrate our results using an in-network linear\nregression problem, which is formulated as two average consensus problems.",
          "arxiv_id": "2206.09916v1"
        },
        {
          "title": "Secure Distributed Optimization Under Gradient Attacks",
          "year": "2022-10",
          "abstract": "In this paper, we study secure distributed optimization against arbitrary\ngradient attack in multi-agent networks. In distributed optimization, there is\nno central server to coordinate local updates, and each agent can only\ncommunicate with its neighbors on a predefined network. We consider the\nscenario where out of $n$ networked agents, a fixed but unknown fraction $\\rho$\nof the agents are under arbitrary gradient attack in that their stochastic\ngradient oracles return arbitrary information to derail the optimization\nprocess, and the goal is to minimize the sum of local objective functions on\nunattacked agents. We propose a distributed stochastic gradient method that\ncombines local variance reduction and clipping (CLIP-VRG). We show that, in a\nconnected network, when unattacked local objective functions are convex and\nsmooth, share a common minimizer, and their sum is strongly convex, CLIP-VRG\nleads to almost sure convergence of the iterates to the exact sum cost\nminimizer at all agents. We quantify a tight upper bound of the fraction $\\rho$\nof attacked agents in terms of problem parameters such as the condition number\nof the associated sum cost that guarantee exact convergence of CLIP-VRG, and\ncharacterize its asymptotic convergence rate. Finally, we empirically\ndemonstrate the effectiveness of the proposed method under gradient attacks in\nboth synthetic dataset and image classification datasets.",
          "arxiv_id": "2210.15821v1"
        },
        {
          "title": "Distributed design of deterministic discrete-time privacy preserving average consensus for multi-agent systems through network augmentation",
          "year": "2021-12",
          "abstract": "Average consensus protocols emerge with a central role in distributed systems\nand decision-making such as distributed information fusion, distributed\noptimization, distributed estimation, and control. A key advantage of these\nprotocols is that agents exchange and reveal their state information only to\ntheir neighbors. Yet, it can raise privacy concerns in situations where the\nagents' states contain sensitive information. In this paper, we propose a novel\n(noiseless) privacy preserving distributed algorithms for multi-agent systems\nto reach an average consensus. The main idea of the algorithms is that each\nagent runs a (small) network with a crafted structure and dynamics to form a\nnetwork of networks (i.e., the connection between the newly created networks\nand their interconnections respecting the initial network connections).\nTogether with a re-weighting of the dynamic parameters dictating the\ninter-agent dynamics and the initial states, we show that it is possible to\nensure that the value of each node converges to the consensus value of the\noriginal network. Furthermore, we show that, under mild assumptions, it is\npossible to craft the dynamics such that the design can be achieved in a\ndistributed fashion. Finally, we illustrate the proposed algorithm with\nexamples.",
          "arxiv_id": "2112.09914v1"
        }
      ],
      "4": [
        {
          "title": "Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning",
          "year": "2025-04",
          "abstract": "Traffic congestion remains a significant challenge in modern urban networks.\nAutonomous driving technologies have emerged as a potential solution. Among\ntraffic control methods, reinforcement learning has shown superior performance\nover traffic signals in various scenarios. However, prior research has largely\nfocused on small-scale networks or isolated intersections, leaving large-scale\nmixed traffic control largely unexplored. This study presents the first attempt\nto use decentralized multi-agent reinforcement learning for large-scale mixed\ntraffic control in which some intersections are managed by traffic signals and\nothers by robot vehicles. Evaluating a real-world network in Colorado Springs,\nCO, USA with 14 intersections, we measure traffic efficiency via average\nwaiting time of vehicles at intersections and the number of vehicles reaching\ntheir destinations within a time window (i.e., throughput). At 80% RV\npenetration rate, our method reduces waiting time from 6.17s to 5.09s and\nincreases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500\nseconds, outperforming the baseline of fully signalized intersections. These\nfindings suggest that integrating reinforcement learning-based control\nlarge-scale traffic can improve overall efficiency and may inform future urban\nplanning strategies.",
          "arxiv_id": "2504.04691v2"
        },
        {
          "title": "Traffic-Aware Autonomous Driving with Differentiable Traffic Simulation",
          "year": "2022-10",
          "abstract": "While there have been advancements in autonomous driving control and traffic\nsimulation, there have been little to no works exploring their unification with\ndeep learning. Works in both areas seem to focus on entirely different\nexclusive problems, yet traffic and driving are inherently related in the real\nworld. In this paper, we present Traffic-Aware Autonomous Driving (TrAAD), a\ngeneralizable distillation-style method for traffic-informed imitation learning\nthat directly optimizes for faster traffic flow and lower energy consumption.\nTrAAD focuses on the supervision of speed control in imitation learning\nsystems, as most driving research focuses on perception and steering. Moreover,\nour method addresses the lack of co-simulation between traffic and driving\nsimulators and provides a basis for directly involving traffic simulation with\nautonomous driving in future work. Our results show that, with information from\ntraffic simulation involved in the supervision of imitation learning methods,\nan autonomous vehicle can learn how to accelerate in a fashion that is\nbeneficial for traffic flow and overall energy consumption for all nearby\nvehicles.",
          "arxiv_id": "2210.03772v5"
        },
        {
          "title": "Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections",
          "year": "2023-01",
          "abstract": "Intersections are essential road infrastructures for traffic in modern\nmetropolises. However, they can also be the bottleneck of traffic flows as a\nresult of traffic incidents or the absence of traffic coordination mechanisms\nsuch as traffic lights. Recently, various control and coordination mechanisms\nthat are beyond traditional control methods have been proposed to improve the\nefficiency of intersection traffic by leveraging the ability of autonomous\nvehicles. Amongst these methods, the control of foreseeable mixed traffic that\nconsists of human-driven vehicles (HVs) and robot vehicles (RVs) has emerged.\nWe propose a decentralized multi-agent reinforcement learning approach for the\ncontrol and coordination of mixed traffic by RVs at real-world, complex\nintersections -- an open challenge to date. We design comprehensive experiments\nto evaluate the effectiveness, robustness, generalizablility, and adaptability\nof our approach. In particular, our method can prevent congestion formation via\nmerely 5% RVs under a real-world traffic demand of 700 vehicles per hour. In\ncontrast, without RVs, congestion will form when the traffic demand reaches as\nlow as 200 vehicles per hour. Moreover, when the RV penetration rate exceeds\n60%, our method starts to outperform traffic signal control in terms of the\naverage waiting time of all vehicles. Our method is not only robust against\nblackout events, sudden RV percentage drops, and V2V communication error, but\nalso enjoys excellent generalizablility, evidenced by its successful deployment\nin five unseen intersections. Lastly, our method performs well under various\ntraffic rules, demonstrating its adaptability to diverse scenarios. Videos and\ncode of our work are available at\nhttps://sites.google.com/view/mixedtrafficcontrol",
          "arxiv_id": "2301.05294v4"
        }
      ],
      "5": [
        {
          "title": "Learning enables adaptation in cooperation for multi-player stochastic games",
          "year": "2020-07",
          "abstract": "Interactions among individuals in natural populations often occur in a\ndynamically changing environment. Understanding the role of environmental\nvariation in population dynamics has long been a central topic in theoretical\necology and population biology. However, the key question of how individuals,\nin the middle of challenging social dilemmas (e.g., the \"tragedy of the\ncommons\"), modulate their behaviors to adapt to the fluctuation of the\nenvironment has not yet been addressed satisfactorily. Utilizing evolutionary\ngame theory and stochastic games, we develop a game-theoretical framework that\nincorporates the adaptive mechanism of reinforcement learning to investigate\nwhether cooperative behaviors can evolve in the ever-changing group interaction\nenvironment. When the action choices of players are just slightly influenced by\npast reinforcements, we construct an analytical condition to determine whether\ncooperation can be favored over defection. Intuitively, this condition reveals\nwhy and how the environment can mediate cooperative dilemmas. Under our model\narchitecture, we also compare this learning mechanism with two non-learning\ndecision rules, and we find that learning significantly improves the propensity\nfor cooperation in weak social dilemmas, and, in sharp contrast, hinders\ncooperation in strong social dilemmas. Our results suggest that in complex\nsocial-ecological dilemmas, learning enables the adaptation of individuals to\nvarying environments.",
          "arxiv_id": "2007.14957v1"
        },
        {
          "title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents",
          "year": "2024-03",
          "abstract": "Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.",
          "arxiv_id": "2403.04202v7"
        },
        {
          "title": "Deconstructing Cooperation and Ostracism via Multi-Agent Reinforcement Learning",
          "year": "2023-10",
          "abstract": "Cooperation is challenging in biological systems, human societies, and\nmulti-agent systems in general. While a group can benefit when everyone\ncooperates, it is tempting for each agent to act selfishly instead. Prior human\nstudies show that people can overcome such social dilemmas while choosing\ninteraction partners, i.e., strategic network rewiring. However, little is\nknown about how agents, including humans, can learn about cooperation from\nstrategic rewiring and vice versa. Here, we perform multi-agent reinforcement\nlearning simulations in which two agents play the Prisoner's Dilemma game\niteratively. Each agent has two policies: one controls whether to cooperate or\ndefect; the other controls whether to rewire connections with another agent.\nThis setting enables us to disentangle complex causal dynamics between\ncooperation and network rewiring. We find that network rewiring facilitates\nmutual cooperation even when one agent always offers cooperation, which is\nvulnerable to free-riding. We then confirm that the network-rewiring effect is\nexerted through agents' learning of ostracism, that is, connecting to\ncooperators and disconnecting from defectors. However, we also find that\nostracism alone is not sufficient to make cooperation emerge. Instead,\nostracism emerges from the learning of cooperation, and existing cooperation is\nsubsequently reinforced due to the presence of ostracism. Our findings provide\ninsights into the conditions and mechanisms necessary for the emergence of\ncooperation with network rewiring.",
          "arxiv_id": "2310.04623v1"
        }
      ],
      "6": [
        {
          "title": "Robustness and sample complexity of model-based MARL for general-sum Markov games",
          "year": "2021-10",
          "abstract": "Multi-agent reinforcement learning (MARL) is often modeled using the\nframework of Markov games (also called stochastic games or dynamic games). Most\nof the existing literature on MARL concentrates on zero-sum Markov games but is\nnot applicable to general-sum Markov games. It is known that the best-response\ndynamics in general-sum Markov games are not a contraction. Therefore,\ndifferent equilibria in general-sum Markov games can have different values.\nMoreover, the Q-function is not sufficient to completely characterize the\nequilibrium. Given these challenges, model based learning is an attractive\napproach for MARL in general-sum Markov games.\n  In this paper, we investigate the fundamental question of \\emph{sample\ncomplexity} for model-based MARL algorithms in general-sum Markov games. We\nshow two results. We first use Hoeffding inequality based bounds to show that\n$\\tilde{\\mathcal{O}}( (1-\\gamma)^{-4} \\alpha^{-2})$ samples per state-action\npair are sufficient to obtain a $\\alpha$-approximate Markov perfect equilibrium\nwith high probability, where $\\gamma$ is the discount factor, and the\n$\\tilde{\\mathcal{O}}(\\cdot)$ notation hides logarithmic terms. We then use\nBernstein inequality based bounds to show that $\\tilde{\\mathcal{O}}(\n(1-\\gamma)^{-1} \\alpha^{-2} )$ samples are sufficient. To obtain these results,\nwe study the robustness of Markov perfect equilibrium to model approximations.\nWe show that the Markov perfect equilibrium of an approximate (or perturbed)\ngame is always an approximate Markov perfect equilibrium of the original game\nand provide explicit bounds on the approximation error. We illustrate the\nresults via a numerical example.",
          "arxiv_id": "2110.02355v2"
        },
        {
          "title": "Efficiently Computing Nash Equilibria in Adversarial Team Markov Games",
          "year": "2022-08",
          "abstract": "Computing Nash equilibrium policies is a central problem in multi-agent\nreinforcement learning that has received extensive attention both in theory and\nin practice. However, provable guarantees have been thus far either limited to\nfully competitive or cooperative scenarios or impose strong assumptions that\nare difficult to meet in most practical applications. In this work, we depart\nfrom those prior results by investigating infinite-horizon \\emph{adversarial\nteam Markov games}, a natural and well-motivated class of games in which a team\nof identically-interested players -- in the absence of any explicit\ncoordination or communication -- is competing against an adversarial player.\nThis setting allows for a unifying treatment of zero-sum Markov games and\nMarkov potential games, and serves as a step to model more realistic strategic\ninteractions that feature both competing and cooperative interests. Our main\ncontribution is the first algorithm for computing stationary\n$\\epsilon$-approximate Nash equilibria in adversarial team Markov games with\ncomputational complexity that is polynomial in all the natural parameters of\nthe game, as well as $1/\\epsilon$. The proposed algorithm is particularly\nnatural and practical, and it is based on performing independent policy\ngradient steps for each player in the team, in tandem with best responses from\nthe side of the adversary; in turn, the policy for the adversary is then\nobtained by solving a carefully constructed linear program. Our analysis\nleverages non-standard techniques to establish the KKT optimality conditions\nfor a nonlinear program with nonconvex constraints, thereby leading to a\nnatural interpretation of the induced Lagrange multipliers. Along the way, we\nsignificantly extend an important characterization of optimal policies in\nadversarial (normal-form) team games due to Von Stengel and Koller (GEB `97).",
          "arxiv_id": "2208.02204v1"
        },
        {
          "title": "Zero-sum Polymatrix Markov Games: Equilibrium Collapse and Efficient Computation of Nash Equilibria",
          "year": "2023-05",
          "abstract": "The works of (Daskalakis et al., 2009, 2022; Jin et al., 2022; Deng et al.,\n2023) indicate that computing Nash equilibria in multi-player Markov games is a\ncomputationally hard task. This fact raises the question of whether or not\ncomputational intractability can be circumvented if one focuses on specific\nclasses of Markov games. One such example is two-player zero-sum Markov games,\nin which efficient ways to compute a Nash equilibrium are known. Inspired by\nzero-sum polymatrix normal-form games (Cai et al., 2016), we define a class of\nzero-sum multi-agent Markov games in which there are only pairwise interactions\ndescribed by a graph that changes per state. For this class of Markov games, we\nshow that an $\\epsilon$-approximate Nash equilibrium can be found efficiently.\nTo do so, we generalize the techniques of (Cai et al., 2016), by showing that\nthe set of coarse-correlated equilibria collapses to the set of Nash\nequilibria. Afterwards, it is possible to use any algorithm in the literature\nthat computes approximate coarse-correlated equilibria Markovian policies to\nget an approximate Nash equilibrium.",
          "arxiv_id": "2305.14329v2"
        }
      ],
      "7": [
        {
          "title": "Agent based network modelling of COVID-19 disease dynamics and vaccination uptake in a New South Wales Country Township",
          "year": "2024-01",
          "abstract": "We employ an agent-based contact network model to study the relationship\nbetween vaccine uptake and disease dynamics in a hypothetical country town from\nNew South Wales, Australia, undergoing a COVID-19 epidemic, over a period of\nthree years. We model the contact network in this hypothetical township of N =\n10000 people as a scale-free network, and simulate the spread of COVID-19 and\nvaccination program using disease and vaccination uptake parameters typically\nobserved in such a NSW town. We simulate the spread of the ancestral variant of\nCOVID-19 in this town, and study the disease dynamics while the town maintains\nlimited but non-negligible contact with the rest of the country which is\nassumed to be undergoing a severe COVID-19 epidemic. We also simulate a maximum\nthree doses of Pfizer Comirnaty vaccine being administered in this town, with\nlimited vaccine supply at first which gradually increases, and analyse how the\nvaccination uptake affects the disease dynamics in this town, which is captured\nusing an extended compartmental model with epidemic parameters typical for a\nCOVID-19 epidemic in Australia. Our results show that, in such a township,\nthree vaccination doses are sufficient to contain but not eradicate COVID-19,\nand the disease essentially becomes endemic. We also show that the average\ndegree of infected nodes (the average number of contacts for infected people)\npredicts the proportion of infected people. Therefore, if the hubs (people with\na relatively high number of contacts) are disproportionately infected, this\nindicates an oncoming peak of the infection, though the lag time thereof\ndepends on the maximum number of vaccines administered to the populace.\nOverall, our analysis provides interesting insights in understanding the\ninterplay between network topology, vaccination levels, and COVID-19 disease\ndynamics in a typical remote NSW country town.",
          "arxiv_id": "2401.03610v1"
        },
        {
          "title": "CoV-ABM: A stochastic discrete-event agent-based framework to simulate spatiotemporal dynamics of COVID-19",
          "year": "2020-07",
          "abstract": "The paper develops a stochastic Agent-Based Model (ABM) mimicking the spread\nof infectious diseases in geographical domains. The model is designed to\nsimulate the spatiotemporal spread of SARS-CoV2 disease, known as COVID-19. Our\nSARS-CoV2-based ABM framework (CoV-ABM) simulates the spread at any\ngeographical scale, ranging from a village to a country and considers unique\ncharacteristics of SARS-CoV2 viruses such as its persistence in the\nenvironment. Therefore, unlike other simulators, CoV-ABM computes the density\nof active viruses inside each location space to get the virus transmission\nprobability for each agent. It also uses the local census and health data to\ncreate health and risk factor profiles for each individual. The proposed model\nrelies on a flexible timestamp scale to optimize the computational speed and\nthe level of detail. In our framework each agent represents a person\ninteracting with the surrounding space and other adjacent agents inside the\nsame space. Moreover, families stochastic daily tasks are formulated to get\ntracked by the corresponding family members. The model also formulates the\npossibility of meetings for each subset of friendships and relatives. The main\naim of the proposed framework is threefold: to illustrate the dynamics of\nSARS-CoV diseases, to identify places which have a higher probability to become\ninfection hubs and to provide a decision-support system to design efficient\ninterventions in order to fight against pandemics. The framework employs SEIHRD\ndynamics of viral diseases with different intervention scenarios. The paper\nsimulates the spread of COVID-19 in the State of Delaware, United States, with\nnear one million stochastic agents. The results achieved over a period of 15\nweeks with a timestamp of 1 hour show which places become the hubs of\ninfection. The paper also illustrates how hospitals get overwhelmed as the\noutbreak reaches its pick.",
          "arxiv_id": "2007.13231v1"
        },
        {
          "title": "Agent-based modeling for realistic reproduction of human mobility and contact behavior to evaluate test and isolation strategies in epidemic infectious disease spread",
          "year": "2024-10",
          "abstract": "Agent-based models have proven to be useful tools in supporting\ndecision-making processes in different application domains. The advent of\nmodern computers and supercomputers has enabled these bottom-up approaches to\nrealistically model human mobility and contact behavior. The COVID-19 pandemic\nshowcased the urgent need for detailed and informative models that can answer\nresearch questions on transmission dynamics. We present a sophisticated\nagent-based model to simulate the spread of respiratory diseases. The model is\nhighly modularized and can be used on various scales, from a small collection\nof buildings up to cities or countries. Although not being the focus of this\npaper, the model has undergone performance engineering on a single core and\nprovides an efficient intra- and inter-simulation parallelization for\ntime-critical decision-making processes.\n  In order to allow answering research questions on individual level\nresolution, nonpharmaceutical intervention strategies such as face masks or\nvenue closures can be implemented for particular locations or agents. In\nparticular, we allow for sophisticated testing and isolation strategies to\nstudy the effects of minimal-invasive infectious disease mitigation. With\nrealistic human mobility patterns for the region of Brunswick, Germany, we\nstudy the effects of different interventions between March 1st and May 30, 2021\nin the SARS-CoV-2 pandemic. Our analyses suggest that symptom-independent\ntesting has limited impact on the mitigation of disease dynamics if the dark\nfigure in symptomatic cases is high. Furthermore, we found that quarantine\nlength is more important than quarantine efficiency but that, with sufficient\nsymptomatic control, also short quarantines can have a substantial effect.",
          "arxiv_id": "2410.08050v2"
        }
      ],
      "8": [
        {
          "title": "Automated Temporal Equilibrium Analysis: Verification and Synthesis of Multi-Player Games",
          "year": "2020-08",
          "abstract": "In the context of multi-agent systems, the rational verification problem is\nconcerned with checking which temporal logic properties will hold in a system\nwhen its constituent agents are assumed to behave rationally and strategically\nin pursuit of individual objectives. Typically, those objectives are expressed\nas temporal logic formulae which the relevant agent desires to see satisfied.\nUnfortunately, rational verification is computationally complex, and requires\nspecialised techniques in order to obtain practically useable implementations.\nIn this paper, we present such a technique. This technique relies on a\nreduction of the rational verification problem to the solution of a collection\nof parity games. Our approach has been implemented in the Equilibrium\nVerification Environment (EVE) system. The EVE system takes as input a model of\na concurrent/multi-agent system represented using the Simple Reactive Modules\nLanguage (SRML), where agent goals are represented as Linear Temporal Logic\n(LTL) formulae, together with a claim about the equilibrium behaviour of the\nsystem, also expressed as an LTL formula. EVE can then check whether the LTL\nclaim holds on some (or every) computation of the system that could arise\nthrough agents choosing Nash equilibrium strategies; it can also check whether\na system has a Nash equilibrium, and synthesise individual strategies for\nplayers in the multi-player game. After presenting our basic framework, we\ndescribe our new technique and prove its correctness. We then describe our\nimplementation in the EVE system, and present experimental results which show\nthat EVE performs favourably in comparison to other existing tools that support\nrational verification.",
          "arxiv_id": "2008.05638v1"
        },
        {
          "title": "On the Complexity of Rational Verification",
          "year": "2022-07",
          "abstract": "Rational verification refers to the problem of checking which temporal logic\nproperties hold of a concurrent multiagent system, under the assumption that\nagents in the system choose strategies that form a game-theoretic equilibrium.\nRational verification can be understood as a counterpart to model checking for\nmultiagent systems, but while classical model checking can be done in\npolynomial time for some temporal logic specification languages such as CTL,\nand polynomial space with LTL specifications, rational verification is much\nharder: the key decision problems for rational verification are\n2EXPTIME-complete with LTL specifications, even when using explicit-state\nsystem representations. Against this background, our contributions in this\npaper are threefold. First, we show that the complexity of rational\nverification can be greatly reduced by restricting specifications to GR(1), a\nfragment of LTL that can represent a broad and practically useful class of\nresponse properties of reactive systems. In particular, we show that for a\nnumber of relevant settings, rational verification can be done in polynomial\nspace and even in polynomial time. Second, we provide improved complexity\nresults for rational verification when considering players' goals given by\nmean-payoff utility functions; arguably the most widely used approach for\nquantitative objectives in concurrent and multiagent systems. Finally, we\nconsider the problem of computing outcomes that satisfy social welfare\nconstraints. To this end, we consider both utilitarian and egalitarian social\nwelfare and show that computing such outcomes is either PSPACE-complete or\nNP-complete.",
          "arxiv_id": "2207.02637v1"
        },
        {
          "title": "Multi-Valued Verification of Strategic Ability",
          "year": "2023-10",
          "abstract": "Some multi-agent scenarios call for the possibility of evaluating\nspecifications in a richer domain of truth values. Examples include runtime\nmonitoring of a temporal property over a growing prefix of an infinite path,\ninconsistency analysis in distributed databases, and verification methods that\nuse incomplete anytime algorithms, such as bounded model checking. In this\npaper, we present multi-valued alternating-time temporal logic (mv-ATL*), an\nexpressive logic to specify strategic abilities in multi-agent systems. It is\nwell known that, for branching-time logics, a general method for\nmodel-independent translation from multi-valued to two-valued model checking\nexists. We show that the method cannot be directly extended to mv-ATL*. We also\npropose two ways of overcoming the problem. Firstly, we identify constraints on\nformulas for which the model-independent translation can be suitably adapted.\nSecondly, we present a model-dependent reduction that can be applied to all\nformulas of mv-ATL*. We show that, in all cases, the complexity of verification\nincreases only linearly when new truth values are added to the evaluation\ndomain. We also consider several examples that show possible applications of\nmv-ATL* and motivate its use for model checking multi-agent systems.",
          "arxiv_id": "2310.20344v1"
        }
      ],
      "9": [
        {
          "title": "Approximate Proportionality in Online Fair Division",
          "year": "2025-08",
          "abstract": "We study the online fair division problem, where indivisible goods arrive\nsequentially and must be allocated immediately and irrevocably to agents. Prior\nwork has established strong impossibility results for approximating classic\nfairness notions, such as envy-freeness and maximin share fairness, in this\nsetting. In contrast, we focus on proportionality up to one good (PROP1), a\nnatural relaxation of proportionality whose approximability remains unresolved.\nWe begin by showing that three natural greedy algorithms fail to guarantee any\npositive approximation to PROP1 in general, against an adaptive adversary. This\nis surprising because greedy algorithms are commonly used in fair division and\na natural greedy algorithm is known to be able to achieve PROP1 under\nadditional information assumptions. This hardness result motivates the study of\nnon-adaptive adversaries and the use of side-information, in the spirit of\nlearning-augmented algorithms. For non-adaptive adversaries, we show that the\nsimple uniformly random allocation can achieve a meaningful PROP1 approximation\nwith high probability. Meanwhile, we present an algorithm that obtain robust\napproximation ratios against PROP1 when given predictions of the maximum item\nvalue (MIV). Interestingly, we also show that stronger fairness notions such as\nEF1, MMS, and PROPX remain inapproximable even with perfect MIV predictions.",
          "arxiv_id": "2508.03253v1"
        },
        {
          "title": "Fairness Criteria for Allocating Indivisible Chores: Connections and Efficiencies",
          "year": "2021-01",
          "abstract": "We study several fairness notions in allocating indivisible chores (i.e.,\nitems with non-positive values) to agents who have additive and submodular cost\nfunctions. The fairness criteria we are concern with are envy-free up to any\nitem (EFX), envy-free up to one item (EF1), maximin share (MMS), and pairwise\nmaximin share (PMMS), which are proposed as relaxations of envy-freeness in the\nsetting of additive cost functions. For allocations under each fairness\ncriterion, we establish their approximation guarantee for other fairness\ncriteria. Under the additive setting, our results show strong connections\nbetween these fairness criteria and, at the same time, reveal intrinsic\ndifferences between goods allocation and chores allocation. However, such\nstrong relationships cannot be inherited by the submodular setting, under which\nPMMS and MMS are no longer relaxations of envy-freeness and, even worse, few\nnon-trivial guarantees exist. We also investigate efficiency loss under these\nfairness constraints and establish their prices of fairness.",
          "arxiv_id": "2101.07435v3"
        },
        {
          "title": "Class Fairness in Online Matching",
          "year": "2022-03",
          "abstract": "In the classical version of online bipartite matching, there is a given set\nof offline vertices (aka agents) and another set of vertices (aka items) that\narrive online. When each item arrives, its incident edges -- the agents who\nlike the item -- are revealed and the algorithm must irrevocably match the item\nto such agents. We initiate the study of class fairness in this setting, where\nagents are partitioned into a set of classes and the matching is required to be\nfair with respect to the classes. We adopt popular fairness notions from the\nfair division literature such as envy-freeness (up to one item),\nproportionality, and maximin share fairness to our setting. Our class versions\nof these notions demand that all classes, regardless of their sizes, receive a\nfair treatment. We study deterministic and randomized algorithms for matching\nindivisible items (leading to integral matchings) and for matching divisible\nitems (leading to fractional matchings). We design and analyze three novel\nalgorithms. For matching indivisible items, we propose an\nadaptive-priority-based algorithm, MATCH-AND-SHIFT, prove that it achieves\n1/2-approximation of both class envy-freeness up to one item and class maximin\nshare fairness, and show that each guarantee is tight. For matching divisible\nitems, we design a water-filling-based algorithm, EQUAL-FILLING, that achieves\n(1-1/e)-approximation of class envy-freeness and class proportionality; we\nprove (1-1/e) to be tight for class proportionality and establish a 3/4 upper\nbound on class envy-freeness. Finally, we build upon EQUAL-FILLING to design a\nrandomized algorithm for matching indivisible items, EQAUL-FILLING-OCS, which\nachieves 0.593-approximation of class proportionality. The algorithm and its\nanalysis crucially leverage the recently introduced technique of online\ncorrelated selection (OCS) [Fahrbach et al., 2020].",
          "arxiv_id": "2203.03751v1"
        }
      ],
      "10": [
        {
          "title": "Vehicular Network Slicing for Reliable Access and Deadline-Constrained Data Offloading: A Multi-Agent On-Device Learning Approach",
          "year": "2020-12",
          "abstract": "Efficient data offloading plays a pivotal role in computational-intensive\nplatforms as data rate over wireless channels is fundamentally limited. On top\nof that, high mobility adds an extra burden in vehicular edge networks (VENs),\nbolstering the desire for efficient user-centric solutions. Therefore, unlike\nthe legacy inflexible network-centric approach, this paper exploits a\nsoftware-defined flexible, open, and programmable networking platform for an\nefficient user-centric, fast, reliable, and deadline-constrained offloading\nsolution in VENs. In the proposed model, each active vehicle user (VU) is\nserved from multiple low-powered access points (APs) by creating a noble\nvirtual cell (VC). A joint node association, power allocation, and distributed\nresource allocation problem is formulated. As centralized learning is not\npractical in many real-world problems, following the distributed nature of\nautonomous VUs, each VU is considered an edge learning agent. To that end,\nconsidering practical location-aware node associations, a joint radio and power\nresource allocation non-cooperative stochastic game is formulated. Leveraging\nreinforcement learning's (RL) efficacy, a multi-agent RL (MARL) solution is\nproposed where the edge learners aim to learn the Nash equilibrium (NE)\nstrategies to solve the game efficiently. Besides, real-world map data, with a\npractical microscopic mobility model, are used for the simulation. Results\nsuggest that the proposed user-centric approach can deliver remarkable\nperformances in VENs. Moreover, the proposed MARL solution delivers\nnear-optimal performances with approximately 3% collision probabilities in case\nof distributed random access in the uplink.",
          "arxiv_id": "2012.15545v2"
        },
        {
          "title": "Multi-Agent Reinforcement Learning for Network Selection and Resource Allocation in Heterogeneous multi-RAT Networks",
          "year": "2022-02",
          "abstract": "The rapid production of mobile devices along with the wireless applications\nboom is continuing to evolve daily. This motivates the exploitation of wireless\nspectrum using multiple Radio Access Technologies (multi-RAT) and developing\ninnovative network selection techniques to cope with such intensive demand\nwhile improving Quality of Service (QoS). Thus, we propose a distributed\nframework for dynamic network selection at the edge level, and resource\nallocation at the Radio Access Network (RAN) level, while taking into\nconsideration diverse applications' characteristics. In particular, our\nframework employs a deep Multi-Agent Reinforcement Learning (DMARL) algorithm,\nthat aims to maximize the edge nodes' quality of experience while extending the\nbattery lifetime of the nodes and leveraging adaptive compression schemes.\nIndeed, our framework enables data transfer from the network's edge nodes, with\nmulti-RAT capabilities, to the cloud in a cost and energy-efficient manner,\nwhile maintaining QoS requirements of different supported applications. Our\nresults depict that our solution outperforms state-of-the-art techniques of\nnetwork selection in terms of energy consumption, latency, and cost.",
          "arxiv_id": "2202.10308v1"
        },
        {
          "title": "Heterogeneous Task Offloading and Resource Allocations via Deep Recurrent Reinforcement Learning in Partial Observable Multi-Fog Networks",
          "year": "2020-07",
          "abstract": "As wireless services and applications become more sophisticated and require\nfaster and higher-capacity networks, there is a need for an efficient\nmanagement of the execution of increasingly complex tasks based on the\nrequirements of each application. In this regard, fog computing enables the\nintegration of virtualized servers into networks and brings cloud services\ncloser to end devices. In contrast to the cloud server, the computing capacity\nof fog nodes is limited and thus a single fog node might not be capable of\ncomputing-intensive tasks. In this context, task offloading can be particularly\nuseful at the fog nodes by selecting the suitable nodes and proper resource\nmanagement while guaranteeing the Quality-of-Service (QoS) requirements of the\nusers. This paper studies the design of a joint task offloading and resource\nallocation control for heterogeneous service tasks in multi-fog nodes systems.\nThis problem is formulated as a partially observable stochastic game, in which\neach fog node cooperates to maximize the aggregated local rewards while the\nnodes only have access to local observations. To deal with partial\nobservability, we apply a deep recurrent Q-network (DRQN) approach to\napproximate the optimal value functions. The solution is then compared to a\ndeep Q-network (DQN) and deep convolutional Q-network (DCQN) approach to\nevaluate the performance of different neural networks. Moreover, to guarantee\nthe convergence and accuracy of the neural network, an adjusted\nexploration-exploitation method is adopted. Provided numerical results show\nthat the proposed algorithm can achieve a higher average success rate and lower\naverage overflow than baseline methods.",
          "arxiv_id": "2007.10581v1"
        }
      ],
      "11": [
        {
          "title": "MARL for Decentralized Electric Vehicle Charging Coordination with V2V Energy Exchange",
          "year": "2023-08",
          "abstract": "Effective energy management of electric vehicle (EV) charging stations is\ncritical to supporting the transport sector's sustainable energy transition.\nThis paper addresses the EV charging coordination by considering\nvehicle-to-vehicle (V2V) energy exchange as the flexibility to harness in EV\ncharging stations. Moreover, this paper takes into account EV user experiences,\nsuch as charging satisfaction and fairness. We propose a Multi-Agent\nReinforcement Learning (MARL) approach to coordinate EV charging with V2V\nenergy exchange while considering uncertainties in the EV arrival time, energy\nprice, and solar energy generation. The exploration capability of MARL is\nenhanced by introducing parameter noise into MARL's neural network models.\nExperimental results demonstrate the superior performance and scalability of\nour proposed method compared to traditional optimization baselines. The\ndecentralized execution of the algorithm enables it to effectively deal with\npartial system faults in the charging station.",
          "arxiv_id": "2308.14111v1"
        },
        {
          "title": "Renewable energy integration and microgrid energy trading using multi-agent deep reinforcement learning",
          "year": "2021-11",
          "abstract": "In this paper, multi-agent reinforcement learning is used to control a hybrid\nenergy storage system working collaboratively to reduce the energy costs of a\nmicrogrid through maximising the value of renewable energy and trading. The\nagents must learn to control three different types of energy storage system\nsuited for short, medium, and long-term storage under fluctuating demand,\ndynamic wholesale energy prices, and unpredictable renewable energy generation.\nTwo case studies are considered: the first looking at how the energy storage\nsystems can better integrate renewable energy generation under dynamic pricing,\nand the second with how those same agents can be used alongside an aggregator\nagent to sell energy to self-interested external microgrids looking to reduce\ntheir own energy bills. This work found that the centralised learning with\ndecentralised execution of the multi-agent deep deterministic policy gradient\nand its state-of-the-art variants allowed the multi-agent methods to perform\nsignificantly better than the control from a single global agent. It was also\nfound that using separate reward functions in the multi-agent approach\nperformed much better than using a single control agent. Being able to trade\nwith the other microgrids, rather than just selling back to the utility grid,\nalso was found to greatly increase the grid's savings.",
          "arxiv_id": "2111.10898v2"
        },
        {
          "title": "Battery and Hydrogen Energy Storage Control in a Smart Energy Network with Flexible Energy Demand using Deep Reinforcement Learning",
          "year": "2022-08",
          "abstract": "Smart energy networks provide for an effective means to accommodate high\npenetrations of variable renewable energy sources like solar and wind, which\nare key for deep decarbonisation of energy production. However, given the\nvariability of the renewables as well as the energy demand, it is imperative to\ndevelop effective control and energy storage schemes to manage the variable\nenergy generation and achieve desired system economics and environmental goals.\nIn this paper, we introduce a hybrid energy storage system composed of battery\nand hydrogen energy storage to handle the uncertainties related to electricity\nprices, renewable energy production and consumption. We aim to improve\nrenewable energy utilisation and minimise energy costs and carbon emissions\nwhile ensuring energy reliability and stability within the network. To achieve\nthis, we propose a multi-agent deep deterministic policy gradient approach,\nwhich is a deep reinforcement learning-based control strategy to optimise the\nscheduling of the hybrid energy storage system and energy demand in real-time.\nThe proposed approach is model-free and does not require explicit knowledge and\nrigorous mathematical models of the smart energy network environment.\nSimulation results based on real-world data show that: (i) integration and\noptimised operation of the hybrid energy storage system and energy demand\nreduces carbon emissions by 78.69%, improves cost savings by 23.5% and\nrenewable energy utilisation by over 13.2% compared to other baseline models\nand (ii) the proposed algorithm outperforms the state-of-the-art self-learning\nalgorithms like deep-Q network.",
          "arxiv_id": "2208.12779v1"
        }
      ],
      "12": [
        {
          "title": "Upgrading Democracies with Fairer Voting Methods",
          "year": "2025-05",
          "abstract": "Voting methods are instrumental design element of democracies. Citizens use\nthem to express and aggregate their preferences to reach a collective decision.\nHowever, voting outcomes can be as sensitive to voting rules as they are to\npeople's voting choices. Despite the significance and inter-disciplinary\nscientific progress on voting methods, several democracies keep relying on\noutdated voting methods that do not fit modern, pluralistic societies well,\nwhile lacking social innovation. Here, we demonstrate how one can upgrade\nreal-world democracies, namely by using alternative preferential voting methods\nsuch as cumulative voting and the method of equal shares designed for a\nproportional representation of voters' preferences. By rigorously assessing a\nnew participatory budgeting approach applied in the city of Aarau, Switzerland,\nwe unravel the striking voting outcomes of fair voting methods: more winning\nprojects with the same budget and broader geographic and preference\nrepresentation of citizens by the elected projects, in particular for voters\nwho used to be under-represented, while promoting novel project ideas. We\nprovide profound causal evidence showing that citizens prefer proportional\nvoting methods, which possess strong legitimacy without the need of very\ntechnical specialized explanations. We also reveal strong underlying democratic\nvalues exhibited by citizens who support fair voting methods such as altruism\nand compromise. These findings come with a global momentum to unleash a new and\nlong-awaited participation blueprint of how to upgrade democracies.",
          "arxiv_id": "2505.14349v1"
        },
        {
          "title": "Proportionality and Strategyproofness in Multiwinner Elections",
          "year": "2021-04",
          "abstract": "Multiwinner voting rules can be used to select a fixed-size committee from a\nlarger set of candidates. We consider approval-based committee rules, which\nallow voters to approve or disapprove candidates. In this setting, several\nvoting rules such as Proportional Approval Voting (PAV) and Phragm\\'en's rules\nhave been shown to produce committees that are proportional, in the sense that\nthey proportionally represent voters' preferences; all of these rules are\nstrategically manipulable by voters. On the other hand, a generalisation of\nApproval Voting gives a non-proportional but strategyproof voting rule. We show\nthat there is a fundamental tradeoff between these two properties: we prove\nthat no multiwinner voting rule can simultaneously satisfy a weak form of\nproportionality (a weakening of justified representation) and a weak form of\nstrategyproofness. Our impossibility is obtained using a formulation of the\nproblem in propositional logic and applying SAT solvers; a human-readable\nversion of the computer-generated proof is obtained by extracting a minimal\nunsatisfiable set (MUS). We also discuss several related axiomatic questions in\nthe domain of committee elections.",
          "arxiv_id": "2104.08594v2"
        },
        {
          "title": "Characterizations of voting rules based on majority margins",
          "year": "2025-01",
          "abstract": "In the context of voting with ranked ballots, an important class of voting\nrules is the class of margin-based rules (also called pairwise rules). A voting\nrule is margin-based if whenever two elections generate the same head-to-head\nmargins of victory or loss between candidates, then the voting rule yields the\nsame outcome in both elections. Although this is a mathematically natural\ninvariance property to consider, whether it should be regarded as a normative\naxiom on voting rules is less clear. In this paper, we address this question\nfor voting rules with any kind of output, whether a set of candidates, a\nranking, a probability distribution, etc. We prove that a voting rule is\nmargin-based if and only if it satisfies some axioms with clearer normative\ncontent. A key axiom is what we call Preferential Equality, stating that if two\nvoters both rank a candidate $x$ immediately above a candidate $y$, then either\nvoter switching to rank $y$ immediately above $x$ will have the same effect on\nthe election outcome as if the other voter made the switch, so each voter's\npreference for $y$ over $x$ is treated equally.",
          "arxiv_id": "2501.08595v2"
        }
      ],
      "13": [
        {
          "title": "Towards Realistic Market Simulations: a Generative Adversarial Networks Approach",
          "year": "2021-10",
          "abstract": "Simulated environments are increasingly used by trading firms and investment\nbanks to evaluate trading strategies before approaching real markets.\nBacktesting, a widely used approach, consists of simulating experimental\nstrategies while replaying historical market scenarios. Unfortunately, this\napproach does not capture the market response to the experimental agents'\nactions. In contrast, multi-agent simulation presents a natural bottom-up\napproach to emulating agent interaction in financial markets. It allows to set\nup pools of traders with diverse strategies to mimic the financial market\ntrader population, and test the performance of new experimental strategies.\nSince individual agent-level historical data is typically proprietary and not\navailable for public use, it is difficult to calibrate multiple market agents\nto obtain the realism required for testing trading strategies. To addresses\nthis challenge we propose a synthetic market generator based on Conditional\nGenerative Adversarial Networks (CGANs) trained on real aggregate-level\nhistorical data. A CGAN-based \"world\" agent can generate meaningful orders in\nresponse to an experimental agent. We integrate our synthetic market generator\ninto ABIDES, an open source simulator of financial markets. By means of\nextensive simulations we show that our proposal outperforms previous work in\nterms of stylized facts reflecting market responsiveness and realism.",
          "arxiv_id": "2110.13287v1"
        },
        {
          "title": "Learning who is in the market from time series: market participant discovery through adversarial calibration of multi-agent simulators",
          "year": "2021-08",
          "abstract": "In electronic trading markets often only the price or volume time series,\nthat result from interaction of multiple market participants, are directly\nobservable. In order to test trading strategies before deploying them to\nreal-time trading, multi-agent market environments calibrated so that the time\nseries that result from interaction of simulated agents resemble historical are\noften used. To ensure adequate testing, one must test trading strategies in a\nvariety of market scenarios -- which includes both scenarios that represent\nordinary market days as well as stressed markets (most recently observed due to\nthe beginning of COVID pandemic). In this paper, we address the problem of\nmulti-agent simulator parameter calibration to allow simulator capture\ncharacteristics of different market regimes. We propose a novel two-step method\nto train a discriminator that is able to distinguish between \"real\" and \"fake\"\nprice and volume time series as a part of GAN with self-attention, and then\nutilize it within an optimization framework to tune parameters of a simulator\nmodel with known agent archetypes to represent a market scenario. We conclude\nwith experimental results that demonstrate effectiveness of our method.",
          "arxiv_id": "2108.00664v1"
        },
        {
          "title": "Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study",
          "year": "2024-05",
          "abstract": "Exploring complex adaptive financial trading environments through multi-agent\nbased simulation methods presents an innovative approach within the realm of\nquantitative finance. Despite the dominance of multi-agent reinforcement\nlearning approaches in financial markets with observable data, there exists a\nset of systematically significant financial markets that pose challenges due to\ntheir partial or obscured data availability. We, therefore, devise a\nmulti-agent simulation approach employing small-scale meta-heuristic methods.\nThis approach aims to represent the opaque bilateral market for Australian\ngovernment bond trading, capturing the bilateral nature of bank-to-bank\ntrading, also referred to as \"over-the-counter\" (OTC) trading, and commonly\noccurring between \"market makers\". The uniqueness of the bilateral market,\ncharacterized by negotiated transactions and a limited number of agents, yields\nvaluable insights for agent-based modelling and quantitative finance. The\ninherent rigidity of this market structure, which is at odds with the global\nproliferation of multilateral platforms and the decentralization of finance,\nunderscores the unique insights offered by our agent-based model. We explore\nthe implications of market rigidity on market structure and consider the\nelement of stability, in market design. This extends the ongoing discourse on\ncomplex financial trading environments, providing an enhanced understanding of\ntheir dynamics and implications.",
          "arxiv_id": "2405.02849v1"
        }
      ],
      "14": [
        {
          "title": "On the Role of Memory in Robust Opinion Dynamics",
          "year": "2023-02",
          "abstract": "We investigate opinion dynamics in a fully-connected system, consisting of\n$n$ identical and anonymous agents, where one of the opinions (which is called\ncorrect) represents a piece of information to disseminate. In more detail, one\nsource agent initially holds the correct opinion and remains with this opinion\nthroughout the execution. The goal for non-source agents is to quickly agree on\nthis correct opinion, and do that robustly, i.e., from any initial\nconfiguration. The system evolves in rounds. In each round, one agent chosen\nuniformly at random is activated: unless it is the source, the agent pulls the\nopinions of $\\ell$ random agents and then updates its opinion according to some\nrule. We consider a restricted setting, in which agents have no memory and they\nonly revise their opinions on the basis of those of the agents they currently\nsample. As restricted as it is, this setting encompasses very popular opinion\ndynamics, such as the voter model and best-of-$k$ majority rules.\n  Qualitatively speaking, we show that lack of memory prevents efficient\nconvergence. Specifically, we prove that no dynamics can achieve correct\nconvergence in an expected number of steps that is sub-quadratic in $n$, even\nunder a strong version of the model in which activated agents have complete\naccess to the current configuration of the entire system, i.e., the case\n$\\ell=n$. Conversely, we prove that the simple voter model (in which $\\ell=1$)\ncorrectly solves the problem, while almost matching the aforementioned lower\nbound.\n  These results suggest that, in contrast to symmetric consensus problems (that\ndo not involve a notion of correct opinion), fast convergence on the correct\nopinion using stochastic opinion dynamics may indeed require the use of memory.\nThis insight may reflect on natural information dissemination processes that\nrely on a few knowledgeable individuals.",
          "arxiv_id": "2302.08600v1"
        },
        {
          "title": "The Sound of Silence in Social Networks",
          "year": "2024-10",
          "abstract": "We generalize the classic multi-agent DeGroot model for opinion dynamics to\nincorporate the Spiral of Silence theory from political science. This theory\nstates that individuals may withhold their opinions when they perceive them to\nbe in the minority. As in the DeGroot model, a community of agents is\nrepresented as a weighted directed graph whose edges indicate how much agents\ninfluence one another. However, agents whose current opinions are in the\nminority become silent (i.e., they do not express their opinion). Two models\nfor opinion update are then introduced. In the memoryless opinion model (SOM-),\nagents update their opinion by taking the weighted average of their non-silent\nneighbors' opinions. In the memory based opinion model (SOM+), agents update\ntheir opinions by taking the weighted average of the opinions of all their\nneighbors, but for silent neighbors, their most recent opinion is considered.\nWe show that for SOM- convergence to consensus is guaranteed for clique graphs\nbut, unlike for the classic DeGroot, not guaranteed for strongly-connected\naperiodic graphs. In contrast, we show that for SOM+ convergence to consensus\nis not guaranteed even for clique graphs. We showcase our models through\nsimulations offering experimental insights that align with key aspects of the\nSpiral of Silence theory. These findings reveal the impact of silence dynamics\non opinion formation and highlight the limitations of consensus in more nuanced\nsocial models.",
          "arxiv_id": "2410.19685v2"
        },
        {
          "title": "On a Voter Model with Context-Dependent Opinion Adoption",
          "year": "2023-05",
          "abstract": "Opinion diffusion is a crucial phenomenon in social networks, often\nunderlying the way in which a collective of agents develops a consensus on\nrelevant decisions. The voter model is a well-known theoretical model to study\nopinion spreading in social networks and structured populations. Its simplest\nversion assumes that an updating agent will adopt the opinion of a neighboring\nagent chosen at random. The model allows us to study, for example, the\nprobability that a certain opinion will fixate into a consensus opinion, as\nwell as the expected time it takes for a consensus opinion to emerge.\n  Standard voter models are oblivious to the opinions held by the agents\ninvolved in the opinion adoption process. We propose and study a\ncontext-dependent opinion spreading process on an arbitrary social graph, in\nwhich the probability that an agent abandons opinion $a$ in favor of opinion\n$b$ depends on both $a$ and $b$. We discuss the relations of the model with\nexisting voter models and then derive theoretical results for both the fixation\nprobability and the expected consensus time for two opinions, for both the\nsynchronous and the asynchronous update models.",
          "arxiv_id": "2305.07377v2"
        }
      ],
      "15": [
        {
          "title": "Simulating Ride-Pooling Services with Pre-Booking and On-Demand Customers",
          "year": "2022-10",
          "abstract": "If private vehicle trips can be replaced, ride-pooling services can decrease\nparking space needed by higher vehicle utilization and increase traffic\nefficiency by increasing vehicle occupancy. Nevertheless, substantial benefits\ncan only be achieved if a certain market penetration is passed to find enough\nshareable rides for pooling to take place. Additionally, because of their\nhighly dynamic and stochastic nature on-demand ride-pooling services cannot\nalways guarantee that a request is served. Allowing customers to pre-book their\ntrip in advance could provide benefits for both aspects. Additional knowledge\nhelps an operator to better plan vehicle schedules to improve service\nefficiency while an accepted trip or a rejection can be communicated early on\nto the customer. This study presents a simulation framework where a\nride-pooling provider offers a service in mixed operation: Customers can either\nuse the service on-demand or pre-book trips. A graph-based batch optimization\nformulation is proposed to create offline schedules for pre-booking customers.\nUsing two rolling horizons, this offline solution is forwarded to an online\noptimization for on-demand and pre-booking customers simultaneously. The\nframework is tested in a case study for Manhattan, NYC. That the graph-based\nbatch optimization is superior to a basic insertion method in terms of solution\nquality and run-time. Due to additional knowledge, the ride-pooling operator\ncan improve the solution quality significantly by serving more customers while\npooling efficiency can be increased. Additionally, customers have shorter\nwaiting and detour times the more customers book a trip in advance.",
          "arxiv_id": "2210.06972v1"
        },
        {
          "title": "A generalized ride-matching approach for sustainable shared mobility",
          "year": "2021-01",
          "abstract": "On-demand shared mobility is a promising and sustainable transportation\napproach that can mitigate vehicle externalities, such as traffic congestion\nand emission. On-demand shared mobility systems require matching of one\n(one-to-one) or multiple riders (many-to-one) to a vehicle based on real-time\ninformation. We propose a novel Graph-based Many-to-One ride-Matching\n(GMOMatch) algorithm for the dynamic many-to-one matching problem in the\npresence of traffic congestion. GMOMatch, which is an iterative two-step\nmethod, provides high service quality and is efficient in terms of\ncomputational complexity. It starts with a one-to-one matching in Step 1 and is\nfollowed by solving a maximum weight matching problem in Step 2 to combine the\ntravel requests. To evaluate the performance, it is compared with a\nride-matching algorithm developed by Simonetto et al. (2019). Both algorithms\nare implemented in a micro-traffic simulator to assess their performance and\ntheir impact on traffic congestion in Downtown, Toronto road network. In\ncomparison to the Simonetto, GMOMatch improved the service rate, vehicle\nkilometer traveled and traffic travel time by 32%, 16.07%, and 4%,\nrespectively. The sensitivity analysis indicated that utilizing vehicles with a\ncapacity of 10 can achieve 25% service rate improvement compared to a capacity\nof 4.",
          "arxiv_id": "2101.08657v3"
        },
        {
          "title": "Integrating Parcel Deliveries into a Ride-Pooling Service -- An Agent-Based Simulation Study",
          "year": "2022-05",
          "abstract": "This paper examines the integration of freight delivery into the passenger\ntransport of an on-demand ride-pooling service. The goal of this research is to\nuse existing passenger trips for logistics services and thus reduce additional\nvehicle kilometers for freight delivery and the total number of vehicles on the\nroad network. This is achieved by merging the need for two separate fleets into\na single one by combining the services. To evaluate the potential of such a\nmobility-on-demand service, this paper uses an agent-based simulation framework\nand integrates three heuristic parcel assignment strategies into a ride-pooling\nfleet control algorithm. Two integration scenarios (moderate and full) are set\nup. While in both scenarios passengers and parcels share rides in one vehicle,\nin the moderate scenario no stops for parcel pick-up and delivery are allowed\nduring a passenger ride to decrease customer inconvenience. Using real-world\ndemand data for a case study of Munich, Germany, the two integration scenarios\ntogether with the three assignment strategies are compared to the status quo,\nwhich uses two separate vehicle fleets for passenger and logistics transport.\nThe results indicate that the integration of logistics services into a\nride-pooling service is possible and can exploit unused system capacities\nwithout deteriorating passenger transport. Depending on the assignment\nstrategies nearly all parcels can be served until a parcel to passenger demand\nratio of 1:10 while the overall fleet kilometers can be deceased compared to\nthe status quo.",
          "arxiv_id": "2205.04718v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:18:29Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}