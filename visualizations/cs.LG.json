{
  "topics": {
    "data": {
      "0": {
        "name": "0_policy_RL_reinforcement_reinforcement learning",
        "keywords": [
          [
            "policy",
            0.01892273333225932
          ],
          [
            "RL",
            0.01825458402175323
          ],
          [
            "reinforcement",
            0.014228103293609927
          ],
          [
            "reinforcement learning",
            0.01416543999827494
          ],
          [
            "agent",
            0.013509525820505734
          ],
          [
            "Reinforcement",
            0.012003353264303768
          ],
          [
            "agents",
            0.011078872819574072
          ],
          [
            "learning",
            0.010353489677212223
          ],
          [
            "reward",
            0.01017224432221072
          ],
          [
            "Learning",
            0.01014609201649125
          ]
        ],
        "count": 15426
      },
      "1": {
        "name": "1_privacy_FL_Federated_adversarial",
        "keywords": [
          [
            "privacy",
            0.02051379369554836
          ],
          [
            "FL",
            0.01807741773021934
          ],
          [
            "Federated",
            0.01581884985295384
          ],
          [
            "adversarial",
            0.014187492619632223
          ],
          [
            "attacks",
            0.013627812874447653
          ],
          [
            "clients",
            0.011595128327313318
          ],
          [
            "training",
            0.01014322203486474
          ],
          [
            "data",
            0.010086662993268176
          ],
          [
            "communication",
            0.009091597119843984
          ],
          [
            "model",
            0.00901913170622878
          ]
        ],
        "count": 13212
      },
      "2": {
        "name": "2_LLMs_language_LLM_language models",
        "keywords": [
          [
            "LLMs",
            0.017822003011803967
          ],
          [
            "language",
            0.016649648432830268
          ],
          [
            "LLM",
            0.011837841627740632
          ],
          [
            "language models",
            0.01120704624414113
          ],
          [
            "models",
            0.010960210512655746
          ],
          [
            "reasoning",
            0.010942750287683855
          ],
          [
            "Language",
            0.010384132036667399
          ],
          [
            "tasks",
            0.009360476099926985
          ],
          [
            "Large",
            0.00856173782317167
          ],
          [
            "training",
            0.007903498526358153
          ]
        ],
        "count": 10742
      },
      "3": {
        "name": "3_graph_Graph_node_graphs",
        "keywords": [
          [
            "graph",
            0.04518512672047848
          ],
          [
            "Graph",
            0.026018518619940877
          ],
          [
            "node",
            0.02376876299733115
          ],
          [
            "graphs",
            0.022537990213167592
          ],
          [
            "GNNs",
            0.021422775548251456
          ],
          [
            "GNN",
            0.015066056147739825
          ],
          [
            "nodes",
            0.01492336942421552
          ],
          [
            "Networks",
            0.009872409381514253
          ],
          [
            "networks",
            0.009281065729529732
          ],
          [
            "Neural",
            0.009050996564357792
          ]
        ],
        "count": 6795
      },
      "4": {
        "name": "4_equations_neural_PDEs_physics",
        "keywords": [
          [
            "equations",
            0.016303661314686947
          ],
          [
            "neural",
            0.013704876129883234
          ],
          [
            "PDEs",
            0.012572947351723342
          ],
          [
            "physics",
            0.01206822079063775
          ],
          [
            "differential",
            0.01192038111184616
          ],
          [
            "differential equations",
            0.011007716439618844
          ],
          [
            "systems",
            0.010826151062111152
          ],
          [
            "dynamics",
            0.01048637551619226
          ],
          [
            "PDE",
            0.010186161591828844
          ],
          [
            "physical",
            0.009763807391893144
          ]
        ],
        "count": 5580
      },
      "5": {
        "name": "5_molecular_protein_drug_molecules",
        "keywords": [
          [
            "molecular",
            0.02252738279063011
          ],
          [
            "protein",
            0.019752172879101224
          ],
          [
            "drug",
            0.017998625551105273
          ],
          [
            "molecules",
            0.013764451823167984
          ],
          [
            "chemical",
            0.011001925407668152
          ],
          [
            "prediction",
            0.010307721830946094
          ],
          [
            "materials",
            0.010232697297492323
          ],
          [
            "discovery",
            0.009514455539084038
          ],
          [
            "structure",
            0.00831208103997084
          ],
          [
            "properties",
            0.00787396176172602
          ]
        ],
        "count": 5307
      },
      "6": {
        "name": "6_segmentation_images_medical_image",
        "keywords": [
          [
            "segmentation",
            0.022870450107784948
          ],
          [
            "images",
            0.019830937562576912
          ],
          [
            "medical",
            0.0173392385699582
          ],
          [
            "image",
            0.017167640059062794
          ],
          [
            "imaging",
            0.013231213973673281
          ],
          [
            "MRI",
            0.013066584273398026
          ],
          [
            "CT",
            0.012820218028935125
          ],
          [
            "clinical",
            0.009399991674369492
          ],
          [
            "deep",
            0.00924363882292
          ],
          [
            "cancer",
            0.009150755503945035
          ]
        ],
        "count": 5208
      },
      "7": {
        "name": "7_speech_audio_speaker_music",
        "keywords": [
          [
            "speech",
            0.03984954861791602
          ],
          [
            "audio",
            0.029012374240954524
          ],
          [
            "speaker",
            0.017119496052201384
          ],
          [
            "music",
            0.015324986250629665
          ],
          [
            "Speech",
            0.014744528023362409
          ],
          [
            "ASR",
            0.013096912081811334
          ],
          [
            "recognition",
            0.012134522691861748
          ],
          [
            "Audio",
            0.00920317135583068
          ],
          [
            "acoustic",
            0.00876790856551084
          ],
          [
            "model",
            0.008439755007830846
          ]
        ],
        "count": 5171
      },
      "8": {
        "name": "8_gradient_networks_convergence_neural",
        "keywords": [
          [
            "gradient",
            0.019435259181493848
          ],
          [
            "networks",
            0.017133467899732188
          ],
          [
            "convergence",
            0.015361923520085469
          ],
          [
            "neural",
            0.015274723973815144
          ],
          [
            "SGD",
            0.013784987910469457
          ],
          [
            "neural networks",
            0.013524209757319994
          ],
          [
            "optimization",
            0.01290974715013314
          ],
          [
            "convex",
            0.012586618001593186
          ],
          [
            "descent",
            0.01234467338845483
          ],
          [
            "stochastic",
            0.012014664673937593
          ]
        ],
        "count": 4803
      },
      "9": {
        "name": "9_diffusion_image_generative_Diffusion",
        "keywords": [
          [
            "diffusion",
            0.023420229830487492
          ],
          [
            "image",
            0.02263020805970074
          ],
          [
            "generative",
            0.017750886841038586
          ],
          [
            "Diffusion",
            0.01501930919124956
          ],
          [
            "diffusion models",
            0.014881989423276374
          ],
          [
            "generation",
            0.014621687105286505
          ],
          [
            "models",
            0.012328401588354397
          ],
          [
            "images",
            0.011603580062592789
          ],
          [
            "GANs",
            0.010523007615566432
          ],
          [
            "text",
            0.009709224324284118
          ]
        ],
        "count": 4068
      },
      "10": {
        "name": "10_pruning_hardware_NAS_quantization",
        "keywords": [
          [
            "pruning",
            0.016894155255637193
          ],
          [
            "hardware",
            0.014355567852104708
          ],
          [
            "NAS",
            0.013413866043732039
          ],
          [
            "quantization",
            0.011842333897889181
          ],
          [
            "memory",
            0.01152140928608762
          ],
          [
            "training",
            0.011308717409236153
          ],
          [
            "accuracy",
            0.011253308384775298
          ],
          [
            "search",
            0.011252857844429262
          ],
          [
            "DNN",
            0.010699537944195097
          ],
          [
            "neural",
            0.010617308926406185
          ]
        ],
        "count": 3751
      },
      "11": {
        "name": "11_channel_wireless_communication_network",
        "keywords": [
          [
            "channel",
            0.018918869062309055
          ],
          [
            "wireless",
            0.016434164943093504
          ],
          [
            "communication",
            0.013712140652193892
          ],
          [
            "network",
            0.013599525367747334
          ],
          [
            "networks",
            0.008792224313477382
          ],
          [
            "UAV",
            0.008586269464510375
          ],
          [
            "MIMO",
            0.008170156734584927
          ],
          [
            "radio",
            0.007327404384222032
          ],
          [
            "communications",
            0.007201277224684705
          ],
          [
            "signal",
            0.007165615754349841
          ]
        ],
        "count": 2979
      },
      "12": {
        "name": "12_regret_bandit_bandits_algorithm",
        "keywords": [
          [
            "regret",
            0.04624800572691651
          ],
          [
            "bandit",
            0.031000395142802662
          ],
          [
            "bandits",
            0.02222929607916555
          ],
          [
            "algorithm",
            0.021549573481564676
          ],
          [
            "arm",
            0.019609285042477047
          ],
          [
            "bound",
            0.018504368062655728
          ],
          [
            "Bandits",
            0.017559803941345067
          ],
          [
            "online",
            0.017029627732865864
          ],
          [
            "problem",
            0.016655186428299402
          ],
          [
            "algorithms",
            0.015622418804743278
          ]
        ],
        "count": 2707
      },
      "13": {
        "name": "13_causal_treatment_Causal_variables",
        "keywords": [
          [
            "causal",
            0.05923434668373164
          ],
          [
            "treatment",
            0.024198367786979594
          ],
          [
            "Causal",
            0.021248178154108666
          ],
          [
            "variables",
            0.015344687019263843
          ],
          [
            "effect",
            0.014240644010284841
          ],
          [
            "observational",
            0.013449948202302726
          ],
          [
            "effects",
            0.012217675572663551
          ],
          [
            "causal inference",
            0.010631313198821515
          ],
          [
            "causal discovery",
            0.009972441994354122
          ],
          [
            "discovery",
            0.009492883064617296
          ]
        ],
        "count": 2573
      },
      "14": {
        "name": "14_clinical_patient_medical_patients",
        "keywords": [
          [
            "clinical",
            0.02634300865077601
          ],
          [
            "patient",
            0.02046222121640753
          ],
          [
            "medical",
            0.018718143605645707
          ],
          [
            "patients",
            0.01768760631433714
          ],
          [
            "health",
            0.014260506954050319
          ],
          [
            "prediction",
            0.010014456418928737
          ],
          [
            "risk",
            0.009317004678276534
          ],
          [
            "models",
            0.009131761421545597
          ],
          [
            "disease",
            0.009019335735556452
          ],
          [
            "data",
            0.008637867355905795
          ]
        ],
        "count": 2527
      },
      "15": {
        "name": "15_recommendation_user_item_items",
        "keywords": [
          [
            "recommendation",
            0.03267614692252243
          ],
          [
            "user",
            0.030519429094603853
          ],
          [
            "item",
            0.02195811492734439
          ],
          [
            "items",
            0.019357088516535934
          ],
          [
            "recommender",
            0.018617637426891643
          ],
          [
            "users",
            0.017702443639611386
          ],
          [
            "recommender systems",
            0.013499355751583923
          ],
          [
            "Recommendation",
            0.012983024897166831
          ],
          [
            "systems",
            0.011822265685749844
          ],
          [
            "ranking",
            0.011414352891043892
          ]
        ],
        "count": 2481
      },
      "16": {
        "name": "16_visual_video_image_multimodal",
        "keywords": [
          [
            "visual",
            0.021420207786751157
          ],
          [
            "video",
            0.018597858616978432
          ],
          [
            "image",
            0.016620194917393066
          ],
          [
            "multimodal",
            0.016297118622180463
          ],
          [
            "language",
            0.014632718490020374
          ],
          [
            "text",
            0.014536618624608078
          ],
          [
            "modal",
            0.013313327506084001
          ],
          [
            "vision",
            0.013296370395404827
          ],
          [
            "CLIP",
            0.012525779612431119
          ],
          [
            "modalities",
            0.011007204278420044
          ]
        ],
        "count": 2334
      },
      "17": {
        "name": "17_label_labels_supervised_class",
        "keywords": [
          [
            "label",
            0.027508419043185052
          ],
          [
            "labels",
            0.021945350883386283
          ],
          [
            "supervised",
            0.01567324020883414
          ],
          [
            "class",
            0.013460720813180603
          ],
          [
            "SSL",
            0.012407552461268193
          ],
          [
            "noisy",
            0.010958847446858709
          ],
          [
            "supervised learning",
            0.010503702428763414
          ],
          [
            "learning",
            0.010273739077283281
          ],
          [
            "noise",
            0.010000497928612469
          ],
          [
            "loss",
            0.009885835589080814
          ]
        ],
        "count": 2142
      },
      "18": {
        "name": "18_quantum_Quantum_classical_circuit",
        "keywords": [
          [
            "quantum",
            0.12105247068860281
          ],
          [
            "Quantum",
            0.04266334548859334
          ],
          [
            "classical",
            0.030606523699058214
          ],
          [
            "circuit",
            0.014541979739905458
          ],
          [
            "circuits",
            0.014113814509069178
          ],
          [
            "machine",
            0.013822556988220252
          ],
          [
            "machine learning",
            0.013398726766111934
          ],
          [
            "states",
            0.010786057635539262
          ],
          [
            "computing",
            0.009840286771582881
          ],
          [
            "learning",
            0.00962211770964122
          ]
        ],
        "count": 2006
      },
      "19": {
        "name": "19_explanations_explanation_XAI_interpretability",
        "keywords": [
          [
            "explanations",
            0.037358887260886924
          ],
          [
            "explanation",
            0.021605650916670285
          ],
          [
            "XAI",
            0.018058362764277813
          ],
          [
            "interpretability",
            0.012791400863980881
          ],
          [
            "methods",
            0.011782721484025193
          ],
          [
            "Explanations",
            0.011099498095803887
          ],
          [
            "counterfactual",
            0.01081069796298671
          ],
          [
            "interpretable",
            0.010810000402829673
          ],
          [
            "explainability",
            0.01041722481381271
          ],
          [
            "AI",
            0.010136630658928858
          ]
        ],
        "count": 1878
      },
      "20": {
        "name": "20_series_time series_time_forecasting",
        "keywords": [
          [
            "series",
            0.057465192100122055
          ],
          [
            "time series",
            0.05416729028870162
          ],
          [
            "time",
            0.03531865457898893
          ],
          [
            "forecasting",
            0.03315603992996961
          ],
          [
            "Time",
            0.025220342980258124
          ],
          [
            "Series",
            0.022084196855411936
          ],
          [
            "series forecasting",
            0.01891868378443007
          ],
          [
            "time series forecasting",
            0.01684365195629308
          ],
          [
            "Forecasting",
            0.01322849374472022
          ],
          [
            "series data",
            0.011580980109038352
          ]
        ],
        "count": 1814
      },
      "21": {
        "name": "21_3D_point_object_scene",
        "keywords": [
          [
            "3D",
            0.047960965305507496
          ],
          [
            "point",
            0.019163139354877217
          ],
          [
            "object",
            0.01786968836136593
          ],
          [
            "scene",
            0.01580870881770968
          ],
          [
            "LiDAR",
            0.013103769679714689
          ],
          [
            "view",
            0.012534139714190366
          ],
          [
            "point cloud",
            0.012415145065673523
          ],
          [
            "shape",
            0.0116334486232548
          ],
          [
            "clouds",
            0.011412979537885547
          ],
          [
            "point clouds",
            0.011407436920382439
          ]
        ],
        "count": 1718
      },
      "22": {
        "name": "22_news_media_social_social media",
        "keywords": [
          [
            "news",
            0.027407875008925376
          ],
          [
            "media",
            0.026243944567975616
          ],
          [
            "social",
            0.026195241857215258
          ],
          [
            "social media",
            0.02223651783409859
          ],
          [
            "sentiment",
            0.01871157328954459
          ],
          [
            "fake",
            0.017804898681965445
          ],
          [
            "Twitter",
            0.015456670088738767
          ],
          [
            "detection",
            0.014207374272128863
          ],
          [
            "tweets",
            0.013687000308479929
          ],
          [
            "content",
            0.013076843989656996
          ]
        ],
        "count": 1507
      },
      "23": {
        "name": "23_fairness_fair_Fairness_bias",
        "keywords": [
          [
            "fairness",
            0.08444952473915587
          ],
          [
            "fair",
            0.02775897610404963
          ],
          [
            "Fairness",
            0.023991457551500502
          ],
          [
            "bias",
            0.01968056712236672
          ],
          [
            "groups",
            0.01484792309142468
          ],
          [
            "group",
            0.014264146973153306
          ],
          [
            "sensitive",
            0.013731017346790593
          ],
          [
            "Fair",
            0.012453510609575167
          ],
          [
            "attributes",
            0.011324064605825381
          ],
          [
            "decision",
            0.010403601865380295
          ]
        ],
        "count": 1504
      },
      "24": {
        "name": "24_energy_power_electricity_grid",
        "keywords": [
          [
            "energy",
            0.032966449694306836
          ],
          [
            "power",
            0.0290380175283629
          ],
          [
            "electricity",
            0.01823111690028078
          ],
          [
            "grid",
            0.01779191951695084
          ],
          [
            "load",
            0.017183554675552867
          ],
          [
            "forecasting",
            0.016780215549811364
          ],
          [
            "renewable",
            0.012811203549888924
          ],
          [
            "consumption",
            0.012072194728003435
          ],
          [
            "demand",
            0.010215896907596236
          ],
          [
            "Power",
            0.009714678703014673
          ]
        ],
        "count": 1364
      },
      "25": {
        "name": "25_traffic_temporal_spatial_Traffic",
        "keywords": [
          [
            "traffic",
            0.05101860837288561
          ],
          [
            "temporal",
            0.020346100822776395
          ],
          [
            "spatial",
            0.01838049317916934
          ],
          [
            "Traffic",
            0.01674586027036114
          ],
          [
            "prediction",
            0.01625410605944518
          ],
          [
            "mobility",
            0.0157281226041175
          ],
          [
            "transportation",
            0.014172830636754952
          ],
          [
            "road",
            0.013737964757457253
          ],
          [
            "travel",
            0.012584512896860937
          ],
          [
            "urban",
            0.012268724092313886
          ]
        ],
        "count": 1309
      },
      "26": {
        "name": "26_driving_autonomous_traffic_vehicles",
        "keywords": [
          [
            "driving",
            0.04584138207709966
          ],
          [
            "autonomous",
            0.02424566473873142
          ],
          [
            "traffic",
            0.020466011658422066
          ],
          [
            "vehicles",
            0.01986526665302392
          ],
          [
            "vehicle",
            0.019545165855869816
          ],
          [
            "trajectory",
            0.01783992085026405
          ],
          [
            "autonomous driving",
            0.015777218546605073
          ],
          [
            "scenarios",
            0.014474847857153974
          ],
          [
            "Driving",
            0.014210788919709533
          ],
          [
            "Autonomous",
            0.013489077789588863
          ]
        ],
        "count": 1259
      },
      "27": {
        "name": "27_continual_forgetting_continual learning_Continual",
        "keywords": [
          [
            "continual",
            0.03806427049772744
          ],
          [
            "forgetting",
            0.03779412206779848
          ],
          [
            "continual learning",
            0.03553435845102451
          ],
          [
            "Continual",
            0.029135495671494194
          ],
          [
            "catastrophic forgetting",
            0.022726687302568396
          ],
          [
            "catastrophic",
            0.022471446447547488
          ],
          [
            "tasks",
            0.017492186004005403
          ],
          [
            "incremental",
            0.017319338554278828
          ],
          [
            "new",
            0.01635269344589709
          ],
          [
            "learning",
            0.016185422874016923
          ]
        ],
        "count": 1244
      },
      "28": {
        "name": "28_detection_IoT_traffic_attacks",
        "keywords": [
          [
            "detection",
            0.02680207037046015
          ],
          [
            "IoT",
            0.02258113366997385
          ],
          [
            "traffic",
            0.020728986048269383
          ],
          [
            "attacks",
            0.019356246256258713
          ],
          [
            "intrusion",
            0.015927411744351024
          ],
          [
            "security",
            0.0150265526203306
          ],
          [
            "network",
            0.014546245970295912
          ],
          [
            "Detection",
            0.013944958267715423
          ],
          [
            "attack",
            0.013020066850359563
          ],
          [
            "malicious",
            0.010778726691829861
          ]
        ],
        "count": 1202
      },
      "29": {
        "name": "29_matrix_tensor_rank_low rank",
        "keywords": [
          [
            "matrix",
            0.032422714484446856
          ],
          [
            "tensor",
            0.03231428464106086
          ],
          [
            "rank",
            0.03009010534222033
          ],
          [
            "low rank",
            0.018045991255834774
          ],
          [
            "low",
            0.01459243576054815
          ],
          [
            "algorithm",
            0.014409783114247031
          ],
          [
            "sparse",
            0.014293814896910013
          ],
          [
            "completion",
            0.013483810148447638
          ],
          [
            "matrices",
            0.012585120163788011
          ],
          [
            "problem",
            0.012035672866282428
          ]
        ],
        "count": 1186
      },
      "30": {
        "name": "30_optimization_Bayesian_BO_Optimization",
        "keywords": [
          [
            "optimization",
            0.03468922208355215
          ],
          [
            "Bayesian",
            0.027370823425925984
          ],
          [
            "BO",
            0.02685300766940057
          ],
          [
            "Optimization",
            0.020182342170170096
          ],
          [
            "Bayesian optimization",
            0.01859204969073719
          ],
          [
            "objective",
            0.015739686121332176
          ],
          [
            "function",
            0.014918600022611474
          ],
          [
            "problems",
            0.013312453848436746
          ],
          [
            "box",
            0.012961330363885611
          ],
          [
            "black",
            0.012394646295186985
          ]
        ],
        "count": 1154
      },
      "31": {
        "name": "31_market_stock_financial_trading",
        "keywords": [
          [
            "market",
            0.04311834869790509
          ],
          [
            "stock",
            0.04160406820635024
          ],
          [
            "financial",
            0.03242627555675866
          ],
          [
            "trading",
            0.026435956514064315
          ],
          [
            "price",
            0.024634698894790613
          ],
          [
            "portfolio",
            0.01830498631847848
          ],
          [
            "prices",
            0.014760484056332285
          ],
          [
            "markets",
            0.01395507578923777
          ],
          [
            "volatility",
            0.013891423699646754
          ],
          [
            "risk",
            0.011289826038306349
          ]
        ],
        "count": 1107
      },
      "32": {
        "name": "32_clustering_clusters_means_Clustering",
        "keywords": [
          [
            "clustering",
            0.05257849777226792
          ],
          [
            "clusters",
            0.021825690051840856
          ],
          [
            "means",
            0.020143846206986102
          ],
          [
            "Clustering",
            0.018683934832345508
          ],
          [
            "cluster",
            0.018120918891595873
          ],
          [
            "algorithm",
            0.01705695716265215
          ],
          [
            "data",
            0.013713938230638913
          ],
          [
            "dimensional",
            0.013552570244251312
          ],
          [
            "points",
            0.012467987323922841
          ],
          [
            "SNE",
            0.010750342228391368
          ]
        ],
        "count": 1087
      },
      "33": {
        "name": "33_code_software_Code_program",
        "keywords": [
          [
            "code",
            0.06625424749972719
          ],
          [
            "software",
            0.021940372483994785
          ],
          [
            "Code",
            0.01866599081186088
          ],
          [
            "program",
            0.016170500231324287
          ],
          [
            "code generation",
            0.014799602565442936
          ],
          [
            "language",
            0.01375777229835113
          ],
          [
            "source code",
            0.013081688125401865
          ],
          [
            "source",
            0.012736469005669354
          ],
          [
            "generation",
            0.01196895088860716
          ],
          [
            "programs",
            0.011471704097362555
          ]
        ],
        "count": 1062
      },
      "34": {
        "name": "34_entity_entities_knowledge_Knowledge",
        "keywords": [
          [
            "entity",
            0.030907631010102625
          ],
          [
            "entities",
            0.029580743815286168
          ],
          [
            "knowledge",
            0.02503053814929659
          ],
          [
            "Knowledge",
            0.02501242430776111
          ],
          [
            "KG",
            0.022391320062233322
          ],
          [
            "knowledge graph",
            0.018699051171667005
          ],
          [
            "Entity",
            0.01669965550531347
          ],
          [
            "graph",
            0.015794152795102177
          ],
          [
            "knowledge graphs",
            0.015693850846144063
          ],
          [
            "relations",
            0.014844078908448686
          ]
        ],
        "count": 1013
      },
      "35": {
        "name": "35_EEG_brain_signals_subject",
        "keywords": [
          [
            "EEG",
            0.094030839621759
          ],
          [
            "brain",
            0.023142889723218492
          ],
          [
            "signals",
            0.020003080885395905
          ],
          [
            "subject",
            0.01378133244035033
          ],
          [
            "classification",
            0.01255278218962202
          ],
          [
            "decoding",
            0.012393062845102587
          ],
          [
            "emotion",
            0.01022195044703162
          ],
          [
            "subjects",
            0.009695881966037524
          ],
          [
            "Brain",
            0.009627364861091515
          ],
          [
            "features",
            0.00893703484507353
          ]
        ],
        "count": 1012
      },
      "36": {
        "name": "36_LLMs_attacks_safety_attack",
        "keywords": [
          [
            "LLMs",
            0.029580762008645223
          ],
          [
            "attacks",
            0.02774735161901144
          ],
          [
            "safety",
            0.024551090768281734
          ],
          [
            "attack",
            0.023012517046703276
          ],
          [
            "adversarial",
            0.021684577121392656
          ],
          [
            "harmful",
            0.01850110327016784
          ],
          [
            "LLM",
            0.018334478112283444
          ],
          [
            "Language",
            0.014210999955081563
          ],
          [
            "language",
            0.013451599765727339
          ],
          [
            "Large",
            0.012942210899428527
          ]
        ],
        "count": 987
      },
      "37": {
        "name": "37_segmentation_object_image_semantic segmentation",
        "keywords": [
          [
            "segmentation",
            0.030632492190849476
          ],
          [
            "object",
            0.019724426714828176
          ],
          [
            "image",
            0.015241481916407193
          ],
          [
            "semantic segmentation",
            0.015239361758851423
          ],
          [
            "attention",
            0.01498186955599607
          ],
          [
            "semantic",
            0.014976023112082064
          ],
          [
            "vision",
            0.013163835857452904
          ],
          [
            "Segmentation",
            0.012448705053408235
          ],
          [
            "Vision",
            0.011872419518890617
          ],
          [
            "object detection",
            0.010706330885653589
          ]
        ],
        "count": 963
      },
      "38": {
        "name": "38_Gaussian_GP_variational_posterior",
        "keywords": [
          [
            "Gaussian",
            0.036308966527493776
          ],
          [
            "GP",
            0.021523663547964263
          ],
          [
            "variational",
            0.020074436381072473
          ],
          [
            "posterior",
            0.01971833688601113
          ],
          [
            "inference",
            0.018932996299497636
          ],
          [
            "Bayesian",
            0.018684089783118245
          ],
          [
            "GPs",
            0.015477916627292289
          ],
          [
            "Processes",
            0.014999916517153199
          ],
          [
            "MCMC",
            0.013676659571821261
          ],
          [
            "Monte",
            0.013032129689512199
          ]
        ],
        "count": 950
      },
      "39": {
        "name": "39_weather_climate_precipitation_forecasting",
        "keywords": [
          [
            "weather",
            0.037394767808381246
          ],
          [
            "climate",
            0.03336635078508811
          ],
          [
            "precipitation",
            0.026810810011355882
          ],
          [
            "forecasting",
            0.019978609286481583
          ],
          [
            "forecasts",
            0.01639358292944563
          ],
          [
            "resolution",
            0.015286265831024043
          ],
          [
            "Weather",
            0.013200382207200233
          ],
          [
            "forecast",
            0.012884494176943779
          ],
          [
            "weather forecasting",
            0.011710951983319711
          ],
          [
            "prediction",
            0.011617308681247836
          ]
        ],
        "count": 936
      },
      "40": {
        "name": "40_domain_Domain_source_target",
        "keywords": [
          [
            "domain",
            0.06495863481096488
          ],
          [
            "Domain",
            0.03563390521261517
          ],
          [
            "source",
            0.03435419300702152
          ],
          [
            "target",
            0.03417012359152368
          ],
          [
            "adaptation",
            0.03143773124177906
          ],
          [
            "domain adaptation",
            0.02809877457216538
          ],
          [
            "domains",
            0.026799784922293138
          ],
          [
            "target domain",
            0.02305633737289782
          ],
          [
            "Adaptation",
            0.021312117816235144
          ],
          [
            "UDA",
            0.01742418914673842
          ]
        ],
        "count": 853
      },
      "41": {
        "name": "41_uncertainty_conformal_prediction_Conformal",
        "keywords": [
          [
            "uncertainty",
            0.04135143454267384
          ],
          [
            "conformal",
            0.03290692770639203
          ],
          [
            "prediction",
            0.027077998397177217
          ],
          [
            "Conformal",
            0.023189610591918465
          ],
          [
            "calibration",
            0.02172807912363559
          ],
          [
            "conformal prediction",
            0.02055818636608526
          ],
          [
            "coverage",
            0.020281388416936368
          ],
          [
            "quantification",
            0.013773829960233734
          ],
          [
            "prediction sets",
            0.013561402953375754
          ],
          [
            "distribution",
            0.013145058035639787
          ]
        ],
        "count": 840
      },
      "42": {
        "name": "42_simulations_spectra_surveys_mass",
        "keywords": [
          [
            "simulations",
            0.011307356278770216
          ],
          [
            "spectra",
            0.010969695355137175
          ],
          [
            "surveys",
            0.01008971606523276
          ],
          [
            "mass",
            0.010054475880442141
          ],
          [
            "astronomical",
            0.008990367122899348
          ],
          [
            "planet",
            0.008899706002420524
          ],
          [
            "data",
            0.008796750054910752
          ],
          [
            "light",
            0.008545674750620218
          ],
          [
            "neural",
            0.008399015553353808
          ],
          [
            "matter",
            0.008119473176900765
          ]
        ],
        "count": 745
      },
      "43": {
        "name": "43_satellite_crop_remote sensing_imagery",
        "keywords": [
          [
            "satellite",
            0.019685714803155757
          ],
          [
            "crop",
            0.019282046654810373
          ],
          [
            "remote sensing",
            0.019253979179769648
          ],
          [
            "imagery",
            0.019076162637960074
          ],
          [
            "remote",
            0.018207648600280294
          ],
          [
            "sensing",
            0.016659940997720307
          ],
          [
            "land",
            0.014446725267529261
          ],
          [
            "images",
            0.013914064828274522
          ],
          [
            "resolution",
            0.011050935545397062
          ],
          [
            "segmentation",
            0.010180532229606763
          ]
        ],
        "count": 705
      },
      "44": {
        "name": "44_HAR_activity_Activity_activity recognition",
        "keywords": [
          [
            "HAR",
            0.03746214910703934
          ],
          [
            "activity",
            0.025425708382260864
          ],
          [
            "Activity",
            0.021358679661288404
          ],
          [
            "activity recognition",
            0.01857029187036817
          ],
          [
            "sensor",
            0.018320236338925686
          ],
          [
            "recognition",
            0.018018599694774377
          ],
          [
            "activities",
            0.01786226372306669
          ],
          [
            "Human",
            0.017544074370720805
          ],
          [
            "Recognition",
            0.017053706870025526
          ],
          [
            "human",
            0.015278454514367146
          ]
        ],
        "count": 681
      },
      "45": {
        "name": "45_SNN_spiking_neuromorphic_spike",
        "keywords": [
          [
            "SNN",
            0.03867199361973268
          ],
          [
            "spiking",
            0.037161921990679615
          ],
          [
            "neuromorphic",
            0.030022742057847256
          ],
          [
            "spike",
            0.023944669605348426
          ],
          [
            "energy",
            0.020458911313543417
          ],
          [
            "Neural",
            0.01618830170268778
          ],
          [
            "neural",
            0.015065884628264036
          ],
          [
            "neurons",
            0.014460660622470997
          ],
          [
            "Networks",
            0.014459356576782179
          ],
          [
            "networks",
            0.01413593524448348
          ]
        ],
        "count": 662
      },
      "46": {
        "name": "46_brain_fMRI_Alzheimer_AD",
        "keywords": [
          [
            "brain",
            0.057864712951113235
          ],
          [
            "fMRI",
            0.027668977967053133
          ],
          [
            "Alzheimer",
            0.0275525620239271
          ],
          [
            "AD",
            0.02594627462718702
          ],
          [
            "disease",
            0.02212901981721499
          ],
          [
            "functional",
            0.021980267767345046
          ],
          [
            "connectivity",
            0.016245395857327226
          ],
          [
            "MRI",
            0.015193989752951153
          ],
          [
            "neuroimaging",
            0.013941199293027057
          ],
          [
            "Brain",
            0.013890461689293922
          ]
        ],
        "count": 653
      },
      "47": {
        "name": "47_problems_combinatorial_instances_combinatorial optimization",
        "keywords": [
          [
            "problems",
            0.023169661289603478
          ],
          [
            "combinatorial",
            0.021119656965529466
          ],
          [
            "instances",
            0.021113557758470694
          ],
          [
            "combinatorial optimization",
            0.01740221415476985
          ],
          [
            "problem",
            0.01737094475745326
          ],
          [
            "Problem",
            0.016670898346618234
          ],
          [
            "optimization",
            0.014920508532323074
          ],
          [
            "solutions",
            0.014857956176312786
          ],
          [
            "solvers",
            0.01445031919801771
          ],
          [
            "solution",
            0.014227137998594272
          ]
        ],
        "count": 650
      },
      "48": {
        "name": "48_ECG_signals_heart_signal",
        "keywords": [
          [
            "ECG",
            0.0957306621653438
          ],
          [
            "signals",
            0.02281809710756085
          ],
          [
            "heart",
            0.020839216863306666
          ],
          [
            "signal",
            0.015027082503297965
          ],
          [
            "classification",
            0.011968052899043981
          ],
          [
            "lead",
            0.011804097582926078
          ],
          [
            "clinical",
            0.010874572658471367
          ],
          [
            "monitoring",
            0.00920720269329968
          ],
          [
            "detection",
            0.008728013939431994
          ],
          [
            "AF",
            0.008025203863433516
          ]
        ],
        "count": 615
      },
      "49": {
        "name": "49_active learning_active_Active_AL",
        "keywords": [
          [
            "active learning",
            0.05989096354075194
          ],
          [
            "active",
            0.05890904051145342
          ],
          [
            "Active",
            0.04678508532449181
          ],
          [
            "AL",
            0.031709105552737614
          ],
          [
            "learning",
            0.016948404132155696
          ],
          [
            "labeling",
            0.014721428303050586
          ],
          [
            "Learning",
            0.013772207205831862
          ],
          [
            "Active learning",
            0.012793773478228613
          ],
          [
            "label",
            0.0127053046282786
          ],
          [
            "unlabeled",
            0.012525966619137883
          ]
        ],
        "count": 610
      },
      "50": {
        "name": "50_fault_maintenance_diagnosis_faults",
        "keywords": [
          [
            "fault",
            0.041917143938284995
          ],
          [
            "maintenance",
            0.01861020031400156
          ],
          [
            "diagnosis",
            0.017032124641267073
          ],
          [
            "faults",
            0.0142956697847309
          ],
          [
            "Fault",
            0.01427654962684853
          ],
          [
            "industrial",
            0.014160497675685502
          ],
          [
            "vibration",
            0.012891584283142764
          ],
          [
            "fault detection",
            0.012795827465412439
          ],
          [
            "detection",
            0.0111403763074659
          ],
          [
            "monitoring",
            0.011128481724329654
          ]
        ],
        "count": 593
      },
      "51": {
        "name": "51_shot_meta_shot learning_meta learning",
        "keywords": [
          [
            "shot",
            0.05287204827106603
          ],
          [
            "meta",
            0.04042930142777045
          ],
          [
            "shot learning",
            0.03267399316492577
          ],
          [
            "meta learning",
            0.03018078633411623
          ],
          [
            "Few",
            0.023207978136323575
          ],
          [
            "Shot",
            0.01931709109767495
          ],
          [
            "Meta",
            0.017604158882297813
          ],
          [
            "learning",
            0.017426306755027654
          ],
          [
            "classes",
            0.015613327054652562
          ],
          [
            "shot classification",
            0.01351732830523349
          ]
        ],
        "count": 591
      },
      "52": {
        "name": "52_AI_ML_systems_risks",
        "keywords": [
          [
            "AI",
            0.08660127236785226
          ],
          [
            "ML",
            0.019070026272553955
          ],
          [
            "systems",
            0.018638102284931454
          ],
          [
            "risks",
            0.014089031827984778
          ],
          [
            "Artificial",
            0.014017870674977145
          ],
          [
            "research",
            0.013361932971234038
          ],
          [
            "development",
            0.013037841132176549
          ],
          [
            "Intelligence",
            0.012961200082219035
          ],
          [
            "ethical",
            0.012517353591033188
          ],
          [
            "intelligence",
            0.012067602296403352
          ]
        ],
        "count": 544
      },
      "53": {
        "name": "53_anomaly_detection_Anomaly_anomalies",
        "keywords": [
          [
            "anomaly",
            0.06318802484783971
          ],
          [
            "detection",
            0.04658972338071199
          ],
          [
            "Anomaly",
            0.030815061180588094
          ],
          [
            "anomalies",
            0.02751299325677136
          ],
          [
            "outlier",
            0.02091355544013476
          ],
          [
            "Detection",
            0.020478209125012442
          ],
          [
            "normal",
            0.01793379725925601
          ],
          [
            "outlier detection",
            0.014977169980681528
          ],
          [
            "Outlier",
            0.012747230382639993
          ],
          [
            "anomalous",
            0.012454326179433716
          ]
        ],
        "count": 506
      },
      "54": {
        "name": "54_PAC_bounds_dimension_learnability",
        "keywords": [
          [
            "PAC",
            0.03488774580025422
          ],
          [
            "bounds",
            0.026437877291210866
          ],
          [
            "dimension",
            0.01843090581991293
          ],
          [
            "learnability",
            0.016701050431422455
          ],
          [
            "sample",
            0.016182715340323412
          ],
          [
            "class",
            0.015824822218246135
          ],
          [
            "algorithm",
            0.015699535333308975
          ],
          [
            "complexity",
            0.015376888001517477
          ],
          [
            "learning",
            0.015227358995582104
          ],
          [
            "distribution",
            0.015214059206377544
          ]
        ],
        "count": 502
      },
      "55": {
        "name": "55_COVID_epidemic_pandemic_spread",
        "keywords": [
          [
            "COVID",
            0.051137259071346766
          ],
          [
            "epidemic",
            0.024336894866591536
          ],
          [
            "pandemic",
            0.02296367842810757
          ],
          [
            "spread",
            0.019629496297336027
          ],
          [
            "disease",
            0.01873299922475632
          ],
          [
            "forecasting",
            0.015670390223879123
          ],
          [
            "cases",
            0.014834052166270015
          ],
          [
            "infection",
            0.012896869508122783
          ],
          [
            "health",
            0.012206105134684144
          ],
          [
            "countries",
            0.012177795651575785
          ]
        ],
        "count": 501
      },
      "56": {
        "name": "56_trees_tree_boosting_decision",
        "keywords": [
          [
            "trees",
            0.041480641952531254
          ],
          [
            "tree",
            0.03168002091270415
          ],
          [
            "boosting",
            0.023720243813189025
          ],
          [
            "decision",
            0.022615397762204306
          ],
          [
            "decision trees",
            0.020141617304901546
          ],
          [
            "Trees",
            0.014502321937797383
          ],
          [
            "regression",
            0.01441930187422253
          ],
          [
            "forest",
            0.014209159186791978
          ],
          [
            "ensemble",
            0.013504228630836857
          ],
          [
            "random",
            0.013195030562100312
          ]
        ],
        "count": 473
      },
      "57": {
        "name": "57_anomaly_series_time series_detection",
        "keywords": [
          [
            "anomaly",
            0.054181549624304666
          ],
          [
            "series",
            0.04173284512879375
          ],
          [
            "time series",
            0.03969877243954506
          ],
          [
            "detection",
            0.03863552079842754
          ],
          [
            "time",
            0.02807292742443429
          ],
          [
            "Anomaly",
            0.02799804097085454
          ],
          [
            "anomalies",
            0.027254979308460953
          ],
          [
            "Time",
            0.020151668591210314
          ],
          [
            "Detection",
            0.01927650119279166
          ],
          [
            "Series",
            0.017906729996078202
          ]
        ],
        "count": 447
      },
      "58": {
        "name": "58_particle_physics_detector_high energy",
        "keywords": [
          [
            "particle",
            0.03381786171697658
          ],
          [
            "physics",
            0.033619741293133845
          ],
          [
            "detector",
            0.023700921698521366
          ],
          [
            "high energy",
            0.019299266456881067
          ],
          [
            "energy",
            0.01634841446418653
          ],
          [
            "high",
            0.013515898078535355
          ],
          [
            "events",
            0.01228537135702867
          ],
          [
            "simulation",
            0.011475277295622642
          ],
          [
            "particles",
            0.011352950223843387
          ],
          [
            "event",
            0.011142746394950344
          ]
        ],
        "count": 436
      }
    },
    "correlations": [
      [
        1.0,
        -0.7494931954156634,
        -0.7119863402322035,
        -0.7437521759356736,
        -0.7413647361254071,
        -0.7564416183092675,
        -0.7524137671041368,
        -0.7624721468497522,
        -0.7165452177331084,
        -0.7364754044140998,
        -0.7398972469372038,
        -0.7238529828861586,
        -0.658997802684021,
        -0.7426136548635608,
        -0.7495618733050043,
        -0.7301574529157734,
        -0.7237356845536973,
        -0.74191435058858,
        -0.7507250666327137,
        -0.7511763734597048,
        -0.7294613513119417,
        -0.738517396687435,
        -0.7520976977415016,
        -0.752912695105975,
        -0.7179542685853249,
        -0.7315858900940968,
        -0.6778000080847527,
        -0.7438724079490339,
        -0.7365923457556443,
        -0.7490687873225073,
        -0.686548765419583,
        -0.7347975094524848,
        -0.7214764230029025,
        -0.70856209947624,
        -0.7268508199553243,
        -0.760601384429952,
        -0.6971632830900547,
        -0.7463233299491461,
        -0.7349396074221795,
        -0.7576990395759771,
        -0.6959706656526841,
        -0.7139549795566917,
        -0.7623503540326865,
        -0.7534591936124488,
        -0.7457014357052528,
        -0.7442019541457935,
        -0.7557455971896396,
        -0.6622559235106206,
        -0.7586927386268972,
        -0.7384792808656772,
        -0.7566430402769134,
        -0.7145672326783492,
        -0.7309772388563243,
        -0.7585917689209265,
        -0.6945722864369113,
        -0.7571629972942076,
        -0.739654340294505,
        -0.7353057008595583,
        -0.740756816920159
      ],
      [
        -0.7494931954156634,
        1.0,
        -0.7369439969016282,
        -0.7391620080154757,
        -0.7498401350244966,
        -0.7626365734683839,
        -0.7330004291083763,
        -0.7527937154654067,
        -0.6920398435461412,
        -0.7387120712452349,
        -0.7288049761734461,
        -0.6778127192967591,
        -0.7316107533983298,
        -0.7588941218360977,
        -0.7330059920568983,
        -0.7113581409053769,
        -0.7439540438188544,
        -0.7247771472119069,
        -0.7553558287002371,
        -0.7535121088268766,
        -0.7409114094884635,
        -0.7562260894152959,
        -0.7533343332246517,
        -0.72371488613169,
        -0.7229597639899497,
        -0.7458586720135548,
        -0.7452778317467661,
        -0.741516158561375,
        -0.5855533316144548,
        -0.7472645967593586,
        -0.714646826020098,
        -0.7506253771018312,
        -0.7231066940186142,
        -0.72916475347202,
        -0.7287880895959251,
        -0.7617904169150116,
        -0.5550201409434861,
        -0.7462572688981588,
        -0.7399200104139412,
        -0.76209724951643,
        -0.7263006017771161,
        -0.7493084475251266,
        -0.7622067350625832,
        -0.7529867372178719,
        -0.7486648360522851,
        -0.7364558597826842,
        -0.7567539220299346,
        -0.7344183529949384,
        -0.7593151108423724,
        -0.751977497697913,
        -0.7539056287856765,
        -0.7456950177071966,
        -0.7335443948343526,
        -0.7390898383559712,
        -0.7301564516181032,
        -0.7547231350027221,
        -0.7508906471945838,
        -0.7344311242342452,
        -0.7581228627154506
      ],
      [
        -0.7119863402322035,
        -0.7369439969016282,
        1.0,
        -0.7424107766017191,
        -0.7538083222473067,
        -0.7362664555757028,
        -0.7415073160543217,
        -0.7317001417734198,
        -0.7382108545730508,
        -0.7175337990044368,
        -0.7216214109023684,
        -0.7484051615948102,
        -0.7528971606243774,
        -0.7444396041209145,
        -0.735856648910671,
        -0.7184736429348892,
        -0.5507561404098928,
        -0.7339344266624919,
        -0.7598101143867868,
        -0.738276704172393,
        -0.7283607770726523,
        -0.7508057743896328,
        -0.7207239457928385,
        -0.7480033736016016,
        -0.7407475442071305,
        -0.7496678393293015,
        -0.7485847965753859,
        -0.7377385631967275,
        -0.7327839163559173,
        -0.7223661312775655,
        -0.7238378031480148,
        -0.7485764859396722,
        -0.7461572803733246,
        -0.4959250828680765,
        -0.6999404168091419,
        -0.7608529581731271,
        0.02457471218361862,
        -0.7322109413436069,
        -0.7474937995172939,
        -0.7598828633419465,
        -0.6812330835140933,
        -0.7360828690468393,
        -0.7634455602224209,
        -0.760109130309464,
        -0.7468058120907495,
        -0.7500229070614093,
        -0.7541808872699136,
        -0.7314571755413428,
        -0.7593116585155415,
        -0.7511235720630023,
        -0.756704004270286,
        -0.6770766872704532,
        -0.6833677967596562,
        -0.7414077659540987,
        -0.7565796974341163,
        -0.7533504359409939,
        -0.74710842884542,
        -0.7299214679609087,
        -0.7543203827737386
      ],
      [
        -0.7437521759356736,
        -0.7391620080154757,
        -0.7424107766017191,
        1.0,
        -0.7204864909976558,
        -0.6827503184815961,
        -0.7428282242512991,
        -0.7600214277709756,
        -0.7043634109678603,
        -0.7351598112797372,
        -0.7170016682852856,
        -0.7256604817291425,
        -0.7420490556289441,
        -0.7336026356010739,
        -0.7495887511545395,
        -0.7196538415937979,
        -0.7452331409904737,
        -0.702426709860252,
        -0.7476910977838824,
        -0.7359852048294909,
        -0.7248339742931076,
        -0.7333420686035586,
        -0.7309804968124283,
        -0.7479212029029727,
        -0.7132059627779308,
        -0.7006383210448643,
        -0.7456792836621068,
        -0.7533774461735944,
        -0.7137428890878608,
        -0.7191131222569584,
        -0.7282905753744713,
        -0.747384890935887,
        -0.6937964177597777,
        -0.7091331153803208,
        -0.6481249541720293,
        -0.7535116523546661,
        -0.7296781403496571,
        -0.7407044272299382,
        -0.7407147781812397,
        -0.7514294193934439,
        -0.7152545470084155,
        -0.7346599719936512,
        -0.7611348289617397,
        -0.7588207361764592,
        -0.7479417068049925,
        -0.6423016996964216,
        -0.7332318786614993,
        -0.6987250630731847,
        -0.7529913318427772,
        -0.7534755266542412,
        -0.755649318835051,
        -0.7364316157340197,
        -0.7513152894881558,
        -0.7310183672651809,
        -0.7453790710138352,
        -0.7543816055871514,
        -0.7343716873667248,
        -0.7195544977007302,
        -0.7386351975403087
      ],
      [
        -0.7413647361254071,
        -0.7498401350244966,
        -0.7538083222473067,
        -0.7204864909976558,
        1.0,
        -0.7475102575579247,
        -0.7260812115131505,
        -0.752694623649497,
        -0.3846433945270383,
        -0.7184383609849325,
        -0.40936851549010295,
        -0.7097411049513951,
        -0.7458329642844428,
        -0.751274305761831,
        -0.7521069174859972,
        -0.7541946236520737,
        -0.7467823265899028,
        -0.7418243321847431,
        -0.7374025040011727,
        -0.7463785583300309,
        -0.7059546130395524,
        -0.7325654739909482,
        -0.7519332965610473,
        -0.7583170738448506,
        -0.7069982022174903,
        -0.7290338326483198,
        -0.738982420501699,
        -0.7539380265831083,
        -0.7167509890385566,
        -0.7271226448333676,
        -0.7052196359419631,
        -0.7541453403611065,
        -0.7336469359906849,
        -0.7343654575856413,
        -0.7403188466632851,
        -0.7565008221292497,
        -0.7439652130065935,
        -0.7455150952989125,
        -0.6989931068746855,
        -0.7334346808030479,
        -0.7087273498237596,
        -0.7091889847854363,
        -0.7413089830483481,
        -0.7556392263834821,
        -0.7493781282600385,
        -0.45407938204027676,
        -0.7408838236460304,
        -0.6975282991170542,
        -0.7567412612579072,
        -0.7494829197721907,
        -0.7560455990805344,
        -0.744958177281442,
        -0.7472880964560631,
        -0.7520922187101851,
        -0.7352038383261106,
        -0.7545718269710908,
        -0.7472927242087535,
        -0.7171291800255513,
        -0.5520243119719974
      ],
      [
        -0.7564416183092675,
        -0.7626365734683839,
        -0.7362664555757028,
        -0.6827503184815961,
        -0.7475102575579247,
        1.0,
        -0.747657312561893,
        -0.7647316124964607,
        -0.7495725005461772,
        -0.7116648880757676,
        -0.75346634318765,
        -0.7581827503606182,
        -0.760842628289969,
        -0.7485713054452101,
        -0.7352009385398146,
        -0.7580124681584479,
        -0.7530254405719816,
        -0.7503658342612556,
        -0.7314429551301317,
        -0.7552581280396763,
        -0.7529334604951673,
        -0.7028815900523906,
        -0.7571729428921246,
        -0.7621415477618904,
        -0.7282335917087441,
        -0.7469899122782263,
        -0.7589675344849083,
        -0.7639591595340626,
        -0.7585829528251404,
        -0.7536182590757429,
        -0.7343112265056387,
        -0.7623062934356404,
        -0.7499926166317895,
        -0.7379636420172226,
        -0.7389943623057319,
        -0.7626495221732852,
        -0.7482435802019287,
        -0.7575712661951022,
        -0.7475427779303152,
        -0.759374976577353,
        -0.732408169021932,
        -0.725621088370155,
        -0.7487732327361383,
        -0.761525982644718,
        -0.7449464903251367,
        -0.7445429776180723,
        -0.7435240839638413,
        -0.7505514738878633,
        -0.7617474602715872,
        -0.7448998196401344,
        -0.76286745751655,
        -0.7467648751111479,
        -0.7391901404699527,
        -0.7583280206542804,
        -0.76396655215146,
        -0.7518338506861415,
        -0.7539385389056599,
        -0.753745864083939,
        -0.7384738399213695
      ],
      [
        -0.7524137671041368,
        -0.7330004291083763,
        -0.7415073160543217,
        -0.7428282242512991,
        -0.7260812115131505,
        -0.747657312561893,
        1.0,
        -0.7476542153658987,
        -0.7019578234424085,
        -0.5576080077376064,
        -0.7140208708426207,
        -0.7301039001647803,
        -0.7540500345315654,
        -0.7404709511456309,
        -0.5572034659780737,
        -0.7445280618587892,
        -0.608290322928928,
        -0.6884889436049049,
        -0.7545746791848706,
        -0.7401427191721837,
        -0.7235988748729663,
        -0.6490354199600739,
        -0.7195537010125392,
        -0.7536336367401592,
        -0.7341700099557742,
        -0.7359317998618766,
        -0.7393537997383274,
        -0.7534937773216482,
        -0.6973884142072991,
        -0.7428859786735298,
        -0.7337133468398249,
        -0.755218559911365,
        -0.7287064049221245,
        -0.7046377651567606,
        -0.734753112798779,
        -0.7407040895088455,
        -0.7313086089372639,
        -0.3103057887899015,
        -0.7382135630454373,
        -0.7402536742557886,
        -0.6837551555361124,
        -0.7235342492207761,
        -0.7538606854951444,
        -0.6666257105104514,
        -0.729345911297677,
        -0.7205477873707684,
        -0.6564369324808849,
        -0.7270493675009924,
        -0.7473933189477269,
        -0.7479016616159417,
        -0.7212898717675724,
        -0.7286638528285816,
        -0.7206516211816759,
        -0.6903233706462841,
        -0.756448949029042,
        -0.6278375052789422,
        -0.7440105396164314,
        -0.7004533004517776,
        -0.7349453796505505
      ],
      [
        -0.7624721468497522,
        -0.7527937154654067,
        -0.7317001417734198,
        -0.7600214277709756,
        -0.752694623649497,
        -0.7647316124964607,
        -0.7476542153658987,
        1.0,
        -0.7477067458011043,
        -0.7284730483691115,
        -0.7426687414235758,
        -0.74484781616696,
        -0.7625932872410848,
        -0.7582143416056529,
        -0.7512221937019596,
        -0.7447632296195769,
        -0.682253667787042,
        -0.7242951567489295,
        -0.762167220615934,
        -0.7593025577606882,
        -0.7465325168762893,
        -0.7521486494338225,
        -0.7260635014438411,
        -0.7602315296218569,
        -0.7528848047919986,
        -0.7519468979578897,
        -0.760206933375947,
        -0.7584493613850902,
        -0.7333128020298185,
        -0.7559143755408415,
        -0.7560166159282473,
        -0.7613671170774357,
        -0.7515469918567426,
        -0.7331508848351095,
        -0.7467563314147833,
        -0.7480926850124383,
        -0.7417551390462476,
        -0.7417966260077672,
        -0.7548266144329163,
        -0.763159606452795,
        -0.7086523878981328,
        -0.7544403787251972,
        -0.7634657005100376,
        -0.7613004605881921,
        -0.7253017378128928,
        -0.7466924999173674,
        -0.7500100481262839,
        -0.7542696715275743,
        -0.7534966668846941,
        -0.7546590181447079,
        -0.7607660312043735,
        -0.7386498176699314,
        -0.7479401605821372,
        -0.7325791303190761,
        -0.7633404563395849,
        -0.7522504799518193,
        -0.7589742455778086,
        -0.7378895215944614,
        -0.7602110080248056
      ],
      [
        -0.7165452177331084,
        -0.6920398435461412,
        -0.7382108545730508,
        -0.7043634109678603,
        -0.3846433945270383,
        -0.7495725005461772,
        -0.7019578234424085,
        -0.7477067458011043,
        1.0,
        -0.7203946249901778,
        -0.2713568117072226,
        -0.6723417939893576,
        -0.7026664942970673,
        -0.7516908388558632,
        -0.7494444813202448,
        -0.7426602387365874,
        -0.7324104693641119,
        -0.7127953392458632,
        -0.7344958236687597,
        -0.7301336144885073,
        -0.7120235192336903,
        -0.7278181581390881,
        -0.7524223657406723,
        -0.7492619267044449,
        -0.7023494636570937,
        -0.7388187203468937,
        -0.7372761818239504,
        -0.7412775469991532,
        -0.6891797806049539,
        -0.6891396630958487,
        -0.6136875273512975,
        -0.7520493536501793,
        -0.7011749230651505,
        -0.7206715367170287,
        -0.7218408433835881,
        -0.7549390342763695,
        -0.7159700696763889,
        -0.7269327119152169,
        -0.6791260812068247,
        -0.7514612258127582,
        -0.7065602320601356,
        -0.7111400545013069,
        -0.7522812641058534,
        -0.7511336204777699,
        -0.7444707306433278,
        -0.36846087074328165,
        -0.7375104438717203,
        -0.6642379524095037,
        -0.75331160189735,
        -0.7491999645757806,
        -0.7541750722024494,
        -0.737880046429148,
        -0.7439269798730661,
        -0.7445649204592797,
        -0.6762374594565304,
        -0.7554206413268993,
        -0.7325569752811075,
        -0.7162161380693157,
        -0.7110821158533972
      ],
      [
        -0.7364754044140998,
        -0.7387120712452349,
        -0.7175337990044368,
        -0.7351598112797372,
        -0.7184383609849325,
        -0.7116648880757676,
        -0.5576080077376064,
        -0.7284730483691115,
        -0.7203946249901778,
        1.0,
        -0.7324245897906871,
        -0.7368418798000576,
        -0.7522799045282897,
        -0.750645980693858,
        -0.7392306015174261,
        -0.7405170417467243,
        -0.5560517435286589,
        -0.7233087859016334,
        -0.7533353169954873,
        -0.7454442646796775,
        -0.7281725271641517,
        -0.6935592324940936,
        -0.7439329344229,
        -0.7535880976139749,
        -0.7283196755071778,
        -0.7417434022284572,
        -0.7425070210839013,
        -0.7533639573984905,
        -0.7291881107003723,
        -0.7447926055729994,
        -0.7235229945858268,
        -0.7563968522364992,
        -0.7338939356947585,
        -0.6950323428838143,
        -0.7372672595576024,
        -0.7569949841439226,
        -0.7178524017371201,
        -0.6383271508019996,
        -0.700337857184932,
        -0.7418793298262931,
        -0.6975123632009596,
        -0.7368011277394851,
        -0.7617580880191077,
        -0.7322571273331198,
        -0.747821910414969,
        -0.7396608947336073,
        -0.7414539896624098,
        -0.7265432354805883,
        -0.756274063108201,
        -0.7554881757763944,
        -0.7581612239355907,
        -0.7256979550786407,
        -0.7272712706339128,
        -0.7386080854022574,
        -0.747753408017129,
        -0.7571653701535632,
        -0.7520440272280219,
        -0.7263616302255711,
        -0.7245895275069383
      ],
      [
        -0.7398972469372038,
        -0.7288049761734461,
        -0.7216214109023684,
        -0.7170016682852856,
        -0.40936851549010295,
        -0.75346634318765,
        -0.7140208708426207,
        -0.7426687414235758,
        -0.2713568117072226,
        -0.7324245897906871,
        1.0,
        -0.679642876309366,
        -0.7455360513156001,
        -0.7548814350471951,
        -0.7500955662433921,
        -0.7449774403527311,
        -0.734000613883264,
        -0.7292821082432692,
        -0.744892290242784,
        -0.7398830923888962,
        -0.7201443890687274,
        -0.7393548788976947,
        -0.755828309039533,
        -0.7532056475235408,
        -0.6752142228866937,
        -0.7440798473693017,
        -0.7400410302712885,
        -0.7461431654490219,
        -0.6902548380580839,
        -0.7161809113044679,
        -0.6948067611556714,
        -0.7530990350046615,
        -0.7313575101623448,
        -0.7066379028173537,
        -0.7351697175202001,
        -0.755008135934643,
        -0.7073456745751372,
        -0.7220582042323753,
        -0.7179677456003373,
        -0.7532958117124584,
        -0.7161185574888156,
        -0.7216853868849193,
        -0.7484950689117537,
        -0.7510750339016043,
        -0.7458917806687494,
        -0.40103502606336305,
        -0.7431320296144331,
        -0.7140171870181498,
        -0.7540854963276553,
        -0.7513289695054103,
        -0.7498678470715656,
        -0.7314488167159601,
        -0.7335066390726043,
        -0.7441227728566269,
        -0.7400630680690427,
        -0.7527296848001204,
        -0.7364484475889728,
        -0.7211855537906903,
        -0.72369171966461
      ],
      [
        -0.7238529828861586,
        -0.6778127192967591,
        -0.7484051615948102,
        -0.7256604817291425,
        -0.7097411049513951,
        -0.7581827503606182,
        -0.7301039001647803,
        -0.74484781616696,
        -0.6723417939893576,
        -0.7368418798000576,
        -0.679642876309366,
        1.0,
        -0.7345367943800549,
        -0.757867691737899,
        -0.7536126515860342,
        -0.7235815565814365,
        -0.7409065192766103,
        -0.7452452455087979,
        -0.7520752720353814,
        -0.7568922838017338,
        -0.727453498637604,
        -0.7422465091111623,
        -0.7487574761767795,
        -0.757054760217526,
        -0.6958565039152425,
        -0.7230432052677975,
        -0.7333175926855892,
        -0.754824121312264,
        -0.5107810279340563,
        -0.7370620127969127,
        -0.725384581887847,
        -0.7570338354663536,
        -0.7342187699136051,
        -0.7398013156512078,
        -0.7398105349338292,
        -0.745675830152008,
        -0.7388799127706991,
        -0.7339850709008431,
        -0.743163583528467,
        -0.7565726324449484,
        -0.7302792657939506,
        -0.7423476838248362,
        -0.7557864321478902,
        -0.7342014819480526,
        -0.7401434858531422,
        -0.7060996905132038,
        -0.7507385434244468,
        -0.7262790268251925,
        -0.7473366102548782,
        -0.7517759406949336,
        -0.7532598045406842,
        -0.7477914563427888,
        -0.7380741971809294,
        -0.7380983019447129,
        -0.7459657699859958,
        -0.7577262767770645,
        -0.7520789420726821,
        -0.726312423364643,
        -0.7434079609561952
      ],
      [
        -0.658997802684021,
        -0.7316107533983298,
        -0.7528971606243774,
        -0.7420490556289441,
        -0.7458329642844428,
        -0.760842628289969,
        -0.7540500345315654,
        -0.7625932872410848,
        -0.7026664942970673,
        -0.7522799045282897,
        -0.7455360513156001,
        -0.7345367943800549,
        1.0,
        -0.7430072066711387,
        -0.7541464848372874,
        -0.7135289985893672,
        -0.7550836831821448,
        -0.744338524322077,
        -0.7460623282454191,
        -0.7560849190872623,
        -0.7331951389509329,
        -0.7530173694503034,
        -0.7562063487902551,
        -0.7450446239238315,
        -0.7446341640752749,
        -0.7530551041699334,
        -0.7502858318206995,
        -0.7569406468469109,
        -0.7471108976227219,
        -0.7249944367267923,
        -0.6833389541052965,
        -0.733696280783882,
        -0.5398296866407764,
        -0.7505340186390406,
        -0.7348837511036352,
        -0.7624540715438777,
        -0.7436385262368301,
        -0.7556683292812337,
        -0.7158962892721419,
        -0.7616217430161911,
        -0.738556972699503,
        -0.728457169514944,
        -0.763445470268262,
        -0.7607317665578135,
        -0.7595038674824229,
        -0.7487095359050822,
        -0.7600275660173157,
        -0.6690151915039246,
        -0.7585488219408517,
        -0.7472575834948922,
        -0.7615380349006909,
        -0.7465939897374494,
        -0.75741018634145,
        -0.7555814684054412,
        -0.5473539256683142,
        -0.759507091081342,
        -0.7374683340067363,
        -0.7358543946044779,
        -0.7563326893536795
      ],
      [
        -0.7426136548635608,
        -0.7588941218360977,
        -0.7444396041209145,
        -0.7336026356010739,
        -0.751274305761831,
        -0.7485713054452101,
        -0.7404709511456309,
        -0.7582143416056529,
        -0.7516908388558632,
        -0.750645980693858,
        -0.7548814350471951,
        -0.757867691737899,
        -0.7430072066711387,
        1.0,
        -0.6953871754724354,
        -0.7454050019611886,
        -0.7495120564951501,
        -0.7504486716031364,
        -0.7605641686603455,
        -0.739142215790325,
        -0.727882917851455,
        -0.7576277599185623,
        -0.7504734575480303,
        -0.7312024488252327,
        -0.7535527478605508,
        -0.7476046607429728,
        -0.7546504661016437,
        -0.7624182683692498,
        -0.7544939536812562,
        -0.7547275581460622,
        -0.7470403779620245,
        -0.7541775964117821,
        -0.7491135692781397,
        -0.7474706108285665,
        -0.7493553411238632,
        -0.758759570663277,
        -0.747493785246172,
        -0.7506644975081365,
        -0.7417588113433152,
        -0.749682118956204,
        -0.7329724970915451,
        -0.7402439543519848,
        -0.7598933033270185,
        -0.7573767480049902,
        -0.7546237230488422,
        -0.7529379743367077,
        -0.7363628061615317,
        -0.7463227792624727,
        -0.7597330474068332,
        -0.7540110962975999,
        -0.7523249214263579,
        -0.7525948260200516,
        -0.7463182880020813,
        -0.7525288852106329,
        -0.7509943275381405,
        -0.7543416949901613,
        -0.7457582839669954,
        -0.7308760705450819,
        -0.7564353097417065
      ],
      [
        -0.7495618733050043,
        -0.7330059920568983,
        -0.735856648910671,
        -0.7495887511545395,
        -0.7521069174859972,
        -0.7352009385398146,
        -0.5572034659780737,
        -0.7512221937019596,
        -0.7494444813202448,
        -0.7392306015174261,
        -0.7500955662433921,
        -0.7536126515860342,
        -0.7541464848372874,
        -0.6953871754724354,
        1.0,
        -0.7459368961414001,
        -0.7316476231603188,
        -0.7306571859571347,
        -0.7612106573021289,
        -0.7363964006097249,
        -0.7200742425285485,
        -0.7394613449972327,
        -0.7421690039735216,
        -0.7416710988212174,
        -0.7522573200337981,
        -0.7403357281552475,
        -0.7566421556532801,
        -0.7595546562274506,
        -0.7308756849486799,
        -0.7542803551711552,
        -0.7483942605607157,
        -0.7559564841580273,
        -0.7394626836868035,
        -0.7309331531829549,
        -0.7355671590837656,
        -0.7240475242926765,
        -0.7364128953976932,
        -0.7145175990436576,
        -0.752891601582802,
        -0.7567802707525308,
        -0.7188990324821913,
        -0.7197034192225871,
        -0.7592340911767259,
        -0.7489135025830174,
        -0.7292024597165738,
        -0.7480305574919502,
        -0.6400907845594155,
        -0.7500079366067418,
        -0.7057228922237291,
        -0.7542451267200545,
        -0.7078325200950277,
        -0.745874560707932,
        -0.6994035772045103,
        -0.716716061025914,
        -0.7616963159367711,
        -0.6976422044694355,
        -0.7313739769522551,
        -0.7116911300551816,
        -0.7584055018021878
      ],
      [
        -0.7301574529157734,
        -0.7113581409053769,
        -0.7184736429348892,
        -0.7196538415937979,
        -0.7541946236520737,
        -0.7580124681584479,
        -0.7445280618587892,
        -0.7447632296195769,
        -0.7426602387365874,
        -0.7405170417467243,
        -0.7449774403527311,
        -0.7235815565814365,
        -0.7135289985893672,
        -0.7454050019611886,
        -0.7459368961414001,
        1.0,
        -0.7299221826760325,
        -0.7398063738964968,
        -0.7614184288520132,
        -0.7284397304600285,
        -0.7356274611658589,
        -0.75086803461843,
        -0.6980054484117522,
        -0.7332002999695355,
        -0.7399096002228214,
        -0.7319286621250791,
        -0.7451433707197925,
        -0.7557814968033948,
        -0.7275206531863725,
        -0.7276678728570487,
        -0.7280198294603797,
        -0.7425501935816377,
        -0.7319407507544626,
        -0.7234159676670697,
        -0.7184930689939609,
        -0.7571102453866563,
        -0.7178633543692381,
        -0.7454113031870839,
        -0.7501318080779,
        -0.7589935939470702,
        -0.7216092160839152,
        -0.7347275212402457,
        -0.761976512847535,
        -0.7558516240232986,
        -0.7322480726792465,
        -0.748812205451841,
        -0.7586981960645618,
        -0.7312795887128472,
        -0.7554988873902039,
        -0.7433113463835908,
        -0.754037340907888,
        -0.7417326195762153,
        -0.7280220145564239,
        -0.7434383181550184,
        -0.7520104434388968,
        -0.7545536240469093,
        -0.7447989470979357,
        -0.7345596640505595,
        -0.7591004883860522
      ],
      [
        -0.7237356845536973,
        -0.7439540438188544,
        -0.5507561404098928,
        -0.7452331409904737,
        -0.7467823265899028,
        -0.7530254405719816,
        -0.608290322928928,
        -0.682253667787042,
        -0.7324104693641119,
        -0.5560517435286589,
        -0.734000613883264,
        -0.7409065192766103,
        -0.7550836831821448,
        -0.7495120564951501,
        -0.7316476231603188,
        -0.7299221826760325,
        1.0,
        -0.7081150202548148,
        -0.7615840644041385,
        -0.7382896581816258,
        -0.7327690140046623,
        -0.6941924580372116,
        -0.7145768949562727,
        -0.7552598452376602,
        -0.7461796172013369,
        -0.7328936137241024,
        -0.737020897514447,
        -0.7473229268619284,
        -0.7250608406899997,
        -0.7453369521035678,
        -0.7418227677860849,
        -0.7569885643801982,
        -0.7412982942059458,
        -0.6739319463076594,
        -0.719107509870784,
        -0.7560647790853534,
        -0.6449361603560388,
        -0.6043079729177068,
        -0.7473047401934707,
        -0.7560200901801353,
        -0.6917755354265385,
        -0.7405507376057325,
        -0.7620029999974034,
        -0.7364575458712453,
        -0.7210125712817266,
        -0.742424869057406,
        -0.7416240581834943,
        -0.7337700402634977,
        -0.7559282961315341,
        -0.7511683758941188,
        -0.7567669156213103,
        -0.6763669651402835,
        -0.7201896033341082,
        -0.7270813324058844,
        -0.756983936003736,
        -0.7556997124690895,
        -0.751566862110941,
        -0.7252646292112852,
        -0.747831991164003
      ],
      [
        -0.74191435058858,
        -0.7247771472119069,
        -0.7339344266624919,
        -0.702426709860252,
        -0.7418243321847431,
        -0.7503658342612556,
        -0.6884889436049049,
        -0.7242951567489295,
        -0.7127953392458632,
        -0.7233087859016334,
        -0.7292821082432692,
        -0.7452452455087979,
        -0.744338524322077,
        -0.7504486716031364,
        -0.7306571859571347,
        -0.7398063738964968,
        -0.7081150202548148,
        1.0,
        -0.7569632549025778,
        -0.7475799326677066,
        -0.7355675313637144,
        -0.7351870290695701,
        -0.7447318001779977,
        -0.7442112230543867,
        -0.7461912116221898,
        -0.7439611974785233,
        -0.7506849433155804,
        -0.7404538107087153,
        -0.721846069860236,
        -0.739325379827631,
        -0.7355697559005797,
        -0.7551412101258012,
        -0.714075532423218,
        -0.7096513060776906,
        -0.725528833811102,
        -0.7534986557690515,
        -0.7272501055272622,
        -0.6963531597626964,
        -0.7395983190404529,
        -0.7595777403778825,
        -0.6855369229814341,
        -0.7150684520977348,
        -0.7613732749984604,
        -0.7407201423986833,
        -0.7377577396510752,
        -0.7356808819082669,
        -0.7468265625464008,
        -0.7082643452282422,
        -0.7520199295746886,
        -0.726371304684938,
        -0.7521863002921014,
        -0.7178522330834576,
        -0.7487375922278403,
        -0.7233596295091086,
        -0.7330175433066208,
        -0.7539262545491898,
        -0.7403979309699392,
        -0.7246171225810712,
        -0.7521215429701795
      ],
      [
        -0.7507250666327137,
        -0.7553558287002371,
        -0.7598101143867868,
        -0.7476910977838824,
        -0.7374025040011727,
        -0.7314429551301317,
        -0.7545746791848706,
        -0.762167220615934,
        -0.7344958236687597,
        -0.7533353169954873,
        -0.744892290242784,
        -0.7520752720353814,
        -0.7460623282454191,
        -0.7605641686603455,
        -0.7612106573021289,
        -0.7614184288520132,
        -0.7615840644041385,
        -0.7569632549025778,
        1.0,
        -0.7608901841392519,
        -0.7498786172470877,
        -0.7559687052307994,
        -0.7615387211329476,
        -0.7620478861655827,
        -0.7429449972466458,
        -0.7587385825310824,
        -0.7587850099329172,
        -0.7627919935793506,
        -0.7525801477136437,
        -0.7350552924607322,
        -0.7408887289018353,
        -0.7546428671581153,
        -0.7481665282876248,
        -0.7558448826565733,
        -0.7576600544652778,
        -0.7624596904721541,
        -0.7583321182336529,
        -0.7607185089732347,
        -0.7411602662938357,
        -0.7605393529981015,
        -0.7537145811111716,
        -0.7535121606984625,
        -0.7579247767904774,
        -0.7601312787754968,
        -0.7618618575098781,
        -0.7331371026050995,
        -0.7587144727619267,
        -0.7331290344354487,
        -0.7626928489148854,
        -0.7585980034360753,
        -0.7577027533252794,
        -0.759038989531102,
        -0.7564921572374737,
        -0.756242623137891,
        -0.7415332216020468,
        -0.7605449910608642,
        -0.7567819056723214,
        -0.7494201543078511,
        -0.7288799469603139
      ],
      [
        -0.7511763734597048,
        -0.7535121088268766,
        -0.738276704172393,
        -0.7359852048294909,
        -0.7463785583300309,
        -0.7552581280396763,
        -0.7401427191721837,
        -0.7593025577606882,
        -0.7301336144885073,
        -0.7454442646796775,
        -0.7398830923888962,
        -0.7568922838017338,
        -0.7560849190872623,
        -0.739142215790325,
        -0.7363964006097249,
        -0.7284397304600285,
        -0.7382896581816258,
        -0.7475799326677066,
        -0.7608901841392519,
        1.0,
        -0.7410098809752204,
        -0.7579757519636531,
        -0.7514870216207477,
        -0.7437737399332206,
        -0.7534216996920365,
        -0.7509526366109609,
        -0.7511397777836517,
        -0.7610326693019044,
        -0.7399089593649169,
        -0.7565800606111739,
        -0.6694365152187576,
        -0.7531445729522905,
        -0.7443935889839108,
        -0.73879144244053,
        -0.743528522860169,
        -0.7620690546009081,
        -0.727285341016574,
        -0.7476256419750723,
        -0.7533711826854839,
        -0.7593011854774365,
        -0.7367160768386206,
        -0.7345934642259969,
        -0.7605987618830283,
        -0.7603919525223526,
        -0.7526861408455907,
        -0.7382573616351877,
        -0.7523304866489168,
        -0.7426796832645373,
        -0.7613973434000163,
        -0.7560290837069159,
        -0.7511897515979286,
        -0.7531813366134247,
        -0.6828695702749796,
        -0.7443129158474824,
        -0.7556164417573017,
        -0.7591500898679783,
        -0.7225300797298729,
        -0.7372230955000636,
        -0.755067742580139
      ],
      [
        -0.7294613513119417,
        -0.7409114094884635,
        -0.7283607770726523,
        -0.7248339742931076,
        -0.7059546130395524,
        -0.7529334604951673,
        -0.7235988748729663,
        -0.7465325168762893,
        -0.7120235192336903,
        -0.7281725271641517,
        -0.7201443890687274,
        -0.727453498637604,
        -0.7331951389509329,
        -0.727882917851455,
        -0.7200742425285485,
        -0.7356274611658589,
        -0.7327690140046623,
        -0.7355675313637144,
        -0.7498786172470877,
        -0.7410098809752204,
        1.0,
        -0.741587847463608,
        -0.7403441222970477,
        -0.7561032734589717,
        -0.6606437084847255,
        -0.6821232358205818,
        -0.7266806777640938,
        -0.7493316175277583,
        -0.7050381204389483,
        -0.7325120096054885,
        -0.7250666481218664,
        -0.6945815422150825,
        -0.71995553442367,
        -0.7197213292885336,
        -0.7316879367684576,
        -0.7440399234897588,
        -0.7262463716560033,
        -0.7331936968226713,
        -0.7271238445271395,
        -0.6309145866242053,
        -0.7111467722129015,
        -0.7112330104157658,
        -0.7577677828627786,
        -0.7350601722788279,
        -0.7198313344545322,
        -0.7161470109044714,
        -0.7377838421192437,
        -0.7221187505667417,
        -0.7406836778450734,
        -0.7481239458731828,
        -0.7389638894113602,
        -0.734271684566014,
        -0.7354044893772409,
        -0.7008510702247184,
        -0.7451814526526019,
        -0.7370211636664272,
        -0.7365618163801612,
        0.27924887046660907,
        -0.7326834611947127
      ],
      [
        -0.738517396687435,
        -0.7562260894152959,
        -0.7508057743896328,
        -0.7333420686035586,
        -0.7325654739909482,
        -0.7028815900523906,
        -0.6490354199600739,
        -0.7521486494338225,
        -0.7278181581390881,
        -0.6935592324940936,
        -0.7393548788976947,
        -0.7422465091111623,
        -0.7530173694503034,
        -0.7576277599185623,
        -0.7394613449972327,
        -0.75086803461843,
        -0.6941924580372116,
        -0.7351870290695701,
        -0.7559687052307994,
        -0.7579757519636531,
        -0.741587847463608,
        1.0,
        -0.7581504688910061,
        -0.7609213373805374,
        -0.7415119713082102,
        -0.735062476924123,
        -0.7072419434132574,
        -0.7578279889586191,
        -0.7300167246458478,
        -0.7486666515975455,
        -0.7383170714083875,
        -0.7617447128050275,
        -0.7403428805987603,
        -0.721322381952598,
        -0.7453457317706136,
        -0.7550171258692122,
        -0.7464995642254028,
        -0.5468330136654234,
        -0.7343089530546131,
        -0.746368319837138,
        -0.7225539669016723,
        -0.7384335401189298,
        -0.7600096787848605,
        -0.7384392139664108,
        -0.7351507971434683,
        -0.7387180850236262,
        -0.7297083006770081,
        -0.7339571973990635,
        -0.7590834910685504,
        -0.748085773801257,
        -0.7568345266750914,
        -0.7379893290385524,
        -0.746247627995968,
        -0.726789989651271,
        -0.7549377882190484,
        -0.756788889368964,
        -0.7515060129571309,
        -0.7314301273599152,
        -0.7167711225959041
      ],
      [
        -0.7520976977415016,
        -0.7533343332246517,
        -0.7207239457928385,
        -0.7309804968124283,
        -0.7519332965610473,
        -0.7571729428921246,
        -0.7195537010125392,
        -0.7260635014438411,
        -0.7524223657406723,
        -0.7439329344229,
        -0.755828309039533,
        -0.7487574761767795,
        -0.7562063487902551,
        -0.7504734575480303,
        -0.7421690039735216,
        -0.6980054484117522,
        -0.7145768949562727,
        -0.7447318001779977,
        -0.7615387211329476,
        -0.7514870216207477,
        -0.7403441222970477,
        -0.7581504688910061,
        1.0,
        -0.7430630597394735,
        -0.7541603219236939,
        -0.7454697532671894,
        -0.7517436267397779,
        -0.7610391881505743,
        -0.7166042222607512,
        -0.7556794179748709,
        -0.7564481837510045,
        -0.714571365509482,
        -0.7430324671526511,
        -0.7365040442401533,
        -0.7308072016449545,
        -0.760986210185489,
        -0.7314276290031033,
        -0.7523428071518321,
        -0.759700016160429,
        -0.7530382205826737,
        -0.7322368163061395,
        -0.7508097088708405,
        -0.7633565741144481,
        -0.7561765654865905,
        -0.7388630474825855,
        -0.7537143695500725,
        -0.7574040831396747,
        -0.7486990170369531,
        -0.7597297509262946,
        -0.7515641772579446,
        -0.7595401761991993,
        -0.7486039885993099,
        -0.7311166103840139,
        -0.7049056775036308,
        -0.7627101020694818,
        -0.6276278124023352,
        -0.7511297961734221,
        -0.7231382165445619,
        -0.754834760943125
      ],
      [
        -0.752912695105975,
        -0.72371488613169,
        -0.7480033736016016,
        -0.7479212029029727,
        -0.7583170738448506,
        -0.7621415477618904,
        -0.7536336367401592,
        -0.7602315296218569,
        -0.7492619267044449,
        -0.7535880976139749,
        -0.7532056475235408,
        -0.757054760217526,
        -0.7450446239238315,
        -0.7312024488252327,
        -0.7416710988212174,
        -0.7332002999695355,
        -0.7552598452376602,
        -0.7442112230543867,
        -0.7620478861655827,
        -0.7437737399332206,
        -0.7561032734589717,
        -0.7609213373805374,
        -0.7430630597394735,
        1.0,
        -0.7557169928962982,
        -0.7571428950170738,
        -0.7596197172020089,
        -0.7607523727725514,
        -0.752555434797088,
        -0.7574769795727747,
        -0.743223412673868,
        -0.7504491527296885,
        -0.739880339297677,
        -0.7491724982375951,
        -0.7536457430221579,
        -0.7640156767751933,
        -0.745868650256313,
        -0.759406064073376,
        -0.7580979433954171,
        -0.7628180346859859,
        -0.74476551928003,
        -0.7467676833661032,
        -0.7626408893802628,
        -0.761543299599974,
        -0.7597935421691024,
        -0.7555668085566041,
        -0.7606961587942898,
        -0.7440675085259407,
        -0.7622324393495307,
        -0.7556199530407076,
        -0.7609580297325595,
        -0.7565039439191611,
        -0.7186839293573173,
        -0.7547961259663738,
        -0.7503190569710629,
        -0.7611824418902857,
        -0.7469584069099967,
        -0.754812809598346,
        -0.7628344499686429
      ],
      [
        -0.7179542685853249,
        -0.7229597639899497,
        -0.7407475442071305,
        -0.7132059627779308,
        -0.7069982022174903,
        -0.7282335917087441,
        -0.7341700099557742,
        -0.7528848047919986,
        -0.7023494636570937,
        -0.7283196755071778,
        -0.6752142228866937,
        -0.6958565039152425,
        -0.7446341640752749,
        -0.7535527478605508,
        -0.7522573200337981,
        -0.7399096002228214,
        -0.7461796172013369,
        -0.7461912116221898,
        -0.7429449972466458,
        -0.7534216996920365,
        -0.6606437084847255,
        -0.7415119713082102,
        -0.7541603219236939,
        -0.7557169928962982,
        1.0,
        -0.7244061428718551,
        -0.732812257787838,
        -0.7566224168225006,
        -0.7018572931277282,
        -0.7343484294066125,
        -0.7089322080616093,
        -0.7233216961467914,
        -0.730805826782755,
        -0.7351884362947484,
        -0.741105702812564,
        -0.7539473317791612,
        -0.7346853051923645,
        -0.7417366730646389,
        -0.7230467714534601,
        -0.6699424302897403,
        -0.7261051982606903,
        -0.7230674761354778,
        -0.7534753369387897,
        -0.7427244518661755,
        -0.7350608317471213,
        -0.564394094924722,
        -0.7453158457550213,
        -0.7246628583146674,
        -0.7500048134607105,
        -0.7414574947619326,
        -0.7379381151936882,
        -0.7483426509432545,
        -0.7195563631067722,
        -0.7352806783721244,
        -0.7491809005615333,
        -0.7523491727913653,
        -0.7394405927398483,
        -0.6985501039109794,
        -0.6450544031595434
      ],
      [
        -0.7315858900940968,
        -0.7458586720135548,
        -0.7496678393293015,
        -0.7006383210448643,
        -0.7290338326483198,
        -0.7469899122782263,
        -0.7359317998618766,
        -0.7519468979578897,
        -0.7388187203468937,
        -0.7417434022284572,
        -0.7440798473693017,
        -0.7230432052677975,
        -0.7530551041699334,
        -0.7476046607429728,
        -0.7403357281552475,
        -0.7319286621250791,
        -0.7328936137241024,
        -0.7439611974785233,
        -0.7587385825310824,
        -0.7509526366109609,
        -0.6821232358205818,
        -0.735062476924123,
        -0.7454697532671894,
        -0.7571428950170738,
        -0.7244061428718551,
        1.0,
        -0.5338411051814324,
        -0.7590451524953833,
        -0.5513205551002742,
        -0.7446673878001955,
        -0.7457502235735535,
        -0.7516912111741454,
        -0.7393378945649378,
        -0.7371515734450991,
        -0.7398897332169905,
        -0.7499501919209807,
        -0.7359131556822207,
        -0.7386147177406972,
        -0.7452922004227722,
        -0.6907747389599359,
        -0.7314176883908623,
        -0.6228648614527696,
        -0.7598398830544448,
        -0.7373622023887723,
        -0.7281112949585369,
        -0.7300689348045166,
        -0.7416786868621352,
        -0.7404215179198851,
        -0.7548852419138786,
        -0.7548260339233765,
        -0.7513430769093103,
        -0.7495537067628527,
        -0.7445455480760231,
        -0.724509313797177,
        -0.7602026460152839,
        -0.7462801774958407,
        -0.7435633961892766,
        -0.6964701295871937,
        -0.7359849498949367
      ],
      [
        -0.6778000080847527,
        -0.7452778317467661,
        -0.7485847965753859,
        -0.7456792836621068,
        -0.738982420501699,
        -0.7589675344849083,
        -0.7393537997383274,
        -0.760206933375947,
        -0.7372761818239504,
        -0.7425070210839013,
        -0.7400410302712885,
        -0.7333175926855892,
        -0.7502858318206995,
        -0.7546504661016437,
        -0.7566421556532801,
        -0.7451433707197925,
        -0.737020897514447,
        -0.7506849433155804,
        -0.7587850099329172,
        -0.7511397777836517,
        -0.7266806777640938,
        -0.7072419434132574,
        -0.7517436267397779,
        -0.7596197172020089,
        -0.732812257787838,
        -0.5338411051814324,
        1.0,
        -0.7527961441280381,
        -0.6683145568249673,
        -0.757397708100751,
        -0.7359108784036128,
        -0.7538878719898716,
        -0.7449946126028342,
        -0.737245925941068,
        -0.7468877155631783,
        -0.7573110526690224,
        -0.7116957776851675,
        -0.7186666532119919,
        -0.7457608045317876,
        -0.7389597923653605,
        -0.7328869425164743,
        -0.7214890635221978,
        -0.7614225776179263,
        -0.7489896760623072,
        -0.7336008865772417,
        -0.7398205821094554,
        -0.7522534007939914,
        -0.7388953325938147,
        -0.7577293193306429,
        -0.7459671055874467,
        -0.7521565891324414,
        -0.7491578998054943,
        -0.729056534464072,
        -0.7303369677712582,
        -0.7561277995288083,
        -0.7583568249667189,
        -0.7496314862086713,
        -0.7254378455475505,
        -0.7406346769026573
      ],
      [
        -0.7438724079490339,
        -0.741516158561375,
        -0.7377385631967275,
        -0.7533774461735944,
        -0.7539380265831083,
        -0.7639591595340626,
        -0.7534937773216482,
        -0.7584493613850902,
        -0.7412775469991532,
        -0.7533639573984905,
        -0.7461431654490219,
        -0.754824121312264,
        -0.7569406468469109,
        -0.7624182683692498,
        -0.7595546562274506,
        -0.7557814968033948,
        -0.7473229268619284,
        -0.7404538107087153,
        -0.7627919935793506,
        -0.7610326693019044,
        -0.7493316175277583,
        -0.7578279889586191,
        -0.7610391881505743,
        -0.7607523727725514,
        -0.7566224168225006,
        -0.7590451524953833,
        -0.7527961441280381,
        1.0,
        -0.7507873408966831,
        -0.7536939856989651,
        -0.7549335598499587,
        -0.7619669965847674,
        -0.7550144791919803,
        -0.7360471385555483,
        -0.7374262600031498,
        -0.7609970004855913,
        -0.7422756610936607,
        -0.7524337148025614,
        -0.7560550405390476,
        -0.7636826608508719,
        -0.7324003492649411,
        -0.7564146438866008,
        -0.7644408186793268,
        -0.7626590718170903,
        -0.7554858619111942,
        -0.7451342402373038,
        -0.7496689150626223,
        -0.7487406263448114,
        -0.7628576185241287,
        -0.7540806886168838,
        -0.7574639743027698,
        -0.7262174219196714,
        -0.7506670395821111,
        -0.7554459870941161,
        -0.7599759922655669,
        -0.7622354154144761,
        -0.7613531687156602,
        -0.7495836387410714,
        -0.7607596130253242
      ],
      [
        -0.7365923457556443,
        -0.5855533316144548,
        -0.7327839163559173,
        -0.7137428890878608,
        -0.7167509890385566,
        -0.7585829528251404,
        -0.6973884142072991,
        -0.7333128020298185,
        -0.6891797806049539,
        -0.7291881107003723,
        -0.6902548380580839,
        -0.5107810279340563,
        -0.7471108976227219,
        -0.7544939536812562,
        -0.7308756849486799,
        -0.7275206531863725,
        -0.7250608406899997,
        -0.721846069860236,
        -0.7525801477136437,
        -0.7399089593649169,
        -0.7050381204389483,
        -0.7300167246458478,
        -0.7166042222607512,
        -0.752555434797088,
        -0.7018572931277282,
        -0.5513205551002742,
        -0.6683145568249673,
        -0.7507873408966831,
        1.0,
        -0.7453120776766644,
        -0.7258477091384556,
        -0.7425200537592744,
        -0.7306154828115572,
        -0.7117357164277862,
        -0.7265307483102055,
        -0.7449916558011416,
        -0.5031651455644223,
        -0.7027191019124439,
        -0.7433610689664412,
        -0.7454111008100198,
        -0.7081695404220442,
        -0.7318768404720885,
        -0.7518423311745717,
        -0.7306668878186404,
        -0.7146904071721222,
        -0.6982372155052043,
        -0.7387902983324486,
        -0.7289797870193395,
        -0.7421913589934885,
        -0.7449955025097239,
        -0.7295611352117914,
        -0.7415034567101776,
        -0.7182549487197347,
        -0.229981438884494,
        -0.7521059457488093,
        -0.7464471800121091,
        -0.7369751850129689,
        -0.4284468634787466,
        -0.7285123197604298
      ],
      [
        -0.7490687873225073,
        -0.7472645967593586,
        -0.7223661312775655,
        -0.7191131222569584,
        -0.7271226448333676,
        -0.7536182590757429,
        -0.7428859786735298,
        -0.7559143755408415,
        -0.6891396630958487,
        -0.7447926055729994,
        -0.7161809113044679,
        -0.7370620127969127,
        -0.7249944367267923,
        -0.7547275581460622,
        -0.7542803551711552,
        -0.7276678728570487,
        -0.7453369521035678,
        -0.739325379827631,
        -0.7350552924607322,
        -0.7565800606111739,
        -0.7325120096054885,
        -0.7486666515975455,
        -0.7556794179748709,
        -0.7574769795727747,
        -0.7343484294066125,
        -0.7446673878001955,
        -0.757397708100751,
        -0.7536939856989651,
        -0.7453120776766644,
        1.0,
        -0.7144525576828858,
        -0.7558447308904284,
        -0.7008870364495215,
        -0.731023636188816,
        -0.7412933898379763,
        -0.7586374058805208,
        -0.7351352559341937,
        -0.7428637472126913,
        -0.7133429127884382,
        -0.7584189874054934,
        -0.7390618167097727,
        -0.744220394665996,
        -0.7609948566464475,
        -0.757500468252176,
        -0.7581632797827078,
        -0.7269920954339002,
        -0.7492836022412677,
        -0.7149966670492072,
        -0.7594013856584554,
        -0.7554773176920822,
        -0.7605976402939834,
        -0.7518825234072524,
        -0.754551882839003,
        -0.7525517382170661,
        -0.7165318022913578,
        -0.7599294518773068,
        -0.7498563522030178,
        -0.7360178737811448,
        -0.7487631510893062
      ],
      [
        -0.686548765419583,
        -0.714646826020098,
        -0.7238378031480148,
        -0.7282905753744713,
        -0.7052196359419631,
        -0.7343112265056387,
        -0.7337133468398249,
        -0.7560166159282473,
        -0.6136875273512975,
        -0.7235229945858268,
        -0.6948067611556714,
        -0.725384581887847,
        -0.6833389541052965,
        -0.7470403779620245,
        -0.7483942605607157,
        -0.7280198294603797,
        -0.7418227677860849,
        -0.7355697559005797,
        -0.7408887289018353,
        -0.6694365152187576,
        -0.7250666481218664,
        -0.7383170714083875,
        -0.7564481837510045,
        -0.743223412673868,
        -0.7089322080616093,
        -0.7457502235735535,
        -0.7359108784036128,
        -0.7549335598499587,
        -0.7258477091384556,
        -0.7144525576828858,
        1.0,
        -0.7462432630109967,
        -0.7049432249951644,
        -0.7167715815866869,
        -0.7303561230352307,
        -0.7608720272107763,
        -0.6969563795684282,
        -0.7422679758603752,
        -0.6321530061747827,
        -0.7535350338743563,
        -0.7104600186222797,
        -0.6977991581187274,
        -0.7574273375255827,
        -0.7555687011928799,
        -0.7531849566997071,
        -0.7096018757863416,
        -0.750856044152403,
        -0.5660356501693404,
        -0.7589886474950562,
        -0.7331740139637692,
        -0.7541734077011891,
        -0.7279375197203029,
        -0.7363279855720609,
        -0.7499777633295595,
        -0.7099110612147816,
        -0.7563674219013398,
        -0.7274421462415022,
        -0.7289477639519978,
        -0.7261994326152199
      ],
      [
        -0.7347975094524848,
        -0.7506253771018312,
        -0.7485764859396722,
        -0.747384890935887,
        -0.7541453403611065,
        -0.7623062934356404,
        -0.755218559911365,
        -0.7613671170774357,
        -0.7520493536501793,
        -0.7563968522364992,
        -0.7530990350046615,
        -0.7570338354663536,
        -0.733696280783882,
        -0.7541775964117821,
        -0.7559564841580273,
        -0.7425501935816377,
        -0.7569885643801982,
        -0.7551412101258012,
        -0.7546428671581153,
        -0.7531445729522905,
        -0.6945815422150825,
        -0.7617447128050275,
        -0.714571365509482,
        -0.7504491527296885,
        -0.7233216961467914,
        -0.7516912111741454,
        -0.7538878719898716,
        -0.7619669965847674,
        -0.7425200537592744,
        -0.7558447308904284,
        -0.7462432630109967,
        1.0,
        -0.7464060499191985,
        -0.7522029421058676,
        -0.7489907486095793,
        -0.7638688455477758,
        -0.7463527922249872,
        -0.7583961137755384,
        -0.7555157821773568,
        -0.7403232787652496,
        -0.7471304396676173,
        -0.7433031785925953,
        -0.7637426317618474,
        -0.7585201083593556,
        -0.7528399335873253,
        -0.7501841322018661,
        -0.7632803098605399,
        -0.7441269302406541,
        -0.761634663702739,
        -0.7580668814101446,
        -0.760785195317446,
        -0.7567698660981987,
        -0.7390717228820616,
        -0.7409319115723125,
        -0.7567589137726602,
        -0.7543256681222887,
        -0.7423795516222059,
        -0.7092431841493025,
        -0.7606362302346937
      ],
      [
        -0.7214764230029025,
        -0.7231066940186142,
        -0.7461572803733246,
        -0.6937964177597777,
        -0.7336469359906849,
        -0.7499926166317895,
        -0.7287064049221245,
        -0.7515469918567426,
        -0.7011749230651505,
        -0.7338939356947585,
        -0.7313575101623448,
        -0.7342187699136051,
        -0.5398296866407764,
        -0.7491135692781397,
        -0.7394626836868035,
        -0.7319407507544626,
        -0.7412982942059458,
        -0.714075532423218,
        -0.7481665282876248,
        -0.7443935889839108,
        -0.71995553442367,
        -0.7403428805987603,
        -0.7430324671526511,
        -0.739880339297677,
        -0.730805826782755,
        -0.7393378945649378,
        -0.7449946126028342,
        -0.7550144791919803,
        -0.7306154828115572,
        -0.7008870364495215,
        -0.7049432249951644,
        -0.7464060499191985,
        1.0,
        -0.7322263714032478,
        -0.7296960144742148,
        -0.7581850246399193,
        -0.7422514923383224,
        -0.7353638302236736,
        -0.7121325232777955,
        -0.751973805082003,
        -0.7250794620601839,
        -0.7379395222061887,
        -0.7623661972333207,
        -0.7519619893252123,
        -0.7453531668474772,
        -0.7353955651100925,
        -0.746406310302222,
        -0.6952281227236901,
        -0.7555750532678346,
        -0.7463970169955229,
        -0.7537422913694455,
        -0.744265038820388,
        -0.7499686873089235,
        -0.733064192949509,
        -0.7111613119363066,
        -0.7526488294950827,
        -0.7259017851883753,
        -0.7164889256525608,
        -0.7424241892529019
      ],
      [
        -0.70856209947624,
        -0.72916475347202,
        -0.4959250828680765,
        -0.7091331153803208,
        -0.7343654575856413,
        -0.7379636420172226,
        -0.7046377651567606,
        -0.7331508848351095,
        -0.7206715367170287,
        -0.6950323428838143,
        -0.7066379028173537,
        -0.7398013156512078,
        -0.7505340186390406,
        -0.7474706108285665,
        -0.7309331531829549,
        -0.7234159676670697,
        -0.6739319463076594,
        -0.7096513060776906,
        -0.7558448826565733,
        -0.73879144244053,
        -0.7197213292885336,
        -0.721322381952598,
        -0.7365040442401533,
        -0.7491724982375951,
        -0.7351884362947484,
        -0.7371515734450991,
        -0.737245925941068,
        -0.7360471385555483,
        -0.7117357164277862,
        -0.731023636188816,
        -0.7167715815866869,
        -0.7522029421058676,
        -0.7322263714032478,
        1.0,
        -0.7138901659693428,
        -0.7558360984471397,
        -0.5154765057669428,
        -0.7012776043807005,
        -0.7396736204694906,
        -0.75158927354512,
        -0.6889346633841542,
        -0.7307524252890409,
        -0.7583822710205543,
        -0.7475307663868257,
        -0.7405102570646894,
        -0.7310475330132845,
        -0.7435499156871657,
        -0.7232663579582596,
        -0.7569659163912039,
        -0.7483125531964545,
        -0.7468259416473241,
        -0.7081521757582931,
        -0.7078622720017054,
        -0.7230676736963264,
        -0.7559595100391565,
        -0.751015481471288,
        -0.7325616223434546,
        -0.7138777570713628,
        -0.7431808281448258
      ],
      [
        -0.7268508199553243,
        -0.7287880895959251,
        -0.6999404168091419,
        -0.6481249541720293,
        -0.7403188466632851,
        -0.7389943623057319,
        -0.734753112798779,
        -0.7467563314147833,
        -0.7218408433835881,
        -0.7372672595576024,
        -0.7351697175202001,
        -0.7398105349338292,
        -0.7348837511036352,
        -0.7493553411238632,
        -0.7355671590837656,
        -0.7184930689939609,
        -0.719107509870784,
        -0.725528833811102,
        -0.7576600544652778,
        -0.743528522860169,
        -0.7316879367684576,
        -0.7453457317706136,
        -0.7308072016449545,
        -0.7536457430221579,
        -0.741105702812564,
        -0.7398897332169905,
        -0.7468877155631783,
        -0.7374262600031498,
        -0.7265307483102055,
        -0.7412933898379763,
        -0.7303561230352307,
        -0.7489907486095793,
        -0.7296960144742148,
        -0.7138901659693428,
        1.0,
        -0.760024409841402,
        -0.7060896736814556,
        -0.7355211387755674,
        -0.7450675107934144,
        -0.756199431521242,
        -0.6988641892221015,
        -0.7381259229241934,
        -0.7589110052077724,
        -0.7563686295315886,
        -0.7409800182360203,
        -0.7361860524208685,
        -0.7516589696899949,
        -0.7200014731159483,
        -0.7579006961931609,
        -0.7523509055188029,
        -0.7556575758050021,
        -0.7231316976777986,
        -0.7384484547126542,
        -0.740870778327161,
        -0.7407087213556083,
        -0.7543729219632235,
        -0.7441870299955993,
        -0.7295140778485056,
        -0.7502678597182191
      ],
      [
        -0.760601384429952,
        -0.7617904169150116,
        -0.7608529581731271,
        -0.7535116523546661,
        -0.7565008221292497,
        -0.7626495221732852,
        -0.7407040895088455,
        -0.7480926850124383,
        -0.7549390342763695,
        -0.7569949841439226,
        -0.755008135934643,
        -0.745675830152008,
        -0.7624540715438777,
        -0.758759570663277,
        -0.7240475242926765,
        -0.7571102453866563,
        -0.7560647790853534,
        -0.7534986557690515,
        -0.7624596904721541,
        -0.7620690546009081,
        -0.7440399234897588,
        -0.7550171258692122,
        -0.760986210185489,
        -0.7640156767751933,
        -0.7539473317791612,
        -0.7499501919209807,
        -0.7573110526690224,
        -0.7609970004855913,
        -0.7449916558011416,
        -0.7586374058805208,
        -0.7608720272107763,
        -0.7638688455477758,
        -0.7581850246399193,
        -0.7558360984471397,
        -0.760024409841402,
        1.0,
        -0.7602035278309078,
        -0.7499466626500321,
        -0.7598970146534154,
        -0.7631446009345151,
        -0.7435644774446928,
        -0.7555094112471882,
        -0.7632293400069062,
        -0.7555630727973456,
        -0.7278052258806833,
        -0.7467467129042533,
        -0.5117395202633434,
        -0.7591237876199015,
        -0.5890101569429711,
        -0.7606551716015704,
        -0.7514333296707093,
        -0.7578672051917941,
        -0.7580334178340778,
        -0.7402610984798088,
        -0.7631751966214975,
        -0.7612252485337085,
        -0.7598736331204675,
        -0.7369893742159705,
        -0.7587490241592084
      ],
      [
        -0.6971632830900547,
        -0.5550201409434861,
        0.02457471218361862,
        -0.7296781403496571,
        -0.7439652130065935,
        -0.7482435802019287,
        -0.7313086089372639,
        -0.7417551390462476,
        -0.7159700696763889,
        -0.7178524017371201,
        -0.7073456745751372,
        -0.7388799127706991,
        -0.7436385262368301,
        -0.747493785246172,
        -0.7364128953976932,
        -0.7178633543692381,
        -0.6449361603560388,
        -0.7272501055272622,
        -0.7583321182336529,
        -0.727285341016574,
        -0.7262463716560033,
        -0.7464995642254028,
        -0.7314276290031033,
        -0.745868650256313,
        -0.7346853051923645,
        -0.7359131556822207,
        -0.7116957776851675,
        -0.7422756610936607,
        -0.5031651455644223,
        -0.7351352559341937,
        -0.6969563795684282,
        -0.7463527922249872,
        -0.7422514923383224,
        -0.5154765057669428,
        -0.7060896736814556,
        -0.7602035278309078,
        1.0,
        -0.7311256974434779,
        -0.7438094139268463,
        -0.7582258180052077,
        -0.6881150135930505,
        -0.7262767587269358,
        -0.7625238081228586,
        -0.7581000790403896,
        -0.7470663590590805,
        -0.728452782008178,
        -0.7548606626862266,
        -0.7309180709814365,
        -0.7588108505049367,
        -0.7501064812367694,
        -0.752388019023167,
        -0.7004706103483117,
        -0.680922094352451,
        -0.7116056810036862,
        -0.7466344699835172,
        -0.7525852956156005,
        -0.7430811593355846,
        -0.7125180049012387,
        -0.7491555344925582
      ],
      [
        -0.7463233299491461,
        -0.7462572688981588,
        -0.7322109413436069,
        -0.7407044272299382,
        -0.7455150952989125,
        -0.7575712661951022,
        -0.3103057887899015,
        -0.7417966260077672,
        -0.7269327119152169,
        -0.6383271508019996,
        -0.7220582042323753,
        -0.7339850709008431,
        -0.7556683292812337,
        -0.7506644975081365,
        -0.7145175990436576,
        -0.7454113031870839,
        -0.6043079729177068,
        -0.6963531597626964,
        -0.7607185089732347,
        -0.7476256419750723,
        -0.7331936968226713,
        -0.5468330136654234,
        -0.7523428071518321,
        -0.759406064073376,
        -0.7417366730646389,
        -0.7386147177406972,
        -0.7186666532119919,
        -0.7524337148025614,
        -0.7027191019124439,
        -0.7428637472126913,
        -0.7422679758603752,
        -0.7583961137755384,
        -0.7353638302236736,
        -0.7012776043807005,
        -0.7355211387755674,
        -0.7499466626500321,
        -0.7311256974434779,
        1.0,
        -0.7459563639601208,
        -0.7468975013941374,
        -0.7009526586344439,
        -0.7329085181863222,
        -0.7614508562499724,
        -0.6994877219589124,
        -0.7339489692884769,
        -0.736074179060846,
        -0.7192461101136063,
        -0.733241262879158,
        -0.7542640159009474,
        -0.7462833953024364,
        -0.746523016699845,
        -0.724399604786758,
        -0.7415962680982927,
        -0.6910096727303213,
        -0.7583491228415784,
        -0.7482433455379196,
        -0.749700241538201,
        -0.7097942716460425,
        -0.7387252492667922
      ],
      [
        -0.7349396074221795,
        -0.7399200104139412,
        -0.7474937995172939,
        -0.7407147781812397,
        -0.6989931068746855,
        -0.7475427779303152,
        -0.7382135630454373,
        -0.7548266144329163,
        -0.6791260812068247,
        -0.700337857184932,
        -0.7179677456003373,
        -0.743163583528467,
        -0.7158962892721419,
        -0.7417588113433152,
        -0.752891601582802,
        -0.7501318080779,
        -0.7473047401934707,
        -0.7395983190404529,
        -0.7411602662938357,
        -0.7533711826854839,
        -0.7271238445271395,
        -0.7343089530546131,
        -0.759700016160429,
        -0.7580979433954171,
        -0.7230467714534601,
        -0.7452922004227722,
        -0.7457608045317876,
        -0.7560550405390476,
        -0.7433610689664412,
        -0.7133429127884382,
        -0.6321530061747827,
        -0.7555157821773568,
        -0.7121325232777955,
        -0.7396736204694906,
        -0.7450675107934144,
        -0.7598970146534154,
        -0.7438094139268463,
        -0.7459563639601208,
        1.0,
        -0.7501141315508604,
        -0.7255271017204277,
        -0.6576090517233272,
        -0.7542244204305442,
        -0.753809893829777,
        -0.7528440889159881,
        -0.7159515212176538,
        -0.7479527688665133,
        -0.7122880873308179,
        -0.7594538650889424,
        -0.738874335360979,
        -0.7590717018368378,
        -0.7471216591825696,
        -0.7552723726646466,
        -0.7500846398581393,
        -0.7000037708943627,
        -0.7576677863351607,
        -0.7421763270908994,
        -0.7305551215204802,
        -0.710522992281825
      ],
      [
        -0.7576990395759771,
        -0.76209724951643,
        -0.7598828633419465,
        -0.7514294193934439,
        -0.7334346808030479,
        -0.759374976577353,
        -0.7402536742557886,
        -0.763159606452795,
        -0.7514612258127582,
        -0.7418793298262931,
        -0.7532958117124584,
        -0.7565726324449484,
        -0.7616217430161911,
        -0.749682118956204,
        -0.7567802707525308,
        -0.7589935939470702,
        -0.7560200901801353,
        -0.7595777403778825,
        -0.7605393529981015,
        -0.7593011854774365,
        -0.6309145866242053,
        -0.746368319837138,
        -0.7530382205826737,
        -0.7628180346859859,
        -0.6699424302897403,
        -0.6907747389599359,
        -0.7389597923653605,
        -0.7636826608508719,
        -0.7454111008100198,
        -0.7584189874054934,
        -0.7535350338743563,
        -0.7403232787652496,
        -0.751973805082003,
        -0.75158927354512,
        -0.756199431521242,
        -0.7631446009345151,
        -0.7582258180052077,
        -0.7468975013941374,
        -0.7501141315508604,
        1.0,
        -0.7423063240018482,
        -0.720007621000788,
        -0.7580302597308628,
        -0.6619997851183698,
        -0.7440452132539386,
        -0.7452063443574566,
        -0.7597922868814404,
        -0.7555148593543285,
        -0.7625998650108143,
        -0.7563452070190093,
        -0.7605354123147112,
        -0.7577678975320064,
        -0.7375471851194009,
        -0.7492695104051952,
        -0.7641461302197587,
        -0.7492319982333928,
        -0.7481627258284118,
        -0.7048651912279413,
        -0.7214529710977826
      ],
      [
        -0.6959706656526841,
        -0.7263006017771161,
        -0.6812330835140933,
        -0.7152545470084155,
        -0.7087273498237596,
        -0.732408169021932,
        -0.6837551555361124,
        -0.7086523878981328,
        -0.7065602320601356,
        -0.6975123632009596,
        -0.7161185574888156,
        -0.7302792657939506,
        -0.738556972699503,
        -0.7329724970915451,
        -0.7188990324821913,
        -0.7216092160839152,
        -0.6917755354265385,
        -0.6855369229814341,
        -0.7537145811111716,
        -0.7367160768386206,
        -0.7111467722129015,
        -0.7225539669016723,
        -0.7322368163061395,
        -0.74476551928003,
        -0.7261051982606903,
        -0.7314176883908623,
        -0.7328869425164743,
        -0.7324003492649411,
        -0.7081695404220442,
        -0.7390618167097727,
        -0.7104600186222797,
        -0.7471304396676173,
        -0.7250794620601839,
        -0.6889346633841542,
        -0.6988641892221015,
        -0.7435644774446928,
        -0.6881150135930505,
        -0.7009526586344439,
        -0.7255271017204277,
        -0.7423063240018482,
        1.0,
        -0.7166663488689871,
        -0.759079319779168,
        -0.7362051899715429,
        -0.7315299580467913,
        -0.7209376990923979,
        -0.7397360623613445,
        -0.7029636400635367,
        -0.7498433506804191,
        -0.7411797238721378,
        -0.7442920593675613,
        -0.6912523930869288,
        -0.7238362990610019,
        -0.7220316630770742,
        -0.7376923110727538,
        -0.7503262773266428,
        -0.7393107436337097,
        -0.7058421950903948,
        -0.7293930680873092
      ],
      [
        -0.7139549795566917,
        -0.7493084475251266,
        -0.7360828690468393,
        -0.7346599719936512,
        -0.7091889847854363,
        -0.725621088370155,
        -0.7235342492207761,
        -0.7544403787251972,
        -0.7111400545013069,
        -0.7368011277394851,
        -0.7216853868849193,
        -0.7423476838248362,
        -0.728457169514944,
        -0.7402439543519848,
        -0.7197034192225871,
        -0.7347275212402457,
        -0.7405507376057325,
        -0.7150684520977348,
        -0.7535121606984625,
        -0.7345934642259969,
        -0.7112330104157658,
        -0.7384335401189298,
        -0.7508097088708405,
        -0.7467676833661032,
        -0.7230674761354778,
        -0.6228648614527696,
        -0.7214890635221978,
        -0.7564146438866008,
        -0.7318768404720885,
        -0.744220394665996,
        -0.6977991581187274,
        -0.7433031785925953,
        -0.7379395222061887,
        -0.7307524252890409,
        -0.7381259229241934,
        -0.7555094112471882,
        -0.7262767587269358,
        -0.7329085181863222,
        -0.6576090517233272,
        -0.720007621000788,
        -0.7166663488689871,
        1.0,
        -0.7558993298229999,
        -0.7461075956878078,
        -0.7455556699466553,
        -0.7165666189058197,
        -0.7454711389783686,
        -0.7220911121495126,
        -0.756061073470814,
        -0.7215967660828378,
        -0.7474397790003638,
        -0.7440081954177129,
        -0.7373826637446692,
        -0.7363867837706908,
        -0.7363051104809413,
        -0.7526536350509341,
        -0.7315441013042295,
        -0.7175291941585078,
        -0.7281820459734378
      ],
      [
        -0.7623503540326865,
        -0.7622067350625832,
        -0.7634455602224209,
        -0.7611348289617397,
        -0.7413089830483481,
        -0.7487732327361383,
        -0.7538606854951444,
        -0.7634657005100376,
        -0.7522812641058534,
        -0.7617580880191077,
        -0.7484950689117537,
        -0.7557864321478902,
        -0.763445470268262,
        -0.7598933033270185,
        -0.7592340911767259,
        -0.761976512847535,
        -0.7620029999974034,
        -0.7613732749984604,
        -0.7579247767904774,
        -0.7605987618830283,
        -0.7577677828627786,
        -0.7600096787848605,
        -0.7633565741144481,
        -0.7626408893802628,
        -0.7534753369387897,
        -0.7598398830544448,
        -0.7614225776179263,
        -0.7644408186793268,
        -0.7518423311745717,
        -0.7609948566464475,
        -0.7574273375255827,
        -0.7637426317618474,
        -0.7623661972333207,
        -0.7583822710205543,
        -0.7589110052077724,
        -0.7632293400069062,
        -0.7625238081228586,
        -0.7614508562499724,
        -0.7542244204305442,
        -0.7580302597308628,
        -0.759079319779168,
        -0.7558993298229999,
        1.0,
        -0.7554880274855138,
        -0.7591780630346849,
        -0.7554547208796407,
        -0.7611028674869029,
        -0.7596997451983006,
        -0.7609464286905041,
        -0.7624835197541422,
        -0.7580986583435004,
        -0.7625981759509799,
        -0.7613617639499162,
        -0.7528002787342202,
        -0.7630084665597943,
        -0.7610685036534717,
        -0.7599958272590266,
        -0.7541766626095454,
        -0.7455325808675353
      ],
      [
        -0.7534591936124488,
        -0.7529867372178719,
        -0.760109130309464,
        -0.7588207361764592,
        -0.7556392263834821,
        -0.761525982644718,
        -0.6666257105104514,
        -0.7613004605881921,
        -0.7511336204777699,
        -0.7322571273331198,
        -0.7510750339016043,
        -0.7342014819480526,
        -0.7607317665578135,
        -0.7573767480049902,
        -0.7489135025830174,
        -0.7558516240232986,
        -0.7364575458712453,
        -0.7407201423986833,
        -0.7601312787754968,
        -0.7603919525223526,
        -0.7350601722788279,
        -0.7384392139664108,
        -0.7561765654865905,
        -0.761543299599974,
        -0.7427244518661755,
        -0.7373622023887723,
        -0.7489896760623072,
        -0.7626590718170903,
        -0.7306668878186404,
        -0.757500468252176,
        -0.7555687011928799,
        -0.7585201083593556,
        -0.7519619893252123,
        -0.7475307663868257,
        -0.7563686295315886,
        -0.7555630727973456,
        -0.7581000790403896,
        -0.6994877219589124,
        -0.753809893829777,
        -0.6619997851183698,
        -0.7362051899715429,
        -0.7461075956878078,
        -0.7554880274855138,
        1.0,
        -0.7264135008198226,
        -0.7509366886201169,
        -0.753250708216967,
        -0.7532284989570727,
        -0.7545664054471143,
        -0.7521945060545034,
        -0.754488383534484,
        -0.7529934902236333,
        -0.7441324963609011,
        -0.7295743680513542,
        -0.7637681842738754,
        -0.7532798719731371,
        -0.7461476813786414,
        -0.7285096731165657,
        -0.7511754672217328
      ],
      [
        -0.7457014357052528,
        -0.7486648360522851,
        -0.7468058120907495,
        -0.7479417068049925,
        -0.7493781282600385,
        -0.7449464903251367,
        -0.729345911297677,
        -0.7253017378128928,
        -0.7444707306433278,
        -0.747821910414969,
        -0.7458917806687494,
        -0.7401434858531422,
        -0.7595038674824229,
        -0.7546237230488422,
        -0.7292024597165738,
        -0.7322480726792465,
        -0.7210125712817266,
        -0.7377577396510752,
        -0.7618618575098781,
        -0.7526861408455907,
        -0.7198313344545322,
        -0.7351507971434683,
        -0.7388630474825855,
        -0.7597935421691024,
        -0.7350608317471213,
        -0.7281112949585369,
        -0.7336008865772417,
        -0.7554858619111942,
        -0.7146904071721222,
        -0.7581632797827078,
        -0.7531849566997071,
        -0.7528399335873253,
        -0.7453531668474772,
        -0.7405102570646894,
        -0.7409800182360203,
        -0.7278052258806833,
        -0.7470663590590805,
        -0.7339489692884769,
        -0.7528440889159881,
        -0.7440452132539386,
        -0.7315299580467913,
        -0.7455556699466553,
        -0.7591780630346849,
        -0.7264135008198226,
        1.0,
        -0.7336417878553513,
        -0.7166703682332968,
        -0.7472072736553397,
        -0.729172887796501,
        -0.7464983474952464,
        -0.7467705788742518,
        -0.7462793405200637,
        -0.7404528420640751,
        -0.7192890688638388,
        -0.7624516247084463,
        -0.7515044350908755,
        -0.7520266404987619,
        -0.7121611576974807,
        -0.7481188369492736
      ],
      [
        -0.7442019541457935,
        -0.7364558597826842,
        -0.7500229070614093,
        -0.6423016996964216,
        -0.45407938204027676,
        -0.7445429776180723,
        -0.7205477873707684,
        -0.7466924999173674,
        -0.36846087074328165,
        -0.7396608947336073,
        -0.40103502606336305,
        -0.7060996905132038,
        -0.7487095359050822,
        -0.7529379743367077,
        -0.7480305574919502,
        -0.748812205451841,
        -0.742424869057406,
        -0.7356808819082669,
        -0.7331371026050995,
        -0.7382573616351877,
        -0.7161470109044714,
        -0.7387180850236262,
        -0.7537143695500725,
        -0.7555668085566041,
        -0.564394094924722,
        -0.7300689348045166,
        -0.7398205821094554,
        -0.7451342402373038,
        -0.6982372155052043,
        -0.7269920954339002,
        -0.7096018757863416,
        -0.7501841322018661,
        -0.7353955651100925,
        -0.7310475330132845,
        -0.7361860524208685,
        -0.7467467129042533,
        -0.728452782008178,
        -0.736074179060846,
        -0.7159515212176538,
        -0.7452063443574566,
        -0.7209376990923979,
        -0.7165666189058197,
        -0.7554547208796407,
        -0.7509366886201169,
        -0.7336417878553513,
        1.0,
        -0.7195985480018541,
        -0.7164172322988662,
        -0.7514860170135713,
        -0.7502338992566331,
        -0.7449223328394265,
        -0.747426632401831,
        -0.7326895758902927,
        -0.7399794345770987,
        -0.7352246239843836,
        -0.751470883303832,
        -0.7391471843681039,
        -0.7190049863128054,
        -0.6860270473787218
      ],
      [
        -0.7557455971896396,
        -0.7567539220299346,
        -0.7541808872699136,
        -0.7332318786614993,
        -0.7408838236460304,
        -0.7435240839638413,
        -0.6564369324808849,
        -0.7500100481262839,
        -0.7375104438717203,
        -0.7414539896624098,
        -0.7431320296144331,
        -0.7507385434244468,
        -0.7600275660173157,
        -0.7363628061615317,
        -0.6400907845594155,
        -0.7586981960645618,
        -0.7416240581834943,
        -0.7468265625464008,
        -0.7587144727619267,
        -0.7523304866489168,
        -0.7377838421192437,
        -0.7297083006770081,
        -0.7574040831396747,
        -0.7606961587942898,
        -0.7453158457550213,
        -0.7416786868621352,
        -0.7522534007939914,
        -0.7496689150626223,
        -0.7387902983324486,
        -0.7492836022412677,
        -0.750856044152403,
        -0.7632803098605399,
        -0.746406310302222,
        -0.7435499156871657,
        -0.7516589696899949,
        -0.5117395202633434,
        -0.7548606626862266,
        -0.7192461101136063,
        -0.7479527688665133,
        -0.7597922868814404,
        -0.7397360623613445,
        -0.7454711389783686,
        -0.7611028674869029,
        -0.753250708216967,
        -0.7166703682332968,
        -0.7195985480018541,
        1.0,
        -0.7516591350218123,
        -0.7376295963895051,
        -0.754327689253044,
        -0.7370833736902991,
        -0.7508038829235426,
        -0.7411512990496179,
        -0.723869548121622,
        -0.7599319184458995,
        -0.7366146714088886,
        -0.7509098844449388,
        -0.7238855321932838,
        -0.7535360130588422
      ],
      [
        -0.6622559235106206,
        -0.7344183529949384,
        -0.7314571755413428,
        -0.6987250630731847,
        -0.6975282991170542,
        -0.7505514738878633,
        -0.7270493675009924,
        -0.7542696715275743,
        -0.6642379524095037,
        -0.7265432354805883,
        -0.7140171870181498,
        -0.7262790268251925,
        -0.6690151915039246,
        -0.7463227792624727,
        -0.7500079366067418,
        -0.7312795887128472,
        -0.7337700402634977,
        -0.7082643452282422,
        -0.7331290344354487,
        -0.7426796832645373,
        -0.7221187505667417,
        -0.7339571973990635,
        -0.7486990170369531,
        -0.7440675085259407,
        -0.7246628583146674,
        -0.7404215179198851,
        -0.7388953325938147,
        -0.7487406263448114,
        -0.7289797870193395,
        -0.7149966670492072,
        -0.5660356501693404,
        -0.7441269302406541,
        -0.6952281227236901,
        -0.7232663579582596,
        -0.7200014731159483,
        -0.7591237876199015,
        -0.7309180709814365,
        -0.733241262879158,
        -0.7122880873308179,
        -0.7555148593543285,
        -0.7029636400635367,
        -0.7220911121495126,
        -0.7596997451983006,
        -0.7532284989570727,
        -0.7472072736553397,
        -0.7164172322988662,
        -0.7516591350218123,
        1.0,
        -0.7560257971454892,
        -0.7432679716809132,
        -0.7566420127650695,
        -0.7311506753217472,
        -0.7413162291228685,
        -0.7379933267668948,
        -0.6900267027115667,
        -0.7575622502222512,
        -0.7227448497055731,
        -0.7223071902720786,
        -0.7320808519206274
      ],
      [
        -0.7586927386268972,
        -0.7593151108423724,
        -0.7593116585155415,
        -0.7529913318427772,
        -0.7567412612579072,
        -0.7617474602715872,
        -0.7473933189477269,
        -0.7534966668846941,
        -0.75331160189735,
        -0.756274063108201,
        -0.7540854963276553,
        -0.7473366102548782,
        -0.7585488219408517,
        -0.7597330474068332,
        -0.7057228922237291,
        -0.7554988873902039,
        -0.7559282961315341,
        -0.7520199295746886,
        -0.7626928489148854,
        -0.7613973434000163,
        -0.7406836778450734,
        -0.7590834910685504,
        -0.7597297509262946,
        -0.7622324393495307,
        -0.7500048134607105,
        -0.7548852419138786,
        -0.7577293193306429,
        -0.7628576185241287,
        -0.7421913589934885,
        -0.7594013856584554,
        -0.7589886474950562,
        -0.761634663702739,
        -0.7555750532678346,
        -0.7569659163912039,
        -0.7579006961931609,
        -0.5890101569429711,
        -0.7588108505049367,
        -0.7542640159009474,
        -0.7594538650889424,
        -0.7625998650108143,
        -0.7498433506804191,
        -0.756061073470814,
        -0.7609464286905041,
        -0.7545664054471143,
        -0.729172887796501,
        -0.7514860170135713,
        -0.7376295963895051,
        -0.7560257971454892,
        1.0,
        -0.7617832885209082,
        -0.7473575720714865,
        -0.7588621151306227,
        -0.7555748540453591,
        -0.7354908796912496,
        -0.7615798812769545,
        -0.7553507506526007,
        -0.7556724067368856,
        -0.7330553676062701,
        -0.755480329033273
      ],
      [
        -0.7384792808656772,
        -0.751977497697913,
        -0.7511235720630023,
        -0.7534755266542412,
        -0.7494829197721907,
        -0.7448998196401344,
        -0.7479016616159417,
        -0.7546590181447079,
        -0.7491999645757806,
        -0.7554881757763944,
        -0.7513289695054103,
        -0.7517759406949336,
        -0.7472575834948922,
        -0.7540110962975999,
        -0.7542451267200545,
        -0.7433113463835908,
        -0.7511683758941188,
        -0.726371304684938,
        -0.7585980034360753,
        -0.7560290837069159,
        -0.7481239458731828,
        -0.748085773801257,
        -0.7515641772579446,
        -0.7556199530407076,
        -0.7414574947619326,
        -0.7548260339233765,
        -0.7459671055874467,
        -0.7540806886168838,
        -0.7449955025097239,
        -0.7554773176920822,
        -0.7331740139637692,
        -0.7580668814101446,
        -0.7463970169955229,
        -0.7483125531964545,
        -0.7523509055188029,
        -0.7606551716015704,
        -0.7501064812367694,
        -0.7462833953024364,
        -0.738874335360979,
        -0.7563452070190093,
        -0.7411797238721378,
        -0.7215967660828378,
        -0.7624835197541422,
        -0.7521945060545034,
        -0.7464983474952464,
        -0.7502338992566331,
        -0.754327689253044,
        -0.7432679716809132,
        -0.7617832885209082,
        1.0,
        -0.7574580375368086,
        -0.7512859597375657,
        -0.7490949644822482,
        -0.7457347824965954,
        -0.7508432825930792,
        -0.7569858554521794,
        -0.7534048511631789,
        -0.7443880635119875,
        -0.7476325888803226
      ],
      [
        -0.7566430402769134,
        -0.7539056287856765,
        -0.756704004270286,
        -0.755649318835051,
        -0.7560455990805344,
        -0.76286745751655,
        -0.7212898717675724,
        -0.7607660312043735,
        -0.7541750722024494,
        -0.7581612239355907,
        -0.7498678470715656,
        -0.7532598045406842,
        -0.7615380349006909,
        -0.7523249214263579,
        -0.7078325200950277,
        -0.754037340907888,
        -0.7567669156213103,
        -0.7521863002921014,
        -0.7577027533252794,
        -0.7511897515979286,
        -0.7389638894113602,
        -0.7568345266750914,
        -0.7595401761991993,
        -0.7609580297325595,
        -0.7379381151936882,
        -0.7513430769093103,
        -0.7521565891324414,
        -0.7574639743027698,
        -0.7295611352117914,
        -0.7605976402939834,
        -0.7541734077011891,
        -0.760785195317446,
        -0.7537422913694455,
        -0.7468259416473241,
        -0.7556575758050021,
        -0.7514333296707093,
        -0.752388019023167,
        -0.746523016699845,
        -0.7590717018368378,
        -0.7605354123147112,
        -0.7442920593675613,
        -0.7474397790003638,
        -0.7580986583435004,
        -0.754488383534484,
        -0.7467705788742518,
        -0.7449223328394265,
        -0.7370833736902991,
        -0.7566420127650695,
        -0.7473575720714865,
        -0.7574580375368086,
        1.0,
        -0.7563563655246023,
        -0.7424047321304137,
        -0.7106583302076557,
        -0.7637185262499051,
        -0.7532074763083102,
        -0.7489463466277193,
        -0.7187596949550794,
        -0.7537990647758953
      ],
      [
        -0.7145672326783492,
        -0.7456950177071966,
        -0.6770766872704532,
        -0.7364316157340197,
        -0.744958177281442,
        -0.7467648751111479,
        -0.7286638528285816,
        -0.7386498176699314,
        -0.737880046429148,
        -0.7256979550786407,
        -0.7314488167159601,
        -0.7477914563427888,
        -0.7465939897374494,
        -0.7525948260200516,
        -0.745874560707932,
        -0.7417326195762153,
        -0.6763669651402835,
        -0.7178522330834576,
        -0.759038989531102,
        -0.7531813366134247,
        -0.734271684566014,
        -0.7379893290385524,
        -0.7486039885993099,
        -0.7565039439191611,
        -0.7483426509432545,
        -0.7495537067628527,
        -0.7491578998054943,
        -0.7262174219196714,
        -0.7415034567101776,
        -0.7518825234072524,
        -0.7279375197203029,
        -0.7567698660981987,
        -0.744265038820388,
        -0.7081521757582931,
        -0.7231316976777986,
        -0.7578672051917941,
        -0.7004706103483117,
        -0.724399604786758,
        -0.7471216591825696,
        -0.7577678975320064,
        -0.6912523930869288,
        -0.7440081954177129,
        -0.7625981759509799,
        -0.7529934902236333,
        -0.7462793405200637,
        -0.747426632401831,
        -0.7508038829235426,
        -0.7311506753217472,
        -0.7588621151306227,
        -0.7512859597375657,
        -0.7563563655246023,
        1.0,
        -0.7455177672473972,
        -0.7430249101443632,
        -0.7493243920939581,
        -0.7571769047431358,
        -0.7522498079548722,
        -0.7329221437244988,
        -0.7507115946433043
      ],
      [
        -0.7309772388563243,
        -0.7335443948343526,
        -0.6833677967596562,
        -0.7513152894881558,
        -0.7472880964560631,
        -0.7391901404699527,
        -0.7206516211816759,
        -0.7479401605821372,
        -0.7439269798730661,
        -0.7272712706339128,
        -0.7335066390726043,
        -0.7380741971809294,
        -0.75741018634145,
        -0.7463182880020813,
        -0.6994035772045103,
        -0.7280220145564239,
        -0.7201896033341082,
        -0.7487375922278403,
        -0.7564921572374737,
        -0.6828695702749796,
        -0.7354044893772409,
        -0.746247627995968,
        -0.7311166103840139,
        -0.7186839293573173,
        -0.7195563631067722,
        -0.7445455480760231,
        -0.729056534464072,
        -0.7506670395821111,
        -0.7182549487197347,
        -0.754551882839003,
        -0.7363279855720609,
        -0.7390717228820616,
        -0.7499686873089235,
        -0.7078622720017054,
        -0.7384484547126542,
        -0.7580334178340778,
        -0.680922094352451,
        -0.7415962680982927,
        -0.7552723726646466,
        -0.7375471851194009,
        -0.7238362990610019,
        -0.7373826637446692,
        -0.7613617639499162,
        -0.7441324963609011,
        -0.7404528420640751,
        -0.7326895758902927,
        -0.7411512990496179,
        -0.7413162291228685,
        -0.7555748540453591,
        -0.7490949644822482,
        -0.7424047321304137,
        -0.7455177672473972,
        1.0,
        -0.7322831977793407,
        -0.7608522676129874,
        -0.7453203177240596,
        -0.7424568232278572,
        -0.7296396954367887,
        -0.7421476438062611
      ],
      [
        -0.7585917689209265,
        -0.7390898383559712,
        -0.7414077659540987,
        -0.7310183672651809,
        -0.7520922187101851,
        -0.7583280206542804,
        -0.6903233706462841,
        -0.7325791303190761,
        -0.7445649204592797,
        -0.7386080854022574,
        -0.7441227728566269,
        -0.7380983019447129,
        -0.7555814684054412,
        -0.7525288852106329,
        -0.716716061025914,
        -0.7434383181550184,
        -0.7270813324058844,
        -0.7233596295091086,
        -0.756242623137891,
        -0.7443129158474824,
        -0.7008510702247184,
        -0.726789989651271,
        -0.7049056775036308,
        -0.7547961259663738,
        -0.7352806783721244,
        -0.724509313797177,
        -0.7303369677712582,
        -0.7554459870941161,
        -0.229981438884494,
        -0.7525517382170661,
        -0.7499777633295595,
        -0.7409319115723125,
        -0.733064192949509,
        -0.7230676736963264,
        -0.740870778327161,
        -0.7402610984798088,
        -0.7116056810036862,
        -0.6910096727303213,
        -0.7500846398581393,
        -0.7492695104051952,
        -0.7220316630770742,
        -0.7363867837706908,
        -0.7528002787342202,
        -0.7295743680513542,
        -0.7192890688638388,
        -0.7399794345770987,
        -0.723869548121622,
        -0.7379933267668948,
        -0.7354908796912496,
        -0.7457347824965954,
        -0.7106583302076557,
        -0.7430249101443632,
        -0.7322831977793407,
        1.0,
        -0.7590689263827729,
        -0.7425880908117264,
        -0.7429730698159227,
        -0.0913903901198243,
        -0.7251420657569702
      ],
      [
        -0.6945722864369113,
        -0.7301564516181032,
        -0.7565796974341163,
        -0.7453790710138352,
        -0.7352038383261106,
        -0.76396655215146,
        -0.756448949029042,
        -0.7633404563395849,
        -0.6762374594565304,
        -0.747753408017129,
        -0.7400630680690427,
        -0.7459657699859958,
        -0.5473539256683142,
        -0.7509943275381405,
        -0.7616963159367711,
        -0.7520104434388968,
        -0.756983936003736,
        -0.7330175433066208,
        -0.7415332216020468,
        -0.7556164417573017,
        -0.7451814526526019,
        -0.7549377882190484,
        -0.7627101020694818,
        -0.7503190569710629,
        -0.7491809005615333,
        -0.7602026460152839,
        -0.7561277995288083,
        -0.7599759922655669,
        -0.7521059457488093,
        -0.7165318022913578,
        -0.7099110612147816,
        -0.7567589137726602,
        -0.7111613119363066,
        -0.7559595100391565,
        -0.7407087213556083,
        -0.7631751966214975,
        -0.7466344699835172,
        -0.7583491228415784,
        -0.7000037708943627,
        -0.7641461302197587,
        -0.7376923110727538,
        -0.7363051104809413,
        -0.7630084665597943,
        -0.7637681842738754,
        -0.7624516247084463,
        -0.7352246239843836,
        -0.7599319184458995,
        -0.6900267027115667,
        -0.7615798812769545,
        -0.7508432825930792,
        -0.7637185262499051,
        -0.7493243920939581,
        -0.7608522676129874,
        -0.7590689263827729,
        1.0,
        -0.7610471616328749,
        -0.7413248908885651,
        -0.7470798825085498,
        -0.7586598654966712
      ],
      [
        -0.7571629972942076,
        -0.7547231350027221,
        -0.7533504359409939,
        -0.7543816055871514,
        -0.7545718269710908,
        -0.7518338506861415,
        -0.6278375052789422,
        -0.7522504799518193,
        -0.7554206413268993,
        -0.7571653701535632,
        -0.7527296848001204,
        -0.7577262767770645,
        -0.759507091081342,
        -0.7543416949901613,
        -0.6976422044694355,
        -0.7545536240469093,
        -0.7556997124690895,
        -0.7539262545491898,
        -0.7605449910608642,
        -0.7591500898679783,
        -0.7370211636664272,
        -0.756788889368964,
        -0.6276278124023352,
        -0.7611824418902857,
        -0.7523491727913653,
        -0.7462801774958407,
        -0.7583568249667189,
        -0.7622354154144761,
        -0.7464471800121091,
        -0.7599294518773068,
        -0.7563674219013398,
        -0.7543256681222887,
        -0.7526488294950827,
        -0.751015481471288,
        -0.7543729219632235,
        -0.7612252485337085,
        -0.7525852956156005,
        -0.7482433455379196,
        -0.7576677863351607,
        -0.7492319982333928,
        -0.7503262773266428,
        -0.7526536350509341,
        -0.7610685036534717,
        -0.7532798719731371,
        -0.7515044350908755,
        -0.751470883303832,
        -0.7366146714088886,
        -0.7575622502222512,
        -0.7553507506526007,
        -0.7569858554521794,
        -0.7532074763083102,
        -0.7571769047431358,
        -0.7453203177240596,
        -0.7425880908117264,
        -0.7610471616328749,
        1.0,
        -0.7529916141391342,
        -0.7386715884783506,
        -0.758258857522279
      ],
      [
        -0.739654340294505,
        -0.7508906471945838,
        -0.74710842884542,
        -0.7343716873667248,
        -0.7472927242087535,
        -0.7539385389056599,
        -0.7440105396164314,
        -0.7589742455778086,
        -0.7325569752811075,
        -0.7520440272280219,
        -0.7364484475889728,
        -0.7520789420726821,
        -0.7374683340067363,
        -0.7457582839669954,
        -0.7313739769522551,
        -0.7447989470979357,
        -0.751566862110941,
        -0.7403979309699392,
        -0.7567819056723214,
        -0.7225300797298729,
        -0.7365618163801612,
        -0.7515060129571309,
        -0.7511297961734221,
        -0.7469584069099967,
        -0.7394405927398483,
        -0.7435633961892766,
        -0.7496314862086713,
        -0.7613531687156602,
        -0.7369751850129689,
        -0.7498563522030178,
        -0.7274421462415022,
        -0.7423795516222059,
        -0.7259017851883753,
        -0.7325616223434546,
        -0.7441870299955993,
        -0.7598736331204675,
        -0.7430811593355846,
        -0.749700241538201,
        -0.7421763270908994,
        -0.7481627258284118,
        -0.7393107436337097,
        -0.7315441013042295,
        -0.7599958272590266,
        -0.7461476813786414,
        -0.7520266404987619,
        -0.7391471843681039,
        -0.7509098844449388,
        -0.7227448497055731,
        -0.7556724067368856,
        -0.7534048511631789,
        -0.7489463466277193,
        -0.7522498079548722,
        -0.7424568232278572,
        -0.7429730698159227,
        -0.7413248908885651,
        -0.7529916141391342,
        1.0,
        -0.7360956870280351,
        -0.7520456161493886
      ],
      [
        -0.7353057008595583,
        -0.7344311242342452,
        -0.7299214679609087,
        -0.7195544977007302,
        -0.7171291800255513,
        -0.753745864083939,
        -0.7004533004517776,
        -0.7378895215944614,
        -0.7162161380693157,
        -0.7263616302255711,
        -0.7211855537906903,
        -0.726312423364643,
        -0.7358543946044779,
        -0.7308760705450819,
        -0.7116911300551816,
        -0.7345596640505595,
        -0.7252646292112852,
        -0.7246171225810712,
        -0.7494201543078511,
        -0.7372230955000636,
        0.27924887046660907,
        -0.7314301273599152,
        -0.7231382165445619,
        -0.754812809598346,
        -0.6985501039109794,
        -0.6964701295871937,
        -0.7254378455475505,
        -0.7495836387410714,
        -0.4284468634787466,
        -0.7360178737811448,
        -0.7289477639519978,
        -0.7092431841493025,
        -0.7164889256525608,
        -0.7138777570713628,
        -0.7295140778485056,
        -0.7369893742159705,
        -0.7125180049012387,
        -0.7097942716460425,
        -0.7305551215204802,
        -0.7048651912279413,
        -0.7058421950903948,
        -0.7175291941585078,
        -0.7541766626095454,
        -0.7285096731165657,
        -0.7121611576974807,
        -0.7190049863128054,
        -0.7238855321932838,
        -0.7223071902720786,
        -0.7330553676062701,
        -0.7443880635119875,
        -0.7187596949550794,
        -0.7329221437244988,
        -0.7296396954367887,
        -0.0913903901198243,
        -0.7470798825085498,
        -0.7386715884783506,
        -0.7360956870280351,
        1.0,
        -0.7249449269475394
      ],
      [
        -0.740756816920159,
        -0.7581228627154506,
        -0.7543203827737386,
        -0.7386351975403087,
        -0.5520243119719974,
        -0.7384738399213695,
        -0.7349453796505505,
        -0.7602110080248056,
        -0.7110821158533972,
        -0.7245895275069383,
        -0.72369171966461,
        -0.7434079609561952,
        -0.7563326893536795,
        -0.7564353097417065,
        -0.7584055018021878,
        -0.7591004883860522,
        -0.747831991164003,
        -0.7521215429701795,
        -0.7288799469603139,
        -0.755067742580139,
        -0.7326834611947127,
        -0.7167711225959041,
        -0.754834760943125,
        -0.7628344499686429,
        -0.6450544031595434,
        -0.7359849498949367,
        -0.7406346769026573,
        -0.7607596130253242,
        -0.7285123197604298,
        -0.7487631510893062,
        -0.7261994326152199,
        -0.7606362302346937,
        -0.7424241892529019,
        -0.7431808281448258,
        -0.7502678597182191,
        -0.7587490241592084,
        -0.7491555344925582,
        -0.7387252492667922,
        -0.710522992281825,
        -0.7214529710977826,
        -0.7293930680873092,
        -0.7281820459734378,
        -0.7455325808675353,
        -0.7511754672217328,
        -0.7481188369492736,
        -0.6860270473787218,
        -0.7535360130588422,
        -0.7320808519206274,
        -0.755480329033273,
        -0.7476325888803226,
        -0.7537990647758953,
        -0.7507115946433043,
        -0.7421476438062611,
        -0.7251420657569702,
        -0.7586598654966712,
        -0.758258857522279,
        -0.7520456161493886,
        -0.7249449269475394,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        121,
        23,
        8,
        49,
        24,
        10,
        51,
        48,
        70,
        19,
        10,
        18,
        32,
        14,
        18,
        40,
        13,
        20,
        19,
        15,
        52,
        23,
        28,
        13,
        23,
        15,
        25,
        10,
        13,
        34,
        39,
        11,
        34,
        37,
        28,
        5,
        22,
        23,
        38,
        5,
        54,
        23,
        0,
        12,
        10,
        16,
        8,
        16,
        6,
        18,
        3,
        16,
        27,
        28,
        15,
        2,
        31,
        2,
        21
      ],
      "2020-02": [
        202,
        40,
        14,
        91,
        19,
        18,
        29,
        64,
        158,
        37,
        18,
        25,
        96,
        28,
        18,
        45,
        22,
        58,
        26,
        14,
        78,
        40,
        25,
        32,
        39,
        26,
        22,
        6,
        8,
        62,
        61,
        20,
        56,
        46,
        44,
        12,
        62,
        33,
        77,
        5,
        80,
        47,
        1,
        5,
        11,
        26,
        19,
        32,
        7,
        25,
        15,
        39,
        25,
        31,
        47,
        9,
        31,
        3,
        17
      ],
      "2020-03": [
        121,
        27,
        12,
        81,
        15,
        11,
        35,
        31,
        100,
        31,
        18,
        20,
        54,
        20,
        22,
        36,
        26,
        43,
        16,
        18,
        72,
        54,
        25,
        15,
        36,
        37,
        34,
        16,
        9,
        25,
        45,
        23,
        43,
        42,
        36,
        9,
        41,
        48,
        47,
        9,
        76,
        29,
        0,
        7,
        14,
        15,
        18,
        22,
        4,
        24,
        9,
        46,
        25,
        44,
        19,
        29,
        25,
        3,
        16
      ],
      "2020-04": [
        165,
        23,
        23,
        51,
        18,
        15,
        49,
        50,
        79,
        33,
        15,
        21,
        34,
        21,
        33,
        51,
        30,
        33,
        17,
        21,
        70,
        44,
        33,
        13,
        47,
        22,
        30,
        23,
        16,
        39,
        37,
        12,
        48,
        56,
        47,
        14,
        37,
        51,
        34,
        8,
        80,
        30,
        0,
        16,
        25,
        17,
        21,
        23,
        8,
        22,
        7,
        52,
        28,
        48,
        22,
        63,
        24,
        5,
        22
      ],
      "2020-05": [
        132,
        21,
        20,
        56,
        22,
        17,
        34,
        72,
        82,
        26,
        16,
        24,
        26,
        19,
        26,
        56,
        25,
        38,
        19,
        27,
        70,
        39,
        30,
        30,
        41,
        33,
        30,
        9,
        15,
        33,
        43,
        24,
        50,
        52,
        47,
        13,
        43,
        50,
        45,
        11,
        79,
        22,
        1,
        6,
        18,
        20,
        20,
        24,
        7,
        21,
        2,
        34,
        25,
        53,
        18,
        50,
        28,
        3,
        18
      ],
      "2020-06": [
        285,
        50,
        14,
        157,
        45,
        17,
        47,
        64,
        234,
        50,
        36,
        19,
        115,
        40,
        35,
        76,
        28,
        77,
        27,
        28,
        92,
        78,
        28,
        50,
        63,
        31,
        38,
        35,
        12,
        93,
        103,
        23,
        94,
        68,
        37,
        13,
        73,
        49,
        101,
        15,
        123,
        65,
        0,
        15,
        10,
        25,
        42,
        49,
        7,
        37,
        10,
        66,
        37,
        50,
        61,
        54,
        49,
        0,
        32
      ],
      "2020-07": [
        178,
        58,
        12,
        94,
        20,
        14,
        38,
        79,
        129,
        48,
        16,
        21,
        66,
        33,
        33,
        66,
        24,
        67,
        23,
        17,
        66,
        59,
        38,
        30,
        41,
        35,
        43,
        22,
        9,
        37,
        61,
        21,
        53,
        56,
        30,
        7,
        55,
        56,
        52,
        10,
        88,
        57,
        0,
        13,
        15,
        24,
        28,
        34,
        8,
        29,
        7,
        59,
        27,
        49,
        26,
        51,
        38,
        5,
        20
      ],
      "2020-08": [
        150,
        41,
        11,
        53,
        12,
        14,
        36,
        90,
        56,
        22,
        13,
        14,
        30,
        18,
        19,
        62,
        16,
        40,
        17,
        15,
        54,
        46,
        30,
        15,
        35,
        26,
        33,
        7,
        14,
        47,
        37,
        17,
        40,
        49,
        25,
        10,
        36,
        52,
        36,
        11,
        65,
        29,
        0,
        7,
        12,
        14,
        23,
        18,
        13,
        22,
        12,
        36,
        32,
        39,
        15,
        50,
        22,
        3,
        9
      ],
      "2020-09": [
        138,
        41,
        11,
        77,
        18,
        9,
        33,
        45,
        87,
        17,
        20,
        14,
        41,
        23,
        27,
        51,
        18,
        26,
        14,
        28,
        67,
        33,
        22,
        26,
        33,
        24,
        19,
        14,
        11,
        39,
        36,
        25,
        62,
        45,
        33,
        10,
        48,
        40,
        46,
        7,
        57,
        22,
        1,
        13,
        15,
        20,
        26,
        25,
        9,
        21,
        8,
        41,
        21,
        50,
        16,
        55,
        25,
        3,
        18
      ],
      "2020-10": [
        256,
        43,
        32,
        133,
        20,
        21,
        45,
        113,
        139,
        44,
        13,
        25,
        84,
        48,
        37,
        54,
        29,
        71,
        25,
        21,
        100,
        60,
        30,
        50,
        60,
        26,
        38,
        21,
        17,
        70,
        66,
        30,
        56,
        67,
        60,
        17,
        65,
        48,
        87,
        12,
        104,
        68,
        1,
        15,
        22,
        26,
        37,
        36,
        9,
        27,
        4,
        70,
        36,
        45,
        36,
        58,
        44,
        7,
        19
      ],
      "2020-11": [
        193,
        46,
        9,
        87,
        24,
        17,
        46,
        56,
        111,
        21,
        22,
        18,
        65,
        30,
        33,
        44,
        21,
        49,
        29,
        32,
        77,
        61,
        31,
        30,
        39,
        22,
        31,
        19,
        10,
        44,
        44,
        21,
        57,
        52,
        41,
        9,
        47,
        38,
        50,
        10,
        76,
        37,
        1,
        12,
        13,
        14,
        34,
        33,
        9,
        31,
        8,
        41,
        34,
        40,
        17,
        47,
        29,
        2,
        23
      ],
      "2020-12": [
        160,
        50,
        15,
        86,
        23,
        23,
        41,
        37,
        113,
        21,
        19,
        17,
        57,
        26,
        32,
        51,
        20,
        47,
        30,
        24,
        59,
        47,
        26,
        32,
        48,
        23,
        32,
        17,
        13,
        48,
        43,
        20,
        58,
        42,
        27,
        9,
        45,
        44,
        50,
        13,
        96,
        47,
        1,
        16,
        15,
        17,
        24,
        28,
        12,
        37,
        10,
        40,
        38,
        51,
        26,
        54,
        39,
        3,
        18
      ],
      "2021-01": [
        115,
        32,
        10,
        58,
        16,
        14,
        30,
        34,
        77,
        15,
        8,
        17,
        35,
        19,
        22,
        41,
        20,
        26,
        24,
        17,
        85,
        32,
        32,
        19,
        35,
        24,
        25,
        15,
        18,
        38,
        31,
        16,
        34,
        50,
        29,
        9,
        39,
        26,
        33,
        16,
        63,
        28,
        0,
        11,
        22,
        5,
        23,
        22,
        4,
        27,
        6,
        33,
        32,
        42,
        16,
        40,
        27,
        4,
        15
      ],
      "2021-02": [
        213,
        62,
        13,
        96,
        25,
        15,
        40,
        67,
        122,
        33,
        16,
        29,
        99,
        36,
        30,
        43,
        14,
        52,
        35,
        19,
        84,
        28,
        25,
        36,
        45,
        15,
        25,
        19,
        15,
        35,
        55,
        21,
        57,
        58,
        29,
        17,
        49,
        44,
        70,
        11,
        76,
        63,
        0,
        5,
        16,
        22,
        17,
        37,
        7,
        30,
        6,
        48,
        27,
        31,
        36,
        33,
        24,
        4,
        19
      ],
      "2021-03": [
        172,
        50,
        10,
        82,
        17,
        11,
        51,
        55,
        110,
        23,
        14,
        19,
        50,
        36,
        27,
        44,
        20,
        58,
        26,
        24,
        76,
        61,
        35,
        31,
        48,
        22,
        39,
        23,
        18,
        35,
        42,
        19,
        55,
        76,
        34,
        11,
        57,
        41,
        44,
        9,
        80,
        42,
        0,
        9,
        19,
        24,
        24,
        24,
        8,
        20,
        5,
        45,
        53,
        41,
        26,
        28,
        28,
        3,
        25
      ],
      "2021-04": [
        157,
        45,
        14,
        82,
        28,
        10,
        35,
        84,
        85,
        29,
        15,
        19,
        26,
        16,
        35,
        40,
        18,
        44,
        23,
        28,
        73,
        49,
        29,
        22,
        44,
        33,
        26,
        19,
        19,
        29,
        39,
        15,
        49,
        49,
        51,
        12,
        45,
        31,
        38,
        8,
        85,
        36,
        2,
        12,
        17,
        19,
        21,
        32,
        5,
        40,
        8,
        42,
        38,
        47,
        12,
        53,
        25,
        3,
        23
      ],
      "2021-05": [
        146,
        38,
        13,
        98,
        25,
        18,
        35,
        63,
        102,
        34,
        20,
        21,
        50,
        31,
        22,
        44,
        18,
        42,
        20,
        31,
        71,
        48,
        27,
        33,
        32,
        24,
        32,
        22,
        15,
        56,
        49,
        24,
        39,
        50,
        29,
        13,
        40,
        36,
        54,
        9,
        68,
        44,
        0,
        12,
        16,
        15,
        25,
        32,
        10,
        20,
        5,
        39,
        43,
        34,
        18,
        43,
        19,
        4,
        32
      ],
      "2021-06": [
        240,
        78,
        19,
        166,
        34,
        22,
        36,
        75,
        146,
        42,
        22,
        22,
        93,
        52,
        24,
        52,
        32,
        83,
        30,
        51,
        93,
        50,
        30,
        49,
        64,
        30,
        28,
        32,
        23,
        61,
        74,
        28,
        63,
        72,
        54,
        9,
        65,
        48,
        77,
        14,
        126,
        65,
        1,
        14,
        17,
        26,
        33,
        47,
        8,
        42,
        11,
        67,
        41,
        52,
        43,
        35,
        38,
        6,
        38
      ],
      "2021-07": [
        171,
        65,
        6,
        72,
        20,
        12,
        36,
        62,
        96,
        30,
        20,
        20,
        44,
        34,
        31,
        47,
        20,
        44,
        23,
        30,
        76,
        40,
        25,
        23,
        45,
        20,
        26,
        16,
        8,
        36,
        41,
        27,
        51,
        44,
        20,
        12,
        42,
        46,
        56,
        9,
        99,
        54,
        0,
        13,
        15,
        21,
        26,
        29,
        10,
        35,
        3,
        44,
        39,
        34,
        23,
        27,
        28,
        4,
        18
      ],
      "2021-08": [
        113,
        50,
        12,
        74,
        20,
        15,
        32,
        38,
        62,
        22,
        23,
        12,
        32,
        19,
        25,
        45,
        17,
        38,
        24,
        19,
        46,
        32,
        29,
        25,
        51,
        36,
        31,
        18,
        15,
        32,
        32,
        17,
        49,
        45,
        19,
        5,
        30,
        32,
        25,
        8,
        57,
        26,
        0,
        9,
        12,
        8,
        17,
        23,
        9,
        24,
        6,
        24,
        38,
        41,
        6,
        29,
        32,
        2,
        15
      ],
      "2021-09": [
        169,
        47,
        18,
        86,
        32,
        16,
        42,
        50,
        73,
        18,
        15,
        18,
        48,
        28,
        26,
        53,
        19,
        51,
        23,
        32,
        69,
        47,
        35,
        40,
        46,
        37,
        31,
        14,
        16,
        42,
        42,
        19,
        56,
        48,
        44,
        12,
        40,
        39,
        41,
        13,
        74,
        39,
        1,
        11,
        12,
        16,
        29,
        33,
        10,
        31,
        9,
        55,
        38,
        37,
        16,
        43,
        36,
        7,
        24
      ],
      "2021-10": [
        256,
        81,
        22,
        166,
        42,
        17,
        39,
        101,
        135,
        37,
        19,
        29,
        83,
        43,
        32,
        68,
        29,
        68,
        37,
        25,
        97,
        59,
        33,
        50,
        53,
        32,
        42,
        38,
        13,
        82,
        62,
        23,
        62,
        66,
        48,
        7,
        59,
        48,
        67,
        12,
        107,
        63,
        0,
        13,
        13,
        16,
        36,
        31,
        14,
        39,
        10,
        71,
        41,
        44,
        34,
        46,
        32,
        4,
        22
      ],
      "2021-11": [
        179,
        56,
        9,
        97,
        19,
        27,
        40,
        67,
        102,
        38,
        20,
        25,
        59,
        50,
        32,
        46,
        25,
        38,
        20,
        24,
        85,
        65,
        29,
        30,
        53,
        30,
        33,
        19,
        15,
        41,
        50,
        16,
        59,
        58,
        32,
        16,
        39,
        37,
        55,
        16,
        71,
        36,
        3,
        18,
        18,
        15,
        23,
        23,
        13,
        22,
        5,
        41,
        59,
        40,
        24,
        33,
        26,
        5,
        26
      ],
      "2021-12": [
        183,
        50,
        18,
        109,
        22,
        16,
        36,
        45,
        97,
        36,
        15,
        20,
        57,
        34,
        24,
        43,
        29,
        50,
        33,
        22,
        65,
        59,
        27,
        28,
        36,
        37,
        30,
        33,
        18,
        29,
        36,
        31,
        58,
        48,
        54,
        11,
        32,
        48,
        44,
        13,
        83,
        41,
        0,
        11,
        9,
        15,
        21,
        33,
        14,
        22,
        15,
        54,
        47,
        36,
        14,
        26,
        33,
        7,
        25
      ],
      "2022-01": [
        165,
        56,
        12,
        109,
        21,
        27,
        36,
        38,
        86,
        26,
        17,
        18,
        45,
        26,
        17,
        45,
        17,
        31,
        15,
        18,
        73,
        29,
        16,
        28,
        40,
        22,
        22,
        22,
        14,
        43,
        48,
        22,
        41,
        56,
        35,
        7,
        30,
        21,
        39,
        6,
        73,
        41,
        2,
        7,
        20,
        16,
        12,
        35,
        12,
        23,
        4,
        33,
        44,
        34,
        20,
        22,
        24,
        11,
        14
      ],
      "2022-02": [
        189,
        70,
        23,
        130,
        32,
        26,
        32,
        86,
        129,
        30,
        11,
        18,
        94,
        46,
        23,
        58,
        23,
        55,
        37,
        36,
        86,
        41,
        17,
        49,
        46,
        33,
        19,
        27,
        11,
        51,
        57,
        20,
        58,
        56,
        34,
        15,
        47,
        30,
        58,
        10,
        81,
        58,
        1,
        9,
        27,
        13,
        30,
        35,
        5,
        33,
        9,
        42,
        55,
        43,
        40,
        26,
        33,
        10,
        38
      ],
      "2022-03": [
        195,
        51,
        23,
        105,
        35,
        27,
        46,
        107,
        95,
        30,
        21,
        15,
        47,
        42,
        35,
        54,
        23,
        44,
        44,
        26,
        87,
        50,
        22,
        36,
        58,
        36,
        40,
        32,
        13,
        52,
        52,
        20,
        55,
        66,
        40,
        7,
        54,
        53,
        48,
        13,
        79,
        47,
        0,
        10,
        12,
        17,
        37,
        35,
        6,
        32,
        13,
        55,
        58,
        58,
        23,
        28,
        28,
        6,
        39
      ],
      "2022-04": [
        136,
        53,
        22,
        82,
        30,
        20,
        40,
        68,
        63,
        20,
        14,
        13,
        34,
        28,
        35,
        56,
        20,
        45,
        24,
        24,
        72,
        45,
        19,
        29,
        45,
        15,
        32,
        26,
        18,
        36,
        47,
        17,
        47,
        63,
        32,
        18,
        42,
        38,
        35,
        10,
        75,
        33,
        0,
        12,
        12,
        13,
        20,
        26,
        14,
        21,
        6,
        44,
        50,
        45,
        9,
        19,
        33,
        4,
        27
      ],
      "2022-05": [
        238,
        94,
        23,
        129,
        40,
        33,
        43,
        57,
        117,
        33,
        15,
        16,
        88,
        42,
        30,
        52,
        29,
        60,
        46,
        38,
        88,
        50,
        35,
        48,
        58,
        28,
        40,
        32,
        16,
        54,
        49,
        28,
        46,
        63,
        54,
        9,
        51,
        51,
        62,
        11,
        86,
        49,
        3,
        14,
        13,
        28,
        43,
        41,
        11,
        26,
        11,
        71,
        49,
        44,
        28,
        15,
        44,
        12,
        37
      ],
      "2022-06": [
        240,
        98,
        10,
        145,
        38,
        16,
        41,
        70,
        132,
        58,
        17,
        19,
        94,
        65,
        28,
        59,
        37,
        78,
        34,
        51,
        83,
        58,
        13,
        56,
        57,
        31,
        36,
        32,
        15,
        60,
        67,
        26,
        55,
        71,
        45,
        12,
        61,
        37,
        70,
        11,
        111,
        73,
        0,
        6,
        14,
        22,
        41,
        41,
        12,
        34,
        8,
        66,
        58,
        52,
        35,
        28,
        33,
        6,
        37
      ],
      "2022-07": [
        147,
        65,
        15,
        89,
        26,
        28,
        49,
        59,
        76,
        38,
        14,
        13,
        43,
        24,
        34,
        47,
        19,
        55,
        30,
        28,
        71,
        58,
        15,
        42,
        41,
        24,
        24,
        36,
        17,
        45,
        45,
        17,
        39,
        65,
        30,
        13,
        35,
        50,
        23,
        12,
        79,
        53,
        1,
        11,
        16,
        18,
        19,
        29,
        11,
        33,
        9,
        51,
        41,
        44,
        26,
        32,
        33,
        3,
        31
      ],
      "2022-08": [
        124,
        59,
        10,
        96,
        26,
        26,
        40,
        42,
        75,
        36,
        9,
        16,
        45,
        35,
        24,
        53,
        17,
        35,
        34,
        23,
        62,
        40,
        17,
        29,
        50,
        19,
        17,
        21,
        7,
        37,
        38,
        21,
        48,
        53,
        24,
        16,
        42,
        26,
        19,
        16,
        69,
        41,
        0,
        12,
        12,
        19,
        23,
        19,
        8,
        29,
        6,
        43,
        49,
        40,
        15,
        22,
        27,
        6,
        23
      ],
      "2022-09": [
        169,
        63,
        15,
        99,
        27,
        28,
        47,
        44,
        83,
        50,
        13,
        16,
        52,
        34,
        34,
        44,
        20,
        47,
        26,
        39,
        84,
        55,
        26,
        42,
        56,
        28,
        31,
        24,
        9,
        40,
        49,
        19,
        60,
        57,
        45,
        13,
        33,
        34,
        51,
        11,
        73,
        48,
        1,
        9,
        17,
        20,
        31,
        30,
        2,
        32,
        8,
        49,
        40,
        43,
        14,
        26,
        34,
        4,
        28
      ],
      "2022-10": [
        282,
        102,
        46,
        174,
        53,
        30,
        66,
        82,
        123,
        61,
        21,
        19,
        69,
        58,
        25,
        80,
        33,
        78,
        43,
        37,
        97,
        61,
        35,
        52,
        60,
        33,
        32,
        35,
        17,
        56,
        78,
        30,
        65,
        88,
        46,
        18,
        71,
        68,
        62,
        21,
        133,
        83,
        0,
        17,
        23,
        23,
        38,
        37,
        12,
        45,
        10,
        86,
        53,
        57,
        43,
        31,
        41,
        3,
        25
      ],
      "2022-11": [
        220,
        77,
        28,
        153,
        34,
        42,
        34,
        97,
        98,
        65,
        14,
        19,
        61,
        55,
        31,
        48,
        31,
        61,
        47,
        31,
        81,
        57,
        19,
        44,
        48,
        37,
        23,
        39,
        14,
        41,
        49,
        23,
        71,
        65,
        51,
        17,
        48,
        47,
        54,
        26,
        97,
        48,
        2,
        12,
        19,
        14,
        48,
        23,
        8,
        32,
        9,
        63,
        55,
        41,
        28,
        24,
        32,
        7,
        38
      ],
      "2022-12": [
        158,
        59,
        25,
        75,
        36,
        29,
        26,
        55,
        62,
        35,
        9,
        20,
        33,
        33,
        33,
        46,
        30,
        38,
        43,
        28,
        76,
        29,
        31,
        29,
        54,
        22,
        25,
        15,
        8,
        24,
        39,
        23,
        49,
        48,
        36,
        9,
        54,
        37,
        36,
        15,
        85,
        37,
        0,
        13,
        14,
        16,
        23,
        21,
        8,
        23,
        9,
        50,
        72,
        40,
        15,
        19,
        23,
        5,
        19
      ],
      "2023-01": [
        170,
        49,
        20,
        91,
        38,
        22,
        26,
        42,
        77,
        45,
        13,
        10,
        62,
        44,
        13,
        41,
        13,
        40,
        30,
        26,
        73,
        35,
        17,
        31,
        45,
        33,
        25,
        26,
        20,
        26,
        50,
        19,
        41,
        67,
        38,
        10,
        37,
        21,
        30,
        9,
        56,
        28,
        0,
        8,
        15,
        7,
        25,
        22,
        11,
        22,
        7,
        55,
        52,
        35,
        19,
        17,
        20,
        4,
        35
      ],
      "2023-02": [
        206,
        77,
        26,
        119,
        33,
        45,
        30,
        56,
        114,
        77,
        18,
        16,
        112,
        49,
        33,
        65,
        19,
        61,
        25,
        29,
        78,
        55,
        27,
        61,
        60,
        32,
        29,
        24,
        12,
        49,
        64,
        21,
        58,
        78,
        44,
        6,
        50,
        38,
        55,
        19,
        79,
        64,
        0,
        9,
        11,
        20,
        28,
        30,
        8,
        20,
        14,
        53,
        48,
        37,
        31,
        28,
        40,
        3,
        29
      ],
      "2023-03": [
        188,
        78,
        35,
        115,
        34,
        34,
        40,
        64,
        99,
        81,
        16,
        22,
        68,
        45,
        48,
        39,
        37,
        51,
        40,
        39,
        100,
        63,
        17,
        39,
        47,
        39,
        37,
        46,
        13,
        38,
        51,
        33,
        54,
        104,
        37,
        18,
        77,
        40,
        54,
        11,
        89,
        53,
        0,
        7,
        15,
        22,
        36,
        27,
        14,
        38,
        11,
        54,
        82,
        52,
        39,
        24,
        30,
        9,
        32
      ],
      "2023-04": [
        152,
        64,
        40,
        87,
        36,
        24,
        48,
        41,
        80,
        68,
        10,
        19,
        33,
        33,
        33,
        41,
        19,
        47,
        37,
        26,
        63,
        49,
        34,
        38,
        61,
        29,
        30,
        28,
        15,
        32,
        46,
        25,
        46,
        84,
        21,
        10,
        49,
        32,
        35,
        27,
        73,
        46,
        0,
        10,
        16,
        26,
        25,
        16,
        17,
        25,
        16,
        45,
        62,
        40,
        14,
        14,
        24,
        8,
        26
      ],
      "2023-05": [
        217,
        99,
        78,
        152,
        46,
        38,
        52,
        95,
        156,
        95,
        20,
        20,
        90,
        58,
        44,
        69,
        37,
        78,
        33,
        40,
        113,
        71,
        31,
        60,
        58,
        38,
        45,
        49,
        9,
        72,
        70,
        31,
        74,
        127,
        62,
        12,
        105,
        51,
        81,
        11,
        127,
        71,
        0,
        18,
        22,
        23,
        40,
        41,
        9,
        26,
        9,
        75,
        91,
        51,
        33,
        16,
        46,
        7,
        32
      ],
      "2023-06": [
        279,
        96,
        45,
        159,
        34,
        50,
        44,
        100,
        137,
        83,
        15,
        18,
        71,
        75,
        30,
        69,
        36,
        55,
        43,
        38,
        103,
        61,
        17,
        56,
        55,
        43,
        48,
        47,
        10,
        66,
        57,
        31,
        61,
        100,
        41,
        12,
        109,
        48,
        67,
        28,
        80,
        62,
        0,
        22,
        17,
        21,
        32,
        24,
        10,
        35,
        13,
        66,
        91,
        52,
        36,
        16,
        36,
        5,
        39
      ],
      "2023-07": [
        195,
        72,
        37,
        106,
        36,
        29,
        56,
        46,
        99,
        63,
        19,
        11,
        53,
        45,
        24,
        50,
        31,
        47,
        43,
        37,
        79,
        45,
        21,
        48,
        50,
        30,
        23,
        26,
        10,
        34,
        46,
        28,
        48,
        69,
        40,
        16,
        88,
        39,
        37,
        16,
        87,
        44,
        0,
        7,
        18,
        18,
        30,
        26,
        6,
        30,
        15,
        50,
        77,
        48,
        26,
        17,
        28,
        8,
        28
      ],
      "2023-08": [
        140,
        79,
        41,
        118,
        41,
        28,
        45,
        58,
        84,
        47,
        9,
        18,
        40,
        38,
        41,
        64,
        34,
        39,
        37,
        24,
        72,
        55,
        23,
        25,
        45,
        36,
        41,
        32,
        19,
        32,
        38,
        33,
        50,
        52,
        25,
        17,
        98,
        47,
        33,
        16,
        79,
        48,
        0,
        7,
        11,
        14,
        26,
        22,
        11,
        31,
        11,
        36,
        68,
        51,
        18,
        17,
        26,
        14,
        18
      ],
      "2023-09": [
        162,
        62,
        45,
        115,
        30,
        29,
        35,
        85,
        88,
        74,
        13,
        14,
        41,
        36,
        41,
        52,
        29,
        38,
        55,
        38,
        100,
        55,
        27,
        34,
        52,
        40,
        46,
        40,
        15,
        48,
        45,
        31,
        47,
        75,
        33,
        22,
        100,
        44,
        38,
        21,
        93,
        65,
        0,
        14,
        17,
        15,
        31,
        17,
        10,
        38,
        13,
        56,
        72,
        55,
        31,
        24,
        30,
        6,
        40
      ],
      "2023-10": [
        250,
        107,
        112,
        181,
        51,
        67,
        50,
        62,
        129,
        114,
        18,
        22,
        91,
        80,
        44,
        54,
        46,
        64,
        46,
        41,
        139,
        72,
        30,
        60,
        85,
        49,
        28,
        50,
        13,
        64,
        75,
        33,
        60,
        134,
        62,
        14,
        192,
        64,
        78,
        35,
        113,
        79,
        1,
        19,
        20,
        20,
        43,
        53,
        12,
        32,
        9,
        95,
        104,
        57,
        34,
        13,
        40,
        8,
        41
      ],
      "2023-11": [
        172,
        82,
        50,
        118,
        34,
        46,
        54,
        48,
        92,
        97,
        18,
        15,
        45,
        52,
        56,
        42,
        34,
        42,
        60,
        26,
        91,
        47,
        26,
        41,
        63,
        33,
        38,
        46,
        16,
        47,
        53,
        28,
        48,
        89,
        29,
        14,
        125,
        51,
        52,
        26,
        90,
        50,
        1,
        17,
        18,
        17,
        42,
        33,
        13,
        30,
        15,
        64,
        83,
        50,
        24,
        21,
        18,
        9,
        42
      ],
      "2023-12": [
        200,
        91,
        80,
        131,
        35,
        37,
        42,
        61,
        66,
        93,
        18,
        21,
        54,
        56,
        40,
        40,
        31,
        53,
        34,
        31,
        122,
        61,
        21,
        48,
        53,
        45,
        30,
        44,
        9,
        41,
        57,
        31,
        56,
        69,
        31,
        9,
        109,
        29,
        44,
        36,
        101,
        57,
        1,
        13,
        16,
        27,
        30,
        27,
        11,
        33,
        12,
        54,
        93,
        58,
        20,
        15,
        23,
        6,
        38
      ],
      "2024-01": [
        168,
        65,
        54,
        114,
        48,
        36,
        35,
        61,
        74,
        56,
        8,
        24,
        39,
        46,
        36,
        47,
        24,
        40,
        29,
        29,
        107,
        40,
        28,
        43,
        57,
        50,
        28,
        33,
        15,
        41,
        52,
        27,
        45,
        67,
        23,
        18,
        112,
        37,
        28,
        31,
        71,
        44,
        0,
        18,
        24,
        19,
        27,
        20,
        9,
        27,
        18,
        34,
        91,
        44,
        16,
        17,
        26,
        12,
        17
      ],
      "2024-02": [
        275,
        84,
        151,
        202,
        56,
        51,
        39,
        60,
        129,
        129,
        18,
        12,
        105,
        73,
        54,
        52,
        43,
        67,
        43,
        46,
        139,
        64,
        33,
        60,
        71,
        43,
        35,
        43,
        14,
        74,
        81,
        29,
        82,
        138,
        42,
        13,
        275,
        55,
        82,
        30,
        110,
        100,
        0,
        9,
        20,
        24,
        35,
        53,
        13,
        43,
        12,
        82,
        148,
        61,
        49,
        20,
        55,
        6,
        37
      ],
      "2024-03": [
        209,
        83,
        79,
        144,
        31,
        38,
        44,
        59,
        96,
        107,
        18,
        14,
        51,
        54,
        47,
        61,
        46,
        59,
        41,
        36,
        127,
        68,
        32,
        43,
        65,
        37,
        34,
        63,
        19,
        46,
        55,
        28,
        63,
        123,
        33,
        16,
        166,
        44,
        39,
        30,
        127,
        60,
        0,
        16,
        17,
        17,
        35,
        37,
        11,
        37,
        20,
        77,
        90,
        58,
        31,
        12,
        32,
        7,
        38
      ],
      "2024-04": [
        164,
        68,
        88,
        104,
        36,
        29,
        39,
        43,
        69,
        74,
        7,
        16,
        30,
        45,
        38,
        39,
        37,
        36,
        44,
        36,
        75,
        59,
        22,
        46,
        44,
        26,
        43,
        43,
        12,
        35,
        37,
        13,
        38,
        93,
        32,
        18,
        159,
        39,
        37,
        19,
        85,
        65,
        1,
        23,
        14,
        17,
        24,
        26,
        11,
        28,
        9,
        49,
        123,
        45,
        16,
        10,
        25,
        9,
        30
      ],
      "2024-05": [
        281,
        126,
        98,
        144,
        51,
        67,
        38,
        58,
        141,
        126,
        18,
        25,
        95,
        72,
        44,
        53,
        48,
        74,
        56,
        57,
        156,
        52,
        25,
        58,
        67,
        44,
        47,
        50,
        13,
        97,
        71,
        40,
        64,
        122,
        35,
        16,
        207,
        37,
        64,
        34,
        120,
        97,
        0,
        20,
        19,
        18,
        43,
        40,
        13,
        44,
        15,
        71,
        111,
        56,
        44,
        13,
        37,
        16,
        41
      ],
      "2024-06": [
        269,
        88,
        148,
        196,
        53,
        54,
        31,
        101,
        110,
        128,
        16,
        15,
        71,
        75,
        34,
        53,
        55,
        61,
        41,
        46,
        134,
        84,
        12,
        61,
        49,
        32,
        39,
        49,
        12,
        60,
        82,
        29,
        54,
        153,
        34,
        11,
        259,
        38,
        51,
        27,
        115,
        95,
        0,
        12,
        21,
        25,
        26,
        39,
        12,
        29,
        13,
        78,
        150,
        50,
        33,
        10,
        38,
        5,
        27
      ],
      "2024-07": [
        190,
        77,
        98,
        143,
        43,
        39,
        32,
        67,
        86,
        72,
        16,
        22,
        37,
        63,
        45,
        53,
        49,
        37,
        50,
        28,
        108,
        46,
        25,
        36,
        52,
        32,
        34,
        48,
        12,
        49,
        41,
        26,
        48,
        127,
        34,
        13,
        194,
        34,
        42,
        22,
        85,
        68,
        1,
        13,
        21,
        23,
        21,
        23,
        16,
        29,
        14,
        46,
        98,
        47,
        18,
        10,
        25,
        9,
        31
      ],
      "2024-08": [
        145,
        66,
        71,
        115,
        44,
        46,
        20,
        62,
        70,
        58,
        15,
        19,
        41,
        48,
        34,
        55,
        31,
        40,
        46,
        37,
        113,
        40,
        12,
        40,
        66,
        34,
        24,
        26,
        10,
        50,
        46,
        27,
        52,
        71,
        38,
        26,
        131,
        24,
        45,
        33,
        78,
        53,
        0,
        13,
        11,
        22,
        26,
        21,
        13,
        25,
        15,
        57,
        101,
        49,
        18,
        13,
        26,
        9,
        29
      ],
      "2024-09": [
        188,
        84,
        87,
        115,
        44,
        38,
        33,
        92,
        109,
        82,
        15,
        20,
        42,
        50,
        47,
        67,
        34,
        32,
        50,
        22,
        122,
        47,
        20,
        44,
        67,
        45,
        39,
        36,
        14,
        54,
        53,
        35,
        43,
        107,
        25,
        20,
        166,
        37,
        45,
        31,
        89,
        56,
        0,
        16,
        26,
        26,
        49,
        25,
        9,
        30,
        14,
        51,
        112,
        56,
        21,
        7,
        28,
        8,
        39
      ],
      "2024-10": [
        313,
        117,
        238,
        189,
        70,
        88,
        41,
        78,
        160,
        200,
        18,
        30,
        85,
        97,
        66,
        64,
        68,
        61,
        56,
        40,
        215,
        69,
        37,
        55,
        75,
        52,
        54,
        66,
        14,
        108,
        88,
        51,
        87,
        166,
        54,
        23,
        347,
        37,
        101,
        39,
        158,
        108,
        0,
        17,
        20,
        28,
        46,
        43,
        25,
        43,
        7,
        99,
        158,
        62,
        53,
        23,
        66,
        13,
        44
      ],
      "2024-11": [
        169,
        73,
        121,
        134,
        42,
        56,
        35,
        47,
        73,
        113,
        13,
        20,
        51,
        67,
        53,
        54,
        62,
        53,
        51,
        31,
        129,
        74,
        26,
        47,
        65,
        47,
        47,
        47,
        12,
        46,
        57,
        50,
        67,
        97,
        24,
        25,
        180,
        36,
        46,
        34,
        97,
        62,
        1,
        33,
        15,
        25,
        35,
        27,
        13,
        44,
        17,
        49,
        113,
        72,
        23,
        13,
        23,
        8,
        35
      ],
      "2024-12": [
        196,
        104,
        95,
        160,
        47,
        43,
        43,
        57,
        89,
        133,
        9,
        14,
        40,
        59,
        46,
        60,
        85,
        36,
        46,
        23,
        123,
        84,
        21,
        50,
        55,
        63,
        40,
        50,
        19,
        62,
        77,
        39,
        63,
        120,
        36,
        25,
        164,
        54,
        49,
        39,
        94,
        63,
        0,
        11,
        16,
        11,
        35,
        37,
        14,
        37,
        18,
        58,
        146,
        59,
        26,
        15,
        23,
        7,
        47
      ],
      "2025-01": [
        203,
        74,
        118,
        119,
        47,
        51,
        29,
        55,
        76,
        97,
        12,
        21,
        77,
        58,
        51,
        33,
        45,
        33,
        43,
        29,
        95,
        55,
        30,
        47,
        69,
        38,
        38,
        30,
        12,
        60,
        52,
        34,
        70,
        70,
        23,
        20,
        185,
        41,
        40,
        24,
        89,
        60,
        1,
        15,
        10,
        23,
        38,
        25,
        6,
        20,
        11,
        63,
        132,
        69,
        28,
        18,
        30,
        17,
        40
      ],
      "2025-02": [
        295,
        87,
        231,
        172,
        52,
        70,
        21,
        61,
        110,
        171,
        12,
        22,
        98,
        83,
        46,
        66,
        85,
        52,
        59,
        42,
        160,
        57,
        25,
        43,
        87,
        33,
        54,
        54,
        9,
        77,
        78,
        49,
        73,
        169,
        36,
        32,
        334,
        29,
        80,
        33,
        120,
        105,
        2,
        19,
        27,
        29,
        47,
        31,
        23,
        42,
        16,
        90,
        197,
        60,
        64,
        16,
        37,
        14,
        49
      ],
      "2025-03": [
        259,
        104,
        144,
        133,
        51,
        47,
        18,
        52,
        92,
        125,
        16,
        28,
        68,
        74,
        46,
        44,
        61,
        37,
        67,
        26,
        135,
        81,
        24,
        65,
        79,
        47,
        48,
        60,
        12,
        71,
        61,
        23,
        61,
        125,
        26,
        11,
        212,
        45,
        59,
        30,
        106,
        92,
        0,
        17,
        27,
        19,
        41,
        40,
        16,
        43,
        19,
        57,
        140,
        49,
        27,
        10,
        33,
        13,
        41
      ],
      "2025-04": [
        174,
        73,
        134,
        109,
        37,
        44,
        26,
        31,
        78,
        73,
        9,
        15,
        40,
        50,
        50,
        40,
        47,
        43,
        64,
        32,
        104,
        61,
        28,
        41,
        72,
        37,
        49,
        37,
        16,
        57,
        64,
        32,
        49,
        113,
        27,
        19,
        177,
        30,
        45,
        31,
        77,
        64,
        0,
        17,
        17,
        20,
        31,
        17,
        14,
        34,
        23,
        59,
        164,
        52,
        21,
        14,
        22,
        14,
        40
      ],
      "2025-05": [
        363,
        113,
        299,
        180,
        67,
        68,
        27,
        89,
        144,
        175,
        20,
        26,
        89,
        93,
        63,
        62,
        102,
        71,
        69,
        58,
        202,
        85,
        15,
        69,
        107,
        46,
        52,
        85,
        15,
        104,
        79,
        47,
        83,
        195,
        51,
        19,
        319,
        33,
        100,
        43,
        158,
        137,
        2,
        18,
        33,
        31,
        42,
        50,
        17,
        59,
        20,
        106,
        204,
        81,
        43,
        14,
        38,
        14,
        56
      ],
      "2025-06": [
        301,
        89,
        181,
        150,
        43,
        60,
        26,
        83,
        92,
        143,
        14,
        18,
        73,
        92,
        49,
        64,
        60,
        34,
        70,
        41,
        137,
        55,
        18,
        50,
        78,
        64,
        41,
        46,
        11,
        71,
        81,
        39,
        61,
        173,
        38,
        23,
        246,
        37,
        66,
        28,
        94,
        94,
        2,
        11,
        21,
        28,
        38,
        26,
        22,
        49,
        11,
        78,
        166,
        63,
        35,
        14,
        36,
        10,
        47
      ],
      "2025-07": [
        232,
        72,
        142,
        135,
        60,
        63,
        35,
        61,
        70,
        110,
        17,
        24,
        34,
        65,
        60,
        75,
        47,
        49,
        75,
        30,
        101,
        48,
        19,
        42,
        54,
        44,
        53,
        54,
        11,
        66,
        61,
        39,
        59,
        107,
        36,
        25,
        173,
        32,
        54,
        22,
        93,
        104,
        0,
        21,
        18,
        11,
        36,
        29,
        24,
        33,
        11,
        63,
        207,
        52,
        23,
        13,
        38,
        13,
        45
      ],
      "2025-08": [
        197,
        84,
        137,
        136,
        39,
        50,
        27,
        77,
        75,
        82,
        11,
        15,
        49,
        60,
        66,
        85,
        54,
        43,
        65,
        34,
        146,
        37,
        17,
        54,
        53,
        43,
        32,
        61,
        13,
        52,
        62,
        36,
        55,
        121,
        29,
        28,
        210,
        29,
        52,
        31,
        101,
        77,
        2,
        17,
        23,
        26,
        33,
        31,
        24,
        40,
        12,
        61,
        136,
        79,
        21,
        14,
        29,
        9,
        47
      ],
      "2025-09": [
        106,
        41,
        49,
        68,
        24,
        21,
        18,
        29,
        35,
        34,
        3,
        7,
        19,
        34,
        42,
        27,
        19,
        23,
        40,
        11,
        57,
        37,
        14,
        30,
        27,
        21,
        11,
        13,
        9,
        28,
        23,
        22,
        29,
        59,
        14,
        13,
        82,
        11,
        20,
        20,
        42,
        49,
        0,
        8,
        12,
        11,
        18,
        10,
        12,
        12,
        11,
        28,
        85,
        36,
        10,
        8,
        19,
        8,
        27
      ]
    },
    "papers": {
      "0": [
        {
          "title": "ARC - Actor Residual Critic for Adversarial Imitation Learning",
          "year": "2022-06",
          "abstract": "Adversarial Imitation Learning (AIL) is a class of popular state-of-the-art\nImitation Learning algorithms commonly used in robotics. In AIL, an artificial\nadversary's misclassification is used as a reward signal that is optimized by\nany standard Reinforcement Learning (RL) algorithm. Unlike most RL settings,\nthe reward in AIL is $differentiable$ but current model-free RL algorithms do\nnot make use of this property to train a policy. The reward is AIL is also\nshaped since it comes from an adversary. We leverage the differentiability\nproperty of the shaped AIL reward function and formulate a class of Actor\nResidual Critic (ARC) RL algorithms. ARC algorithms draw a parallel to the\nstandard Actor-Critic (AC) algorithms in RL literature and uses a residual\ncritic, $C$ function (instead of the standard $Q$ function) to approximate only\nthe discounted future return (excluding the immediate reward). ARC algorithms\nhave similar convergence properties as the standard AC algorithms with the\nadditional advantage that the gradient through the immediate reward is exact.\nFor the discrete (tabular) case with finite states, actions, and known\ndynamics, we prove that policy iteration with $C$ function converges to an\noptimal policy. In the continuous case with function approximation and unknown\ndynamics, we experimentally show that ARC aided AIL outperforms standard AIL in\nsimulated continuous-control and real robotic manipulation tasks. ARC\nalgorithms are simple to implement and can be incorporated into any existing\nAIL implementation with an AC algorithm. Video and link to code are available\nat: https://sites.google.com/view/actor-residual-critic.",
          "arxiv_id": "2206.02095v4"
        },
        {
          "title": "BRAC+: Improved Behavior Regularized Actor Critic for Offline Reinforcement Learning",
          "year": "2021-10",
          "abstract": "Online interactions with the environment to collect data samples for training\na Reinforcement Learning (RL) agent is not always feasible due to economic and\nsafety concerns. The goal of Offline Reinforcement Learning is to address this\nproblem by learning effective policies using previously collected datasets.\nStandard off-policy RL algorithms are prone to overestimations of the values of\nout-of-distribution (less explored) actions and are hence unsuitable for\nOffline RL. Behavior regularization, which constraints the learned policy\nwithin the support set of the dataset, has been proposed to tackle the\nlimitations of standard off-policy algorithms. In this paper, we improve the\nbehavior regularized offline reinforcement learning and propose BRAC+. First,\nwe propose quantification of the out-of-distribution actions and conduct\ncomparisons between using Kullback-Leibler divergence versus using Maximum Mean\nDiscrepancy as the regularization protocol. We propose an analytical upper\nbound on the KL divergence as the behavior regularizer to reduce variance\nassociated with sample based estimations. Second, we mathematically show that\nthe learned Q values can diverge even using behavior regularized policy update\nunder mild assumptions. This leads to large overestimations of the Q values and\nperformance deterioration of the learned policy. To mitigate this issue, we add\na gradient penalty term to the policy evaluation objective. By doing so, the Q\nvalues are guaranteed to converge. On challenging offline RL benchmarks, BRAC+\noutperforms the baseline behavior regularized approaches by 40%~87% and the\nstate-of-the-art approach by 6%.",
          "arxiv_id": "2110.00894v1"
        },
        {
          "title": "Human AI interaction loop training: New approach for interactive reinforcement learning",
          "year": "2020-03",
          "abstract": "Reinforcement Learning (RL) in various decision-making tasks of machine\nlearning provides effective results with an agent learning from a stand-alone\nreward function. However, it presents unique challenges with large amounts of\nenvironment states and action spaces, as well as in the determination of\nrewards. This complexity, coming from high dimensionality and continuousness of\nthe environments considered herein, calls for a large number of learning trials\nto learn about the environment through Reinforcement Learning. Imitation\nLearning (IL) offers a promising solution for those challenges using a teacher.\nIn IL, the learning process can take advantage of human-sourced assistance\nand/or control over the agent and environment. A human teacher and an agent\nlearner are considered in this study. The teacher takes part in the agent\ntraining towards dealing with the environment, tackling a specific objective,\nand achieving a predefined goal. Within that paradigm, however, existing IL\napproaches have the drawback of expecting extensive demonstration information\nin long-horizon problems. This paper proposes a novel approach combining IL\nwith different types of RL methods, namely state action reward state action\n(SARSA) and asynchronous advantage actor-critic (A3C) agents, to overcome the\nproblems of both stand-alone systems. It is addressed how to effectively\nleverage the teacher feedback, be it direct binary or indirect detailed for the\nagent learner to learn sequential decision-making policies. The results of this\nstudy on various OpenAI Gym environments show that this algorithmic method can\nbe incorporated with different combinations, significantly decreases both human\nendeavor and tedious exploration process.",
          "arxiv_id": "2003.04203v1"
        }
      ],
      "1": [
        {
          "title": "Acceleration of Federated Learning with Alleviated Forgetting in Local Training",
          "year": "2022-03",
          "abstract": "Federated learning (FL) enables distributed optimization of machine learning\nmodels while protecting privacy by independently training local models on each\nclient and then aggregating parameters on a central server, thereby producing\nan effective global model. Although a variety of FL algorithms have been\nproposed, their training efficiency remains low when the data are not\nindependently and identically distributed (non-i.i.d.) across different\nclients. We observe that the slow convergence rates of the existing methods are\n(at least partially) caused by the catastrophic forgetting issue during the\nlocal training stage on each individual client, which leads to a large increase\nin the loss function concerning the previous training data at the other\nclients. Here, we propose FedReg, an algorithm to accelerate FL with alleviated\nknowledge forgetting in the local training stage by regularizing locally\ntrained parameters with the loss on generated pseudo data, which encode the\nknowledge of previous training data learned by the global model. Our\ncomprehensive experiments demonstrate that FedReg not only significantly\nimproves the convergence rate of FL, especially when the neural network\narchitecture is deep and the clients' data are extremely non-i.i.d., but is\nalso able to protect privacy better in classification problems and more robust\nagainst gradient inversion attacks. The code is available at:\nhttps://github.com/Zoesgithub/FedReg.",
          "arxiv_id": "2203.02645v1"
        },
        {
          "title": "PRECAD: Privacy-Preserving and Robust Federated Learning via Crypto-Aided Differential Privacy",
          "year": "2021-10",
          "abstract": "Federated Learning (FL) allows multiple participating clients to train\nmachine learning models collaboratively by keeping their datasets local and\nonly exchanging model updates. Existing FL protocol designs have been shown to\nbe vulnerable to attacks that aim to compromise data privacy and/or model\nrobustness. Recently proposed defenses focused on ensuring either privacy or\nrobustness, but not both. In this paper, we develop a framework called PRECAD,\nwhich simultaneously achieves differential privacy (DP) and enhances robustness\nagainst model poisoning attacks with the help of cryptography. Using secure\nmulti-party computation (MPC) techniques (e.g., secret sharing), noise is added\nto the model updates by the honest-but-curious server(s) (instead of each\nclient) without revealing clients' inputs, which achieves the benefit of\ncentralized DP in terms of providing a better privacy-utility tradeoff than\nlocal DP based solutions. Meanwhile, a crypto-aided secure validation protocol\nis designed to verify that the contribution of model update from each client is\nbounded without leaking privacy. We show analytically that the noise added to\nensure DP also provides enhanced robustness against malicious model\nsubmissions. We experimentally demonstrate that our PRECAD framework achieves\nhigher privacy-utility tradeoff and enhances robustness for the trained models.",
          "arxiv_id": "2110.11578v1"
        },
        {
          "title": "Federated Learning with Heterogeneous Differential Privacy",
          "year": "2021-10",
          "abstract": "Federated learning (FL) takes a first step towards privacy-preserving machine\nlearning by training models while keeping client data local. Models trained\nusing FL may still leak private client information through model updates during\ntraining. Differential privacy (DP) may be employed on model updates to provide\nprivacy guarantees within FL, typically at the cost of degraded performance of\nthe final trained model. Both non-private FL and DP-FL can be solved using\nvariants of the federated averaging (FedAvg) algorithm. In this work, we\nconsider a heterogeneous DP setup where clients require varying degrees of\nprivacy guarantees. First, we analyze the optimal solution to the federated\nlinear regression problem with heterogeneous DP in a Bayesian setup. We find\nthat unlike the non-private setup, where the optimal solution for homogeneous\ndata amounts to a single global solution for all clients learned through\nFedAvg, the optimal solution for each client in this setup would be a\npersonalized one even for homogeneous data. We also analyze the privacy-utility\ntrade-off for this setup, where we characterize the gain obtained from\nheterogeneous privacy where some clients opt for less strict privacy\nguarantees. We propose a new algorithm for FL with heterogeneous DP, named\nFedHDP, which employs personalization and weighted averaging at the server\nusing the privacy choices of clients, to achieve better performance on clients'\nlocal models. Through numerical experiments, we show that FedHDP provides up to\n$9.27\\%$ performance gain compared to the baseline DP-FL for the considered\ndatasets where $5\\%$ of clients opt out of DP. Additionally, we show a gap in\nthe average performance of local models between non-private and private clients\nof up to $3.49\\%$, empirically illustrating that the baseline DP-FL might incur\na large utility cost when not all clients require the stricter privacy\nguarantees.",
          "arxiv_id": "2110.15252v2"
        }
      ],
      "2": [
        {
          "title": "Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves",
          "year": "2023-11",
          "abstract": "Misunderstandings arise not only in interpersonal communication but also\nbetween humans and Large Language Models (LLMs). Such discrepancies can make\nLLMs interpret seemingly unambiguous questions in unexpected ways, yielding\nincorrect responses. While it is widely acknowledged that the quality of a\nprompt, such as a question, significantly impacts the quality of the response\nprovided by LLMs, a systematic method for crafting questions that LLMs can\nbetter comprehend is still underdeveloped. In this paper, we present a method\nnamed `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand\nquestions posed by humans and provide responses in a single prompt. This\napproach serves as a simple yet effective prompting method for improving\nperformance. We also introduce a two-step variant of RaR, where a rephrasing\nLLM first rephrases the question and then passes the original and rephrased\nquestions together to a different responding LLM. This facilitates the\neffective utilization of rephrased questions generated by one LLM with another.\nOur experiments demonstrate that our methods significantly improve the\nperformance of different models across a wide range to tasks. We further\nprovide a comprehensive comparison between RaR and the popular Chain-of-Thought\n(CoT) methods, both theoretically and empirically. We show that RaR is\ncomplementary to CoT and can be combined with CoT to achieve even better\nperformance. Our work not only contributes to enhancing LLM performance\nefficiently and effectively but also sheds light on a fair evaluation of LLM\ncapabilities. Data and codes are available at\nhttps://github.com/uclaml/Rephrase-and-Respond.",
          "arxiv_id": "2311.04205v2"
        },
        {
          "title": "Stacking Small Language Models for Generalizability",
          "year": "2024-10",
          "abstract": "Recent advances show that large language models (LLMs) generalize strong\nperformance across different natural language benchmarks. However, the large\nsize of LLMs makes training and inference expensive and impractical to run in\nresource-limited settings. This paper introduces a new approach called\nfine-tuning stacks of language models (FSLM), which involves stacking small\nlanguage models (SLM) as an alternative to LLMs. By fine-tuning each SLM to\nperform a specific task, this approach breaks down high level reasoning into\nmultiple lower-level steps that specific SLMs are responsible for. As a result,\nFSLM allows for lower training and inference costs, and also improves model\ninterpretability as each SLM communicates with the subsequent one through\nnatural language. By evaluating FSLM on common natural language benchmarks,\nthis paper highlights promising early results toward generalizable performance\nusing FSLM as a cost-effective alternative to LLMs.",
          "arxiv_id": "2410.15570v1"
        },
        {
          "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
          "year": "2025-04",
          "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
          "arxiv_id": "2504.02810v2"
        }
      ],
      "3": [
        {
          "title": "Graph Decipher: A transparent dual-attention graph neural network to understand the message-passing mechanism for the node classification",
          "year": "2022-01",
          "abstract": "Graph neural networks can be effectively applied to find solutions for many\nreal-world problems across widely diverse fields. The success of graph neural\nnetworks is linked to the message-passing mechanism on the graph, however, the\nmessage-aggregating behavior is still not entirely clear in most algorithms. To\nimprove functionality, we propose a new transparent network called Graph\nDecipher to investigate the message-passing mechanism by prioritizing in two\nmain components: the graph structure and node attributes, at the graph,\nfeature, and global levels on a graph under the node classification task.\nHowever, the computation burden now becomes the most significant issue because\nthe relevance of both graph structure and node attributes are computed on a\ngraph. In order to solve this issue, only relevant representative node\nattributes are extracted by graph feature filters, allowing calculations to be\nperformed in a category-oriented manner. Experiments on seven datasets show\nthat Graph Decipher achieves state-of-the-art performance while imposing a\nsubstantially lower computation burden under the node classification task.\nAdditionally, since our algorithm has the ability to explore the representative\nnode attributes by category, it is utilized to alleviate the imbalanced node\nclassification problem on multi-class graph datasets.",
          "arxiv_id": "2201.01381v1"
        },
        {
          "title": "Multilevel Graph Matching Networks for Deep Graph Similarity Learning",
          "year": "2020-07",
          "abstract": "While the celebrated graph neural networks yield effective representations\nfor individual nodes of a graph, there has been relatively less success in\nextending to the task of graph similarity learning. Recent work on graph\nsimilarity learning has considered either global-level graph-graph interactions\nor low-level node-node interactions, however ignoring the rich cross-level\ninteractions (e.g., between each node of one graph and the other whole graph).\nIn this paper, we propose a multi-level graph matching network (MGMN) framework\nfor computing the graph similarity between any pair of graph-structured objects\nin an end-to-end fashion. In particular, the proposed MGMN consists of a\nnode-graph matching network for effectively learning cross-level interactions\nbetween each node of one graph and the other whole graph, and a siamese graph\nneural network to learn global-level interactions between two input graphs.\nFurthermore, to compensate for the lack of standard benchmark datasets, we have\ncreated and collected a set of datasets for both the graph-graph classification\nand graph-graph regression tasks with different sizes in order to evaluate the\neffectiveness and robustness of our models. Comprehensive experiments\ndemonstrate that MGMN consistently outperforms state-of-the-art baseline models\non both the graph-graph classification and graph-graph regression tasks.\nCompared with previous work, MGMN also exhibits stronger robustness as the\nsizes of the two input graphs increase.",
          "arxiv_id": "2007.04395v4"
        },
        {
          "title": "Co-embedding of Nodes and Edges with Graph Neural Networks",
          "year": "2020-10",
          "abstract": "Graph, as an important data representation, is ubiquitous in many real world\napplications ranging from social network analysis to biology. How to correctly\nand effectively learn and extract information from graph is essential for a\nlarge number of machine learning tasks. Graph embedding is a way to transform\nand encode the data structure in high dimensional and non-Euclidean feature\nspace to a low dimensional and structural space, which is easily exploited by\nother machine learning algorithms. We have witnessed a huge surge of such\nembedding methods, from statistical approaches to recent deep learning methods\nsuch as the graph convolutional networks (GCN). Deep learning approaches\nusually outperform the traditional methods in most graph learning benchmarks by\nbuilding an end-to-end learning framework to optimize the loss function\ndirectly. However, most of the existing GCN methods can only perform\nconvolution operations with node features, while ignoring the handy information\nin edge features, such as relations in knowledge graphs. To address this\nproblem, we present CensNet, Convolution with Edge-Node Switching graph neural\nnetwork, for learning tasks in graph-structured data with both node and edge\nfeatures. CensNet is a general graph embedding framework, which embeds both\nnodes and edges to a latent feature space. By using line graph of the original\nundirected graph, the role of nodes and edges are switched, and two novel graph\nconvolution operations are proposed for feature propagation. Experimental\nresults on real-world academic citation networks and quantum chemistry graphs\nshow that our approach achieves or matches the state-of-the-art performance in\nfour graph learning tasks, including semi-supervised node classification,\nmulti-task graph classification, graph regression, and link prediction.",
          "arxiv_id": "2010.13242v1"
        }
      ],
      "4": [
        {
          "title": "Semi-analytic PINN methods for singularly perturbed boundary value problems",
          "year": "2022-08",
          "abstract": "We propose a new semi-analytic physics informed neural network (PINN) to\nsolve singularly perturbed boundary value problems. The PINN is a scientific\nmachine learning framework that offers a promising perspective for finding\nnumerical solutions to partial differential equations. The PINNs have shown\nimpressive performance in solving various differential equations including\ntime-dependent and multi-dimensional equations involved in a complex geometry\nof the domain. However, when considering stiff differential equations, neural\nnetworks in general fail to capture the sharp transition of solutions, due to\nthe spectral bias. To resolve this issue, here we develop the semi-analytic\nPINN methods, enriched by using the so-called corrector functions obtained from\nthe boundary layer analysis. Our new enriched PINNs accurately predict\nnumerical solutions to the singular perturbation problems. Numerical\nexperiments include various types of singularly perturbed linear and nonlinear\ndifferential equations.",
          "arxiv_id": "2208.09145v1"
        },
        {
          "title": "Hyena Neural Operator for Partial Differential Equations",
          "year": "2023-06",
          "abstract": "Numerically solving partial differential equations typically requires fine\ndiscretization to resolve necessary spatiotemporal scales, which can be\ncomputationally expensive. Recent advances in deep learning have provided a new\napproach to solving partial differential equations that involves the use of\nneural operators. Neural operators are neural network architectures that learn\nmappings between function spaces and have the capability to solve partial\ndifferential equations based on data. This study utilizes a novel neural\noperator called Hyena, which employs a long convolutional filter that is\nparameterized by a multilayer perceptron. The Hyena operator is an operation\nthat enjoys sub-quadratic complexity and state space model to parameterize long\nconvolution that enjoys a global receptive field. This mechanism enhances the\nmodel's comprehension of the input's context and enables data-dependent weight\nfor different partial differential equations instances. To measure how\neffective the layers are in solving partial differential equations, we conduct\nexperiments on Diffusion-Reaction equation and Navier Stokes equation. Our\nfindings indicate Hyena Neural operator can serve as an efficient and accurate\nmodel for learning partial differential equations solution operator. The data\nand code used can be found at:\nhttps://github.com/Saupatil07/Hyena-Neural-Operator",
          "arxiv_id": "2306.16524v2"
        },
        {
          "title": "SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs",
          "year": "2021-02",
          "abstract": "We introduce a class of Sparse, Physics-based, and partially Interpretable\nNeural Networks (SPINN) for solving ordinary and partial differential equations\n(PDEs). By reinterpreting a traditional meshless representation of solutions of\nPDEs we develop a class of sparse neural network architectures that are\npartially interpretable. The SPINN model we propose here serves as a seamless\nbridge between two extreme modeling tools for PDEs, namely dense neural network\nbased methods like Physics Informed Neural Networks (PINNs) and traditional\nmesh-free numerical methods, thereby providing a novel means to develop a new\nclass of hybrid algorithms that build on the best of both these viewpoints. A\nunique feature of the SPINN model that distinguishes it from other neural\nnetwork based approximations proposed earlier is that it is (i) interpretable,\nin a particular sense made precise in the work, and (ii) sparse in the sense\nthat it has much fewer connections than typical dense neural networks used for\nPDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is\nable to handle discontinuities in the solutions. In addition, we demonstrate\nthat Fourier series representations can also be expressed as a special class of\nSPINN and propose generalized neural network analogues of Fourier\nrepresentations. We illustrate the utility of the proposed method with a\nvariety of examples involving ordinary differential equations, elliptic,\nparabolic, hyperbolic and nonlinear partial differential equations, and an\nexample in fluid dynamics.",
          "arxiv_id": "2102.13037v4"
        }
      ],
      "5": [
        {
          "title": "Protein Language Model-Powered 3D Ligand Binding Site Prediction from Protein Sequence",
          "year": "2023-12",
          "abstract": "Prediction of ligand binding sites of proteins is a fundamental and important\ntask for understanding the function of proteins and screening potential drugs.\nMost existing methods require experimentally determined protein holo-structures\nas input. However, such structures can be unavailable on novel or less-studied\nproteins. To tackle this limitation, we propose LaMPSite, which only takes\nprotein sequences and ligand molecular graphs as input for ligand binding site\npredictions. The protein sequences are used to retrieve residue-level\nembeddings and contact maps from the pre-trained ESM-2 protein language model.\nThe ligand molecular graphs are fed into a graph neural network to compute\natom-level embeddings. Then we compute and update the protein-ligand\ninteraction embedding based on the protein residue-level embeddings and ligand\natom-level embeddings, and the geometric constraints in the inferred protein\ncontact map and ligand distance map. A final pooling on protein-ligand\ninteraction embedding would indicate which residues belong to the binding\nsites. Without any 3D coordinate information of proteins, our proposed model\nachieves competitive performance compared to baseline methods that require 3D\nprotein structures when predicting binding sites. Given that less than 50% of\nproteins have reliable structure information in the current stage, LaMPSite\nwill provide new opportunities for drug discovery.",
          "arxiv_id": "2312.03016v1"
        },
        {
          "title": "Pharmacophore-guided de novo drug design with diffusion bridge",
          "year": "2024-12",
          "abstract": "De novo design of bioactive drug molecules with potential to treat desired\nbiological targets is a profound task in the drug discovery process. Existing\napproaches tend to leverage the pocket structure of the target protein to\ncondition the molecule generation. However, even the pocket area of the target\nprotein may contain redundant information since not all atoms in the pocket is\nresponsible for the interaction with the ligand. In this work, we propose\nPharmacoBridge, a phamacophore-guided de novo design approach to generate drug\ncandidates inducing desired bioactivity via diffusion bridge. Our method adapts\nthe diffusion bridge to effectively convert pharmacophore arrangements in the\nspatial space into molecular structures under the manner of SE(3)-equivariant\ntransformation, providing sophisticated control over optimal biochemical\nfeature arrangements on the generated molecules. PharmacoBridge is demonstrated\nto generate hit candidates that exhibit high binding affinity with potential\nprotein targets.",
          "arxiv_id": "2412.19812v2"
        },
        {
          "title": "MoFlow: An Invertible Flow Model for Generating Molecular Graphs",
          "year": "2020-06",
          "abstract": "Generating molecular graphs with desired chemical properties driven by deep\ngraph generative models provides a very promising way to accelerate drug\ndiscovery process. Such graph generative models usually consist of two steps:\nlearning latent representations and generation of molecular graphs. However, to\ngenerate novel and chemically-valid molecular graphs from latent\nrepresentations is very challenging because of the chemical constraints and\ncombinatorial complexity of molecular graphs. In this paper, we propose MoFlow,\na flow-based graph generative model to learn invertible mappings between\nmolecular graphs and their latent representations. To generate molecular\ngraphs, our MoFlow first generates bonds (edges) through a Glow based model,\nthen generates atoms (nodes) given bonds by a novel graph conditional flow, and\nfinally assembles them into a chemically valid molecular graph with a posthoc\nvalidity correction. Our MoFlow has merits including exact and tractable\nlikelihood training, efficient one-pass embedding and generation, chemical\nvalidity guarantees, 100\\% reconstruction of training data, and good\ngeneralization ability. We validate our model by four tasks: molecular graph\ngeneration and reconstruction, visualization of the continuous latent space,\nproperty optimization, and constrained property optimization. Our MoFlow\nachieves state-of-the-art performance, which implies its potential efficiency\nand effectiveness to explore large chemical space for drug discovery.",
          "arxiv_id": "2006.10137v1"
        }
      ],
      "6": [
        {
          "title": "Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review",
          "year": "2021-07",
          "abstract": "The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of\nlives and has affected all aspects of human life. This paper focuses on the\napplication of deep learning (DL) models to medical imaging and drug discovery\nfor managing COVID-19 disease. In this article, we detail various medical\nimaging-based studies such as X-rays and computed tomography (CT) images along\nwith DL methods for classifying COVID-19 affected versus pneumonia. The\napplications of DL techniques to medical images are further described in terms\nof image localization, segmentation, registration, and classification leading\nto COVID-19 detection. The reviews of recent papers indicate that the highest\nclassification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is\napplied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients\nand 365 normal people. Furthermore, it can be seen that the best classification\naccuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT\nimage dataset of 7500 samples where COVID-19 patients, lung tumor patients and\nnormal people are equal in number. Moreover, we illustrate the potential DL\ntechniques in drug or vaccine discovery in combating the coronavirus. Finally,\nwe address a number of problems, concerns and future research directions\nrelevant to DL applications for COVID-19.",
          "arxiv_id": "2107.09602v1"
        },
        {
          "title": "High Tissue Contrast MRI Synthesis Using Multi-Stage Attention-GAN for Glioma Segmentation",
          "year": "2020-06",
          "abstract": "Magnetic resonance imaging (MRI) provides varying tissue contrast images of\ninternal organs based on a strong magnetic field. Despite the non-invasive\nadvantage of MRI in frequent imaging, the low contrast MR images in the target\narea make tissue segmentation a challenging problem. This paper demonstrates\nthe potential benefits of image-to-image translation techniques to generate\nsynthetic high tissue contrast (HTC) images. Notably, we adopt a new cycle\ngenerative adversarial network (CycleGAN) with an attention mechanism to\nincrease the contrast within underlying tissues. The attention block, as well\nas training on HTC images, guides our model to converge on certain tissues. To\nincrease the resolution of HTC images, we employ multi-stage architecture to\nfocus on one particular tissue as a foreground and filter out the irrelevant\nbackground in each stage. This multi-stage structure also alleviates the common\nartifacts of the synthetic images by decreasing the gap between source and\ntarget domains. We show the application of our method for synthesizing HTC\nimages on brain MR scans, including glioma tumor. We also employ HTC MR images\nin both the end-to-end and two-stage segmentation structure to confirm the\neffectiveness of these images. The experiments over three competitive\nsegmentation baselines on BraTS 2018 dataset indicate that incorporating the\nsynthetic HTC images in the multi-modal segmentation framework improves the\naverage Dice scores 0.8%, 0.6%, and 0.5% on the whole tumor, tumor core, and\nenhancing tumor, respectively, while eliminating one real MRI sequence from the\nsegmentation procedure.",
          "arxiv_id": "2006.05030v1"
        },
        {
          "title": "Comprehensive Comparison of Deep Learning Models for Lung and COVID-19 Lesion Segmentation in CT scans",
          "year": "2020-09",
          "abstract": "Recently there has been an explosion in the use of Deep Learning (DL) methods\nfor medical image segmentation. However the field's reliability is hindered by\nthe lack of a common base of reference for accuracy/performance evaluation and\nthe fact that previous research uses different datasets for evaluation. In this\npaper, an extensive comparison of DL models for lung and COVID-19 lesion\nsegmentation in Computerized Tomography (CT) scans is presented, which can also\nbe used as a benchmark for testing medical image segmentation models. Four DL\narchitectures (Unet, Linknet, FPN, PSPNet) are combined with 25 randomly\ninitialized and pretrained encoders (variations of VGG, DenseNet, ResNet,\nResNext, DPN, MobileNet, Xception, Inception-v4, EfficientNet), to construct\n200 tested models. Three experimental setups are conducted for lung\nsegmentation, lesion segmentation and lesion segmentation using the original\nlung masks. A public COVID-19 dataset with 100 CT scan images (80 for train, 20\nfor validation) is used for training/validation and a different public dataset\nconsisting of 829 images from 9 CT scan volumes for testing. Multiple findings\nare provided including the best architecture-encoder models for each experiment\nas well as mean Dice results for each experiment, architecture and encoder\nindependently. Finally, the upper bounds improvements when using lung masks as\na preprocessing step or when using pretrained models are quantified. The source\ncode and 600 pretrained models for the three experiments are provided, suitable\nfor fine-tuning in experimental setups without GPU capabilities.",
          "arxiv_id": "2009.06412v7"
        }
      ],
      "7": [
        {
          "title": "Robust Self-Supervised Audio-Visual Speech Recognition",
          "year": "2022-01",
          "abstract": "Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.",
          "arxiv_id": "2201.01763v3"
        },
        {
          "title": "Speaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric and Elderly Speech Recognition",
          "year": "2022-02",
          "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\ntargeting normal speech in recent decades, accurate recognition of dysarthric\nand elderly speech remains highly challenging tasks to date. Sources of\nheterogeneity commonly found in normal speech including accent or gender, when\nfurther compounded with the variability over age and speech pathology severity\nlevel, create large diversity among speakers. To this end, speaker adaptation\ntechniques play a key role in personalization of ASR systems for such users.\nMotivated by the spectro-temporal level differences between dysarthric, elderly\nand normal speech that systematically manifest in articulatory imprecision,\ndecreased volume and clarity, slower speaking rates and increased dysfluencies,\nnovel spectrotemporal subspace basis deep embedding features derived using SVD\nspeech spectrum decomposition are proposed in this paper to facilitate\nauxiliary feature based speaker adaptation of state-of-the-art hybrid DNN/TDNN\nand end-to-end Conformer speech recognition systems. Experiments were conducted\non four tasks: the English UASpeech and TORGO dysarthric speech corpora; the\nEnglish DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets.\nThe proposed spectro-temporal deep feature adapted systems outperformed\nbaseline i-Vector and xVector adaptation by up to 2.63% absolute (8.63%\nrelative) reduction in word error rate (WER). Consistent performance\nimprovements were retained after model based speaker adaptation using learning\nhidden unit contributions (LHUC) was further applied. The best speaker adapted\nsystem using the proposed spectral basis embedding features produced the lowest\npublished WER of 25.05% on the UASpeech test set of 16 dysarthric speakers.",
          "arxiv_id": "2202.10290v3"
        },
        {
          "title": "Moshi: a speech-text foundation model for real-time dialogue",
          "year": "2024-09",
          "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.",
          "arxiv_id": "2410.00037v2"
        }
      ],
      "8": [
        {
          "title": "The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods",
          "year": "2021-02",
          "abstract": "The adaptive stochastic gradient descent (SGD) with momentum has been widely\nadopted in deep learning as well as convex optimization. In practice, the last\niterate is commonly used as the final solution to make decisions. However, the\navailable regret analysis and the setting of constant momentum parameters only\nguarantee the optimal convergence of the averaged solution. In this paper, we\nfill this theory-practice gap by investigating the convergence of the last\niterate (referred to as individual convergence), which is a more difficult task\nthan convergence analysis of the averaged solution. Specifically, in the\nconstrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB)\nmethod, in which only the step size is updated using the exponential moving\naverage strategy, attains an optimal individual convergence rate of\n$O(\\frac{1}{\\sqrt{t}})$, as opposed to the optimality of $O(\\frac{\\log t}{\\sqrt\n{t}})$ of SGD, where $t$ is the number of iterations. Our new analysis not only\nshows how the HB momentum and its time-varying weight help us to achieve the\nacceleration in convex optimization but also gives valuable hints how the\nmomentum parameters should be scheduled in deep learning. Empirical results on\noptimizing convex functions and training deep networks validate the correctness\nof our convergence analysis and demonstrate the improved performance of the\nadaptive HB methods.",
          "arxiv_id": "2102.07314v1"
        },
        {
          "title": "From Sublinear to Linear: Fast Convergence in Deep Networks via Locally Polyak-Lojasiewicz Regions",
          "year": "2025-07",
          "abstract": "The convergence of gradient descent (GD) on the non-convex loss landscapes of\ndeep neural networks (DNNs) presents a fundamental theoretical challenge. While\nrecent work has established that GD converges to a stationary point at a\nsublinear rate within locally quasi-convex regions (LQCRs), this fails to\nexplain the exponential convergence rates consistently observed in practice. In\nthis paper, we resolve this discrepancy by proving that under a mild assumption\non Neural Tangent Kernel (NTK) stability, these same regions satisfy a local\nPolyak-Lojasiewicz (PL) condition. We introduce the concept of a Locally\nPolyak-Lojasiewicz Region (LPLR), where the squared gradient norm lower-bounds\nthe suboptimality gap, prove that properly initialized finite-width networks\nadmit such regions around initialization, and establish that GD achieves linear\nconvergence within an LPLR, providing the first finite-width guarantee that\nmatches empirically observed rates. We validate our theory across diverse\nsettings, from controlled experiments on fully-connected networks to modern\nResNet architectures trained with stochastic methods, demonstrating that LPLR\nstructure emerges robustly in practical deep learning scenarios. By rigorously\nconnecting local landscape geometry to fast optimization through the NTK\nframework, our work provides a definitive theoretical explanation for the\nremarkable efficiency of gradient-based optimization in deep learning.",
          "arxiv_id": "2507.21429v1"
        },
        {
          "title": "Exponential convergence rates for momentum stochastic gradient descent in the overparametrized setting",
          "year": "2023-02",
          "abstract": "We prove explicit bounds on the exponential rate of convergence for the\nmomentum stochastic gradient descent scheme (MSGD) for arbitrary, fixed\nhyperparameters (learning rate, friction parameter) and its continuous-in-time\ncounterpart in the context of non-convex optimization. In the small step-size\nregime and in the case of flat minima or large noise intensities, these bounds\nprove faster convergence of MSGD compared to plain stochastic gradient descent\n(SGD). The results are shown for objective functions satisfying a local\nPolyak-Lojasiewicz inequality and under assumptions on the variance of MSGD\nthat are satisfied in overparametrized settings. Moreover, we analyze the\noptimal choice of the friction parameter and show that the MSGD process almost\nsurely converges to a local minimum.",
          "arxiv_id": "2302.03550v2"
        }
      ],
      "9": [
        {
          "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion",
          "year": "2021-12",
          "abstract": "Score-based generative models (SGMs) have demonstrated remarkable synthesis\nquality. SGMs rely on a diffusion process that gradually perturbs the data\ntowards a tractable distribution, while the generative model learns to denoise.\nThe complexity of this denoising task is, apart from the data distribution\nitself, uniquely determined by the diffusion process. We argue that current\nSGMs employ overly simplistic diffusions, leading to unnecessarily complex\ndenoising processes, which limit generative modeling performance. Based on\nconnections to statistical mechanics, we propose a novel critically-damped\nLangevin diffusion (CLD) and show that CLD-based SGMs achieve superior\nperformance. CLD can be interpreted as running a joint diffusion in an extended\nspace, where the auxiliary variables can be considered \"velocities\" that are\ncoupled to the data variables as in Hamiltonian dynamics. We derive a novel\nscore matching objective for CLD and show that the model only needs to learn\nthe score function of the conditional distribution of the velocity given data,\nan easier task than learning scores of the data directly. We also derive a new\nsampling scheme for efficient synthesis from CLD-based diffusion models. We\nfind that CLD outperforms previous SGMs in synthesis quality for similar\nnetwork architectures and sampling compute budgets. We show that our novel\nsampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our\nframework provides new insights into score-based denoising diffusion models and\ncan be readily used for high-resolution image synthesis. Project page and code:\nhttps://nv-tlabs.github.io/CLD-SGM.",
          "arxiv_id": "2112.07068v4"
        },
        {
          "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models",
          "year": "2025-07",
          "abstract": "We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.",
          "arxiv_id": "2507.12318v2"
        },
        {
          "title": "Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis",
          "year": "2023-09",
          "abstract": "Conditional generative models typically demand large annotated training sets\nto achieve high-quality synthesis. As a result, there has been significant\ninterest in designing models that perform plug-and-play generation, i.e., to\nuse a predefined or pretrained model, which is not explicitly trained on the\ngenerative task, to guide the generative process (e.g., using language).\nHowever, such guidance is typically useful only towards synthesizing high-level\nsemantics rather than editing fine-grained details as in image-to-image\ntranslation tasks. To this end, and capitalizing on the powerful fine-grained\ngenerative control offered by the recent diffusion-based generative models, we\nintroduce Steered Diffusion, a generalized framework for photorealistic\nzero-shot conditional image generation using a diffusion model trained for\nunconditional generation. The key idea is to steer the image generation of the\ndiffusion model at inference time via designing a loss using a pre-trained\ninverse model that characterizes the conditional task. This loss modulates the\nsampling trajectory of the diffusion process. Our framework allows for easy\nincorporation of multiple conditions during inference. We present experiments\nusing steered diffusion on several tasks including inpainting, colorization,\ntext-guided semantic editing, and image super-resolution. Our results\ndemonstrate clear qualitative and quantitative improvements over\nstate-of-the-art diffusion-based plug-and-play models while adding negligible\nadditional computational cost.",
          "arxiv_id": "2310.00224v1"
        }
      ],
      "10": [
        {
          "title": "NASCaps: A Framework for Neural Architecture Search to Optimize the Accuracy and Hardware Efficiency of Convolutional Capsule Networks",
          "year": "2020-08",
          "abstract": "Deep Neural Networks (DNNs) have made significant improvements to reach the\ndesired accuracy to be employed in a wide variety of Machine Learning (ML)\napplications. Recently the Google Brain's team demonstrated the ability of\nCapsule Networks (CapsNets) to encode and learn spatial correlations between\ndifferent input features, thereby obtaining superior learning capabilities\ncompared to traditional (i.e., non-capsule based) DNNs. However, designing\nCapsNets using conventional methods is a tedious job and incurs significant\ntraining effort. Recent studies have shown that powerful methods to\nautomatically select the best/optimal DNN model configuration for a given set\nof applications and a training dataset are based on the Neural Architecture\nSearch (NAS) algorithms. Moreover, due to their extreme computational and\nmemory requirements, DNNs are employed using the specialized hardware\naccelerators in IoT-Edge/CPS devices. In this paper, we propose NASCaps, an\nautomated framework for the hardware-aware NAS of different types of DNNs,\ncovering both traditional convolutional DNNs and CapsNets. We study the\nefficacy of deploying a multi-objective Genetic Algorithm (e.g., based on the\nNSGA-II algorithm). The proposed framework can jointly optimize the network\naccuracy and the corresponding hardware efficiency, expressed in terms of\nenergy, memory, and latency of a given hardware accelerator executing the DNN\ninference. Besides supporting the traditional DNN layers, our framework is the\nfirst to model and supports the specialized capsule layers and dynamic routing\nin the NAS-flow. We evaluate our framework on different datasets, generating\ndifferent network configurations, and demonstrate the tradeoffs between the\ndifferent output metrics. We will open-source the complete framework and\nconfigurations of the Pareto-optimal architectures at\nhttps://github.com/ehw-fit/nascaps.",
          "arxiv_id": "2008.08476v1"
        },
        {
          "title": "Search-time Efficient Device Constraints-Aware Neural Architecture Search",
          "year": "2023-07",
          "abstract": "Edge computing aims to enable edge devices, such as IoT devices, to process\ndata locally instead of relying on the cloud. However, deep learning techniques\nlike computer vision and natural language processing can be computationally\nexpensive and memory-intensive. Creating manual architectures specialized for\neach device is infeasible due to their varying memory and computational\nconstraints. To address these concerns, we automate the construction of\ntask-specific deep learning architectures optimized for device constraints\nthrough Neural Architecture Search (NAS). We present DCA-NAS, a principled\nmethod of fast neural network architecture search that incorporates edge-device\nconstraints such as model size and floating-point operations. It incorporates\nweight sharing and channel bottleneck techniques to speed up the search time.\nBased on our experiments, we see that DCA-NAS outperforms manual architectures\nfor similar sized models and is comparable to popular mobile architectures on\nvarious image classification datasets like CIFAR-10, CIFAR-100, and\nImagenet-1k. Experiments with search spaces -- DARTS and NAS-Bench-201 show the\ngeneralization capabilities of DCA-NAS. On further evaluating our approach on\nHardware-NAS-Bench, device-specific architectures with low inference latency\nand state-of-the-art performance were discovered.",
          "arxiv_id": "2307.04443v1"
        },
        {
          "title": "Hybrid In-memory Computing Architecture for the Training of Deep Neural Networks",
          "year": "2021-02",
          "abstract": "The cost involved in training deep neural networks (DNNs) on von-Neumann\narchitectures has motivated the development of novel solutions for efficient\nDNN training accelerators. We propose a hybrid in-memory computing (HIC)\narchitecture for the training of DNNs on hardware accelerators that results in\nmemory-efficient inference and outperforms baseline software accuracy in\nbenchmark tasks. We introduce a weight representation technique that exploits\nboth binary and multi-level phase-change memory (PCM) devices, and this leads\nto a memory-efficient inference accelerator. Unlike previous in-memory\ncomputing-based implementations, we use a low precision weight update\naccumulator that results in more memory savings. We trained the ResNet-32\nnetwork to classify CIFAR-10 images using HIC. For a comparable model size,\nHIC-based training outperforms baseline network, trained in floating-point\n32-bit (FP32) precision, by leveraging appropriate network width multiplier.\nFurthermore, we observe that HIC-based training results in about 50% less\ninference model size to achieve baseline comparable accuracy. We also show that\nthe temporal drift in PCM devices has a negligible effect on post-training\ninference accuracy for extended periods (year). Finally, our simulations\nindicate HIC-based training naturally ensures that the number of write-erase\ncycles seen by the devices is a small fraction of the endurance limit of PCM,\ndemonstrating the feasibility of this architecture for achieving hardware\nplatforms that can learn in the field.",
          "arxiv_id": "2102.05271v1"
        }
      ],
      "11": [
        {
          "title": "Deep Joint Transmission-Recognition for Multi-View Cameras",
          "year": "2020-11",
          "abstract": "We propose joint transmission-recognition schemes for efficient inference at\nthe wireless edge. Motivated by the surveillance applications with wireless\ncameras, we consider the person classification task over a wireless channel\ncarried out by multi-view cameras operating as edge devices. We introduce deep\nneural network (DNN) based compression schemes which incorporate digital\n(separate) transmission and joint source-channel coding (JSCC) methods. We\nevaluate the proposed device-edge communication schemes under different channel\nSNRs, bandwidth and power constraints. We show that the JSCC schemes not only\nimprove the end-to-end accuracy but also simplify the encoding process and\nprovide graceful degradation with channel quality.",
          "arxiv_id": "2011.01902v1"
        },
        {
          "title": "Computation Offloading and Resource Allocation in F-RANs: A Federated Deep Reinforcement Learning Approach",
          "year": "2022-06",
          "abstract": "The fog radio access network (F-RAN) is a promising technology in which the\nuser mobile devices (MDs) can offload computation tasks to the nearby fog\naccess points (F-APs). Due to the limited resource of F-APs, it is important to\ndesign an efficient task offloading scheme. In this paper, by considering\ntime-varying network environment, a dynamic computation offloading and resource\nallocation problem in F-RANs is formulated to minimize the task execution delay\nand energy consumption of MDs. To solve the problem, a federated deep\nreinforcement learning (DRL) based algorithm is proposed, where the deep\ndeterministic policy gradient (DDPG) algorithm performs computation offloading\nand resource allocation in each F-AP. Federated learning is exploited to train\nthe DDPG agents in order to decrease the computing complexity of training\nprocess and protect the user privacy. Simulation results show that the proposed\nfederated DDPG algorithm can achieve lower task execution delay and energy\nconsumption of MDs more quickly compared with the other existing strategies.",
          "arxiv_id": "2206.05881v1"
        },
        {
          "title": "Model-Driven Deep Learning Based Channel Estimation and Feedback for Millimeter-Wave Massive Hybrid MIMO Systems",
          "year": "2021-04",
          "abstract": "This paper proposes a model-driven deep learning (MDDL)-based channel\nestimation and feedback scheme for wideband millimeter-wave (mmWave) massive\nhybrid multiple-input multiple-output (MIMO) systems, where the angle-delay\ndomain channels' sparsity is exploited for reducing the overhead. Firstly, we\nconsider the uplink channel estimation for time-division duplexing systems. To\nreduce the uplink pilot overhead for estimating the high-dimensional channels\nfrom a limited number of radio frequency (RF) chains at the base station (BS),\nwe propose to jointly train the phase shift network and the channel estimator\nas an auto-encoder. Particularly, by exploiting the channels' structured\nsparsity from an a priori model and learning the integrated trainable\nparameters from the data samples, the proposed multiple-measurement-vectors\nlearned approximate message passing (MMV-LAMP) network with the devised\nredundant dictionary can jointly recover multiple subcarriers' channels with\nsignificantly enhanced performance. Moreover, we consider the downlink channel\nestimation and feedback for frequency-division duplexing systems. Similarly,\nthe pilots at the BS and channel estimator at the users can be jointly trained\nas an encoder and a decoder, respectively. Besides, to further reduce the\nchannel feedback overhead, only the received pilots on part of the subcarriers\nare fed back to the BS, which can exploit the MMV-LAMP network to reconstruct\nthe spatial-frequency channel matrix. Numerical results show that the proposed\nMDDL-based channel estimation and feedback scheme outperforms the\nstate-of-the-art approaches.",
          "arxiv_id": "2104.11052v3"
        }
      ],
      "12": [
        {
          "title": "Near-optimal Per-Action Regret Bounds for Sleeping Bandits",
          "year": "2024-03",
          "abstract": "We derive near-optimal per-action regret bounds for sleeping bandits, in\nwhich both the sets of available arms and their losses in every round are\nchosen by an adversary. In a setting with $K$ total arms and at most $A$\navailable arms in each round over $T$ rounds, the best known upper bound is\n$O(K\\sqrt{TA\\ln{K}})$, obtained indirectly via minimizing internal sleeping\nregrets. Compared to the minimax $\\Omega(\\sqrt{TA})$ lower bound, this upper\nbound contains an extra multiplicative factor of $K\\ln{K}$. We address this gap\nby directly minimizing the per-action regret using generalized versions of\nEXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal\nbounds of order $O(\\sqrt{TA\\ln{K}})$ and $O(\\sqrt{T\\sqrt{AK}})$. We extend our\nresults to the setting of bandits with advice from sleeping experts,\ngeneralizing EXP4 along the way. This leads to new proofs for a number of\nexisting adaptive and tracking regret bounds for standard non-sleeping bandits.\nExtending our results to the bandit version of experts that report their\nconfidences leads to new bounds for the confidence regret that depends\nprimarily on the sum of experts' confidences. We prove a lower bound, showing\nthat for any minimax optimal algorithms, there exists an action whose regret is\nsublinear in $T$ but linear in the number of its active rounds.",
          "arxiv_id": "2403.01315v2"
        },
        {
          "title": "Tight Regret Bounds for Single-pass Streaming Multi-armed Bandits",
          "year": "2023-06",
          "abstract": "Regret minimization in streaming multi-armed bandits (MABs) has been studied\nextensively in recent years. In the single-pass setting with $K$ arms and $T$\ntrials, a regret lower bound of $\\Omega(T^{2/3})$ has been proved for any\nalgorithm with $o(K)$ memory (Maiti et al. [NeurIPS'21]; Agarwal at al.\n[COLT'22]). On the other hand, however, the previous best regret upper bound is\nstill $O(K^{1/3} T^{2/3}\\log^{1/3}(T))$, which is achieved by the streaming\nimplementation of the simple uniform exploration. The $O(K^{1/3}\\log^{1/3}(T))$\ngap leaves the open question of the tight regret bound in the single-pass MABs\nwith sublinear arm memory.\n  In this paper, we answer this open problem and complete the picture of regret\nminimization in single-pass streaming MABs. We first improve the regret lower\nbound to $\\Omega(K^{1/3}T^{2/3})$ for algorithms with $o(K)$ memory, which\nmatches the uniform exploration regret up to a logarithm factor in $T$. We then\nshow that the $\\log^{1/3}(T)$ factor is not necessary, and we can achieve\n$O(K^{1/3}T^{2/3})$ regret by finding an $\\varepsilon$-best arm and committing\nto it in the rest of the trials. For regret minimization with high constant\nprobability, we can apply the single-memory $\\varepsilon$-best arm algorithms\nin Jin et al. [ICML'21] to obtain the optimal bound. Furthermore, for the\nexpected regret minimization, we design an algorithm with a single-arm memory\nthat achieves $O(K^{1/3} T^{2/3}\\log(K))$ regret, and an algorithm with\n$O(\\log^{*}(n))$-memory with the optimal $O(K^{1/3} T^{2/3})$ regret following\nthe $\\varepsilon$-best arm algorithm in Assadi and Wang [STOC'20].\n  We further tested the empirical performances of our algorithms. The\nsimulation results show that the proposed algorithms consistently outperform\nthe benchmark uniform exploration algorithm by a large margin, and on occasion,\nreduce the regret by up to 70%.",
          "arxiv_id": "2306.02208v1"
        },
        {
          "title": "Combinatorial Bandits for Maximum Value Reward Function under Max Value-Index Feedback",
          "year": "2023-05",
          "abstract": "We consider a combinatorial multi-armed bandit problem for maximum value\nreward function under maximum value and index feedback. This is a new feedback\nstructure that lies in between commonly studied semi-bandit and full-bandit\nfeedback structures. We propose an algorithm and provide a regret bound for\nproblem instances with stochastic arm outcomes according to arbitrary\ndistributions with finite supports. The regret analysis rests on considering an\nextended set of arms, associated with values and probabilities of arm outcomes,\nand applying a smoothness condition. Our algorithm achieves a\n$O((k/\\Delta)\\log(T))$ distribution-dependent and a $\\tilde{O}(\\sqrt{T})$\ndistribution-independent regret where $k$ is the number of arms selected in\neach round, $\\Delta$ is a distribution-dependent reward gap and $T$ is the\nhorizon time. Perhaps surprisingly, the regret bound is comparable to\npreviously-known bound under more informative semi-bandit feedback. We\ndemonstrate the effectiveness of our algorithm through experimental results.",
          "arxiv_id": "2305.16074v1"
        }
      ],
      "13": [
        {
          "title": "Causal DAG Summarization (Full Version)",
          "year": "2025-04",
          "abstract": "Causal inference aids researchers in discovering cause-and-effect\nrelationships, leading to scientific insights. Accurate causal estimation\nrequires identifying confounding variables to avoid false discoveries. Pearl's\ncausal model uses causal DAGs to identify confounding variables, but incorrect\nDAGs can lead to unreliable causal conclusions. However, for high dimensional\ndata, the causal DAGs are often complex beyond human verifiability. Graph\nsummarization is a logical next step, but current methods for general-purpose\ngraph summarization are inadequate for causal DAG summarization. This paper\naddresses these challenges by proposing a causal graph summarization objective\nthat balances graph simplification for better understanding while retaining\nessential causal information for reliable inference. We develop an efficient\ngreedy algorithm and show that summary causal DAGs can be directly used for\ninference and are more robust to misspecification of assumptions, enhancing\nrobustness for causal inference. Experimenting with six real-life datasets, we\ncompared our algorithm to three existing solutions, showing its effectiveness\nin handling high-dimensional data and its ability to generate summary DAGs that\nensure both reliable causal inference and robustness against misspecifications.",
          "arxiv_id": "2504.14937v1"
        },
        {
          "title": "Deep End-to-end Causal Inference",
          "year": "2022-02",
          "abstract": "Causal inference is essential for data-driven decision making across domains\nsuch as business engagement, medical treatment and policy making. However,\nresearch on causal discovery has evolved separately from inference methods,\npreventing straight-forward combination of methods from both fields. In this\nwork, we develop Deep End-to-end Causal Inference (DECI), a single flow-based\nnon-linear additive noise model that takes in observational data and can\nperform both causal discovery and inference, including conditional average\ntreatment effect (CATE) estimation. We provide a theoretical guarantee that\nDECI can recover the ground truth causal graph under standard causal discovery\nassumptions. Motivated by application impact, we extend this model to\nheterogeneous, mixed-type data with missing values, allowing for both\ncontinuous and discrete treatment decisions. Our results show the competitive\nperformance of DECI when compared to relevant baselines for both causal\ndiscovery and (C)ATE estimation in over a thousand experiments on both\nsynthetic datasets and causal machine learning benchmarks across data-types and\nlevels of missingness.",
          "arxiv_id": "2202.02195v2"
        },
        {
          "title": "Causal Mediation Analysis with Hidden Confounders",
          "year": "2021-02",
          "abstract": "An important problem in causal inference is to break down the total effect of\na treatment on an outcome into different causal pathways and to quantify the\ncausal effect in each pathway. For instance, in causal fairness, the total\neffect of being a male employee (i.e., treatment) constitutes its direct effect\non annual income (i.e., outcome) and the indirect effect via the employee's\noccupation (i.e., mediator). Causal mediation analysis (CMA) is a formal\nstatistical framework commonly used to reveal such underlying causal\nmechanisms. One major challenge of CMA in observational studies is handling\nconfounders, variables that cause spurious causal relationships among\ntreatment, mediator, and outcome. Conventional methods assume sequential\nignorability that implies all confounders can be measured, which is often\nunverifiable in practice. This work aims to circumvent the stringent sequential\nignorability assumptions and consider hidden confounders. Drawing upon proxy\nstrategies and recent advances in deep learning, we propose to simultaneously\nuncover the latent variables that characterize hidden confounders and estimate\nthe causal effects. Empirical evaluations using both synthetic and\nsemi-synthetic datasets validate the effectiveness of the proposed method. We\nfurther show the potentials of our approach for causal fairness analysis.",
          "arxiv_id": "2102.11724v3"
        }
      ],
      "14": [
        {
          "title": "Generative Medical Event Models Improve with Scale",
          "year": "2025-08",
          "abstract": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family of\ndecoder-only transformer models pretrained on 118 million patients representing\n115 billion discrete medical events (151 billion tokens). We present the\nlargest scaling-law study for medical event data, establishing a methodology\nfor pretraining and revealing power-law scaling relationships for compute,\ntokens, and model size. Based on this, we pretrained a series of\ncompute-optimal models with up to 1 billion parameters. Conditioned on a\npatient's real-world history, CoMET autoregressively generates the next medical\nevent, simulating patient health timelines. We studied 78 real-world tasks,\nincluding diagnosis prediction, disease prognosis, and healthcare operations.\nRemarkably for a foundation model with generic pretraining and simulation-based\ninference, CoMET generally outperformed or matched task-specific supervised\nmodels on these tasks, without requiring task-specific fine-tuning or few-shot\nexamples. CoMET's predictive power consistently improves as the model and\npretraining scale. Our results show that CoMET, a generative medical event\nfoundation model, can effectively capture complex clinical dynamics, providing\nan extensible and generalizable framework to support clinical decision-making,\nstreamline healthcare operations, and improve patient outcomes.",
          "arxiv_id": "2508.12104v1"
        },
        {
          "title": "A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care",
          "year": "2022-09",
          "abstract": "The COVID-19 pandemic has posed a heavy burden to the healthcare system\nworldwide and caused huge social disruption and economic loss. Many deep\nlearning models have been proposed to conduct clinical predictive tasks such as\nmortality prediction for COVID-19 patients in intensive care units using\nElectronic Health Record (EHR) data. Despite their initial success in certain\nclinical applications, there is currently a lack of benchmarking results to\nachieve a fair comparison so that we can select the optimal model for clinical\nuse. Furthermore, there is a discrepancy between the formulation of traditional\nprediction tasks and real-world clinical practice in intensive care. To fill\nthese gaps, we propose two clinical prediction tasks, Outcome-specific\nlength-of-stay prediction and Early mortality prediction for COVID-19 patients\nin intensive care units. The two tasks are adapted from the naive\nlength-of-stay and mortality prediction tasks to accommodate the clinical\npractice for COVID-19 patients. We propose fair, detailed, open-source\ndata-preprocessing pipelines and evaluate 17 state-of-the-art predictive models\non two tasks, including 5 machine learning models, 6 basic deep learning models\nand 6 deep learning predictive models specifically designed for EHR data. We\nprovide benchmarking results using data from two real-world COVID-19 EHR\ndatasets. One dataset is publicly available without needing any inquiry and\nanother dataset can be accessed on request. We provide fair, reproducible\nbenchmarking results for two tasks. We deploy all experiment results and models\non an online platform. We also allow clinicians and researchers to upload their\ndata to the platform and get quick prediction results using our trained models.\nWe hope our efforts can further facilitate deep learning and machine learning\nresearch for COVID-19 predictive modeling.",
          "arxiv_id": "2209.07805v4"
        },
        {
          "title": "CliBench: A Multifaceted and Multigranular Evaluation of Large Language Models for Clinical Decision Making",
          "year": "2024-06",
          "abstract": "The integration of Artificial Intelligence (AI), especially Large Language\nModels (LLMs), into the clinical diagnosis process offers significant potential\nto improve the efficiency and accessibility of medical care. While LLMs have\nshown some promise in the medical domain, their application in clinical\ndiagnosis remains underexplored, especially in real-world clinical practice,\nwhere highly sophisticated, patient-specific decisions need to be made. Current\nevaluations of LLMs in this field are often narrow in scope, focusing on\nspecific diseases or specialties and employing simplified diagnostic tasks. To\nbridge this gap, we introduce CliBench, a novel benchmark developed from the\nMIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'\ncapabilities in clinical diagnosis. This benchmark not only covers diagnoses\nfrom a diverse range of medical cases across various specialties but also\nincorporates tasks of clinical significance: treatment procedure\nidentification, lab test ordering and medication prescriptions. Supported by\nstructured output ontologies, CliBench enables a precise and multi-granular\nevaluation, offering an in-depth understanding of LLM's capability on diverse\nclinical tasks of desired granularity. We conduct a zero-shot evaluation of\nleading LLMs to assess their proficiency in clinical decision-making. Our\npreliminary results shed light on the potential and limitations of current LLMs\nin clinical settings, providing valuable insights for future advancements in\nLLM-powered healthcare.",
          "arxiv_id": "2406.09923v2"
        }
      ],
      "15": [
        {
          "title": "Creator-Side Recommender System: Challenges, Designs, and Applications",
          "year": "2025-02",
          "abstract": "Users and creators are two crucial components of recommender systems. Typical\nrecommender systems focus on the user side, providing the most suitable items\nbased on each user's request. In such scenarios, a few items receive a majority\nof exposures, while many items receive very few. This imbalance leads to poorer\nexperiences and decreased activity among the creators receiving less feedback,\nharming the recommender system in the long term. To this end, we develop a\ncreator-side recommender system, called DualRec, to answer the following\nquestion: how to find the most suitable users for each item to enhance the\ncreators' experience? We show that typical user-side recommendation algorithms,\nsuch as retrieval and ranking algorithms, can be adapted into the creator-side\nversions with just a few modifications. This greatly simplifies algorithm\ndesign in DualRec. Moreover, we discuss a unique challenge in DualRec: the user\navailability issue, which is not present in user-side recommender systems. To\ntackle this issue, we incorporate a user availability calculation (UAC) module\nto effectively enhance DualRec's performance. DualRec has already been\nimplemented in Kwai, a short video recommendation system with over 100 millions\nuser and over 10 million creators, significantly improving the experience for\ncreators.",
          "arxiv_id": "2502.20497v1"
        },
        {
          "title": "FINN.no Slates Dataset: A new Sequential Dataset Logging Interactions, allViewed Items and Click Responses/No-Click for Recommender Systems Research",
          "year": "2021-11",
          "abstract": "We present a novel recommender systems dataset that records the sequential\ninteractions between users and an online marketplace. The users are\nsequentially presented with both recommendations and search results in the form\nof ranked lists of items, called slates, from the marketplace. The dataset\nincludes the presented slates at each round, whether the user clicked on any of\nthese items and which item the user clicked on. Although the usage of exposure\ndata in recommender systems is growing, to our knowledge there is no open\nlarge-scale recommender systems dataset that includes the slates of items\npresented to the users at each interaction. As a result, most articles on\nrecommender systems do not utilize this exposure information. Instead, the\nproposed models only depend on the user's click responses, and assume that the\nuser is exposed to all the items in the item universe at each step, often\ncalled uniform candidate sampling. This is an incomplete assumption, as it\ntakes into account items the user might not have been exposed to. This way\nitems might be incorrectly considered as not of interest to the user. Taking\ninto account the actually shown slates allows the models to use a more natural\nlikelihood, based on the click probability given the exposure set of items, as\nis prevalent in the bandit and reinforcement learning literature.\n\\cite{Eide2021DynamicSampling} shows that likelihoods based on uniform\ncandidate sampling (and similar assumptions) are implicitly assuming that the\nplatform only shows the most relevant items to the user. This causes the\nrecommender system to implicitly reinforce feedback loops and to be biased\ntowards previously exposed items to the user.",
          "arxiv_id": "2111.03340v1"
        },
        {
          "title": "Multi-Behavior Sequential Recommendation with Temporal Graph Transformer",
          "year": "2022-06",
          "abstract": "Modeling time-evolving preferences of users with their sequential item\ninteractions, has attracted increasing attention in many online applications.\nHence, sequential recommender systems have been developed to learn the dynamic\nuser interests from the historical interactions for suggesting items. However,\nthe interaction pattern encoding functions in most existing sequential\nrecommender systems have focused on single type of user-item interactions. In\nmany real-life online platforms, user-item interactive behaviors are often\nmulti-typed (e.g., click, add-to-favorite, purchase) with complex cross-type\nbehavior inter-dependencies. Learning from informative representations of users\nand items based on their multi-typed interaction data, is of great importance\nto accurately characterize the time-evolving user preference. In this work, we\ntackle the dynamic user-item relation learning with the awareness of\nmulti-behavior interactive patterns. Towards this end, we propose a new\nTemporal Graph Transformer (TGT) recommendation framework to jointly capture\ndynamic short-term and long-range user-item interactive patterns, by exploring\nthe evolving correlations across different types of behaviors. The new TGT\nmethod endows the sequential recommendation architecture to distill dedicated\nknowledge for type-specific behavior relational context and the implicit\nbehavior dependencies. Experiments on the real-world datasets indicate that our\nmethod TGT consistently outperforms various state-of-the-art recommendation\nmethods. Our model implementation codes are available at\nhttps://github.com/akaxlh/TGT.",
          "arxiv_id": "2206.02687v1"
        }
      ],
      "16": [
        {
          "title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training",
          "year": "2024-12",
          "abstract": "Vision-Language Models (VLMs) trained with contrastive loss have achieved\nsignificant advancements in various vision and language tasks. However, the\nglobal nature of the contrastive loss makes VLMs focus predominantly on\nforeground objects, neglecting other crucial information in the image, which\nlimits their effectiveness in downstream tasks. To address these challenges, we\npropose COSMOS: CrOSs-MOdality Self-distillation for vision-language\npre-training that integrates a novel text-cropping strategy and cross-attention\nmodule into a self-supervised learning framework. We create global and local\nviews of images and texts (i.e., multi-modal augmentations), which are\nessential for self-distillation in VLMs. We further introduce a cross-attention\nmodule, enabling COSMOS to learn comprehensive cross-modal representations\noptimized via a cross-modality self-distillation loss. COSMOS consistently\noutperforms previous strong baselines on various zero-shot downstream tasks,\nincluding retrieval, classification, and semantic segmentation. Additionally,\nit surpasses CLIP-based models trained on larger datasets in visual perception\nand contextual understanding tasks. Code is available at\nhttps://github.com/ExplainableML/cosmos.",
          "arxiv_id": "2412.01814v2"
        },
        {
          "title": "Learning the Visualness of Text Using Large Vision-Language Models",
          "year": "2023-05",
          "abstract": "Visual text evokes an image in a person's mind, while non-visual text fails\nto do so. A method to automatically detect visualness in text will enable\ntext-to-image retrieval and generation models to augment text with relevant\nimages. This is particularly challenging with long-form text as text-to-image\ngeneration and retrieval models are often triggered for text that is designed\nto be explicitly visual in nature, whereas long-form text could contain many\nnon-visual sentences. To this end, we curate a dataset of 3,620 English\nsentences and their visualness scores provided by multiple human annotators. We\nalso propose a fine-tuning strategy that adapts large vision-language models\nlike CLIP by modifying the model's contrastive learning objective to map text\nidentified as non-visual to a common NULL image while matching visual text to\ntheir corresponding images in the document. We evaluate the proposed approach\non its ability to (i) classify visual and non-visual text accurately, and (ii)\nattend over words that are identified as visual in psycholinguistic studies.\nEmpirical evaluation indicates that our approach performs better than several\nheuristics and baseline models for the proposed task. Furthermore, to highlight\nthe importance of modeling the visualness of text, we conduct qualitative\nanalyses of text-to-image generation systems like DALL-E. Project webpage:\nhttps://gaurav22verma.github.io/text-visualness/",
          "arxiv_id": "2305.10434v2"
        },
        {
          "title": "Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval",
          "year": "2024-12",
          "abstract": "Cross-modal (e.g. image-text, video-text) retrieval is an important task in\ninformation retrieval and multimodal vision-language understanding field.\nTemporal understanding makes video-text retrieval more challenging than\nimage-text retrieval. However, we find that the widely used video-text\nbenchmarks have shortcomings in comprehensively assessing abilities of models,\nespecially in temporal understanding, causing large-scale image-text\npre-trained models can already achieve comparable zero-shot performance with\nvideo-text pre-trained models. In this paper, we introduce RTime, a novel\ntemporal-emphasized video-text retrieval dataset. We first obtain videos of\nactions or events with significant temporality, and then reverse these videos\nto create harder negative samples. We then recruit annotators to judge the\nsignificance and reversibility of candidate videos, and write captions for\nqualified videos. We further adopt GPT-4 to extend more captions based on\nhuman-written captions. Our RTime dataset currently consists of 21k videos with\n10 captions per video, totalling about 122 hours. Based on RTime, we propose\nthree retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We\nfurther enhance the use of harder-negatives in model training, and benchmark a\nvariety of video-text models on RTime. Extensive experiment analysis proves\nthat RTime indeed poses new and higher challenges to video-text retrieval. We\nrelease our RTime\ndataset\\footnote{\\url{https://github.com/qyr0403/Reversed-in-Time}} to further\nadvance video-text retrieval and multimodal understanding research.",
          "arxiv_id": "2412.19178v1"
        }
      ],
      "17": [
        {
          "title": "DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning",
          "year": "2021-06",
          "abstract": "The capability of the traditional semi-supervised learning (SSL) methods is\nfar from real-world application due to severely biased pseudo-labels caused by\n(1) class imbalance and (2) class distribution mismatch between labeled and\nunlabeled data. This paper addresses such a relatively under-explored problem.\nFirst, we propose a general pseudo-labeling framework that class-adaptively\nblends the semantic pseudo-label from a similarity-based classifier to the\nlinear one from the linear classifier, after making the observation that both\ntypes of pseudo-labels have complementary properties in terms of bias. We\nfurther introduce a novel semantic alignment loss to establish balanced feature\nrepresentation to reduce the biased predictions from the classifier. We term\nthe whole framework as Distribution-Aware Semantics-Oriented (DASO)\nPseudo-label. We conduct extensive experiments in a wide range of imbalanced\nbenchmarks: CIFAR10/100-LT, STL10-LT, and large-scale long-tailed Semi-Aves\nwith open-set class, and demonstrate that, the proposed DASO framework reliably\nimproves SSL learners with unlabeled data especially when both (1) class\nimbalance and (2) distribution mismatch dominate.",
          "arxiv_id": "2106.05682v2"
        },
        {
          "title": "Mixed Blessing: Class-Wise Embedding guided Instance-Dependent Partial Label Learning",
          "year": "2024-12",
          "abstract": "In partial label learning (PLL), every sample is associated with a candidate\nlabel set comprising the ground-truth label and several noisy labels. The\nconventional PLL assumes the noisy labels are randomly generated\n(instance-independent), while in practical scenarios, the noisy labels are\nalways instance-dependent and are highly related to the sample features,\nleading to the instance-dependent partial label learning (IDPLL) problem.\nInstance-dependent noisy label is a double-edged sword. On one side, it may\npromote model training as the noisy labels can depict the sample to some\nextent. On the other side, it brings high label ambiguity as the noisy labels\nare quite undistinguishable from the ground-truth label. To leverage the\nnuances of IDPLL effectively, for the first time we create class-wise\nembeddings for each sample, which allow us to explore the relationship of\ninstance-dependent noisy labels, i.e., the class-wise embeddings in the\ncandidate label set should have high similarity, while the class-wise\nembeddings between the candidate label set and the non-candidate label set\nshould have high dissimilarity. Moreover, to reduce the high label ambiguity,\nwe introduce the concept of class prototypes containing global feature\ninformation to disambiguate the candidate label set. Extensive experimental\ncomparisons with twelve methods on six benchmark data sets, including four\nfine-grained data sets, demonstrate the effectiveness of the proposed method.\nThe code implementation is publicly available at\nhttps://github.com/Yangfc-ML/CEL.",
          "arxiv_id": "2412.05029v1"
        },
        {
          "title": "BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning",
          "year": "2023-05",
          "abstract": "Label-noise learning (LNL) aims to increase the model's generalization given\ntraining data with noisy labels. To facilitate practical LNL algorithms,\nresearchers have proposed different label noise types, ranging from\nclass-conditional to instance-dependent noises. In this paper, we introduce a\nnovel label noise type called BadLabel, which can significantly degrade the\nperformance of existing LNL algorithms by a large margin. BadLabel is crafted\nbased on the label-flipping attack against standard classification, where\nspecific samples are selected and their labels are flipped to other labels so\nthat the loss values of clean and noisy labels become indistinguishable. To\naddress the challenge posed by BadLabel, we further propose a robust LNL method\nthat perturbs the labels in an adversarial manner at each epoch to make the\nloss values of clean and noisy labels again distinguishable. Once we select a\nsmall set of (mostly) clean labeled data, we can apply the techniques of\nsemi-supervised learning to train the model accurately. Empirically, our\nexperimental results demonstrate that existing LNL algorithms are vulnerable to\nthe newly introduced BadLabel noise type, while our proposed robust LNL method\ncan effectively improve the generalization performance of the model under\nvarious types of label noise. The new dataset of noisy labels and the source\ncodes of robust LNL algorithms are available at\nhttps://github.com/zjfheart/BadLabels.",
          "arxiv_id": "2305.18377v2"
        }
      ],
      "18": [
        {
          "title": "Quantum-classical simulation of quantum field theory by quantum circuit learning",
          "year": "2023-11",
          "abstract": "We employ quantum circuit learning to simulate quantum field theories (QFTs).\nTypically, when simulating QFTs with quantum computers, we encounter\nsignificant challenges due to the technical limitations of quantum devices when\nimplementing the Hamiltonian using Pauli spin matrices. To address this\nchallenge, we leverage quantum circuit learning, employing a compact\nconfiguration of qubits and low-depth quantum circuits to predict real-time\ndynamics in quantum field theories. The key advantage of this approach is that\na single-qubit measurement can accurately forecast various physical parameters,\nincluding fully-connected operators. To demonstrate the effectiveness of our\nmethod, we use it to predict quench dynamics, chiral dynamics and jet\nproduction in a 1+1-dimensional model of quantum electrodynamics. We find that\nour predictions closely align with the results of rigorous classical\ncalculations, exhibiting a high degree of accuracy. This hybrid\nquantum-classical approach illustrates the feasibility of efficiently\nsimulating large-scale QFTs on cutting-edge quantum devices.",
          "arxiv_id": "2311.16297v1"
        },
        {
          "title": "Quantum framework for Reinforcement Learning: Integrating Markov decision process, quantum arithmetic, and trajectory search",
          "year": "2024-12",
          "abstract": "This paper introduces a quantum framework for addressing reinforcement\nlearning (RL) tasks, grounded in the quantum principles and leveraging a fully\nquantum model of the classical Markov decision process (MDP). By employing\nquantum concepts and a quantum search algorithm, this work presents the\nimplementation and optimization of the agent-environment interactions entirely\nwithin the quantum domain, eliminating reliance on classical computations. Key\ncontributions include the quantum-based state transitions, return calculation,\nand trajectory search mechanism that utilize quantum principles to demonstrate\nthe realization of RL processes through quantum phenomena. The implementation\nemphasizes the fundamental role of quantum superposition in enhancing\ncomputational efficiency for RL tasks. Results demonstrate the capacity of a\nquantum model to achieve quantum enhancement in RL, highlighting the potential\nof fully quantum implementations in decision-making tasks. This work not only\nunderscores the applicability of quantum computing in machine learning but also\ncontributes to the field of quantum reinforcement learning (QRL) by offering a\nrobust framework for understanding and exploiting quantum computing in RL\nsystems.",
          "arxiv_id": "2412.18208v3"
        },
        {
          "title": "Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers",
          "year": "2024-07",
          "abstract": "When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.",
          "arxiv_id": "2407.14055v2"
        }
      ],
      "19": [
        {
          "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models",
          "year": "2020-08",
          "abstract": "With machine learning models being increasingly applied to various\ndecision-making scenarios, people have spent growing efforts to make machine\nlearning models more transparent and explainable. Among various explanation\ntechniques, counterfactual explanations have the advantages of being\nhuman-friendly and actionable -- a counterfactual explanation tells the user\nhow to gain the desired prediction with minimal changes to the input. Besides,\ncounterfactual explanations can also serve as efficient probes to the models'\ndecisions. In this work, we exploit the potential of counterfactual\nexplanations to understand and explore the behavior of machine learning models.\nWe design DECE, an interactive visualization system that helps understand and\nexplore a model's decisions on individual instances and data subsets,\nsupporting users ranging from decision-subjects to model developers. DECE\nsupports exploratory analysis of model decisions by combining the strengths of\ncounterfactual explanations at instance- and subgroup-levels. We also introduce\na set of interactions that enable users to customize the generation of\ncounterfactual explanations to find more actionable ones that can suit their\nneeds. Through three use cases and an expert interview, we demonstrate the\neffectiveness of DECE in supporting decision exploration tasks and instance\nexplanations.",
          "arxiv_id": "2008.08353v1"
        },
        {
          "title": "Selective Explanations: Leveraging Human Input to Align Explainable AI",
          "year": "2023-01",
          "abstract": "While a vast collection of explainable AI (XAI) algorithms have been\ndeveloped in recent years, they are often criticized for significant gaps with\nhow humans produce and consume explanations. As a result, current XAI\ntechniques are often found to be hard to use and lack effectiveness. In this\nwork, we attempt to close these gaps by making AI explanations selective -- a\nfundamental property of human explanations -- by selectively presenting a\nsubset from a large set of model reasons based on what aligns with the\nrecipient's preferences. We propose a general framework for generating\nselective explanations by leveraging human input on a small sample. This\nframework opens up a rich design space that accounts for different selectivity\ngoals, types of input, and more. As a showcase, we use a decision-support task\nto explore selective explanations based on what the decision-maker would\nconsider relevant to the decision task. We conducted two experimental studies\nto examine three out of a broader possible set of paradigms based on our\nproposed framework: in Study 1, we ask the participants to provide their own\ninput to generate selective explanations, with either open-ended or\ncritique-based input. In Study 2, we show participants selective explanations\nbased on input from a panel of similar users (annotators). Our experiments\ndemonstrate the promise of selective explanations in reducing over-reliance on\nAI and improving decision outcomes and subjective perceptions of the AI, but\nalso paint a nuanced picture that attributes some of these positive effects to\nthe opportunity to provide one's own input to augment AI explanations. Overall,\nour work proposes a novel XAI framework inspired by human communication\nbehaviors and demonstrates its potentials to encourage future work to better\nalign AI explanations with human production and consumption of explanations.",
          "arxiv_id": "2301.09656v3"
        },
        {
          "title": "Evaluating Robustness of Counterfactual Explanations",
          "year": "2021-03",
          "abstract": "Transparency is a fundamental requirement for decision making systems when\nthese should be deployed in the real world. It is usually achieved by providing\nexplanations of the system's behavior. A prominent and intuitive type of\nexplanations are counterfactual explanations. Counterfactual explanations\nexplain a behavior to the user by proposing actions -- as changes to the input\n-- that would cause a different (specified) behavior of the system. However,\nsuch explanation methods can be unstable with respect to small changes to the\ninput -- i.e. even a small change in the input can lead to huge or arbitrary\nchanges in the output and of the explanation. This could be problematic for\ncounterfactual explanations, as two similar individuals might get very\ndifferent explanations. Even worse, if the recommended actions differ\nconsiderably in their complexity, one would consider such unstable\n(counterfactual) explanations as individually unfair.\n  In this work, we formally and empirically study the robustness of\ncounterfactual explanations in general, as well as under different models and\ndifferent kinds of perturbations. Furthermore, we propose that plausible\ncounterfactual explanations can be used instead of closest counterfactual\nexplanations to improve the robustness and consequently the individual fairness\nof counterfactual explanations.",
          "arxiv_id": "2103.02354v3"
        }
      ],
      "20": [
        {
          "title": "Time Series Language Model for Descriptive Caption Generation",
          "year": "2025-01",
          "abstract": "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin.",
          "arxiv_id": "2501.01832v1"
        },
        {
          "title": "Scaling Law for Time Series Forecasting",
          "year": "2024-05",
          "abstract": "Scaling law that rewards large datasets, complex models and enhanced data\ngranularity has been observed in various fields of deep learning. Yet, studies\non time series forecasting have cast doubt on scaling behaviors of deep\nlearning methods for time series forecasting: while more training data improves\nperformance, more capable models do not always outperform less capable models,\nand longer input horizons may hurt performance for some models. We propose a\ntheory for scaling law for time series forecasting that can explain these\nseemingly abnormal behaviors. We take into account the impact of dataset size\nand model complexity, as well as time series data granularity, particularly\nfocusing on the look-back horizon, an aspect that has been unexplored in\nprevious theories. Furthermore, we empirically evaluate various models using a\ndiverse set of time series forecasting datasets, which (1) verifies the\nvalidity of scaling law on dataset size and model complexity within the realm\nof time series forecasting, and (2) validates our theoretical framework,\nparticularly regarding the influence of look back horizon. We hope our findings\nmay inspire new models targeting time series forecasting datasets of limited\nsize, as well as large foundational datasets and models for time series\nforecasting in future work. Code for our experiments has been made public at\nhttps://github.com/JingzheShi/ScalingLawForTimeSeriesForecasting.",
          "arxiv_id": "2405.15124v4"
        },
        {
          "title": "LLM-ABBA: Understanding time series via symbolic approximation",
          "year": "2024-11",
          "abstract": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
          "arxiv_id": "2411.18506v3"
        }
      ],
      "21": [
        {
          "title": "DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries",
          "year": "2021-10",
          "abstract": "We introduce a framework for multi-camera 3D object detection. In contrast to\nexisting works, which estimate 3D bounding boxes directly from monocular images\nor use depth prediction networks to generate input for 3D object detection from\n2D information, our method manipulates predictions directly in 3D space. Our\narchitecture extracts 2D features from multiple camera images and then uses a\nsparse set of 3D object queries to index into these 2D features, linking 3D\npositions to multi-view images using camera transformation matrices. Finally,\nour model makes a bounding box prediction per object query, using a set-to-set\nloss to measure the discrepancy between the ground-truth and the prediction.\nThis top-down approach outperforms its bottom-up counterpart in which object\nbounding box prediction follows per-pixel depth estimation, since it does not\nsuffer from the compounding error introduced by a depth prediction model.\nMoreover, our method does not require post-processing such as non-maximum\nsuppression, dramatically improving inference speed. We achieve\nstate-of-the-art performance on the nuScenes autonomous driving benchmark.",
          "arxiv_id": "2110.06922v1"
        },
        {
          "title": "Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data",
          "year": "2022-03",
          "abstract": "Segmenting or detecting objects in sparse Lidar point clouds are two\nimportant tasks in autonomous driving to allow a vehicle to act safely in its\n3D environment. The best performing methods in 3D semantic segmentation or\nobject detection rely on a large amount of annotated data. Yet annotating 3D\nLidar data for these tasks is tedious and costly. In this context, we propose a\nself-supervised pre-training method for 3D perception models that is tailored\nto autonomous driving data. Specifically, we leverage the availability of\nsynchronized and calibrated image and Lidar sensors in autonomous driving\nsetups for distilling self-supervised pre-trained image representations into 3D\nmodels. Hence, our method does not require any point cloud nor image\nannotations. The key ingredient of our method is the use of superpixels which\nare used to pool 3D point features and 2D pixel features in visually similar\nregions. We then train a 3D network on the self-supervised task of matching\nthese pooled point features with the corresponding pooled image pixel features.\nThe advantages of contrasting regions obtained by superpixels are that: (1)\ngrouping together pixels and points of visually coherent regions leads to a\nmore meaningful contrastive task that produces features well adapted to 3D\nsemantic segmentation and 3D object detection; (2) all the different regions\nhave the same weight in the contrastive loss regardless of the number of 3D\npoints sampled in these regions; (3) it mitigates the noise produced by\nincorrect matching of points and pixels due to occlusions between the different\nsensors. Extensive experiments on autonomous driving datasets demonstrate the\nability of our image-to-Lidar distillation strategy to produce 3D\nrepresentations that transfer well on semantic segmentation and object\ndetection tasks.",
          "arxiv_id": "2203.16258v1"
        },
        {
          "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding",
          "year": "2025-06",
          "abstract": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time\nrendering for novel view synthesis of 3D scenes. However, existing methods\nfocus primarily on geometric and appearance modeling, lacking deeper scene\nunderstanding while also incurring high training costs that complicate the\noriginally streamlined differentiable rendering pipeline. To this end, we\npropose VoteSplat, a novel 3D scene understanding framework that integrates\nHough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized\nfor instance segmentation, extracting objects, and generating 2D vote maps. We\nthen embed spatial offset vectors into Gaussian primitives. These offsets\nconstruct 3D spatial votes by associating them with 2D image votes, while depth\ndistortion constraints refine localization along the depth axis. For\nopen-vocabulary object localization, VoteSplat maps 2D image semantics to 3D\npoint clouds via voting points, reducing training costs associated with\nhigh-dimensional CLIP features while preserving semantic unambiguity. Extensive\nexperiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D\ninstance localization, 3D point cloud understanding, click-based 3D object\nlocalization, hierarchical segmentation, and ablation studies. Our code is\navailable at https://sy-ja.github.io/votesplat/",
          "arxiv_id": "2506.22799v1"
        }
      ],
      "22": [
        {
          "title": "Posting Bot Detection on Blockchain-based Social Media Platform using Machine Learning Techniques",
          "year": "2020-08",
          "abstract": "Steemit is a blockchain-based social media platform, where authors can get\nauthor rewards in the form of cryptocurrencies called STEEM and SBD (Steem\nBlockchain Dollars) if their posts are upvoted. Interestingly, curators (or\nvoters) can also get rewards by voting others' posts, which is called a\ncuration reward. A reward is proportional to a curator's STEEM stakes.\nThroughout this process, Steemit hopes \"good\" content will be automatically\ndiscovered by users in a decentralized way, which is known as the\nProof-of-Brain (PoB). However, there are many bot accounts programmed to post\nautomatically and get rewards, which discourages real human users from creating\ngood content. We call this type of bot a posting bot. While there are many\npapers that studied bots on traditional centralized social media platforms such\nas Facebook and Twitter, we are the first to study posting bots on a\nblockchain-based social media platform. Compared with the bot detection on the\nusual social media platforms, the features we created have an advantage that\nposting bots can be detected without limiting the number or length of posts. We\ncan extract the features of posts by clustering distances between blog data or\nreplies. These features are obtained from the Minimum Average Cluster from\nClustering Distance between Frequent words and Articles (MAC-CDFA), which is\nnot used in any of the previous social media research. Based on the enriched\nfeatures, we enhanced the quality of classification tasks. Comparing the\nF1-scores, the features we created outperformed the features used for bot\ndetection on Facebook and Twitter.",
          "arxiv_id": "2008.12471v1"
        },
        {
          "title": "A comparative analysis of Graph Neural Networks and commonly used machine learning algorithms on fake news detection",
          "year": "2022-03",
          "abstract": "Fake news on social media is increasingly regarded as one of the most\nconcerning issues. Low cost, simple accessibility via social platforms, and a\nplethora of low-budget online news sources are some of the factors that\ncontribute to the spread of false news. Most of the existing fake news\ndetection algorithms are solely focused on the news content only but engaged\nusers prior posts or social activities provide a wealth of information about\ntheir views on news and have significant ability to improve fake news\nidentification. Graph Neural Networks are a form of deep learning approach that\nconducts prediction on graph-described data. Social media platforms are\nfollowed graph structure in their representation, Graph Neural Network are\nspecial types of neural networks that could be usually applied to graphs,\nmaking it much easier to execute edge, node, and graph-level prediction.\nTherefore, in this paper, we present a comparative analysis among some commonly\nused machine learning algorithms and Graph Neural Networks for detecting the\nspread of false news on social media platforms. In this study, we take the UPFD\ndataset and implement several existing machine learning algorithms on text data\nonly. Besides this, we create different GNN layers for fusing graph-structured\nnews propagation data and the text data as the node feature in our GNN models.\nGNNs provide the best solutions to the dilemma of identifying false news in our\nresearch.",
          "arxiv_id": "2203.14132v1"
        },
        {
          "title": "FNR: A Similarity and Transformer-Based Approach to Detect Multi-Modal Fake News in Social Media",
          "year": "2021-12",
          "abstract": "The availability and interactive nature of social media have made them the\nprimary source of news around the globe. The popularity of social media tempts\ncriminals to pursue their immoral intentions by producing and disseminating\nfake news using seductive text and misleading images. Therefore, verifying\nsocial media news and spotting fakes is crucial. This work aims to analyze\nmulti-modal features from texts and images in social media for detecting fake\nnews. We propose a Fake News Revealer (FNR) method that utilizes transform\nlearning to extract contextual and semantic features and contrastive loss to\ndetermine the similarity between image and text. We applied FNR on two real\nsocial media datasets. The results show the proposed method achieves higher\naccuracies in detecting fake news compared to the previous works.",
          "arxiv_id": "2112.01131v1"
        }
      ],
      "23": [
        {
          "title": "OmniFair: A Declarative System for Model-Agnostic Group Fairness in Machine Learning",
          "year": "2021-03",
          "abstract": "Machine learning (ML) is increasingly being used to make decisions in our\nsociety. ML models, however, can be unfair to certain demographic groups (e.g.,\nAfrican Americans or females) according to various fairness metrics. Existing\ntechniques for producing fair ML models either are limited to the type of\nfairness constraints they can handle (e.g., preprocessing) or require\nnontrivial modifications to downstream ML training algorithms (e.g.,\nin-processing).\n  We propose a declarative system OmniFair for supporting group fairness in ML.\nOmniFair features a declarative interface for users to specify desired group\nfairness constraints and supports all commonly used group fairness notions,\nincluding statistical parity, equalized odds, and predictive parity. OmniFair\nis also model-agnostic in the sense that it does not require modifications to a\nchosen ML algorithm. OmniFair also supports enforcing multiple user declared\nfairness constraints simultaneously while most previous techniques cannot. The\nalgorithms in OmniFair maximize model accuracy while meeting the specified\nfairness constraints, and their efficiency is optimized based on the\ntheoretically provable monotonicity property regarding the trade-off between\naccuracy and fairness that is unique to our system.\n  We conduct experiments on commonly used datasets that exhibit bias against\nminority groups in the fairness literature. We show that OmniFair is more\nversatile than existing algorithmic fairness approaches in terms of both\nsupported fairness constraints and downstream ML models. OmniFair reduces the\naccuracy loss by up to $94.8\\%$ compared with the second best method. OmniFair\nalso achieves similar running time to preprocessing methods, and is up to\n$270\\times$ faster than in-processing methods.",
          "arxiv_id": "2103.09055v1"
        },
        {
          "title": "Metric-Free Individual Fairness with Cooperative Contextual Bandits",
          "year": "2020-11",
          "abstract": "Data mining algorithms are increasingly used in automated decision making\nacross all walks of daily life. Unfortunately, as reported in several studies\nthese algorithms inject bias from data and environment leading to inequitable\nand unfair solutions. To mitigate bias in machine learning, different\nformalizations of fairness have been proposed that can be categorized into\ngroup fairness and individual fairness. Group fairness requires that different\ngroups should be treated similarly which might be unfair to some individuals\nwithin a group. On the other hand, individual fairness requires that similar\nindividuals be treated similarly. However, individual fairness remains\nunderstudied due to its reliance on problem-specific similarity metrics. We\npropose a metric-free individual fairness and a cooperative contextual bandits\n(CCB) algorithm. The CCB algorithm utilizes fairness as a reward and attempts\nto maximize it. The advantage of treating fairness as a reward is that the\nfairness criterion does not need to be differentiable. The proposed algorithm\nis tested on multiple real-world benchmark datasets. The results show the\neffectiveness of the proposed algorithm at mitigating bias and at achieving\nboth individual and group fairness.",
          "arxiv_id": "2011.06738v1"
        },
        {
          "title": "Algorithmic Decision Making with Conditional Fairness",
          "year": "2020-06",
          "abstract": "Nowadays fairness issues have raised great concerns in decision-making\nsystems. Various fairness notions have been proposed to measure the degree to\nwhich an algorithm is unfair. In practice, there frequently exist a certain set\nof variables we term as fair variables, which are pre-decision covariates such\nas users' choices. The effects of fair variables are irrelevant in assessing\nthe fairness of the decision support algorithm. We thus define conditional\nfairness as a more sound fairness metric by conditioning on the fairness\nvariables. Given different prior knowledge of fair variables, we demonstrate\nthat traditional fairness notations, such as demographic parity and equalized\nodds, are special cases of our conditional fairness notations. Moreover, we\npropose a Derivable Conditional Fairness Regularizer (DCFR), which can be\nintegrated into any decision-making model, to track the trade-off between\nprecision and fairness of algorithmic decision making. Specifically, an\nadversarial representation based conditional independence loss is proposed in\nour DCFR to measure the degree of unfairness. With extensive experiments on\nthree real-world datasets, we demonstrate the advantages of our conditional\nfairness notation and DCFR.",
          "arxiv_id": "2006.10483v5"
        }
      ],
      "24": [
        {
          "title": "Benchmarks and Custom Package for Energy Forecasting",
          "year": "2023-07",
          "abstract": "Energy (load, wind, photovoltaic) forecasting is significant in the power\nindustry as it can provide a reference for subsequent tasks such as power grid\ndispatch, thus bringing huge economic benefits. However, there are many\ndifferences between energy forecasting and traditional time series forecasting.\nOn the one hand, traditional time series mainly focus on capturing\ncharacteristics like trends and cycles. In contrast, the energy series is\nlargely influenced by many external factors, such as meteorological and\ncalendar variables. On the other hand, energy forecasting aims to minimize the\ncost of subsequent tasks such as power grid dispatch, rather than simply\npursuing prediction accuracy. In addition, the scale of energy data can also\nsignificantly impact the predicted results. In this paper, we collected\nlarge-scale load datasets and released a new renewable energy dataset that\ncontains both station-level and region-level renewable generation data with\nmeteorological data. For load data, we also included load domain-specific\nfeature engineering and provided a method to customize the loss function and\nlink the forecasting error to requirements related to subsequent tasks (such as\npower grid dispatching costs), integrating it into our forecasting framework.\nBased on such a situation, we conducted extensive experiments with 21\nforecasting methods in these energy datasets at different levels under 11\nevaluation metrics, providing a comprehensive reference for researchers to\ncompare different energy forecasting models.",
          "arxiv_id": "2307.07191v2"
        },
        {
          "title": "Combating Uncertainties in Wind and Distributed PV Energy Sources Using Integrated Reinforcement Learning and Time-Series Forecasting",
          "year": "2023-02",
          "abstract": "Renewable energy sources, such as wind and solar power, are increasingly\nbeing integrated into smart grid systems. However, when compared to traditional\nenergy resources, the unpredictability of renewable energy generation poses\nsignificant challenges for both electricity providers and utility companies.\nFurthermore, the large-scale integration of distributed energy resources (such\nas PV systems) creates new challenges for energy management in microgrids. To\ntackle these issues, we propose a novel framework with two objectives: (i)\ncombating uncertainty of renewable energy in smart grid by leveraging\ntime-series forecasting with Long-Short Term Memory (LSTM) solutions, and (ii)\nestablishing distributed and dynamic decision-making framework with multi-agent\nreinforcement learning using Deep Deterministic Policy Gradient (DDPG)\nalgorithm. The proposed framework considers both objectives concurrently to\nfully integrate them, while considering both wholesale and retail markets,\nthereby enabling efficient energy management in the presence of uncertain and\ndistributed renewable energy sources. Through extensive numerical simulations,\nwe demonstrate that the proposed solution significantly improves the profit of\nload serving entities (LSE) by providing a more accurate wind generation\nforecast. Furthermore, our results demonstrate that households with PV and\nbattery installations can increase their profits by using intelligent battery\ncharge/discharge actions determined by the DDPG agents.",
          "arxiv_id": "2302.14094v1"
        },
        {
          "title": "Battery and Hydrogen Energy Storage Control in a Smart Energy Network with Flexible Energy Demand using Deep Reinforcement Learning",
          "year": "2022-08",
          "abstract": "Smart energy networks provide for an effective means to accommodate high\npenetrations of variable renewable energy sources like solar and wind, which\nare key for deep decarbonisation of energy production. However, given the\nvariability of the renewables as well as the energy demand, it is imperative to\ndevelop effective control and energy storage schemes to manage the variable\nenergy generation and achieve desired system economics and environmental goals.\nIn this paper, we introduce a hybrid energy storage system composed of battery\nand hydrogen energy storage to handle the uncertainties related to electricity\nprices, renewable energy production and consumption. We aim to improve\nrenewable energy utilisation and minimise energy costs and carbon emissions\nwhile ensuring energy reliability and stability within the network. To achieve\nthis, we propose a multi-agent deep deterministic policy gradient approach,\nwhich is a deep reinforcement learning-based control strategy to optimise the\nscheduling of the hybrid energy storage system and energy demand in real-time.\nThe proposed approach is model-free and does not require explicit knowledge and\nrigorous mathematical models of the smart energy network environment.\nSimulation results based on real-world data show that: (i) integration and\noptimised operation of the hybrid energy storage system and energy demand\nreduces carbon emissions by 78.69%, improves cost savings by 23.5% and\nrenewable energy utilisation by over 13.2% compared to other baseline models\nand (ii) the proposed algorithm outperforms the state-of-the-art self-learning\nalgorithms like deep-Q network.",
          "arxiv_id": "2208.12779v1"
        }
      ],
      "25": [
        {
          "title": "Causally-Aware Spatio-Temporal Multi-Graph Convolution Network for Accurate and Reliable Traffic Prediction",
          "year": "2024-08",
          "abstract": "Accurate and reliable prediction has profound implications to a wide range of\napplications. In this study, we focus on an instance of spatio-temporal\nlearning problem--traffic prediction--to demonstrate an advanced deep learning\nmodel developed for making accurate and reliable forecast. Despite the\nsignificant progress in traffic prediction, limited studies have incorporated\nboth explicit and implicit traffic patterns simultaneously to improve\nprediction performance. Meanwhile, the variability nature of traffic states\nnecessitates quantifying the uncertainty of model predictions in a\nstatistically principled way; however, extant studies offer no provable\nguarantee on the statistical validity of confidence intervals in reflecting its\nactual likelihood of containing the ground truth. In this paper, we propose an\nend-to-end traffic prediction framework that leverages three primary components\nto generate accurate and reliable traffic predictions: dynamic causal structure\nlearning for discovering implicit traffic patterns from massive traffic data,\ncausally-aware spatio-temporal multi-graph convolution network (CASTMGCN) for\nlearning spatio-temporal dependencies, and conformal prediction for uncertainty\nquantification. CASTMGCN fuses several graphs that characterize different\nimportant aspects of traffic networks and an auxiliary graph that captures the\neffect of exogenous factors on the road network. On this basis, a conformal\nprediction approach tailored to spatio-temporal data is further developed for\nquantifying the uncertainty in node-wise traffic predictions over varying\nprediction horizons. Experimental results on two real-world traffic datasets\ndemonstrate that the proposed method outperforms several state-of-the-art\nmodels in prediction accuracy; moreover, it generates more efficient prediction\nregions than other methods while strictly satisfying the statistical validity\nin coverage.",
          "arxiv_id": "2408.13293v1"
        },
        {
          "title": "FDTI: Fine-grained Deep Traffic Inference with Roadnet-enriched Graph",
          "year": "2023-06",
          "abstract": "This paper proposes the fine-grained traffic prediction task (e.g. interval\nbetween data points is 1 minute), which is essential to traffic-related\ndownstream applications. Under this setting, traffic flow is highly influenced\nby traffic signals and the correlation between traffic nodes is dynamic. As a\nresult, the traffic data is non-smooth between nodes, and hard to utilize\nprevious methods which focus on smooth traffic data. To address this problem,\nwe propose Fine-grained Deep Traffic Inference, termed as FDTI. Specifically,\nwe construct a fine-grained traffic graph based on traffic signals to model the\ninter-road relations. Then, a physically-interpretable dynamic mobility\nconvolution module is proposed to capture vehicle moving dynamics controlled by\nthe traffic signals. Furthermore, traffic flow conservation is introduced to\naccurately infer future volume. Extensive experiments demonstrate that our\nmethod achieves state-of-the-art performance and learned traffic dynamics with\ngood properties. To the best of our knowledge, we are the first to conduct the\ncity-level fine-grained traffic prediction.",
          "arxiv_id": "2306.10945v1"
        },
        {
          "title": "TSSRGCN: Temporal Spectral Spatial Retrieval Graph Convolutional Network for Traffic Flow Forecasting",
          "year": "2020-11",
          "abstract": "Traffic flow forecasting is of great significance for improving the\nefficiency of transportation systems and preventing emergencies. Due to the\nhighly non-linearity and intricate evolutionary patterns of short-term and\nlong-term traffic flow, existing methods often fail to take full advantage of\nspatial-temporal information, especially the various temporal patterns with\ndifferent period shifting and the characteristics of road segments. Besides,\nthe globality representing the absolute value of traffic status indicators and\nthe locality representing the relative value have not been considered\nsimultaneously. This paper proposes a neural network model that focuses on the\nglobality and locality of traffic networks as well as the temporal patterns of\ntraffic data. The cycle-based dilated deformable convolution block is designed\nto capture different time-varying trends on each node accurately. Our model can\nextract both global and local spatial information since we combine two graph\nconvolutional network methods to learn the representations of nodes and edges.\nExperiments on two real-world datasets show that the model can scrutinize the\nspatial-temporal correlation of traffic data, and its performance is better\nthan the compared state-of-the-art methods. Further analysis indicates that the\nlocality and globality of the traffic networks are critical to traffic flow\nprediction and the proposed TSSRGCN model can adapt to the various temporal\ntraffic patterns.",
          "arxiv_id": "2011.14638v1"
        }
      ],
      "26": [
        {
          "title": "Motion Comfort Optimization for Autonomous Vehicles: Concepts, Methods, and Techniques",
          "year": "2023-06",
          "abstract": "This article outlines the architecture of autonomous driving and related\ncomplementary frameworks from the perspective of human comfort. The technical\nelements for measuring Autonomous Vehicle (AV) user comfort and psychoanalysis\nare listed here. At the same time, this article introduces the technology\nrelated to the structure of automatic driving and the reaction time of\nautomatic driving. We also discuss the technical details related to the\nautomatic driving comfort system, the response time of the AV driver, the\ncomfort level of the AV, motion sickness, and related optimization\ntechnologies. The function of the sensor is affected by various factors. Since\nthe sensor of automatic driving mainly senses the environment around a vehicle,\nincluding \"the weather\" which introduces the challenges and limitations of\nsecond-hand sensors in autonomous vehicles under different weather conditions.\nThe comfort and safety of autonomous driving are also factors that affect the\ndevelopment of autonomous driving technologies. This article further analyzes\nthe impact of autonomous driving on the user's physical and psychological\nstates and how the comfort factors of autonomous vehicles affect the automotive\nmarket. Also, part of our focus is on the benefits and shortcomings of\nautonomous driving. The goal is to present an exhaustive overview of the most\nrelevant technical matters to help researchers and application developers\ncomprehend the different comfort factors and systems of autonomous driving.\nFinally, we provide detailed automated driving comfort use cases to illustrate\nthe comfort-related issues of autonomous driving. Then, we provide implications\nand insights for the future of autonomous driving.",
          "arxiv_id": "2306.09462v1"
        },
        {
          "title": "DISC: Dataset for Analyzing Driving Styles In Simulated Crashes for Mixed Autonomy",
          "year": "2025-01",
          "abstract": "Handling pre-crash scenarios is still a major challenge for self-driving cars\ndue to limited practical data and human-driving behavior datasets. We introduce\nDISC (Driving Styles In Simulated Crashes), one of the first datasets designed\nto capture various driving styles and behaviors in pre-crash scenarios for\nmixed autonomy analysis. DISC includes over 8 classes of driving\nstyles/behaviors from hundreds of drivers navigating a simulated vehicle\nthrough a virtual city, encountering rare-event traffic scenarios. This dataset\nenables the classification of pre-crash human driving behaviors in unsafe\nconditions, supporting individualized trajectory prediction based on observed\ndriving patterns. By utilizing a custom-designed VR-based in-house driving\nsimulator, TRAVERSE, data was collected through a driver-centric study\ninvolving human drivers encountering twelve simulated accident scenarios. This\ndataset fills a critical gap in human-centric driving data for rare events\ninvolving interactions with autonomous vehicles. It enables autonomous systems\nto better react to human drivers and optimize trajectory prediction in mixed\nautonomy environments involving both human-driven and self-driving cars. In\naddition, individual driving behaviors are classified through a set of\nstandardized questionnaires, carefully designed to identify and categorize\ndriving behavior traits. We correlate data features with driving behaviors,\nshowing that the simulated environment reflects real-world driving styles. DISC\nis the first dataset to capture how various driving styles respond to accident\nscenarios, offering significant potential to enhance autonomous vehicle safety\nand driving behavior analysis in mixed autonomy environments.",
          "arxiv_id": "2502.00050v1"
        },
        {
          "title": "Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving",
          "year": "2024-02",
          "abstract": "In recent years, the expansion of internet technology and advancements in\nautomation have brought significant attention to autonomous driving technology.\nMajor automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have\nprogressively introduced products ranging from assisted-driving vehicles to\nsemi-autonomous vehicles. However, this period has also witnessed several\ntraffic safety incidents involving self-driving vehicles. For instance, in\nMarch 2016, a Google self-driving car was involved in a minor collision with a\nbus. At the time of the accident, the autonomous vehicle was attempting to\nmerge into the right lane but failed to dynamically respond to the real-time\nenvironmental information during the lane change. It incorrectly assumed that\nthe approaching bus would slow down to avoid it, leading to a low-speed\ncollision with the bus. This incident highlights the current technological\nshortcomings and safety concerns associated with autonomous lane-changing\nbehavior, despite the rapid advancements in autonomous driving technology.\nLane-changing is among the most common and hazardous behaviors in highway\ndriving, significantly impacting traffic safety and flow. Therefore,\nlane-changing is crucial for traffic safety, and accurately predicting drivers'\nlane change intentions can markedly enhance driving safety. This paper\nintroduces a deep learning-based prediction method for autonomous driving lane\nchange behavior, aiming to facilitate safe lane changes and thereby improve\nroad safety.",
          "arxiv_id": "2402.16036v1"
        }
      ],
      "27": [
        {
          "title": "Representational Continuity for Unsupervised Continual Learning",
          "year": "2021-10",
          "abstract": "Continual learning (CL) aims to learn a sequence of tasks without forgetting\nthe previously acquired knowledge. However, recent CL advances are restricted\nto supervised continual learning (SCL) scenarios. Consequently, they are not\nscalable to real-world applications where the data distribution is often biased\nand unannotated. In this work, we focus on unsupervised continual learning\n(UCL), where we learn the feature representations on an unlabelled sequence of\ntasks and show that reliance on annotated data is not necessary for continual\nlearning. We conduct a systematic study analyzing the learned feature\nrepresentations and show that unsupervised visual representations are\nsurprisingly more robust to catastrophic forgetting, consistently achieve\nbetter performance, and generalize better to out-of-distribution tasks than\nSCL. Furthermore, we find that UCL achieves a smoother loss landscape through\nqualitative analysis of the learned representations and learns meaningful\nfeature representations. Additionally, we propose Lifelong Unsupervised Mixup\n(LUMP), a simple yet effective technique that interpolates between the current\ntask and previous tasks' instances to alleviate catastrophic forgetting for\nunsupervised representations.",
          "arxiv_id": "2110.06976v3"
        },
        {
          "title": "The Bayesian Approach to Continual Learning: An Overview",
          "year": "2025-07",
          "abstract": "Continual learning is an online paradigm where a learner continually\naccumulates knowledge from different tasks encountered over sequential time\nsteps. Importantly, the learner is required to extend and update its knowledge\nwithout forgetting about the learning experience acquired from the past, and\nwhile avoiding the need to retrain from scratch. Given its sequential nature\nand its resemblance to the way humans think, continual learning offers an\nopportunity to address several challenges which currently stand in the way of\nwidening the range of applicability of deep models to further real-world\nproblems. The continual need to update the learner with data arriving\nsequentially strikes inherent congruence between continual learning and\nBayesian inference which provides a principal platform to keep updating the\nprior beliefs of a model given new data, without completely forgetting the\nknowledge acquired from the old data. This survey inspects different settings\nof Bayesian continual learning, namely task-incremental learning and\nclass-incremental learning. We begin by discussing definitions of continual\nlearning along with its Bayesian setting, as well as the links with related\nfields, such as domain adaptation, transfer learning and meta-learning.\nAfterwards, we introduce a taxonomy offering a comprehensive categorization of\nalgorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we\nanalyze the state-of-the-art while zooming in on some of the most prominent\nBayesian continual learning algorithms to date. Furthermore, we shed some light\non links between continual learning and developmental psychology, and\ncorrespondingly introduce analogies between both fields. We follow that with a\ndiscussion of current challenges, and finally conclude with potential areas for\nfuture research on Bayesian continual learning.",
          "arxiv_id": "2507.08922v1"
        },
        {
          "title": "Multiband VAE: Latent Space Alignment for Knowledge Consolidation in Continual Learning",
          "year": "2021-06",
          "abstract": "We propose a new method for unsupervised generative continual learning\nthrough realignment of Variational Autoencoder's latent space. Deep generative\nmodels suffer from catastrophic forgetting in the same way as other neural\nstructures. Recent generative continual learning works approach this problem\nand try to learn from new data without forgetting previous knowledge. However,\nthose methods usually focus on artificial scenarios where examples share almost\nno similarity between subsequent portions of data - an assumption not realistic\nin the real-life applications of continual learning. In this work, we identify\nthis limitation and posit the goal of generative continual learning as a\nknowledge accumulation task. We solve it by continuously aligning latent\nrepresentations of new data that we call bands in additional latent space where\nexamples are encoded independently of their source task. In addition, we\nintroduce a method for controlled forgetting of past data that simplifies this\nprocess. On top of the standard continual learning benchmarks, we propose a\nnovel challenging knowledge consolidation scenario and show that the proposed\napproach outperforms state-of-the-art by up to twofold across all experiments\nand the additional real-life evaluation. To our knowledge, Multiband VAE is the\nfirst method to show forward and backward knowledge transfer in generative\ncontinual learning.",
          "arxiv_id": "2106.12196v2"
        }
      ],
      "28": [
        {
          "title": "IoT Malware Network Traffic Classification using Visual Representation and Deep Learning",
          "year": "2020-10",
          "abstract": "With the increase of IoT devices and technologies coming into service,\nMalware has risen as a challenging threat with increased infection rates and\nlevels of sophistication. Without strong security mechanisms, a huge amount of\nsensitive data is exposed to vulnerabilities, and therefore, easily abused by\ncybercriminals to perform several illegal activities. Thus, advanced network\nsecurity mechanisms that are able of performing a real-time traffic analysis\nand mitigation of malicious traffic are required. To address this challenge, we\nare proposing a novel IoT malware traffic analysis approach using deep learning\nand visual representation for faster detection and classification of new\nmalware (zero-day malware). The detection of malicious network traffic in the\nproposed approach works at the package level, significantly reducing the time\nof detection with promising results due to the deep learning technologies used.\nTo evaluate our proposed method performance, a dataset is constructed which\nconsists of 1000 pcap files of normal and malware traffic that are collected\nfrom different network traffic sources. The experimental results of Residual\nNeural Network (ResNet50) are very promising, providing a 94.50% accuracy rate\nfor detection of malware traffic.",
          "arxiv_id": "2010.01712v1"
        },
        {
          "title": "Machine Learning-Assisted Intrusion Detection for Enhancing Internet of Things Security",
          "year": "2024-10",
          "abstract": "Attacks against the Internet of Things (IoT) are rising as devices,\napplications, and interactions become more networked and integrated. The\nincrease in cyber-attacks that target IoT networks poses a considerable\nvulnerability and threat to the privacy, security, functionality, and\navailability of critical systems, which leads to operational disruptions,\nfinancial losses, identity thefts, and data breaches. To efficiently secure IoT\ndevices, real-time detection of intrusion systems is critical, especially those\nusing machine learning to identify threats and mitigate risks and\nvulnerabilities. This paper investigates the latest research on machine\nlearning-based intrusion detection strategies for IoT security, concentrating\non real-time responsiveness, detection accuracy, and algorithm efficiency. Key\nstudies were reviewed from all well-known academic databases, and a taxonomy\nwas provided for the existing approaches. This review also highlights existing\nresearch gaps and outlines the limitations of current IoT security frameworks\nto offer practical insights for future research directions and developments.",
          "arxiv_id": "2410.01016v2"
        },
        {
          "title": "Machine Learning-Enabled IoT Security: Open Issues and Challenges Under Advanced Persistent Threats",
          "year": "2022-04",
          "abstract": "Despite its technological benefits, Internet of Things (IoT) has cyber\nweaknesses due to the vulnerabilities in the wireless medium. Machine learning\n(ML)-based methods are widely used against cyber threats in IoT networks with\npromising performance. Advanced persistent threat (APT) is prominent for\ncybercriminals to compromise networks, and it is crucial to long-term and\nharmful characteristics. However, it is difficult to apply ML-based approaches\nto identify APT attacks to obtain a promising detection performance due to an\nextremely small percentage among normal traffic. There are limited surveys to\nfully investigate APT attacks in IoT networks due to the lack of public\ndatasets with all types of APT attacks. It is worth to bridge the\nstate-of-the-art in network attack detection with APT attack detection in a\ncomprehensive review article. This survey article reviews the security\nchallenges in IoT networks and presents the well-known attacks, APT attacks,\nand threat models in IoT systems. Meanwhile, signature-based, anomaly-based,\nand hybrid intrusion detection systems are summarized for IoT networks. The\narticle highlights statistical insights regarding frequently applied ML-based\nmethods against network intrusion alongside the number of attacks types\ndetected. Finally, open issues and challenges for common network intrusion and\nAPT attacks are presented for future research.",
          "arxiv_id": "2204.03433v2"
        }
      ],
      "29": [
        {
          "title": "Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation from Incomplete Measurements",
          "year": "2021-04",
          "abstract": "Tensors, which provide a powerful and flexible model for representing\nmulti-attribute data and multi-way interactions, play an indispensable role in\nmodern data science across various fields in science and engineering. A\nfundamental task is to faithfully recover the tensor from highly incomplete\nmeasurements in a statistically and computationally efficient manner.\nHarnessing the low-rank structure of tensors in the Tucker decomposition, this\npaper develops a scaled gradient descent (ScaledGD) algorithm to directly\nrecover the tensor factors with tailored spectral initializations, and shows\nthat it provably converges at a linear rate independent of the condition number\nof the ground truth tensor for two canonical problems -- tensor completion and\ntensor regression -- as soon as the sample size is above the order of $n^{3/2}$\nignoring other parameter dependencies, where $n$ is the dimension of the\ntensor. This leads to an extremely scalable approach to low-rank tensor\nestimation compared with prior art, which suffers from at least one of the\nfollowing drawbacks: extreme sensitivity to ill-conditioning, high\nper-iteration costs in terms of memory and computation, or poor sample\ncomplexity guarantees. To the best of our knowledge, ScaledGD is the first\nalgorithm that achieves near-optimal statistical and computational complexities\nsimultaneously for low-rank tensor completion with the Tucker decomposition.\nOur algorithm highlights the power of appropriate preconditioning in\naccelerating nonconvex statistical estimation, where the iteration-varying\npreconditioners promote desirable invariance properties of the trajectory with\nrespect to the underlying symmetry in low-rank tensor factorization.",
          "arxiv_id": "2104.14526v3"
        },
        {
          "title": "Projected Robust PCA with Application to Smooth Image Recovery",
          "year": "2020-07",
          "abstract": "Most high-dimensional matrix recovery problems are studied under the\nassumption that the target matrix has certain intrinsic structures. For image\ndata related matrix recovery problems, approximate low-rankness and smoothness\nare the two most commonly imposed structures. For approximately low-rank matrix\nrecovery, the robust principal component analysis (PCA) is well-studied and\nproved to be effective. For smooth matrix problem, 2d fused Lasso and other\ntotal variation based approaches have played a fundamental role. Although both\nlow-rankness and smoothness are key assumptions for image data analysis, the\ntwo lines of research, however, have very limited interaction. Motivated by\ntaking advantage of both features, we in this paper develop a framework named\nprojected robust PCA (PRPCA), under which the low-rank matrices are projected\nonto a space of smooth matrices. Consequently, a large class of image matrices\ncan be decomposed as a low-rank and smooth component plus a sparse component. A\nkey advantage of this decomposition is that the dimension of the core low-rank\ncomponent can be significantly reduced. Consequently, our framework is able to\naddress a problematic bottleneck of many low-rank matrix problems: singular\nvalue decomposition (SVD) on large matrices. Theoretically, we provide explicit\nstatistical recovery guarantees of PRPCA and include classical robust PCA as a\nspecial case.",
          "arxiv_id": "2009.05478v2"
        },
        {
          "title": "Low-Multi-Rank High-Order Bayesian Robust Tensor Factorization",
          "year": "2023-11",
          "abstract": "The recently proposed tensor robust principal component analysis (TRPCA)\nmethods based on tensor singular value decomposition (t-SVD) have achieved\nnumerous successes in many fields. However, most of these methods are only\napplicable to third-order tensors, whereas the data obtained in practice are\noften of higher order, such as fourth-order color videos, fourth-order\nhyperspectral videos, and fifth-order light-field images. Additionally, in the\nt-SVD framework, the multi-rank of a tensor can describe more fine-grained\nlow-rank structure in the tensor compared with the tubal rank. However,\ndetermining the multi-rank of a tensor is a much more difficult problem than\ndetermining the tubal rank. Moreover, most of the existing TRPCA methods do not\nexplicitly model the noises except the sparse noise, which may compromise the\naccuracy of estimating the low-rank tensor. In this work, we propose a novel\nhigh-order TRPCA method, named as Low-Multi-rank High-order Bayesian Robust\nTensor Factorization (LMH-BRTF), within the Bayesian framework. Specifically,\nwe decompose the observed corrupted tensor into three parts, i.e., the low-rank\ncomponent, the sparse component, and the noise component. By constructing a\nlow-rank model for the low-rank component based on the order-$d$ t-SVD and\nintroducing a proper prior for the model, LMH-BRTF can automatically determine\nthe tensor multi-rank. Meanwhile, benefiting from the explicit modeling of both\nthe sparse and noise components, the proposed method can leverage information\nfrom the noises more effectivly, leading to an improved performance of TRPCA.\nThen, an efficient variational inference algorithm is established for\nparameters estimation. Empirical studies on synthetic and real-world datasets\ndemonstrate the effectiveness of the proposed method in terms of both\nqualitative and quantitative results.",
          "arxiv_id": "2311.05888v1"
        }
      ],
      "30": [
        {
          "title": "Simulation Based Bayesian Optimization",
          "year": "2024-01",
          "abstract": "Bayesian Optimization (BO) is a powerful method for optimizing black-box\nfunctions by combining prior knowledge with ongoing function evaluations. BO\nconstructs a probabilistic surrogate model of the objective function given the\ncovariates, which is in turn used to inform the selection of future evaluation\npoints through an acquisition function. For smooth continuous search spaces,\nGaussian Processes (GPs) are commonly used as the surrogate model as they offer\nanalytical access to posterior predictive distributions, thus facilitating the\ncomputation and optimization of acquisition functions. However, in complex\nscenarios involving optimization over categorical or mixed covariate spaces,\nGPs may not be ideal. This paper introduces Simulation Based Bayesian\nOptimization (SBBO) as a novel approach to optimizing acquisition functions\nthat only requires sampling-based access to posterior predictive distributions.\nSBBO allows the use of surrogate probabilistic models tailored for\ncombinatorial spaces with discrete variables. Any Bayesian model in which\nposterior inference is carried out through Markov chain Monte Carlo can be\nselected as the surrogate model in SBBO. We demonstrate empirically the\neffectiveness of SBBO using various choices of surrogate models in applications\ninvolving combinatorial optimization.",
          "arxiv_id": "2401.10811v3"
        },
        {
          "title": "Optimistic Optimization of Gaussian Process Samples",
          "year": "2022-09",
          "abstract": "Bayesian optimization is a popular formalism for global optimization, but its\ncomputational costs limit it to expensive-to-evaluate functions. A competing,\ncomputationally more efficient, global optimization framework is optimistic\noptimization, which exploits prior knowledge about the geometry of the search\nspace in form of a dissimilarity function. We investigate to which degree the\nconceptual advantages of Bayesian Optimization can be combined with the\ncomputational efficiency of optimistic optimization. By mapping the kernel to a\ndissimilarity, we obtain an optimistic optimization algorithm for the Bayesian\nOptimization setting with a run-time of up to $\\mathcal{O}(N \\log N)$. As a\nhigh-level take-away we find that, when using stationary kernels on objectives\nof relatively low evaluation cost, optimistic optimization can be strongly\npreferable over Bayesian optimization, while for strongly coupled and\nparametric models, good implementations of Bayesian optimization can perform\nmuch better, even at low evaluation cost. We argue that there is a new research\ndomain between geometric and probabilistic search, i.e. methods that run\ndrastically faster than traditional Bayesian optimization, while retaining some\nof the crucial functionality of Bayesian optimization.",
          "arxiv_id": "2209.00895v1"
        },
        {
          "title": "Bayesian Optimization For Multi-Objective Mixed-Variable Problems",
          "year": "2022-01",
          "abstract": "Optimizing multiple, non-preferential objectives for mixed-variable,\nexpensive black-box problems is important in many areas of engineering and\nscience. The expensive, noisy, black-box nature of these problems makes them\nideal candidates for Bayesian optimization (BO). Mixed-variable and\nmulti-objective problems, however, are a challenge due to BO's underlying\nsmooth Gaussian process surrogate model. Current multi-objective BO algorithms\ncannot deal with mixed-variable problems. We present MixMOBO, the first\nmixed-variable, multi-objective Bayesian optimization framework for such\nproblems. Using MixMOBO, optimal Pareto-fronts for multi-objective,\nmixed-variable design spaces can be found efficiently while ensuring diverse\nsolutions. The method is sufficiently flexible to incorporate different kernels\nand acquisition functions, including those that were developed for\nmixed-variable or multi-objective problems by other authors. We also present\nHedgeMO, a modified Hedge strategy that uses a portfolio of acquisition\nfunctions for multi-objective problems. We present a new acquisition function,\nSMC. Our results show that MixMOBO performs well against other mixed-variable\nalgorithms on synthetic problems. We apply MixMOBO to the real-world design of\nan architected material and show that our optimal design, which was\nexperimentally fabricated and validated, has a normalized strain energy density\n$10^4$ times greater than existing structures.",
          "arxiv_id": "2201.12767v2"
        }
      ],
      "31": [
        {
          "title": "Stock Price Prediction Under Anomalous Circumstances",
          "year": "2021-09",
          "abstract": "The stock market is volatile and complicated, especially in 2020. Because of\na series of global and regional \"black swans,\" such as the COVID-19 pandemic,\nthe U.S. stock market triggered the circuit breaker three times within one week\nof March 9 to 16, which is unprecedented throughout history. Affected by the\nwhole circumstance, the stock prices of individual corporations also plummeted\nby rates that were never predicted by any pre-developed forecasting models. It\nreveals that there was a lack of satisfactory models that could predict the\nchanges in stocks prices when catastrophic, highly unlikely events occur. To\nfill the void of such models and to help prevent investors from heavy losses\nduring uncertain times, this paper aims to capture the movement pattern of\nstock prices under anomalous circumstances. First, we detect outliers in\nsequential stock prices by fitting a standard ARIMA model and identifying the\npoints where predictions deviate significantly from actual values. With the\nselected data points, we train ARIMA and LSTM models at the single-stock level,\nindustry level, and general market level, respectively. Since the public moods\naffect the stock market tremendously, a sentiment analysis is also incorporated\ninto the models in the form of sentiment scores, which are converted from\ncomments about specific stocks on Reddit. Based on 100 companies' stock prices\nin the period of 2016 to 2020, the models achieve an average prediction\naccuracy of 98% which can be used to optimize existing prediction\nmethodologies.",
          "arxiv_id": "2109.15059v1"
        },
        {
          "title": "Diffusion Variational Autoencoder for Tackling Stochasticity in Multi-Step Regression Stock Price Prediction",
          "year": "2023-08",
          "abstract": "Multi-step stock price prediction over a long-term horizon is crucial for\nforecasting its volatility, allowing financial institutions to price and hedge\nderivatives, and banks to quantify the risk in their trading books.\nAdditionally, most financial regulators also require a liquidity horizon of\nseveral days for institutional investors to exit their risky assets, in order\nto not materially affect market prices. However, the task of multi-step stock\nprice prediction is challenging, given the highly stochastic nature of stock\ndata. Current solutions to tackle this problem are mostly designed for\nsingle-step, classification-based predictions, and are limited to low\nrepresentation expressiveness. The problem also gets progressively harder with\nthe introduction of the target price sequence, which also contains stochastic\nnoise and reduces generalizability at test-time. To tackle these issues, we\ncombine a deep hierarchical variational-autoencoder (VAE) and diffusion\nprobabilistic techniques to do seq2seq stock prediction through a stochastic\ngenerative process. The hierarchical VAE allows us to learn the complex and\nlow-level latent variables for stock prediction, while the diffusion\nprobabilistic model trains the predictor to handle stock price stochasticity by\nprogressively adding random noise to the stock data. Our Diffusion-VAE (D-Va)\nmodel is shown to outperform state-of-the-art solutions in terms of its\nprediction accuracy and variance. More importantly, the multi-step outputs can\nalso allow us to form a stock portfolio over the prediction length. We\ndemonstrate the effectiveness of our model outputs in the portfolio investment\ntask through the Sharpe ratio metric and highlight the importance of dealing\nwith different types of prediction uncertainties.",
          "arxiv_id": "2309.00073v2"
        },
        {
          "title": "Do Weibo platform experts perform better at predicting stock market?",
          "year": "2024-02",
          "abstract": "Sentiment analysis can be used for stock market prediction. However, existing\nresearch has not studied the impact of a user's financial background on\nsentiment-based forecasting of the stock market using artificial neural\nnetworks. In this work, a novel combination of neural networks is used for the\nassessment of sentiment-based stock market prediction, based on the financial\nbackground of the population that generated the sentiment. The state-of-the-art\nlanguage processing model Bidirectional Encoder Representations from\nTransformers (BERT) is used to classify the sentiment and a Long-Short Term\nMemory (LSTM) model is used for time-series based stock market prediction. For\nevaluation, the Weibo social networking platform is used as a sentiment data\ncollection source. Weibo users (and their comments respectively) are divided\ninto Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor\n(UFA) groups according to their background information, as collected by Weibo.\nThe Hong Kong Hang Seng index is used to extract historical stock market change\ndata. The results indicate that stock market prediction learned from the AFA\ngroup users is 39.67% more precise than that learned from the UFA group users\nand shows the highest accuracy (87%) when compared to existing approaches.",
          "arxiv_id": "2403.00772v1"
        }
      ],
      "32": [
        {
          "title": "Feature Embedding Clustering using POCS-based Clustering Algorithm",
          "year": "2023-03",
          "abstract": "An application of the POCS-based clustering algorithm (POCS stands for\nProjection Onto Convex Set), a novel clustering technique, for feature\nembedding clustering problems is proposed in this paper. The POCS-based\nclustering algorithm applies the POCS's convergence property to clustering\nproblems and has shown competitive performance when compared with that of other\nclassical clustering schemes in terms of clustering error and execution speed.\nSpecifically, the POCS-based clustering algorithm treats each data point as a\nconvex set and applies a parallel projection operation from every cluster\nprototype to corresponding data members in order to minimize the objective\nfunction and update the prototypes. The experimental results on the synthetic\nembedding datasets extracted from the 5 Celebrity Faces and MNIST datasets show\nthat the POCS-based clustering algorithm can perform with favorable results\nwhen compared with those of other classical clustering schemes such as the\nK-Means and Fuzzy C-Means algorithms in feature embedding clustering problems.",
          "arxiv_id": "2305.00001v1"
        },
        {
          "title": "POCS-based Clustering Algorithm",
          "year": "2022-08",
          "abstract": "A novel clustering technique based on the projection onto convex set (POCS)\nmethod, called POCS-based clustering algorithm, is proposed in this paper. The\nproposed POCS-based clustering algorithm exploits a parallel projection method\nof POCS to find appropriate cluster prototypes in the feature space. The\nalgorithm considers each data point as a convex set and projects the cluster\nprototypes parallelly to the member data points. The projections are convexly\ncombined to minimize the objective function for data clustering purpose. The\nperformance of the proposed POCS-based clustering algorithm is verified through\nexperiments on various synthetic datasets. The experimental results show that\nthe proposed POCS-based clustering algorithm is competitive and efficient in\nterms of clustering error and execution speed when compared with other\nconventional clustering methods including Fuzzy C-Means (FCM) and K-means\nclustering algorithms.",
          "arxiv_id": "2208.08888v3"
        },
        {
          "title": "Convex Clustering through MM: An Efficient Algorithm to Perform Hierarchical Clustering",
          "year": "2022-11",
          "abstract": "Convex clustering is a modern method with both hierarchical and $k$-means\nclustering characteristics. Although convex clustering can capture complex\nclustering structures hidden in data, the existing convex clustering algorithms\nare not scalable to large data sets with sample sizes greater than several\nthousands. Moreover, it is known that convex clustering sometimes fails to\nproduce a complete hierarchical clustering structure. This issue arises if\nclusters split up or the minimum number of possible clusters is larger than the\ndesired number of clusters. In this paper, we propose convex clustering through\nmajorization-minimization (CCMM) -- an iterative algorithm that uses cluster\nfusions and a highly efficient updating scheme derived using diagonal\nmajorization. Additionally, we explore different strategies to ensure that the\nhierarchical clustering structure terminates in a single cluster. With a\ncurrent desktop computer, CCMM efficiently solves convex clustering problems\nfeaturing over one million objects in seven-dimensional space, achieving a\nsolution time of 51 seconds on average.",
          "arxiv_id": "2211.01877v2"
        }
      ],
      "33": [
        {
          "title": "PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)",
          "year": "2024-09",
          "abstract": "The capability of generating high-quality source code using large language\nmodels (LLMs) reduces software development time and costs. However, they often\nintroduce security vulnerabilities due to training on insecure open-source\ndata. This highlights the need for ensuring secure and functional code\ngeneration. This paper introduces PromSec, an algorithm for prom optimization\nfor secure and functioning code generation using LLMs. In PromSec, we combine\n1) code vulnerability clearing using a generative adversarial graph neural\nnetwork, dubbed as gGAN, to fix and reduce security vulnerabilities in\ngenerated codes and 2) code generation using an LLM into an interactive loop,\nsuch that the outcome of the gGAN drives the LLM with enhanced prompts to\ngenerate secure codes while preserving their functionality. Introducing a new\ncontrastive learning approach in gGAN, we formulate code-clearing and\ngeneration as a dual-objective optimization problem, enabling PromSec to\nnotably reduce the number of LLM inferences. PromSec offers a cost-effective\nand practical solution for generating secure, functional code. Extensive\nexperiments conducted on Python and Java code datasets confirm that PromSec\neffectively enhances code security while upholding its intended functionality.\nOur experiments show that while a state-of-the-art approach fails to address\nall code vulnerabilities, PromSec effectively resolves them. Moreover, PromSec\nachieves more than an order-of-magnitude reduction in operation time, number of\nLLM queries, and security analysis costs. Furthermore, prompts optimized with\nPromSec for a certain LLM are transferable to other LLMs across programming\nlanguages and generalizable to unseen vulnerabilities in training. This study\nis a step in enhancing the trustworthiness of LLMs for secure and functional\ncode generation, supporting their integration into real-world software\ndevelopment.",
          "arxiv_id": "2409.12699v1"
        },
        {
          "title": "NatGen: Generative pre-training by \"Naturalizing\" source code",
          "year": "2022-06",
          "abstract": "Pre-trained Generative Language models (e.g. PLBART, CodeT5, SPT-Code) for\nsource code yielded strong results on several tasks in the past few years,\nincluding code generation and translation. These models have adopted varying\npre-training objectives to learn statistics of code construction from very\nlarge-scale corpora in a self-supervised fashion; the success of pre-trained\nmodels largely hinges on these pre-training objectives. This paper proposes a\nnew pre-training objective, \"Naturalizing\" of source code, exploiting code's\nbimodal, dual-channel (formal & natural channels) nature. Unlike natural\nlanguage, code's bimodal, dual-channel nature allows us to generate\nsemantically equivalent code at scale. We introduce six classes of semantic\npreserving transformations to introduce un-natural forms of code, and then\nforce our model to produce more natural original programs written by\ndevelopers. Learning to generate equivalent, but more natural code, at scale,\nover large corpora of open-source code, without explicit manual supervision,\nhelps the model learn to both ingest & generate code. We fine-tune our model in\nthree generative Software Engineering tasks: code generation, code translation,\nand code refinement with limited human-curated labeled data and achieve\nstate-of-the-art performance rivaling CodeT5. We show that our pre-trained\nmodel is especially competitive at zero-shot and few-shot learning, and better\nat learning code properties (e.g., syntax, data flow).",
          "arxiv_id": "2206.07585v2"
        },
        {
          "title": "How Accurately Do Large Language Models Understand Code?",
          "year": "2025-04",
          "abstract": "Large Language Models (LLMs) are increasingly used in post-development tasks\nsuch as code repair and testing. A key factor in these tasks' success is the\nmodel's deep understanding of code. However, the extent to which LLMs truly\nunderstand code remains largely unevaluated. Quantifying code comprehension is\nchallenging due to its abstract nature and the lack of a standardized metric.\nPreviously, this was assessed through developer surveys, which are not feasible\nfor evaluating LLMs. Existing LLM benchmarks focus primarily on code\ngeneration, fundamentally different from code comprehension. Additionally,\nfixed benchmarks quickly become obsolete as they become part of the training\ndata. This paper presents the first large-scale empirical investigation into\nLLMs' ability to understand code. Inspired by mutation testing, we use an LLM's\nfault-finding ability as a proxy for its deep code understanding. This approach\nis based on the insight that a model capable of identifying subtle functional\ndiscrepancies must understand the code well. We inject faults in real-world\nprograms and ask the LLM to localize them, ensuring the specifications suffice\nfor fault localization. Next, we apply semantic-preserving code mutations\n(SPMs) to the faulty programs and test whether the LLMs still locate the\nfaults, verifying their confidence in code understanding. We evaluate nine\npopular LLMs on 600,010 debugging tasks from 670 Java and 637 Python programs.\nWe find that LLMs lose the ability to debug the same bug in 78% of faulty\nprograms when SPMs are applied, indicating a shallow understanding of code and\nreliance on features irrelevant to semantics. We also find that LLMs understand\ncode earlier in the program better than later. This suggests that LLMs' code\ncomprehension remains tied to lexical and syntactic features due to\ntokenization designed for natural languages, which overlooks code semantics.",
          "arxiv_id": "2504.04372v2"
        }
      ],
      "34": [
        {
          "title": "Sequence-to-Sequence Knowledge Graph Completion and Question Answering",
          "year": "2022-03",
          "abstract": "Knowledge graph embedding (KGE) models represent each entity and relation of\na knowledge graph (KG) with low-dimensional embedding vectors. These methods\nhave recently been applied to KG link prediction and question answering over\nincomplete KGs (KGQA). KGEs typically create an embedding for each entity in\nthe graph, which results in large model sizes on real-world graphs with\nmillions of entities. For downstream tasks these atomic entity representations\noften need to be integrated into a multi stage pipeline, limiting their\nutility. We show that an off-the-shelf encoder-decoder Transformer model can\nserve as a scalable and versatile KGE model obtaining state-of-the-art results\nfor KG link prediction and incomplete KG question answering. We achieve this by\nposing KG link prediction as a sequence-to-sequence task and exchange the\ntriple scoring approach taken by prior KGE methods with autoregressive\ndecoding. Such a simple but powerful method reduces the model size up to 98%\ncompared to conventional KGE models while keeping inference time tractable.\nAfter finetuning this model on the task of KGQA over incomplete KGs, our\napproach outperforms baselines on multiple large-scale datasets without\nextensive hyperparameter tuning.",
          "arxiv_id": "2203.10321v1"
        },
        {
          "title": "Relational Message Passing for Knowledge Graph Completion",
          "year": "2020-02",
          "abstract": "Knowledge graph completion aims to predict missing relations between entities\nin a knowledge graph. In this work, we propose a relational message passing\nmethod for knowledge graph completion. Different from existing embedding-based\nmethods, relational message passing only considers edge features (i.e.,\nrelation types) without entity IDs in the knowledge graph, and passes\nrelational messages among edges iteratively to aggregate neighborhood\ninformation. Specifically, two kinds of neighborhood topology are modeled for a\ngiven entity pair under the relational message passing framework: (1)\nRelational context, which captures the relation types of edges adjacent to the\ngiven entity pair; (2) Relational paths, which characterize the relative\nposition between the given two entities in the knowledge graph. The two message\npassing modules are combined together for relation prediction. Experimental\nresults on knowledge graph benchmarks as well as our newly proposed dataset\nshow that, our method PathCon outperforms state-of-the-art knowledge graph\ncompletion methods by a large margin. PathCon is also shown applicable to\ninductive settings where entities are not seen in training stage, and it is\nable to provide interpretable explanations for the predicted results. The code\nand all datasets are available at https://github.com/hwwang55/PathCon.",
          "arxiv_id": "2002.06757v2"
        },
        {
          "title": "Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding",
          "year": "2021-10",
          "abstract": "Knowledge graphs (KGs) consisting of a large number of triples have become\nwidespread recently, and many knowledge graph embedding (KGE) methods are\nproposed to embed entities and relations of a KG into continuous vector spaces.\nSuch embedding methods simplify the operations of conducting various in-KG\ntasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering).\nThey can be viewed as general solutions for representing KGs. However, existing\nKGE methods are not applicable to inductive settings, where a model trained on\nsource KGs will be tested on target KGs with entities unseen during model\ntraining. Existing works focusing on KGs in inductive settings can only solve\nthe inductive relation prediction task. They can not handle other out-of-KG\ntasks as general as KGE methods since they don't produce embeddings for\nentities. In this paper, to achieve inductive knowledge graph embedding, we\npropose a model MorsE, which does not learn embeddings for entities but learns\ntransferable meta-knowledge that can be used to produce entity embeddings. Such\nmeta-knowledge is modeled by entity-independent modules and learned by\nmeta-learning. Experimental results show that our model significantly\noutperforms corresponding baselines for in-KG and out-of-KG tasks in inductive\nsettings.",
          "arxiv_id": "2110.14170v3"
        }
      ],
      "35": [
        {
          "title": "Subject-Adaptive Transfer Learning Using Resting State EEG Signals for Cross-Subject EEG Motor Imagery Classification",
          "year": "2024-05",
          "abstract": "Electroencephalography (EEG) motor imagery (MI) classification is a\nfundamental, yet challenging task due to the variation of signals between\nindividuals i.e., inter-subject variability. Previous approaches try to\nmitigate this using task-specific (TS) EEG signals from the target subject in\ntraining. However, recording TS EEG signals requires time and limits its\napplicability in various fields. In contrast, resting state (RS) EEG signals\nare a viable alternative due to ease of acquisition with rich subject\ninformation. In this paper, we propose a novel subject-adaptive transfer\nlearning strategy that utilizes RS EEG signals to adapt models on unseen\nsubject data. Specifically, we disentangle extracted features into task- and\nsubject-dependent features and use them to calibrate RS EEG signals for\nobtaining task information while preserving subject characteristics. The\ncalibrated signals are then used to adapt the model to the target subject,\nenabling the model to simulate processing TS EEG signals of the target subject.\nThe proposed method achieves state-of-the-art accuracy on three public\nbenchmarks, demonstrating the effectiveness of our method in cross-subject EEG\nMI classification. Our findings highlight the potential of leveraging RS EEG\nsignals to advance practical brain-computer interface systems. The code is\navailable at https://github.com/SionAn/MICCAI2024-ResTL.",
          "arxiv_id": "2405.19346v2"
        },
        {
          "title": "EEG-DIF: Early Warning of Epileptic Seizures through Generative Diffusion Model-based Multi-channel EEG Signals Forecasting",
          "year": "2024-10",
          "abstract": "Multi-channel EEG signals are commonly used for the diagnosis and assessment\nof diseases such as epilepsy. Currently, various EEG diagnostic algorithms\nbased on deep learning have been developed. However, most research efforts\nfocus solely on diagnosing and classifying current signal data but do not\nconsider the prediction of future trends for early warning. Additionally, since\nmulti-channel EEG can be essentially regarded as the spatio-temporal signal\ndata received by detectors at different locations in the brain, how to\nconstruct spatio-temporal information representations of EEG signals to\nfacilitate future trend prediction for multi-channel EEG becomes an important\nproblem. This study proposes a multi-signal prediction algorithm based on\ngenerative diffusion models (EEG-DIF), which transforms the multi-signal\nforecasting task into an image completion task, allowing for comprehensive\nrepresentation and learning of the spatio-temporal correlations and future\ndevelopmental patterns of multi-channel EEG signals. Here, we employ a publicly\navailable epilepsy EEG dataset to construct and validate the EEG-DIF. The\nresults demonstrate that our method can accurately predict future trends for\nmulti-channel EEG signals simultaneously. Furthermore, the early warning\naccuracy for epilepsy seizures based on the generated EEG data reaches 0.89. In\ngeneral, EEG-DIF provides a novel approach for characterizing multi-channel EEG\nsignals and an innovative early warning algorithm for epilepsy seizures, aiding\nin optimizing and enhancing the clinical diagnosis process. The code is\navailable at https://github.com/JZK00/EEG-DIF.",
          "arxiv_id": "2410.17343v1"
        },
        {
          "title": "From Video to EEG: Adapting Joint Embedding Predictive Architecture to Uncover Visual Concepts in Brain Signal Analysis",
          "year": "2025-07",
          "abstract": "EEG signals capture brain activity with high temporal and low spatial\nresolution, supporting applications such as neurological diagnosis, cognitive\nmonitoring, and brain-computer interfaces. However, effective analysis is\nhindered by limited labeled data, high dimensionality, and the absence of\nscalable models that fully capture spatiotemporal dependencies. Existing\nself-supervised learning (SSL) methods often focus on either spatial or\ntemporal features, leading to suboptimal representations. To this end, we\npropose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive\nArchitecture (V-JEPA) for EEG classification. By treating EEG as video-like\nsequences, EEG-VJEPA learns semantically meaningful spatiotemporal\nrepresentations using joint embeddings and adaptive masking. To our knowledge,\nthis is the first work that exploits V-JEPA for EEG classification and explores\nthe visual concepts learned by the model. Evaluations on the publicly available\nTemple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA\noutperforms existing state-of-the-art models in classification accuracy. Beyond\nclassification accuracy, EEG-VJEPA captures physiologically relevant spatial\nand temporal signal patterns, offering interpretable embeddings that may\nsupport human-AI collaboration in diagnostic workflows. These findings position\nEEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in\nreal-world clinical settings.",
          "arxiv_id": "2507.03633v4"
        }
      ],
      "36": [
        {
          "title": "Certifying LLM Safety against Adversarial Prompting",
          "year": "2023-09",
          "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that add\nmalicious tokens to an input prompt to bypass the safety guardrails of an LLM\nand cause it to produce harmful content. In this work, we introduce\nerase-and-check, the first framework for defending against adversarial prompts\nwith certifiable safety guarantees. Given a prompt, our procedure erases tokens\nindividually and inspects the resulting subsequences using a safety filter. Our\nsafety certificate guarantees that harmful prompts are not mislabeled as safe\ndue to an adversarial attack up to a certain size. We implement the safety\nfilter in two ways, using Llama 2 and DistilBERT, and compare the performance\nof erase-and-check for the two cases. We defend against three attack modes: i)\nadversarial suffix, where an adversarial sequence is appended at the end of a\nharmful prompt; ii) adversarial insertion, where the adversarial sequence is\ninserted anywhere in the middle of the prompt; and iii) adversarial infusion,\nwhere adversarial tokens are inserted at arbitrary positions in the prompt, not\nnecessarily as a contiguous block. Our experimental results demonstrate that\nthis procedure can obtain strong certified safety guarantees on harmful prompts\nwhile maintaining good empirical performance on safe prompts. Additionally, we\npropose three efficient empirical defenses: i) RandEC, a randomized subsampling\nversion of erase-and-check; ii) GreedyEC, which greedily erases tokens that\nmaximize the softmax score of the harmful class; and iii) GradEC, which uses\ngradient information to optimize tokens to erase. We demonstrate their\neffectiveness against adversarial prompts generated by the Greedy Coordinate\nGradient (GCG) attack algorithm. The code for our experiments is available at\nhttps://github.com/aounon/certified-llm-safety.",
          "arxiv_id": "2309.02705v4"
        },
        {
          "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
          "year": "2023-10",
          "abstract": "Large language models (LLMs) are susceptible to red teaming attacks, which\ncan induce LLMs to generate harmful content. Previous research constructs\nattack prompts via manual or automatic methods, which have their own\nlimitations on construction cost and quality. To address these issues, we\npropose an integrated approach that combines manual and automatic methods to\neconomically generate high-quality attack prompts. Specifically, considering\nthe impressive capabilities of newly emerged LLMs, we propose an attack\nframework to instruct LLMs to mimic human-generated prompts through in-context\nlearning. Furthermore, we propose a defense framework that fine-tunes victim\nLLMs through iterative interactions with the attack framework to enhance their\nsafety against red teaming attacks. Extensive experiments on different LLMs\nvalidate the effectiveness of our proposed attack and defense frameworks.\nAdditionally, we release a series of attack prompts datasets named SAP with\nvarying sizes, facilitating the safety evaluation and enhancement of more LLMs.\nOur code and dataset is available on https://github.com/Aatrox103/SAP .",
          "arxiv_id": "2310.12505v1"
        },
        {
          "title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on Large Language Models",
          "year": "2024-07",
          "abstract": "The rapid development of Large Language Models (LLMs) has brought impressive\nadvancements across various tasks. However, despite these achievements, LLMs\nstill pose inherent safety risks, especially in the context of jailbreak\nattacks. Most existing jailbreak methods follow an input-level manipulation\nparadigm to bypass safety mechanisms. Yet, as alignment techniques improve,\nsuch attacks are becoming increasingly detectable. In this work, we identify an\nunderexplored threat vector: the model's internal reasoning process, which can\nbe manipulated to elicit harmful outputs in a more stealthy way. To explore\nthis overlooked attack surface, we propose a novel black-box jailbreak attack\nmethod, Analyzing-based Jailbreak (ABJ). ABJ comprises two independent attack\npaths: textual and visual reasoning attacks, which exploit the model's\nmultimodal reasoning capabilities to bypass safety mechanisms, comprehensively\nexposing vulnerabilities in its reasoning chain. We conduct extensive\nexperiments on ABJ across various open-source and closed-source LLMs, VLMs, and\nRLMs. In particular, ABJ achieves high attack success rate (ASR) (82.1% on\nGPT-4o-2024-11-20) with exceptional attack efficiency (AE) among all target\nmodels, showcasing its remarkable attack effectiveness, transferability, and\nefficiency. Our work reveals a new type of safety risk and highlights the\nurgent need to mitigate implicit vulnerabilities in the model's reasoning\nprocess.",
          "arxiv_id": "2407.16205v6"
        }
      ],
      "37": [
        {
          "title": "IFSeg: Image-free Semantic Segmentation via Vision-Language Model",
          "year": "2023-03",
          "abstract": "Vision-language (VL) pre-training has recently gained much attention for its\ntransferability and flexibility in novel concepts (e.g., cross-modality\ntransfer) across various visual tasks. However, VL-driven segmentation has been\nunder-explored, and the existing approaches still have the burden of acquiring\nadditional training images or even segmentation annotations to adapt a VL model\nto downstream segmentation tasks. In this paper, we introduce a novel\nimage-free segmentation task where the goal is to perform semantic segmentation\ngiven only a set of the target semantic categories, but without any\ntask-specific images and annotations. To tackle this challenging task, our\nproposed method, coined IFSeg, generates VL-driven artificial\nimage-segmentation pairs and updates a pre-trained VL model to a segmentation\ntask. We construct this artificial training data by creating a 2D map of random\nsemantic categories and another map of their corresponding word tokens. Given\nthat a pre-trained VL model projects visual and text tokens into a common space\nwhere tokens that share the semantics are located closely, this artificially\ngenerated word map can replace the real image inputs for such a VL model.\nThrough an extensive set of experiments, our model not only establishes an\neffective baseline for this novel task but also demonstrates strong\nperformances compared to existing methods that rely on stronger supervision,\nsuch as task-specific images and segmentation masks. Code is available at\nhttps://github.com/alinlab/ifseg.",
          "arxiv_id": "2303.14396v1"
        },
        {
          "title": "Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation",
          "year": "2022-06",
          "abstract": "The task of unsupervised semantic segmentation aims to cluster pixels into\nsemantically meaningful groups. Specifically, pixels assigned to the same\ncluster should share high-level semantic properties like their object or part\ncategory. This paper presents MaskDistill: a novel framework for unsupervised\nsemantic segmentation based on three key ideas. First, we advocate a\ndata-driven strategy to generate object masks that serve as a pixel grouping\nprior for semantic segmentation. This approach omits handcrafted priors, which\nare often designed for specific scene compositions and limit the applicability\nof competing frameworks. Second, MaskDistill clusters the object masks to\nobtain pseudo-ground-truth for training an initial object segmentation model.\nThird, we leverage this model to filter out low-quality object masks. This\nstrategy mitigates the noise in our pixel grouping prior and results in a clean\ncollection of masks which we use to train a final segmentation model. By\ncombining these components, we can considerably outperform previous works for\nunsupervised semantic segmentation on PASCAL (+11% mIoU) and COCO (+4% mask\nAP50). Interestingly, as opposed to existing approaches, our framework does not\nlatch onto low-level image cues and is not limited to object-centric datasets.\nThe code and models will be made available.",
          "arxiv_id": "2206.06363v1"
        },
        {
          "title": "The Missing Point in Vision Transformers for Universal Image Segmentation",
          "year": "2025-05",
          "abstract": "Image segmentation remains a challenging task in computer vision, demanding\nrobust mask generation and precise classification. Recent mask-based approaches\nyield high-quality masks by capturing global context. However, accurately\nclassifying these masks, especially in the presence of ambiguous boundaries and\nimbalanced class distributions, remains an open challenge. In this work, we\nintroduce ViT-P, a novel two-stage segmentation framework that decouples mask\ngeneration from classification. The first stage employs a proposal generator to\nproduce class-agnostic mask proposals, while the second stage utilizes a\npoint-based classification model built on the Vision Transformer (ViT) to\nrefine predictions by focusing on mask central points. ViT-P serves as a\npre-training-free adapter, allowing the integration of various pre-trained\nvision transformers without modifying their architecture, ensuring adaptability\nto dense prediction tasks. Furthermore, we demonstrate that coarse and bounding\nbox annotations can effectively enhance classification without requiring\nadditional training on fine annotation datasets, reducing annotation costs\nwhile maintaining strong performance. Extensive experiments across COCO,\nADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving\nstate-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4\nmIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic\nsegmentation. The code and pretrained models are available at:\nhttps://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.",
          "arxiv_id": "2505.19795v1"
        }
      ],
      "38": [
        {
          "title": "Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters",
          "year": "2025-08",
          "abstract": "Gaussian processes are important models in the field of probabilistic\nnumerics. We present a procedure for optimizing Mat\\'ern kernel temporal\nGaussian processes with respect to the kernel covariance function's\nhyperparameters. It is based on casting the optimization problem as a recursive\nBayesian estimation procedure for the parameters of an autoregressive model. We\ndemonstrate that the proposed procedure outperforms maximizing the marginal\nlikelihood as well as Hamiltonian Monte Carlo sampling, both in terms of\nruntime and ultimate root mean square error in Gaussian process regression.",
          "arxiv_id": "2508.09792v1"
        },
        {
          "title": "Variational Elliptical Processes",
          "year": "2023-11",
          "abstract": "We present elliptical processes, a family of non-parametric probabilistic\nmodels that subsume Gaussian processes and Student's t processes. This\ngeneralization includes a range of new heavy-tailed behaviors while retaining\ncomputational tractability. Elliptical processes are based on a representation\nof elliptical distributions as a continuous mixture of Gaussian distributions.\nWe parameterize this mixture distribution as a spline normalizing flow, which\nwe train using variational inference. The proposed form of the variational\nposterior enables a sparse variational elliptical process applicable to\nlarge-scale problems. We highlight advantages compared to Gaussian processes\nthrough regression and classification experiments. Elliptical processes can\nsupersede Gaussian processes in several settings, including cases where the\nlikelihood is non-Gaussian or when accurate tail modeling is essential.",
          "arxiv_id": "2311.12566v1"
        },
        {
          "title": "Stein Variational Gaussian Processes",
          "year": "2020-09",
          "abstract": "We show how to use Stein variational gradient descent (SVGD) to carry out\ninference in Gaussian process (GP) models with non-Gaussian likelihoods and\nlarge data volumes. Markov chain Monte Carlo (MCMC) is extremely\ncomputationally intensive for these situations, but the parametric assumptions\nrequired for efficient variational inference (VI) result in incorrect inference\nwhen they encounter the multi-modal posterior distributions that are common for\nsuch models. SVGD provides a non-parametric alternative to variational\ninference which is substantially faster than MCMC. We prove that for GP models\nwith Lipschitz gradients the SVGD algorithm monotonically decreases the\nKullback-Leibler divergence from the sampling distribution to the true\nposterior. Our method is demonstrated on benchmark problems in both regression\nand classification, a multimodal posterior, and an air quality example with\n550,134 spatiotemporal observations, showing substantial performance\nimprovements over MCMC and VI.",
          "arxiv_id": "2009.12141v3"
        }
      ],
      "39": [
        {
          "title": "Skillful High-Resolution Ensemble Precipitation Forecasting with an Integrated Deep Learning Framework",
          "year": "2025-01",
          "abstract": "High-resolution precipitation forecasts are crucial for providing accurate\nweather prediction and supporting effective responses to extreme weather\nevents. Traditional numerical models struggle with stochastic subgrid-scale\nprocesses, while recent deep learning models often produce blurry results. To\naddress these challenges, we propose a physics-inspired deep learning framework\nfor high-resolution (0.05\\textdegree{} $\\times$ 0.05\\textdegree{}) ensemble\nprecipitation forecasting. Trained on ERA5 and CMPA high-resolution\nprecipitation datasets, the framework integrates deterministic and\nprobabilistic components. The deterministic model, based on a 3D\nSwinTransformer, captures average precipitation at mesoscale resolution and\nincorporates strategies to enhance performance, particularly for moderate to\nheavy rainfall. The probabilistic model employs conditional diffusion in latent\nspace to account for uncertainties in residual precipitation at convective\nscales. During inference, ensemble members are generated by repeatedly sampling\nlatent variables, enabling the model to represent precipitation uncertainty.\nOur model significantly enhances spatial resolution and forecast accuracy. Rank\nhistogram shows that the ensemble system is reliable and unbiased. In a case\nstudy of heavy precipitation in southern China, the model outputs align more\nclosely with observed precipitation distributions than ERA5, demonstrating\nsuperior capability in capturing extreme precipitation events. Additionally,\n5-day real-time forecasts show good performance in terms of CSI scores.",
          "arxiv_id": "2501.02905v1"
        },
        {
          "title": "Short-term precipitation prediction using deep learning",
          "year": "2021-10",
          "abstract": "Accurate weather prediction is essential for many aspects of life, notably\nthe early warning of extreme weather events such as rainstorms. Short-term\npredictions of these events rely on forecasts from numerical weather models, in\nwhich, despite much improvement in the past decades, outstanding issues remain\nconcerning model uncertainties, and increasing demands for computation and\nstorage resources. In recent years, the advance of deep learning offers a\nviable alternative approach. Here, we show that a 3D convolutional neural\nnetwork using a single frame of meteorology fields as input is capable of\npredicting the precipitation spatial distribution. The network is developed\nbased on 39-years (1980-2018) data of meteorology and daily precipitation over\nthe contiguous United States. The results bring fundamental advancements in\nweather prediction. First, the trained network alone outperforms the\nstate-of-the-art weather models in predicting daily total precipitation, and\nthe superiority of the network extends to forecast leads up to 5 days. Second,\ncombining the network predictions with the weather-model forecasts\nsignificantly improves the accuracy of model forecasts, especially for\nheavy-precipitation events. Third, the millisecond-scale inference time of the\nnetwork facilitates large ensemble predictions for further accuracy\nimprovement. These findings strongly support the use of deep-learning in\nshort-term weather predictions.",
          "arxiv_id": "2110.01843v1"
        },
        {
          "title": "Neural General Circulation Models for Weather and Climate",
          "year": "2023-11",
          "abstract": "General circulation models (GCMs) are the foundation of weather and climate\nprediction. GCMs are physics-based simulators which combine a numerical solver\nfor large-scale dynamics with tuned representations for small-scale processes\nsuch as cloud formation. Recently, machine learning (ML) models trained on\nreanalysis data achieved comparable or better skill than GCMs for deterministic\nweather forecasting. However, these models have not demonstrated improved\nensemble forecasts, or shown sufficient stability for long-term weather and\nclimate simulations. Here we present the first GCM that combines a\ndifferentiable solver for atmospheric dynamics with ML components, and show\nthat it can generate forecasts of deterministic weather, ensemble weather and\nclimate on par with the best ML and physics-based methods. NeuralGCM is\ncompetitive with ML models for 1-10 day forecasts, and with the European Centre\nfor Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts.\nWith prescribed sea surface temperature, NeuralGCM can accurately track climate\nmetrics such as global mean temperature for multiple decades, and climate\nforecasts with 140 km resolution exhibit emergent phenomena such as realistic\nfrequency and trajectories of tropical cyclones. For both weather and climate,\nour approach offers orders of magnitude computational savings over conventional\nGCMs. Our results show that end-to-end deep learning is compatible with tasks\nperformed by conventional GCMs, and can enhance the large-scale physical\nsimulations that are essential for understanding and predicting the Earth\nsystem.",
          "arxiv_id": "2311.07222v3"
        }
      ],
      "40": [
        {
          "title": "Learning to Discover Knowledge: A Weakly-Supervised Partial Domain Adaptation Approach",
          "year": "2024-06",
          "abstract": "Domain adaptation has shown appealing performance by leveraging knowledge\nfrom a source domain with rich annotations. However, for a specific target\ntask, it is cumbersome to collect related and high-quality source domains. In\nreal-world scenarios, large-scale datasets corrupted with noisy labels are easy\nto collect, stimulating a great demand for automatic recognition in a\ngeneralized setting, i.e., weakly-supervised partial domain adaptation\n(WS-PDA), which transfers a classifier from a large source domain with noises\nin labels to a small unlabeled target domain. As such, the key issues of WS-PDA\nare: 1) how to sufficiently discover the knowledge from the noisy labeled\nsource domain and the unlabeled target domain, and 2) how to successfully adapt\nthe knowledge across domains. In this paper, we propose a simple yet effective\ndomain adaptation approach, termed as self-paced transfer classifier learning\n(SP-TCL), to address the above issues, which could be regarded as a\nwell-performing baseline for several generalized domain adaptation tasks. The\nproposed model is established upon the self-paced learning scheme, seeking a\npreferable classifier for the target domain. Specifically, SP-TCL learns to\ndiscover faithful knowledge via a carefully designed prudent loss function and\nsimultaneously adapts the learned knowledge to the target domain by iteratively\nexcluding source examples from training under the self-paced fashion. Extensive\nevaluations on several benchmark datasets demonstrate that SP-TCL significantly\noutperforms state-of-the-art approaches on several generalized domain\nadaptation tasks.",
          "arxiv_id": "2406.14274v1"
        },
        {
          "title": "Unsupervised Domain Adaptation with Progressive Domain Augmentation",
          "year": "2020-04",
          "abstract": "Domain adaptation aims to exploit a label-rich source domain for learning\nclassifiers in a different label-scarce target domain. It is particularly\nchallenging when there are significant divergences between the two domains. In\nthe paper, we propose a novel unsupervised domain adaptation method based on\nprogressive domain augmentation. The proposed method generates virtual\nintermediate domains via domain interpolation, progressively augments the\nsource domain and bridges the source-target domain divergence by conducting\nmultiple subspace alignment on the Grassmann manifold. We conduct experiments\non multiple domain adaptation tasks and the results shows the proposed method\nachieves the state-of-the-art performance.",
          "arxiv_id": "2004.01735v2"
        },
        {
          "title": "Unsupervised Domain Adaptation for Extra Features in the Target Domain Using Optimal Transport",
          "year": "2022-09",
          "abstract": "Domain adaptation aims to transfer knowledge of labeled instances obtained\nfrom a source domain to a target domain to fill the gap between the domains.\nMost domain adaptation methods assume that the source and target domains have\nthe same dimensionality. Methods that are applicable when the number of\nfeatures is different in each domain have rarely been studied, especially when\nno label information is given for the test data obtained from the target\ndomain. In this paper, it is assumed that common features exist in both domains\nand that extra (new additional) features are observed in the target domain;\nhence, the dimensionality of the target domain is higher than that of the\nsource domain. To leverage the homogeneity of the common features, the\nadaptation between these source and target domains is formulated as an optimal\ntransport (OT) problem. In addition, a learning bound in the target domain for\nthe proposed OT-based method is derived. The proposed algorithm is validated\nusing both simulated and real-world data.",
          "arxiv_id": "2209.04594v1"
        }
      ],
      "41": [
        {
          "title": "On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning",
          "year": "2023-11",
          "abstract": "Bayesian deep learning and conformal prediction are two methods that have\nbeen used to convey uncertainty and increase safety in machine learning\nsystems. We focus on combining Bayesian deep learning with split conformal\nprediction and how this combination effects out-of-distribution coverage;\nparticularly in the case of multiclass image classification. We suggest that if\nthe model is generally underconfident on the calibration set, then the\nresultant conformal sets may exhibit worse out-of-distribution coverage\ncompared to simple predictive credible sets. Conversely, if the model is\noverconfident on the calibration set, the use of conformal prediction may\nimprove out-of-distribution coverage. We evaluate prediction sets as a result\nof combining split conformal methods and neural networks trained with (i)\nstochastic gradient descent, (ii) deep ensembles, and (iii) mean-field\nvariational inference. Our results suggest that combining Bayesian deep\nlearning models with split conformal prediction can, in some cases, cause\nunintended consequences such as reducing out-of-distribution coverage.",
          "arxiv_id": "2311.12688v2"
        },
        {
          "title": "An Information Theoretic Perspective on Conformal Prediction",
          "year": "2024-05",
          "abstract": "Conformal Prediction (CP) is a distribution-free uncertainty estimation\nframework that constructs prediction sets guaranteed to contain the true answer\nwith a user-specified probability. Intuitively, the size of the prediction set\nencodes a general notion of uncertainty, with larger sets associated with\nhigher degrees of uncertainty. In this work, we leverage information theory to\nconnect conformal prediction to other notions of uncertainty. More precisely,\nwe prove three different ways to upper bound the intrinsic uncertainty, as\ndescribed by the conditional entropy of the target variable given the inputs,\nby combining CP with information theoretical inequalities. Moreover, we\ndemonstrate two direct and useful applications of such connection between\nconformal prediction and information theory: (i) more principled and effective\nconformal training objectives that generalize previous approaches and enable\nend-to-end training of machine learning models from scratch, and (ii) a natural\nmechanism to incorporate side information into conformal prediction. We\nempirically validate both applications in centralized and federated learning\nsettings, showing our theoretical results translate to lower inefficiency\n(average prediction set size) for popular CP methods.",
          "arxiv_id": "2405.02140v3"
        },
        {
          "title": "Conformal Prediction and Human Decision Making",
          "year": "2025-03",
          "abstract": "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
          "arxiv_id": "2503.11709v2"
        }
      ],
      "42": [
        {
          "title": "Deep learning insights into cosmological structure formation",
          "year": "2020-11",
          "abstract": "The evolution of linear initial conditions present in the early universe into\nextended halos of dark matter at late times can be computed using cosmological\nsimulations. However, a theoretical understanding of this complex process\nremains elusive; in particular, the role of anisotropic information in the\ninitial conditions in establishing the final mass of dark matter halos remains\na long-standing puzzle. Here, we build a deep learning framework to investigate\nthis question. We train a three-dimensional convolutional neural network (CNN)\nto predict the mass of dark matter halos from the initial conditions, and\nquantify in full generality the amounts of information in the isotropic and\nanisotropic aspects of the initial density field about final halo masses. We\nfind that anisotropies add a small, albeit statistically significant amount of\ninformation over that contained within spherical averages of the density field\nabout final halo mass. However, the overall scatter in the final mass\npredictions does not change qualitatively with this additional information,\nonly decreasing from 0.9 dex to 0.7 dex. Given such a small improvement, our\nresults demonstrate that isotropic aspects of the initial density field\nessentially saturate the relevant information about final halo mass. Therefore,\ninstead of searching for information directly encoded in initial conditions\nanisotropies, a more promising route to accurate, fast halo mass predictions is\nto add approximate dynamical information based e.g. on perturbation theory.\nMore broadly, our results indicate that deep learning frameworks can provide a\npowerful tool for extracting physical insight into cosmological structure\nformation.",
          "arxiv_id": "2011.10577v3"
        },
        {
          "title": "Identifying Planetary Transit Candidates in TESS Full-Frame Image Light Curves via Convolutional Neural Networks",
          "year": "2021-01",
          "abstract": "The Transiting Exoplanet Survey Satellite (TESS) mission measured light from\nstars in ~75% of the sky throughout its two year primary mission, resulting in\nmillions of TESS 30-minute cadence light curves to analyze in the search for\ntransiting exoplanets. To search this vast data trove for transit signals, we\naim to provide an approach that is both computationally efficient and produces\nhighly performant predictions. This approach minimizes the required human\nsearch effort. We present a convolutional neural network, which we train to\nidentify planetary transit signals and dismiss false positives. To make a\nprediction for a given light curve, our network requires no prior transit\nparameters identified using other methods. Our network performs inference on a\nTESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large\nscale archival searches. We present 181 new planet candidates identified by our\nnetwork, which pass subsequent human vetting designed to rule out false\npositives. Our neural network model is additionally provided as open-source\ncode for public use and extension.",
          "arxiv_id": "2101.10919v2"
        },
        {
          "title": "Fast and realistic large-scale structure from machine-learning-augmented random field simulations",
          "year": "2022-05",
          "abstract": "Producing thousands of simulations of the dark matter distribution in the\nUniverse with increasing precision is a challenging but critical task to\nfacilitate the exploitation of current and forthcoming cosmological surveys.\nMany inexpensive substitutes to full $N$-body simulations have been proposed,\neven though they often fail to reproduce the statistics of the smaller,\nnon-linear scales. Among these alternatives, a common approximation is\nrepresented by the lognormal distribution, which comes with its own limitations\nas well, while being extremely fast to compute even for high-resolution density\nfields. In this work, we train a generative deep learning model, mainly made of\nconvolutional layers, to transform projected lognormal dark matter density\nfields to more realistic dark matter maps, as obtained from full $N$-body\nsimulations. We detail the procedure that we follow to generate highly\ncorrelated pairs of lognormal and simulated maps, which we use as our training\ndata, exploiting the information of the Fourier phases. We demonstrate the\nperformance of our model comparing various statistical tests with different\nfield resolutions, redshifts and cosmological parameters, proving its\nrobustness and explaining its current limitations. When evaluated on 100 test\nmaps, the augmented lognormal random fields reproduce the power spectrum up to\nwavenumbers of $1 \\ h \\ \\rm{Mpc}^{-1}$, and the bispectrum within 10%, and\nalways within the error bars, of the fiducial target simulations. Finally, we\ndescribe how we plan to integrate our proposed model with existing tools to\nyield more accurate spherical random fields for weak lensing analysis.",
          "arxiv_id": "2205.07898v2"
        }
      ],
      "43": [
        {
          "title": "Towards Global Crop Maps with Transfer Learning",
          "year": "2022-11",
          "abstract": "The continuous increase in global population and the impact of climate change\non crop production are expected to affect the food sector significantly. In\nthis context, there is need for timely, large-scale and precise mapping of\ncrops for evidence-based decision making. A key enabler towards this direction\nare new satellite missions that freely offer big remote sensing data of high\nspatio-temporal resolution and global coverage. During the previous decade and\nbecause of this surge of big Earth observations, deep learning methods have\ndominated the remote sensing and crop mapping literature. Nevertheless, deep\nlearning models require large amounts of annotated data that are scarce and\nhard-to-acquire. To address this problem, transfer learning methods can be used\nto exploit available annotations and enable crop mapping for other regions,\ncrop types and years of inspection. In this work, we have developed and trained\na deep learning model for paddy rice detection in South Korea using Sentinel-1\nVH time-series. We then fine-tune the model for i) paddy rice detection in\nFrance and Spain and ii) barley detection in the Netherlands. Additionally, we\npropose a modification in the pre-trained weights in order to incorporate extra\ninput features (Sentinel-1 VV). Our approach shows excellent performance when\ntransferring in different areas for the same crop type and rather promising\nresults when transferring in a different area and crop type.",
          "arxiv_id": "2211.04755v2"
        },
        {
          "title": "Crop Type Identification for Smallholding Farms: Analyzing Spatial, Temporal and Spectral Resolutions in Satellite Imagery",
          "year": "2022-05",
          "abstract": "The integration of the modern Machine Learning (ML) models into remote\nsensing and agriculture has expanded the scope of the application of satellite\nimages in the agriculture domain. In this paper, we present how the accuracy of\ncrop type identification improves as we move from\nmedium-spatiotemporal-resolution (MSTR) to high-spatiotemporal-resolution\n(HSTR) satellite images. We further demonstrate that high spectral resolution\nin satellite imagery can improve prediction performance for low spatial and\ntemporal resolutions (LSTR) images. The F1-score is increased by 7% when using\nmultispectral data of MSTR images as compared to the best results obtained from\nHSTR images. Similarly, when crop season based time series of multispectral\ndata is used we observe an increase of 1.2% in the F1-score. The outcome\nmotivates further advancements in the field of synthetic band generation.",
          "arxiv_id": "2205.03104v1"
        },
        {
          "title": "Integrating global spatial features in CNN based Hyperspectral/SAR imagery classification",
          "year": "2020-05",
          "abstract": "The land cover classification has played an important role in remote sensing\nbecause it can intelligently identify things in one huge remote sensing image\nto reduce the work of humans. However, a lot of classification methods are\ndesigned based on the pixel feature or limited spatial feature of the remote\nsensing image, which limits the classification accuracy and universality of\ntheir methods. This paper proposed a novel method to take into the information\nof remote sensing image, i.e., geographic latitude-longitude information. In\naddition, a dual-branch convolutional neural network (CNN) classification\nmethod is designed in combination with the global information to mine the pixel\nfeatures of the image. Then, the features of the two neural networks are fused\nwith another fully neural network to realize the classification of remote\nsensing images. Finally, two remote sensing images are used to verify the\neffectiveness of our method, including hyperspectral imaging (HSI) and\npolarimetric synthetic aperture radar (PolSAR) imagery. The result of the\nproposed method is superior to the traditional single-channel convolutional\nneural network.",
          "arxiv_id": "2006.00234v2"
        }
      ],
      "44": [
        {
          "title": "Overview of Human Activity Recognition Using Sensor Data",
          "year": "2023-09",
          "abstract": "Human activity recognition (HAR) is an essential research field that has been\nused in different applications including home and workplace automation,\nsecurity and surveillance as well as healthcare. Starting from conventional\nmachine learning methods to the recently developing deep learning techniques\nand the Internet of things, significant contributions have been shown in the\nHAR area in the last decade. Even though several review and survey studies have\nbeen published, there is a lack of sensor-based HAR overview studies focusing\non summarising the usage of wearable sensors and smart home sensors data as\nwell as applications of HAR and deep learning techniques. Hence, we overview\nsensor-based HAR, discuss several important applications that rely on HAR, and\nhighlight the most common machine learning methods that have been used for HAR.\nFinally, several challenges of HAR are explored that should be addressed to\nfurther improve the robustness of HAR.",
          "arxiv_id": "2309.07170v1"
        },
        {
          "title": "Unsupervised Statistical Feature-Guided Diffusion Model for Sensor-based Human Activity Recognition",
          "year": "2023-05",
          "abstract": "Human activity recognition (HAR) from on-body sensors is a core functionality\nin many AI applications: from personal health, through sports and wellness to\nIndustry 4.0. A key problem holding up progress in wearable sensor-based HAR,\ncompared to other ML areas, such as computer vision, is the unavailability of\ndiverse and labeled training data. Particularly, while there are innumerable\nannotated images available in online repositories, freely available sensor data\nis sparse and mostly unlabeled. We propose an unsupervised statistical\nfeature-guided diffusion model specifically optimized for wearable sensor-based\nhuman activity recognition with devices such as inertial measurement unit (IMU)\nsensors. The method generates synthetic labeled time-series sensor data without\nrelying on annotated training data. Thereby, it addresses the scarcity and\nannotation difficulties associated with real-world sensor data. By conditioning\nthe diffusion model on statistical information such as mean, standard\ndeviation, Z-score, and skewness, we generate diverse and representative\nsynthetic sensor data. We conducted experiments on public human activity\nrecognition datasets and compared the method to conventional oversampling and\nstate-of-the-art generative adversarial network methods. Experimental results\ndemonstrate that this can improve the performance of human activity recognition\nand outperform existing techniques.",
          "arxiv_id": "2306.05285v2"
        },
        {
          "title": "BSDGAN: Balancing Sensor Data Generative Adversarial Networks for Human Activity Recognition",
          "year": "2022-08",
          "abstract": "The development of IoT technology enables a variety of sensors can be\nintegrated into mobile devices. Human Activity Recognition (HAR) based on\nsensor data has become an active research topic in the field of machine\nlearning and ubiquitous computing. However, due to the inconsistent frequency\nof human activities, the amount of data for each activity in the human activity\ndataset is imbalanced. Considering the limited sensor resources and the high\ncost of manually labeled sensor data, human activity recognition is facing the\nchallenge of highly imbalanced activity datasets. In this paper, we propose\nBalancing Sensor Data Generative Adversarial Networks (BSDGAN) to generate\nsensor data for minority human activities. The proposed BSDGAN consists of a\ngenerator model and a discriminator model. Considering the extreme imbalance of\nhuman activity dataset, an autoencoder is employed to initialize the training\nprocess of BSDGAN, ensure the data features of each activity can be learned.\nThe generated activity data is combined with the original dataset to balance\nthe amount of activity data across human activity classes. We deployed multiple\nhuman activity recognition models on two publicly available imbalanced human\nactivity datasets, WISDM and UNIMIB. Experimental results show that the\nproposed BSDGAN can effectively capture the data features of real human\nactivity sensor data, and generate realistic synthetic sensor data. Meanwhile,\nthe balanced activity dataset can effectively help the activity recognition\nmodel to improve the recognition accuracy.",
          "arxiv_id": "2208.03647v1"
        }
      ],
      "45": [
        {
          "title": "RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network",
          "year": "2020-02",
          "abstract": "Spiking Neural Networks (SNNs) have recently attracted significant research\ninterest as the third generation of artificial neural networks that can enable\nlow-power event-driven data analytics. The best performing SNNs for image\nrecognition tasks are obtained by converting a trained Analog Neural Network\n(ANN), consisting of Rectified Linear Units (ReLU), to SNN composed of\nintegrate-and-fire neurons with \"proper\" firing thresholds. The converted SNNs\ntypically incur loss in accuracy compared to that provided by the original ANN\nand require sizable number of inference time-steps to achieve the best\naccuracy. We find that performance degradation in the converted SNN stems from\nusing \"hard reset\" spiking neuron that is driven to fixed reset potential once\nits membrane potential exceeds the firing threshold, leading to information\nloss during SNN inference. We propose ANN-SNN conversion using \"soft reset\"\nspiking neuron model, referred to as Residual Membrane Potential (RMP) spiking\nneuron, which retains the \"residual\" membrane potential above threshold at the\nfiring instants. We demonstrate near loss-less ANN-SNN conversion using RMP\nneurons for VGG-16, ResNet-20, and ResNet-34 SNNs on challenging datasets\nincluding CIFAR-10 (93.63% top-1), CIFAR-100 (70.93% top-1), and ImageNet\n(73.09% top-1 accuracy). Our results also show that RMP-SNN surpasses the best\ninference accuracy provided by the converted SNN with \"hard reset\" spiking\nneurons using 2-8 times fewer inference time-steps across network architectures\nand datasets.",
          "arxiv_id": "2003.01811v2"
        },
        {
          "title": "SPIDE: A Purely Spike-based Method for Training Feedback Spiking Neural Networks",
          "year": "2023-02",
          "abstract": "Spiking neural networks (SNNs) with event-based computation are promising\nbrain-inspired models for energy-efficient applications on neuromorphic\nhardware. However, most supervised SNN training methods, such as conversion\nfrom artificial neural networks or direct training with surrogate gradients,\nrequire complex computation rather than spike-based operations of spiking\nneurons during training. In this paper, we study spike-based implicit\ndifferentiation on the equilibrium state (SPIDE) that extends the recently\nproposed training method, implicit differentiation on the equilibrium state\n(IDE), for supervised learning with purely spike-based computation, which\ndemonstrates the potential for energy-efficient training of SNNs. Specifically,\nwe introduce ternary spiking neuron couples and prove that implicit\ndifferentiation can be solved by spikes based on this design, so the whole\ntraining procedure, including both forward and backward passes, is made as\nevent-driven spike computation, and weights are updated locally with two-stage\naverage firing rates. Then we propose to modify the reset membrane potential to\nreduce the approximation error of spikes. With these key components, we can\ntrain SNNs with flexible structures in a small number of time steps and with\nfiring sparsity during training, and the theoretical estimation of energy costs\ndemonstrates the potential for high efficiency. Meanwhile, experiments show\nthat even with these constraints, our trained models can still achieve\ncompetitive results on MNIST, CIFAR-10, CIFAR-100, and CIFAR10-DVS. Our code is\navailable at https://github.com/pkuxmq/SPIDE-FSNN.",
          "arxiv_id": "2302.00232v1"
        },
        {
          "title": "BioGrad: Biologically Plausible Gradient-Based Learning for Spiking Neural Networks",
          "year": "2021-10",
          "abstract": "Spiking neural networks (SNN) are delivering energy-efficient, massively\nparallel, and low-latency solutions to AI problems, facilitated by the emerging\nneuromorphic chips. To harness these computational benefits, SNN need to be\ntrained by learning algorithms that adhere to brain-inspired neuromorphic\nprinciples, namely event-based, local, and online computations. Yet, the\nstate-of-the-art SNN training algorithms are based on backprop that does not\nfollow the above principles. Due to its limited biological plausibility, the\napplication of backprop to SNN requires non-local feedback pathways for\ntransmitting continuous-valued errors, and relies on gradients from future\ntimesteps. The introduction of biologically plausible modifications to backprop\nhas helped overcome several of its limitations, but limits the degree to which\nbackprop is approximated, which hinders its performance. We propose a\nbiologically plausible gradient-based learning algorithm for SNN that is\nfunctionally equivalent to backprop, while adhering to all three neuromorphic\nprinciples. We introduced multi-compartment spiking neurons with local\neligibility traces to compute the gradients required for learning, and a\nperiodic \"sleep\" phase to further improve the approximation to backprop during\nwhich a local Hebbian rule aligns the feedback and feedforward weights. Our\nmethod achieved the same level of performance as backprop with multi-layer\nfully connected SNN on MNIST (98.13%) and the event-based N-MNIST (97.59%)\ndatasets. We deployed our learning algorithm on Intel's Loihi to train a\n1-hidden-layer network for MNIST, and obtained 93.32% test accuracy while\nconsuming 400 times less energy per training sample than BioGrad on GPU. Our\nwork shows that optimal learning is feasible in neuromorphic computing, and\nfurther pursuing its biological plausibility can better capture the benefits of\nthis emerging computing paradigm.",
          "arxiv_id": "2110.14092v1"
        }
      ],
      "46": [
        {
          "title": "Normative Modeling using Multimodal Variational Autoencoders to Identify Abnormal Brain Structural Patterns in Alzheimer Disease",
          "year": "2021-10",
          "abstract": "Normative modelling is an emerging method for understanding the underlying\nheterogeneity within brain disorders like Alzheimer Disease (AD) by quantifying\nhow each patient deviates from the expected normative pattern that has been\nlearned from a healthy control distribution. Since AD is a multifactorial\ndisease with more than one biological pathways, multimodal magnetic resonance\nimaging (MRI) neuroimaging data can provide complementary information about the\ndisease heterogeneity. However, existing deep learning based normative models\non multimodal MRI data use unimodal autoencoders with a single encoder and\ndecoder that may fail to capture the relationship between brain measurements\nextracted from different MRI modalities. In this work, we propose multi-modal\nvariational autoencoder (mmVAE) based normative modelling framework that can\ncapture the joint distribution between different modalities to identify\nabnormal brain structural patterns in AD. Our multi-modal framework takes as\ninput Freesurfer processed brain region volumes from T1-weighted (cortical and\nsubcortical) and T2-weighed (hippocampal) scans of cognitively normal\nparticipants to learn the morphological characteristics of the healthy brain.\nThe estimated normative model is then applied on Alzheimer Disease (AD)\npatients to quantify the deviation in brain volumes and identify the abnormal\nbrain structural patterns due to the effect of the different AD stages. Our\nexperimental results show that modeling joint distribution between the multiple\nMRI modalities generates deviation maps that are more sensitive to disease\nstaging within AD, have a better correlation with patient cognition and result\nin higher number of brain regions with statistically significant deviations\ncompared to a unimodal baseline model with all modalities concatenated as a\nsingle input.",
          "arxiv_id": "2110.04903v4"
        },
        {
          "title": "Recurrent Brain Graph Mapper for Predicting Time-Dependent Brain Graph Evaluation Trajectory",
          "year": "2021-10",
          "abstract": "Several brain disorders can be detected by observing alterations in the\nbrain's structural and functional connectivities. Neurological findings suggest\nthat early diagnosis of brain disorders, such as mild cognitive impairment\n(MCI), can prevent and even reverse its development into Alzheimer's disease\n(AD). In this context, recent studies aimed to predict the evolution of brain\nconnectivities over time by proposing machine learning models that work on\nbrain images. However, such an approach is costly and time-consuming. Here, we\npropose to use brain connectivities as a more efficient alternative for\ntime-dependent brain disorder diagnosis by regarding the brain as instead a\nlarge interconnected graph characterizing the interconnectivity scheme between\nseveral brain regions. We term our proposed method Recurrent Brain Graph Mapper\n(RBGM), a novel efficient edge-based recurrent graph neural network that\npredicts the time-dependent evaluation trajectory of a brain graph from a\nsingle baseline. Our RBGM contains a set of recurrent neural network-inspired\nmappers for each time point, where each mapper aims to project the ground-truth\nbrain graph onto its next time point. We leverage the teacher forcing method to\nboost training and improve the evolved brain graph quality. To maintain the\ntopological consistency between the predicted brain graphs and their\ncorresponding ground-truth brain graphs at each time point, we further\nintegrate a topological loss. We also use l1 loss to capture time-dependency\nand minimize the distance between the brain graph at consecutive time points\nfor regularization. Benchmarks against several variants of RBGM and\nstate-of-the-art methods prove that we can achieve the same accuracy in\npredicting brain graph evolution more efficiently, paving the way for novel\ngraph neural network architecture and a highly efficient training scheme.",
          "arxiv_id": "2110.11237v1"
        },
        {
          "title": "Multi-Resolution Graph Analysis of Dynamic Brain Network for Classification of Alzheimer's Disease and Mild Cognitive Impairment",
          "year": "2024-09",
          "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder marked by memory\nloss and cognitive decline, making early detection vital for timely\nintervention. However, early diagnosis is challenging due to the heterogeneous\npresentation of symptoms. Resting-state functional magnetic resonance imaging\n(rs-fMRI) captures spontaneous brain activity and functional connectivity,\nwhich are known to be disrupted in AD and mild cognitive impairment (MCI).\nTraditional methods, such as Pearson's correlation, have been used to calculate\nassociation matrices, but these approaches often overlook the dynamic and\nnon-stationary nature of brain activity. In this study, we introduce a novel\nmethod that integrates discrete wavelet transform (DWT) and graph theory to\nmodel the dynamic behavior of brain networks. Our approach captures the\ntime-frequency representation of brain activity, allowing for a more nuanced\nanalysis of the underlying network dynamics. Machine learning was employed to\nautomate the discrimination of different stages of AD based on learned patterns\nfrom brain network at different frequency bands. We applied our method to a\ndataset of rs-fMRI images from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI) database, demonstrating its potential as an early diagnostic tool for AD\nand for monitoring disease progression. Our statistical analysis identifies\nspecific brain regions and connections that are affected in AD and MCI, at\ndifferent frequency bands, offering deeper insights into the disease's impact\non brain function.",
          "arxiv_id": "2409.04072v2"
        }
      ],
      "47": [
        {
          "title": "Modern graph neural networks do worse than classical greedy algorithms in solving combinatorial optimization problems like maximum independent set",
          "year": "2022-06",
          "abstract": "The recent work ``Combinatorial Optimization with Physics-Inspired Graph\nNeural Networks'' [Nat Mach Intell 4 (2022) 367] introduces a physics-inspired\nunsupervised Graph Neural Network (GNN) to solve combinatorial optimization\nproblems on sparse graphs. To test the performances of these GNNs, the authors\nof the work show numerical results for two fundamental problems: maximum cut\nand maximum independent set (MIS). They conclude that \"the graph neural network\noptimizer performs on par or outperforms existing solvers, with the ability to\nscale beyond the state of the art to problems with millions of variables.\"\n  In this comment, we show that a simple greedy algorithm, running in almost\nlinear time, can find solutions for the MIS problem of much better quality than\nthe GNN. The greedy algorithm is faster by a factor of $10^4$ with respect to\nthe GNN for problems with a million variables. We do not see any good reason\nfor solving the MIS with these GNN, as well as for using a sledgehammer to\ncrack nuts.\n  In general, many claims of superiority of neural networks in solving\ncombinatorial problems are at risk of being not solid enough, since we lack\nstandard benchmarks based on really hard problems. We propose one of such hard\nbenchmarks, and we hope to see future neural network optimizers tested on these\nproblems before any claim of superiority is made.",
          "arxiv_id": "2206.13211v2"
        },
        {
          "title": "Enhancing Column Generation by Reinforcement Learning-Based Hyper-Heuristic for Vehicle Routing and Scheduling Problems",
          "year": "2023-10",
          "abstract": "Column generation (CG) is a vital method to solve large-scale problems by\ndynamically generating variables. It has extensive applications in common\ncombinatorial optimization, such as vehicle routing and scheduling problems,\nwhere each iteration step requires solving an NP-hard constrained shortest path\nproblem. Although some heuristic methods for acceleration already exist, they\nare not versatile enough to solve different problems. In this work, we propose\na reinforcement learning-based hyper-heuristic framework, dubbed RLHH, to\nenhance the performance of CG. RLHH is a selection module embedded in CG to\naccelerate convergence and get better integer solutions. In each CG iteration,\nthe RL agent selects a low-level heuristic to construct a reduced network only\ncontaining the edges with a greater chance of being part of the optimal\nsolution. In addition, we specify RLHH to solve two typical combinatorial\noptimization problems: Vehicle Routing Problem with Time Windows (VRPTW) and\nBus Driver Scheduling Problem (BDSP). The total cost can be reduced by up to\n27.9\\% in VRPTW and 15.4\\% in BDSP compared to the best lower-level heuristic\nin our tested scenarios, within equivalent or even less computational time. The\nproposed RLHH is the first RL-based CG method that outperforms traditional\napproaches in terms of solution quality, which can promote the application of\nCG in combinatorial optimization.",
          "arxiv_id": "2310.09686v1"
        },
        {
          "title": "Combining Reinforcement Learning and Constraint Programming for Combinatorial Optimization",
          "year": "2020-06",
          "abstract": "Combinatorial optimization has found applications in numerous fields, from\naerospace to transportation planning and economics. The goal is to find an\noptimal solution among a finite set of possibilities. The well-known challenge\none faces with combinatorial optimization is the state-space explosion problem:\nthe number of possibilities grows exponentially with the problem size, which\nmakes solving intractable for large problems. In the last years, deep\nreinforcement learning (DRL) has shown its promise for designing good\nheuristics dedicated to solve NP-hard combinatorial optimization problems.\nHowever, current approaches have two shortcomings: (1) they mainly focus on the\nstandard travelling salesman problem and they cannot be easily extended to\nother problems, and (2) they only provide an approximate solution with no\nsystematic ways to improve it or to prove optimality. In another context,\nconstraint programming (CP) is a generic tool to solve combinatorial\noptimization problems. Based on a complete search procedure, it will always\nfind the optimal solution if we allow an execution time large enough. A\ncritical design choice, that makes CP non-trivial to use in practice, is the\nbranching decision, directing how the search space is explored. In this work,\nwe propose a general and hybrid approach, based on DRL and CP, for solving\ncombinatorial optimization problems. The core of our approach is based on a\ndynamic programming formulation, that acts as a bridge between both techniques.\nWe experimentally show that our solver is efficient to solve two challenging\nproblems: the traveling salesman problem with time windows, and the 4-moments\nportfolio optimization problem. Results obtained show that the framework\nintroduced outperforms the stand-alone RL and CP solutions, while being\ncompetitive with industrial solvers.",
          "arxiv_id": "2006.01610v1"
        }
      ],
      "48": [
        {
          "title": "ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal",
          "year": "2023-10",
          "abstract": "Electrocardiogram (ECG) is an essential signal in monitoring human heart\nactivities. Researchers have achieved promising results in leveraging ECGs in\nclinical applications with deep learning models. However, the mainstream deep\nlearning approaches usually neglect the periodic and formative attribute of the\nECG heartbeat waveform. In this work, we propose a novel ECG-Segment based\nLearning (ECG-SL) framework to explicitly model the periodic nature of ECG\nsignals. More specifically, ECG signals are first split into heartbeat\nsegments, and then structural features are extracted from each of the segments.\nBased on the structural features, a temporal model is designed to learn the\ntemporal information for various clinical tasks. Further, due to the fact that\nmassive ECG signals are available but the labeled data are very limited, we\nalso explore self-supervised learning strategy to pre-train the models,\nresulting significant improvement for downstream tasks. The proposed method\noutperforms the baseline model and shows competitive performances compared with\ntask-specific methods in three clinical applications: cardiac condition\ndiagnosis, sleep apnea detection, and arrhythmia classification. Further, we\nfind that the ECG-SL tends to focus more on each heartbeat's peak and ST range\nthan ResNet by visualizing the saliency maps.",
          "arxiv_id": "2310.00818v2"
        },
        {
          "title": "Leveraging Statistical Shape Priors in GAN-based ECG Synthesis",
          "year": "2022-10",
          "abstract": "Electrocardiogram (ECG) data collection during emergency situations is\nchallenging, making ECG data generation an efficient solution for dealing with\nhighly imbalanced ECG training datasets. In this paper, we propose a novel\napproach for ECG signal generation using Generative Adversarial Networks (GANs)\nand statistical ECG data modeling. Our approach leverages prior knowledge about\nECG dynamics to synthesize realistic signals, addressing the complex dynamics\nof ECG signals. To validate our approach, we conducted experiments using ECG\nsignals from the MIT-BIH arrhythmia database. Our results demonstrate that our\napproach, which models temporal and amplitude variations of ECG signals as 2-D\nshapes, generates more realistic signals compared to state-of-the-art GAN based\ngeneration baselines. Our proposed approach has significant implications for\nimproving the quality of ECG training datasets, which can ultimately lead to\nbetter performance of ECG classification algorithms. This research contributes\nto the development of more efficient and accurate methods for ECG analysis,\nwhich can aid in the diagnosis and treatment of cardiac diseases.",
          "arxiv_id": "2211.02626v2"
        },
        {
          "title": "Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model",
          "year": "2025-02",
          "abstract": "Electrocardiogram (ECG) is essential for the clinical diagnosis of\narrhythmias and other heart diseases, but deep learning methods based on ECG\noften face limitations due to the need for high-quality annotations. Although\nprevious ECG self-supervised learning (eSSL) methods have made significant\nprogress in representation learning from unannotated ECG data, they typically\ntreat ECG signals as ordinary time-series data, segmenting the signals using\nfixed-size and fixed-step time windows, which often ignore the form and rhythm\ncharacteristics and latent semantic relationships in ECG signals. In this work,\nwe introduce a novel perspective on ECG signals, treating heartbeats as words\nand rhythms as sentences. Based on this perspective, we first designed the\nQRS-Tokenizer, which generates semantically meaningful ECG sentences from the\nraw ECG signals. Building on these, we then propose HeartLang, a novel\nself-supervised learning framework for ECG language processing, learning\ngeneral representations at form and rhythm levels. Additionally, we construct\nthe largest heartbeat-based ECG vocabulary to date, which will further advance\nthe development of ECG language processing. We evaluated HeartLang across six\npublic ECG datasets, where it demonstrated robust competitiveness against other\neSSL methods. Our data and code are publicly available at\nhttps://github.com/PKUDigitalHealth/HeartLang.",
          "arxiv_id": "2502.10707v1"
        }
      ],
      "49": [
        {
          "title": "Efficient Human-in-the-Loop Active Learning: A Novel Framework for Data Labeling in AI Systems",
          "year": "2024-12",
          "abstract": "Modern AI algorithms require labeled data. In real world, majority of data\nare unlabeled. Labeling the data are costly. this is particularly true for some\nareas requiring special skills, such as reading radiology images by physicians.\nTo most efficiently use expert's time for the data labeling, one promising\napproach is human-in-the-loop active learning algorithm. In this work, we\npropose a novel active learning framework with significant potential for\napplication in modern AI systems. Unlike the traditional active learning\nmethods, which only focus on determining which data point should be labeled,\nour framework also introduces an innovative perspective on incorporating\ndifferent query scheme. We propose a model to integrate the information from\ndifferent types of queries. Based on this model, our active learning frame can\nautomatically determine how the next question is queried. We further developed\na data driven exploration and exploitation framework into our active learning\nmethod. This method can be embedded in numerous active learning algorithms.\nThrough simulations on five real-world datasets, including a highly complex\nreal image task, our proposed active learning framework exhibits higher\naccuracy and lower loss compared to other methods.",
          "arxiv_id": "2501.00277v1"
        },
        {
          "title": "GALAXY: Graph-based Active Learning at the Extreme",
          "year": "2022-02",
          "abstract": "Active learning is a label-efficient approach to train highly effective\nmodels while interactively selecting only small subsets of unlabelled data for\nlabelling and training. In \"open world\" settings, the classes of interest can\nmake up a small fraction of the overall dataset -- most of the data may be\nviewed as an out-of-distribution or irrelevant class. This leads to extreme\nclass-imbalance, and our theory and methods focus on this core issue. We\npropose a new strategy for active learning called GALAXY (Graph-based Active\nLearning At the eXtrEme), which blends ideas from graph-based active learning\nand deep learning. GALAXY automatically and adaptively selects more\nclass-balanced examples for labeling than most other methods for active\nlearning. Our theory shows that GALAXY performs a refined form of uncertainty\nsampling that gathers a much more class-balanced dataset than vanilla\nuncertainty sampling. Experimentally, we demonstrate GALAXY's superiority over\nexisting state-of-art deep active learning algorithms in unbalanced vision\nclassification settings generated from popular datasets.",
          "arxiv_id": "2202.01402v2"
        },
        {
          "title": "SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios",
          "year": "2021-07",
          "abstract": "Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet. SIMILAR is available as a part of the DISTIL\ntoolkit: \"https://github.com/decile-team/distil\".",
          "arxiv_id": "2107.00717v2"
        }
      ],
      "50": [
        {
          "title": "Zero-Shot Motor Health Monitoring by Blind Domain Transition",
          "year": "2022-12",
          "abstract": "Continuous long-term monitoring of motor health is crucial for the early\ndetection of abnormalities such as bearing faults (up to 51% of motor failures\nare attributed to bearing faults). Despite numerous methodologies proposed for\nbearing fault detection, most of them require normal (healthy) and abnormal\n(faulty) data for training. Even with the recent deep learning (DL)\nmethodologies trained on the labeled data from the same machine, the\nclassification accuracy significantly deteriorates when one or few conditions\nare altered. Furthermore, their performance suffers significantly or may\nentirely fail when they are tested on another machine with entirely different\nhealthy and faulty signal patterns. To address this need, in this pilot study,\nwe propose a zero-shot bearing fault detection method that can detect any fault\non a new (target) machine regardless of the working conditions, sensor\nparameters, or fault characteristics. To accomplish this objective, a 1D\nOperational Generative Adversarial Network (Op-GAN) first characterizes the\ntransition between normal and fault vibration signals of (a) source machine(s)\nunder various conditions, sensor parameters, and fault types. Then for a target\nmachine, the potential faulty signals can be generated, and over its actual\nhealthy and synthesized faulty signals, a compact, and lightweight 1D Self-ONN\nfault detector can then be trained to detect the real faulty condition in real\ntime whenever it occurs. To validate the proposed approach, a new benchmark\ndataset is created using two different motors working under different\nconditions and sensor locations. Experimental results demonstrate that this\nnovel approach can accurately detect any bearing fault achieving an average\nrecall rate of around 89% and 95% on two target machines regardless of its\ntype, severity, and location.",
          "arxiv_id": "2212.06154v1"
        },
        {
          "title": "An Explainable Artificial Intelligence Approach for Unsupervised Fault Detection and Diagnosis in Rotating Machinery",
          "year": "2021-02",
          "abstract": "The monitoring of rotating machinery is an essential task in today's\nproduction processes. Currently, several machine learning and deep\nlearning-based modules have achieved excellent results in fault detection and\ndiagnosis. Nevertheless, to further increase user adoption and diffusion of\nsuch technologies, users and human experts must be provided with explanations\nand insights by the modules. Another issue is related, in most cases, with the\nunavailability of labeled historical data that makes the use of supervised\nmodels unfeasible. Therefore, a new approach for fault detection and diagnosis\nin rotating machinery is here proposed. The methodology consists of three\nparts: feature extraction, fault detection and fault diagnosis. In the first\npart, the vibration features in the time and frequency domains are extracted.\nSecondly, in the fault detection, the presence of fault is verified in an\nunsupervised manner based on anomaly detection algorithms. The modularity of\nthe methodology allows different algorithms to be implemented. Finally, in\nfault diagnosis, Shapley Additive Explanations (SHAP), a technique to interpret\nblack-box models, is used. Through the feature importance ranking obtained by\nthe model explainability, the fault diagnosis is performed. Two tools for\ndiagnosis are proposed, namely: unsupervised classification and root cause\nanalysis. The effectiveness of the proposed approach is shown on three datasets\ncontaining different mechanical faults in rotating machinery. The study also\npresents a comparison between models used in machine learning explainability:\nSHAP and Local Depth-based Feature Importance for the Isolation Forest (Local-\nDIFFI). Lastly, an analysis of several state-of-art anomaly detection\nalgorithms in rotating machinery is included.",
          "arxiv_id": "2102.11848v1"
        },
        {
          "title": "Synthesizing Rolling Bearing Fault Samples in New Conditions: A framework based on a modified CGAN",
          "year": "2022-06",
          "abstract": "Bearings are one of the vital components of rotating machines that are prone\nto unexpected faults. Therefore, bearing fault diagnosis and condition\nmonitoring is essential for reducing operational costs and downtime in numerous\nindustries. In various production conditions, bearings can be operated under a\nrange of loads and speeds, which causes different vibration patterns associated\nwith each fault type. Normal data is ample as systems usually work in desired\nconditions. On the other hand, fault data is rare, and in many conditions,\nthere is no data recorded for the fault classes. Accessing fault data is\ncrucial for developing data-driven fault diagnosis tools that can improve both\nthe performance and safety of operations. To this end, a novel algorithm based\non Conditional Generative Adversarial Networks (CGANs) is introduced. Trained\non the normal and fault data on any actual fault conditions, this algorithm\ngenerates fault data from normal data of target conditions. The proposed method\nis validated on a real-world bearing dataset, and fault data are generated for\ndifferent conditions. Several state-of-the-art classifiers and visualization\nmodels are implemented to evaluate the quality of the synthesized data. The\nresults demonstrate the efficacy of the proposed algorithm.",
          "arxiv_id": "2206.12076v3"
        }
      ],
      "51": [
        {
          "title": "EMO: Episodic Memory Optimization for Few-Shot Meta-Learning",
          "year": "2023-06",
          "abstract": "Few-shot meta-learning presents a challenge for gradient descent optimization\ndue to the limited number of training samples per task. To address this issue,\nwe propose an episodic memory optimization for meta-learning, we call EMO,\nwhich is inspired by the human ability to recall past learning experiences from\nthe brain's memory. EMO retains the gradient history of past experienced tasks\nin external memory, enabling few-shot learning in a memory-augmented way. By\nlearning to retain and recall the learning process of past training tasks, EMO\nnudges parameter updates in the right direction, even when the gradients\nprovided by a limited number of examples are uninformative. We prove\ntheoretically that our algorithm converges for smooth, strongly convex\nobjectives. EMO is generic, flexible, and model-agnostic, making it a simple\nplug-and-play optimizer that can be seamlessly embedded into existing\noptimization-based few-shot meta-learning approaches. Empirical results show\nthat EMO scales well with most few-shot classification benchmarks and improves\nthe performance of optimization-based meta-learning methods, resulting in\naccelerated convergence.",
          "arxiv_id": "2306.05189v3"
        },
        {
          "title": "Gradient-Based Meta-Learning Using Uncertainty to Weigh Loss for Few-Shot Learning",
          "year": "2022-08",
          "abstract": "Model-Agnostic Meta-Learning (MAML) is one of the most successful\nmeta-learning techniques for few-shot learning. It uses gradient descent to\nlearn commonalities between various tasks, enabling the model to learn the\nmeta-initialization of its own parameters to quickly adapt to new tasks using a\nsmall amount of labeled training data. A key challenge to few-shot learning is\ntask uncertainty. Although a strong prior can be obtained from meta-learning\nwith a large number of tasks, a precision model of the new task cannot be\nguaranteed because the volume of the training dataset is normally too small. In\nthis study, first,in the process of choosing initialization parameters, the new\nmethod is proposed for task-specific learner adaptively learn to select\ninitialization parameters that minimize the loss of new tasks. Then, we propose\ntwo improved methods for the meta-loss part: Method 1 generates weights by\ncomparing meta-loss differences to improve the accuracy when there are few\nclasses, and Method 2 introduces the homoscedastic uncertainty of each task to\nweigh multiple losses based on the original gradient descent,as a way to\nenhance the generalization ability to novel classes while ensuring accuracy\nimprovement. Compared with previous gradient-based meta-learning methods, our\nmodel achieves better performance in regression tasks and few-shot\nclassification and improves the robustness of the model to the learning rate\nand query sets in the meta-test set.",
          "arxiv_id": "2208.08135v1"
        },
        {
          "title": "Few-Shot Learning with a Strong Teacher",
          "year": "2021-07",
          "abstract": "Few-shot learning (FSL) aims to generate a classifier using limited labeled\nexamples. Many existing works take the meta-learning approach, constructing a\nfew-shot learner that can learn from few-shot examples to generate a\nclassifier. Typically, the few-shot learner is constructed or meta-trained by\nsampling multiple few-shot tasks in turn and optimizing the few-shot learner's\nperformance in generating classifiers for those tasks. The performance is\nmeasured by how well the resulting classifiers classify the test (i.e., query)\nexamples of those tasks. In this paper, we point out two potential weaknesses\nof this approach. First, the sampled query examples may not provide sufficient\nsupervision for meta-training the few-shot learner. Second, the effectiveness\nof meta-learning diminishes sharply with the increasing number of shots. To\nresolve these issues, we propose a novel meta-training objective for the\nfew-shot learner, which is to encourage the few-shot learner to generate\nclassifiers that perform like strong classifiers. Concretely, we associate each\nsampled few-shot task with a strong classifier, which is trained with ample\nlabeled examples. The strong classifiers can be seen as the target classifiers\nthat we hope the few-shot learner to generate given few-shot examples, and we\nuse the strong classifiers to supervise the few-shot learner. We present an\nefficient way to construct the strong classifier, making our proposed objective\nan easily plug-and-play term to existing meta-learning based FSL methods. We\nvalidate our approach, LastShot, in combinations with many representative\nmeta-learning methods. On several benchmark datasets, our approach leads to a\nnotable improvement across a variety of tasks. More importantly, with our\napproach, meta-learning based FSL methods can outperform non-meta-learning\nbased methods at different numbers of shots.",
          "arxiv_id": "2107.00197v2"
        }
      ],
      "52": [
        {
          "title": "Responsible Artificial Intelligence Systems: A Roadmap to Society's Trust through Trustworthy AI, Auditability, Accountability, and Governance",
          "year": "2025-02",
          "abstract": "Artificial intelligence (AI) has matured as a technology, necessitating the\ndevelopment of responsibility frameworks that are fair, inclusive, trustworthy,\nsafe and secure, transparent, and accountable. By establishing such frameworks,\nwe can harness the full potential of AI while mitigating its risks,\nparticularly in high-risk scenarios. This requires the design of responsible AI\nsystems based on trustworthy AI technologies and ethical principles, with the\naim of ensuring auditability and accountability throughout their design,\ndevelopment, and deployment, adhering to domain-specific regulations and\nstandards.\n  This paper explores the concept of a responsible AI system from a holistic\nperspective, which encompasses four key dimensions: 1) regulatory context; 2)\ntrustworthy AI technology along with standardization and assessments; 3)\nauditability and accountability; and 4) AI governance. The aim of this paper is\ndouble. First, we analyze and understand these four dimensions and their\ninterconnections in the form of an analysis and overview. Second, the final\ngoal of the paper is to propose a roadmap in the design of responsible AI\nsystems, ensuring that they can gain society's trust. To achieve this\ntrustworthiness, this paper also fosters interdisciplinary discussions on the\nethical, legal, social, economic, and cultural aspects of AI from a global\ngovernance perspective. Last but not least, we also reflect on the current\nstate and those aspects that need to be developed in the near future, as ten\nlessons learned.",
          "arxiv_id": "2503.04739v1"
        },
        {
          "title": "Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation",
          "year": "2023-05",
          "abstract": "Trustworthy Artificial Intelligence (AI) is based on seven technical\nrequirements sustained over three main pillars that should be met throughout\nthe system's entire life cycle: it should be (1) lawful, (2) ethical, and (3)\nrobust, both from a technical and a social perspective. However, attaining\ntruly trustworthy AI concerns a wider vision that comprises the trustworthiness\nof all processes and actors that are part of the system's life cycle, and\nconsiders previous aspects from different lenses. A more holistic vision\ncontemplates four essential axes: the global principles for ethical use and\ndevelopment of AI-based systems, a philosophical take on AI ethics, a\nrisk-based approach to AI regulation, and the mentioned pillars and\nrequirements. The seven requirements (human agency and oversight; robustness\nand safety; privacy and data governance; transparency; diversity,\nnon-discrimination and fairness; societal and environmental wellbeing; and\naccountability) are analyzed from a triple perspective: What each requirement\nfor trustworthy AI is, Why it is needed, and How each requirement can be\nimplemented in practice. On the other hand, a practical approach to implement\ntrustworthy AI systems allows defining the concept of responsibility of\nAI-based systems facing the law, through a given auditing process. Therefore, a\nresponsible AI system is the resulting notion we introduce in this work, and a\nconcept of utmost necessity that can be realized through auditing processes,\nsubject to the challenges posed by the use of regulatory sandboxes. Our\nmultidisciplinary vision of trustworthy AI culminates in a debate on the\ndiverging views published lately about the future of AI. Our reflections in\nthis matter conclude that regulation is a key for reaching a consensus among\nthese views, and that trustworthy and responsible AI systems will be crucial\nfor the present and future of our society.",
          "arxiv_id": "2305.02231v2"
        },
        {
          "title": "Trustworthy AI: From Principles to Practices",
          "year": "2021-10",
          "abstract": "The rapid development of Artificial Intelligence (AI) technology has enabled\nthe deployment of various systems based on it. However, many current AI systems\nare found vulnerable to imperceptible attacks, biased against underrepresented\ngroups, lacking in user privacy protection. These shortcomings degrade user\nexperience and erode people's trust in all AI systems. In this review, we\nprovide AI practitioners with a comprehensive guide for building trustworthy AI\nsystems. We first introduce the theoretical framework of important aspects of\nAI trustworthiness, including robustness, generalization, explainability,\ntransparency, reproducibility, fairness, privacy preservation, and\naccountability. To unify currently available but fragmented approaches toward\ntrustworthy AI, we organize them in a systematic approach that considers the\nentire lifecycle of AI systems, ranging from data acquisition to model\ndevelopment, to system development and deployment, finally to continuous\nmonitoring and governance. In this framework, we offer concrete action items\nfor practitioners and societal stakeholders (e.g., researchers, engineers, and\nregulators) to improve AI trustworthiness. Finally, we identify key\nopportunities and challenges for the future development of trustworthy AI\nsystems, where we identify the need for a paradigm shift toward comprehensively\ntrustworthy AI systems.",
          "arxiv_id": "2110.01167v2"
        }
      ],
      "53": [
        {
          "title": "We Need to Rethink Benchmarking in Anomaly Detection",
          "year": "2025-07",
          "abstract": "Despite the continuous proposal of new anomaly detection algorithms and\nextensive benchmarking efforts, progress seems to stagnate, with only minor\nperformance differences between established baselines and new algorithms. In\nthis position paper, we argue that this stagnation is due to limitations in how\nwe evaluate anomaly detection algorithms. Current benchmarking does not, for\nexample, sufficiently reflect the diversity of anomalies in applications\nranging from predictive maintenance to scientific discovery. Consequently, we\nneed to rethink benchmarking in anomaly detection. In our opinion, anomaly\ndetection should be studied using scenarios that capture the relevant\ncharacteristics of different applications. We identify three key areas for\nimprovement: First, we need to identify anomaly detection scenarios based on a\ncommon taxonomy. Second, anomaly detection pipelines should be analyzed\nend-to-end and by component. Third, evaluating anomaly detection algorithms\nshould be meaningful regarding the scenario's objectives.",
          "arxiv_id": "2507.15584v1"
        },
        {
          "title": "Understanding the Effect of Bias in Deep Anomaly Detection",
          "year": "2021-05",
          "abstract": "Anomaly detection presents a unique challenge in machine learning, due to the\nscarcity of labeled anomaly data. Recent work attempts to mitigate such\nproblems by augmenting training of deep anomaly detection models with\nadditional labeled anomaly samples. However, the labeled data often does not\nalign with the target distribution and introduces harmful bias to the trained\nmodel. In this paper, we aim to understand the effect of a biased anomaly set\non anomaly detection. Concretely, we view anomaly detection as a supervised\nlearning task where the objective is to optimize the recall at a given false\npositive rate. We formally study the relative scoring bias of an anomaly\ndetector, defined as the difference in performance with respect to a baseline\nanomaly detector. We establish the first finite sample rates for estimating the\nrelative scoring bias for deep anomaly detection, and empirically validate our\ntheoretical results on both synthetic and real-world datasets. We also provide\nan extensive empirical study on how a biased training anomaly set affects the\nanomaly score function and therefore the detection performance on different\nanomaly classes. Our study demonstrates scenarios in which the biased anomaly\nset can be useful or problematic, and provides a solid benchmark for future\nresearch.",
          "arxiv_id": "2105.07346v1"
        },
        {
          "title": "AGAD: Adversarial Generative Anomaly Detection",
          "year": "2023-04",
          "abstract": "Anomaly detection suffered from the lack of anomalies due to the diversity of\nabnormalities and the difficulties of obtaining large-scale anomaly data.\nSemi-supervised anomaly detection methods are often used to solely leverage\nnormal data to detect abnormalities that deviated from the learnt normality\ndistributions. Meanwhile, given the fact that limited anomaly data can be\nobtained with a minor cost in practice, some researches also investigated\nanomaly detection methods under supervised scenarios with limited anomaly data.\nIn order to address the lack of abnormal data for robust anomaly detection, we\npropose Adversarial Generative Anomaly Detection (AGAD), a self-contrast-based\nanomaly detection paradigm that learns to detect anomalies by generating\n\\textit{contextual adversarial information} from the massive normal examples.\nEssentially, our method generates pseudo-anomaly data for both supervised and\nsemi-supervised anomaly detection scenarios. Extensive experiments are carried\nout on multiple benchmark datasets and real-world datasets, the results show\nsignificant improvement in both supervised and semi-supervised scenarios.\nImportantly, our approach is data-efficient that can boost up the detection\naccuracy with no more than 5% anomalous training data.",
          "arxiv_id": "2304.04211v1"
        }
      ],
      "54": [
        {
          "title": "Fast Rates for Bandit PAC Multiclass Classification",
          "year": "2024-06",
          "abstract": "We study multiclass PAC learning with bandit feedback, where inputs are\nclassified into one of $K$ possible labels and feedback is limited to whether\nor not the predicted labels are correct. Our main contribution is in designing\na novel learning algorithm for the agnostic $(\\varepsilon,\\delta)$-PAC version\nof the problem, with sample complexity of $O\\big( (\\operatorname{poly}(K) + 1 /\n\\varepsilon^2) \\log (|H| / \\delta) \\big)$ for any finite hypothesis class $H$.\nIn terms of the leading dependence on $\\varepsilon$, this improves upon\nexisting bounds for the problem, that are of the form $O(K/\\varepsilon^2)$. We\nalso provide an extension of this result to general classes and establish\nsimilar sample complexity bounds in which $\\log |H|$ is replaced by the\nNatarajan dimension. This matches the optimal rate in the full-information\nversion of the problem and resolves an open question studied by Daniely,\nSabato, Ben-David, and Shalev-Shwartz (2011) who demonstrated that the\nmultiplicative price of bandit feedback in realizable PAC learning is\n$\\Theta(K)$. We complement this by revealing a stark contrast with the agnostic\ncase, where the price of bandit feedback is only $O(1)$ as $\\varepsilon \\to 0$.\nOur algorithm utilizes a stochastic optimization technique to minimize a\nlog-barrier potential based on Frank-Wolfe updates for computing a low-variance\nexploration distribution over the hypotheses, and is made computationally\nefficient provided access to an ERM oracle over $H$.",
          "arxiv_id": "2406.12406v1"
        },
        {
          "title": "Bandit Multiclass List Classification",
          "year": "2025-02",
          "abstract": "We study the problem of multiclass list classification with (semi-)bandit\nfeedback, where input examples are mapped into subsets of size $m$ of a\ncollection of $K$ possible labels. In each round of the interaction, the\nlearner observes feedback consisting of the predicted labels which lie in some\nunderlying set of ground truth labels associated with the given example. Our\nmain result is for the $(\\varepsilon,\\delta)$-PAC variant of the problem for\nwhich we design an algorithm that returns an $\\varepsilon$-optimal hypothesis\nwith high probability using a sample complexity of $\\widetilde{O} \\big(\n(\\mathrm{poly}(K/m) + sm / \\varepsilon^2) \\log (|H|/\\delta) \\big)$ where $H$ is\nthe underlying (finite) hypothesis class and $s$ is an upper bound on the\nnumber of true labels for a given example. This bound improves upon known\nbounds for combinatorial semi-bandits whenever $s \\ll K$. Moreover, in the\nregime where $s = O(1)$ the leading terms in our bound match the corresponding\nfull-information rates, implying that bandit feedback essentially comes at no\ncost. Our PAC learning algorithm is also computationally efficient given access\nto an ERM oracle for $H$. In the special case of single-label classification\ncorresponding to $s=m=1$, we prove a sample complexity bound of $O \\big((K^7 +\n1/\\varepsilon^2)\\log (|H|/\\delta)\\big)$ which improves upon recent results in\nthis scenario (Erez et al. '24). Additionally, we consider the regret\nminimization setting where data can be generated adversarially, and establish a\nregret bound of $\\widetilde O(|H| + \\sqrt{smT \\log |H|})$. Our results\ngeneralize and extend prior work in the simpler single-label setting (Erez et\nal. '24), and apply more generally to contextual combinatorial semi-bandit\nproblems with $s$-sparse rewards.",
          "arxiv_id": "2502.09257v2"
        },
        {
          "title": "Proper Learning, Helly Number, and an Optimal SVM Bound",
          "year": "2020-05",
          "abstract": "The classical PAC sample complexity bounds are stated for any Empirical Risk\nMinimizer (ERM) and contain an extra logarithmic factor $\\log(1/{\\epsilon})$\nwhich is known to be necessary for ERM in general. It has been recently shown\nby Hanneke (2016) that the optimal sample complexity of PAC learning for any VC\nclass C is achieved by a particular improper learning algorithm, which outputs\na specific majority-vote of hypotheses in C. This leaves the question of when\nthis bound can be achieved by proper learning algorithms, which are restricted\nto always output a hypothesis from C.\n  In this paper we aim to characterize the classes for which the optimal sample\ncomplexity can be achieved by a proper learning algorithm. We identify that\nthese classes can be characterized by the dual Helly number, which is a\ncombinatorial parameter that arises in discrete geometry and abstract\nconvexity. In particular, under general conditions on C, we show that the dual\nHelly number is bounded if and only if there is a proper learner that obtains\nthe optimal joint dependence on $\\epsilon$ and $\\delta$.\n  As further implications of our techniques we resolve a long-standing open\nproblem posed by Vapnik and Chervonenkis (1974) on the performance of the\nSupport Vector Machine by proving that the sample complexity of SVM in the\nrealizable case is $\\Theta((n/{\\epsilon})+(1/{\\epsilon})\\log(1/{\\delta}))$,\nwhere $n$ is the dimension. This gives the first optimal PAC bound for\nHalfspaces achieved by a proper learning algorithm, and moreover is\ncomputationally efficient.",
          "arxiv_id": "2005.11818v1"
        }
      ],
      "55": [
        {
          "title": "Examining Deep Learning Models with Multiple Data Sources for COVID-19 Forecasting",
          "year": "2020-10",
          "abstract": "The COVID-19 pandemic represents the most significant public health disaster\nsince the 1918 influenza pandemic. During pandemics such as COVID-19, timely\nand reliable spatio-temporal forecasting of epidemic dynamics is crucial. Deep\nlearning-based time series models for forecasting have recently gained\npopularity and have been successfully used for epidemic forecasting. Here we\nfocus on the design and analysis of deep learning-based models for COVID-19\nforecasting. We implement multiple recurrent neural network-based deep learning\nmodels and combine them using the stacking ensemble technique. In order to\nincorporate the effects of multiple factors in COVID-19 spread, we consider\nmultiple sources such as COVID-19 confirmed and death case count data and\ntesting data for better predictions. To overcome the sparsity of training data\nand to address the dynamic correlation of the disease, we propose\nclustering-based training for high-resolution forecasting. The methods help us\nto identify the similar trends of certain groups of regions due to various\nspatio-temporal effects. We examine the proposed method for forecasting weekly\nCOVID-19 new confirmed cases at county-, state-, and country-level. A\ncomprehensive comparison between different time series models in COVID-19\ncontext is conducted and analyzed. The results show that simple deep learning\nmodels can achieve comparable or better performance when compared with more\ncomplicated models. We are currently integrating our methods as a part of our\nweekly forecasts that we provide state and federal authorities.",
          "arxiv_id": "2010.14491v2"
        },
        {
          "title": "Modeling time evolving COVID-19 uncertainties with density dependent asymptomatic infections and social reinforcement",
          "year": "2021-08",
          "abstract": "The COVID-19 pandemic has posed significant challenges in modeling its\ncomplex epidemic transmissions, infection and contagion, which are very\ndifferent from known epidemics. The challenges in quantifying COVID-19\ncomplexities include effectively modeling its process and data uncertainties.\nThe uncertainties are embedded in implicit and high-proportional undocumented\ninfections, asymptomatic contagion, social reinforcement of infections, and\nvarious quality issues in the reported data. These uncertainties become even\nmore apparent in the first two months of the COVID-19 pandemic, when the\nrelevant knowledge, case reporting and testing were all limited. Here we\nintroduce a novel hybrid approach Susceptible-Undocumented infected-Documented\ninfected-Recovered (SUDR) model. First, SUDR (1) characterizes and\ndistinguishes Undocumented (U) and Documented (D) infections commonly seen\nduring COVID-19 incubation periods and asymptomatic infections. Second, SUDR\ncharacterizes the probabilistic density of infections by capturing exogenous\nprocesses. Lastly, SUDR approximates the density likelihood of COVID-19\nprevalence over time by incorporating Bayesian inference into SUDR. Different\nfrom existing COVID-19 models, SUDR characterizes the undocumented infections\nduring unknown transmission processes. To capture the uncertainties of temporal\ntransmission and social reinforcement during COVID-19 contagion, the\ntransmission rate is modeled by a time-varying density function of undocumented\ninfectious cases. By sampling from the mean-field posterior distribution with\nreasonable priors, SUDR handles the randomness, noise and sparsity of COVID-19\nobservations widely seen in the public COVID-19 case data. The results\ndemonstrate a deeper quantitative understanding of the above uncertainties, in\ncomparison with classic SIR, time-dependent SIR, and probabilistic SIR models.",
          "arxiv_id": "2108.10029v2"
        },
        {
          "title": "COVID-19 Hospitalizations Forecasts Using Internet Search Data",
          "year": "2022-02",
          "abstract": "As the COVID-19 spread over the globe and new variants of COVID-19 keep\noccurring, reliable real-time forecasts of COVID-19 hospitalizations are\ncritical for public health decision on medical resources allocations such as\nICU beds, ventilators, and personnel to prepare for the surge of COVID-19\npandemics. Inspired by the strong association between public search behavior\nand hospitalization admission, we extended previously-proposed influenza\ntracking model, ARGO (AutoRegression with GOogle search data), to predict\nfuture 2-week national and state-level COVID-19 new hospital admissions.\nLeveraging the COVID-19 related time series information and Google search data,\nour method is able to robustly capture new COVID-19 variants' surges, and\nself-correct at both national and state level. Based on our retrospective\nout-of-sample evaluation over 12-month comparison period, our method achieves\non average 15\\% error reduction over the best alternative models collected from\nCOVID-19 forecast hub. Overall, we showed that our method is flexible,\nself-correcting, robust, accurate, and interpretable, making it a potentially\npowerful tool to assist health-care officials and decision making for the\ncurrent and future infectious disease outbreak.",
          "arxiv_id": "2202.03869v1"
        }
      ],
      "56": [
        {
          "title": "Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm",
          "year": "2025-09",
          "abstract": "Decision trees are a ubiquitous model for classification and regression tasks\ndue to their interpretability and efficiency. However, solving the optimal\ndecision tree (ODT) problem remains a challenging combinatorial optimization\ntask. Even for the simplest splitting rules--axis-parallel hyperplanes--it is\nNP-hard to optimize. In Part I of this series, we rigorously defined the proper\ndecision tree model through four axioms and, based on these, introduced four\nformal definitions of the ODT problem. From these definitions, we derived four\ngeneric algorithms capable of solving ODT problems for arbitrary decision trees\nsatisfying the axioms. We also analyzed the combinatorial geometric properties\nof hypersurfaces, showing that decision trees defined by polynomial\nhypersurface splitting rules satisfy the proper axioms that we proposed.\n  In this second paper (Part II) of this two-part series, building on the\nalgorithmic and geometric foundations established in Part I, we introduce the\nfirst hypersurface decision tree (HODT) algorithm. To the best of our\nknowledge, existing optimal decision tree methods are, to date, limited to\nhyperplane splitting rules--a special case of hypersurfaces--and rely on\ngeneral-purpose solvers. In contrast, our HODT algorithm addresses the general\nhypersurface decision tree model without requiring external solvers.\n  Using synthetic datasets generated from ground-truth hyperplane decision\ntrees, we vary tree size, data size, dimensionality, and label and feature\nnoise. Results showing that our algorithm recovers the ground truth more\naccurately than axis-parallel trees and exhibits greater robustness to noise.\nWe also analyzed generalization performance across 30 real-world datasets,\nshowing that HODT can achieve up to 30% higher accuracy than the\nstate-of-the-art optimal axis-parallel decision tree algorithm when tree\ncomplexity is properly controlled.",
          "arxiv_id": "2509.12057v1"
        },
        {
          "title": "An Algorithmic Framework for Constructing Multiple Decision Trees by Evaluating Their Combination Performance Throughout the Construction Process",
          "year": "2024-02",
          "abstract": "Predictions using a combination of decision trees are known to be effective\nin machine learning. Typical ideas for constructing a combination of decision\ntrees for prediction are bagging and boosting. Bagging independently constructs\ndecision trees without evaluating their combination performance and averages\nthem afterward. Boosting constructs decision trees sequentially, only\nevaluating a combination performance of a new decision tree and the fixed past\ndecision trees at each step. Therefore, neither method directly constructs nor\nevaluates a combination of decision trees for the final prediction. When the\nfinal prediction is based on a combination of decision trees, it is natural to\nevaluate the appropriateness of the combination when constructing them. In this\nstudy, we propose a new algorithmic framework that constructs decision trees\nsimultaneously and evaluates their combination performance throughout the\nconstruction process. Our framework repeats two procedures. In the first\nprocedure, we construct new candidates of combinations of decision trees to\nfind a proper combination of decision trees. In the second procedure, we\nevaluate each combination performance of decision trees under some criteria and\nselect a better combination. To confirm the performance of the proposed\nframework, we perform experiments on synthetic and benchmark data.",
          "arxiv_id": "2402.06452v1"
        },
        {
          "title": "Provably optimal decision trees with arbitrary splitting rules in polynomial time",
          "year": "2025-03",
          "abstract": "In this paper, we introduce a generic data structure called decision trees,\nwhich integrates several well-known data structures, including binary search\ntrees, K-D trees, binary space partition trees, and decision tree models from\nmachine learning. We provide the first axiomatic definition of decision trees.\nThese axioms establish a firm mathematical foundation for studying decision\ntree problems. We refer to decision trees that satisfy the axioms as proper\ndecision trees. We prove that only proper decision trees can be uniquely\ncharacterized as K-permutations. Since permutations are among the most\nwell-studied combinatorial structures, this characterization provides a\nfundamental basis for analyzing the combinatorial and algorithmic properties of\ndecision trees. As a result of this advancement, we develop the first provably\ncorrect polynomial-time algorithm for solving the optimal decision tree\nproblem. Our algorithm is derived using a formal program derivation framework,\nwhich enables step-by-step equational reasoning to construct side-effect-free\nprograms with guaranteed correctness. The derived algorithm is correct by\nconstruction and is applicable to decision tree problems defined by any\nsplitting rules that adhere to the axioms and any objective functions that can\nbe specified in a given form. Examples include the decision tree problems where\nsplitting rules are defined by axis-parallel hyperplanes, arbitrary\nhyperplanes, and hypersurfaces. By extending the axioms, we can potentially\naddress a broader range of problems. Moreover, the derived algorithm can easily\naccommodate various constraints, such as tree depth and leaf size, and is\namenable to acceleration techniques such as thinning method.",
          "arxiv_id": "2503.01455v1"
        }
      ],
      "57": [
        {
          "title": "Understanding Time Series Anomaly State Detection through One-Class Classification",
          "year": "2024-02",
          "abstract": "For a long time, research on time series anomaly detection has mainly focused\non finding outliers within a given time series. Admittedly, this is consistent\nwith some practical problems, but in other practical application scenarios,\npeople are concerned about: assuming a standard time series is given, how to\njudge whether another test time series deviates from the standard time series,\nwhich is more similar to the problem discussed in one-class classification\n(OCC). Therefore, in this article, we try to re-understand and define the time\nseries anomaly detection problem through OCC, which we call 'time series\nanomaly state detection problem'. We first use stochastic processes and\nhypothesis testing to strictly define the 'time series anomaly state detection\nproblem', and its corresponding anomalies. Then, we use the time series\nclassification dataset to construct an artificial dataset corresponding to the\nproblem. We compile 38 anomaly detection algorithms and correct some of the\nalgorithms to adapt to handle this problem. Finally, through a large number of\nexperiments, we fairly compare the actual performance of various time series\nanomaly detection algorithms, providing insights and directions for future\nresearch by researchers.",
          "arxiv_id": "2402.02007v1"
        },
        {
          "title": "Dive into Time-Series Anomaly Detection: A Decade Review",
          "year": "2024-12",
          "abstract": "Recent advances in data collection technology, accompanied by the ever-rising\nvolume and velocity of streaming data, underscore the vital need for time\nseries analytics. In this regard, time-series anomaly detection has been an\nimportant activity, entailing various applications in fields such as cyber\nsecurity, financial markets, law enforcement, and health care. While\ntraditional literature on anomaly detection is centered on statistical\nmeasures, the increasing number of machine learning algorithms in recent years\ncall for a structured, general characterization of the research methods for\ntime-series anomaly detection. This survey groups and summarizes anomaly\ndetection existing solutions under a process-centric taxonomy in the time\nseries context. In addition to giving an original categorization of anomaly\ndetection methods, we also perform a meta-analysis of the literature and\noutline general trends in time-series anomaly detection research.",
          "arxiv_id": "2412.20512v1"
        },
        {
          "title": "Ymir: A Supervised Ensemble Framework for Multivariate Time Series Anomaly Detection",
          "year": "2021-12",
          "abstract": "We proposed a multivariate time series anomaly detection frame-work Ymir,\nwhich leverages ensemble learning and supervisedlearning technology to\nefficiently learn and adapt to anomaliesin real-world system applications. Ymir\nintegrates several currentlywidely used unsupervised anomaly detection models\nthrough anensemble learning method, and thus can provide robust frontalanomaly\ndetection results in unsupervised scenarios. In a super-vised setting, domain\nexperts and system users discuss and providelabels (anomalous or not) for the\ntraining data, which reflects theiranomaly detection criteria for the specific\nsystem. Ymir leveragesthe aforementioned unsupervised methods to extract rich\nand usefulfeature representations from the raw multivariate time series\ndata,then combines the features and labels with a supervised classifier todo\nanomaly detection. We evaluated Ymir on internal multivariatetime series\ndatasets from large monitoring systems and achievedgood anomaly detection\nperformance.",
          "arxiv_id": "2112.04704v1"
        }
      ],
      "58": [
        {
          "title": "Progress towards an improved particle flow algorithm at CMS with machine learning",
          "year": "2023-03",
          "abstract": "The particle-flow (PF) algorithm, which infers particles based on tracks and\ncalorimeter clusters, is of central importance to event reconstruction in the\nCMS experiment at the CERN LHC, and has been a focus of development in light of\nplanned Phase-2 running conditions with an increased pileup and detector\ngranularity. In recent years, the machine learned particle-flow (MLPF)\nalgorithm, a graph neural network that performs PF reconstruction, has been\nexplored in CMS, with the possible advantages of directly optimizing for the\nphysical quantities of interest, being highly reconfigurable to new conditions,\nand being a natural fit for deployment to heterogeneous accelerators. We\ndiscuss progress in CMS towards an improved implementation of the MLPF\nreconstruction, now optimized using generator/simulation-level particle\ninformation as the target for the first time. This paves the way to potentially\nimproving the detector response in terms of physical quantities of interest. We\ndescribe the simulation-based training target, progress and studies on\nevent-based loss terms, details on the model hyperparameter tuning, as well as\nphysics validation with respect to the current PF algorithm in terms of\nhigh-level physical quantities such as the jet and missing transverse momentum\nresolutions. We find that the MLPF algorithm, trained on a generator/simulator\nlevel particle information for the first time, results in broadly compatible\nparticle and jet reconstruction performance with the baseline PF, setting the\nstage for improving the physics performance by additional training statistics\nand model tuning.",
          "arxiv_id": "2303.17657v1"
        },
        {
          "title": "Fine-tuning machine-learned particle-flow reconstruction for new detector geometries in future colliders",
          "year": "2025-02",
          "abstract": "We demonstrate transfer learning capabilities in a machine-learned algorithm\ntrained for particle-flow reconstruction in high energy particle colliders.\nThis paper presents a cross-detector fine-tuning study, where we initially\npretrain the model on a large full simulation dataset from one detector design,\nand subsequently fine-tune the model on a sample with a different collider and\ndetector design. Specifically, we use the Compact Linear Collider detector\n(CLICdet) model for the initial training set and demonstrate successful\nknowledge transfer to the CLIC-like detector (CLD) proposed for the Future\nCircular Collider in electron-positron mode. We show that with an order of\nmagnitude less samples from the second dataset, we can achieve the same\nperformance as a costly training from scratch, across particle-level and\nevent-level performance metrics, including jet and missing transverse momentum\nresolution. Furthermore, we find that the fine-tuned model achieves comparable\nperformance to the traditional rule-based particle-flow approach on event-level\nmetrics after training on 100,000 CLD events, whereas a model trained from\nscratch requires at least 1 million CLD events to achieve similar\nreconstruction performance. To our knowledge, this represents the first\nfull-simulation cross-detector transfer learning study for particle-flow\nreconstruction. These findings offer valuable insights towards building large\nfoundation models that can be fine-tuned across different detector designs and\ngeometries, helping to accelerate the development cycle for new detectors and\nopening the door to rapid detector design and optimization using machine\nlearning.",
          "arxiv_id": "2503.00131v4"
        },
        {
          "title": "Graph Generative Models for Fast Detector Simulations in High Energy Physics",
          "year": "2021-04",
          "abstract": "Accurate and fast simulation of particle physics processes is crucial for the\nhigh-energy physics community. Simulating particle interactions with detectors\nis both time consuming and computationally expensive. With the proton-proton\ncollision energy of 13 TeV, the Large Hadron Collider is uniquely positioned to\ndetect and measure the rare phenomena that can shape our knowledge of new\ninteractions. The High-Luminosity Large Hadron Collider (HL-LHC) upgrade will\nput a significant strain on the computing infrastructure due to increased event\nrate and levels of pile-up. Simulation of high-energy physics collisions needs\nto be significantly faster without sacrificing the physics accuracy. Machine\nlearning approaches can offer faster solutions, while maintaining a high level\nof fidelity. We discuss a graph generative model that provides effective\nreconstruction of LHC events, paving the way for full detector level fast\nsimulation for HL-LHC.",
          "arxiv_id": "2104.01725v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:18:04Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}