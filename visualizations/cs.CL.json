{
  "topics": {
    "data": {
      "0": {
        "name": "0_speech_ASR_Speech_recognition",
        "keywords": [
          [
            "speech",
            0.04131453194459332
          ],
          [
            "ASR",
            0.024318239101485815
          ],
          [
            "Speech",
            0.018972543456157398
          ],
          [
            "recognition",
            0.014297315933295838
          ],
          [
            "audio",
            0.01328177842193607
          ],
          [
            "speech recognition",
            0.012520058974665104
          ],
          [
            "speaker",
            0.01193520623082011
          ],
          [
            "model",
            0.010624118321433607
          ],
          [
            "end",
            0.010420686064406763
          ],
          [
            "data",
            0.008658396644405195
          ]
        ],
        "count": 5300
      },
      "1": {
        "name": "1_medical_clinical_health_biomedical",
        "keywords": [
          [
            "medical",
            0.022868258131929164
          ],
          [
            "clinical",
            0.020310513175510223
          ],
          [
            "health",
            0.011212934456417299
          ],
          [
            "biomedical",
            0.009550146259898293
          ],
          [
            "patient",
            0.009533674753628166
          ],
          [
            "LLMs",
            0.008137792600749828
          ],
          [
            "data",
            0.007993502658150364
          ],
          [
            "models",
            0.007573764616035904
          ],
          [
            "healthcare",
            0.007547245124151129
          ],
          [
            "domain",
            0.0074265680197299665
          ]
        ],
        "count": 4588
      },
      "2": {
        "name": "2_image_visual_video_images",
        "keywords": [
          [
            "image",
            0.0251639099134759
          ],
          [
            "visual",
            0.022089667266282746
          ],
          [
            "video",
            0.01452533055404732
          ],
          [
            "images",
            0.012757422684416438
          ],
          [
            "multimodal",
            0.0126648828512208
          ],
          [
            "vision",
            0.012645964532871052
          ],
          [
            "text",
            0.01117801549497687
          ],
          [
            "modal",
            0.009504199618910995
          ],
          [
            "Vision",
            0.009256546535143368
          ],
          [
            "vision language",
            0.008954632405680748
          ]
        ],
        "count": 3709
      },
      "3": {
        "name": "3_attention_memory_training_quantization",
        "keywords": [
          [
            "attention",
            0.014508241821493016
          ],
          [
            "memory",
            0.012615655140345275
          ],
          [
            "training",
            0.010892491867598433
          ],
          [
            "quantization",
            0.010597717790190447
          ],
          [
            "inference",
            0.010201405299852149
          ],
          [
            "performance",
            0.009819952341728328
          ],
          [
            "model",
            0.00972219106496298
          ],
          [
            "pruning",
            0.009455305616604832
          ],
          [
            "tuning",
            0.009133349462875848
          ],
          [
            "efficient",
            0.009120469228468422
          ]
        ],
        "count": 2909
      },
      "4": {
        "name": "4_attacks_safety_adversarial_attack",
        "keywords": [
          [
            "attacks",
            0.021087666046009663
          ],
          [
            "safety",
            0.019443969399680863
          ],
          [
            "adversarial",
            0.018313723608029617
          ],
          [
            "attack",
            0.017086195890013443
          ],
          [
            "privacy",
            0.015820108449959708
          ],
          [
            "LLMs",
            0.012795028241309169
          ],
          [
            "data",
            0.008882634718418485
          ],
          [
            "models",
            0.00887601224538103
          ],
          [
            "LLM",
            0.008846163276181382
          ],
          [
            "unlearning",
            0.008437082106429013
          ]
        ],
        "count": 2786
      },
      "5": {
        "name": "5_reasoning_Reasoning_LLMs_mathematical",
        "keywords": [
          [
            "reasoning",
            0.0414281411256947
          ],
          [
            "Reasoning",
            0.01317991931211723
          ],
          [
            "LLMs",
            0.012688183143437292
          ],
          [
            "mathematical",
            0.012550751270213024
          ],
          [
            "problems",
            0.009866382102322435
          ],
          [
            "models",
            0.008982961742464511
          ],
          [
            "tasks",
            0.008389919933667552
          ],
          [
            "performance",
            0.008276592178534013
          ],
          [
            "step",
            0.008058192137088643
          ],
          [
            "reasoning tasks",
            0.007672613553197713
          ]
        ],
        "count": 2766
      },
      "6": {
        "name": "6_dialogue_Dialogue_response_user",
        "keywords": [
          [
            "dialogue",
            0.03693325343921711
          ],
          [
            "Dialogue",
            0.01549638774763201
          ],
          [
            "response",
            0.011593912273994002
          ],
          [
            "user",
            0.011041305107359864
          ],
          [
            "conversational",
            0.010750448268251243
          ],
          [
            "responses",
            0.010598540472902493
          ],
          [
            "systems",
            0.010168102295489876
          ],
          [
            "conversation",
            0.009711347904292645
          ],
          [
            "task",
            0.009443392495900201
          ],
          [
            "dialogues",
            0.009348440769760193
          ]
        ],
        "count": 2699
      },
      "7": {
        "name": "7_translation_Translation_machine translation_NMT",
        "keywords": [
          [
            "translation",
            0.053912393022890505
          ],
          [
            "Translation",
            0.02714456357757735
          ],
          [
            "machine translation",
            0.024121233751933452
          ],
          [
            "NMT",
            0.020849515447374723
          ],
          [
            "Machine",
            0.020746635262257607
          ],
          [
            "machine",
            0.020073256047462663
          ],
          [
            "MT",
            0.019089750293795324
          ],
          [
            "languages",
            0.013450627143765291
          ],
          [
            "English",
            0.013148910275799808
          ],
          [
            "translations",
            0.012792486387881233
          ]
        ],
        "count": 1669
      },
      "8": {
        "name": "8_bias_gender_biases_LLMs",
        "keywords": [
          [
            "bias",
            0.037491640312609244
          ],
          [
            "gender",
            0.03164694277727596
          ],
          [
            "biases",
            0.026040804432037993
          ],
          [
            "LLMs",
            0.014532411199316055
          ],
          [
            "fairness",
            0.013825122084479106
          ],
          [
            "Bias",
            0.012917573985728924
          ],
          [
            "social",
            0.01146230679375909
          ],
          [
            "gender bias",
            0.011338912602790047
          ],
          [
            "moral",
            0.011227435934929048
          ],
          [
            "AI",
            0.010385095705219886
          ]
        ],
        "count": 1451
      },
      "9": {
        "name": "9_agents_agent_planning_environments",
        "keywords": [
          [
            "agents",
            0.029528473702829544
          ],
          [
            "agent",
            0.027361372754759236
          ],
          [
            "planning",
            0.012679199418860896
          ],
          [
            "environments",
            0.012560035132432949
          ],
          [
            "LLM",
            0.011728388947411832
          ],
          [
            "tasks",
            0.011319910502121534
          ],
          [
            "navigation",
            0.009980798893397086
          ],
          [
            "language",
            0.009850854204195007
          ],
          [
            "environment",
            0.009821283648207349
          ],
          [
            "Agent",
            0.009536889823005578
          ]
        ],
        "count": 1423
      },
      "10": {
        "name": "10_parsing_syntactic_language_semantic",
        "keywords": [
          [
            "parsing",
            0.020878317930148688
          ],
          [
            "syntactic",
            0.018593978923623817
          ],
          [
            "language",
            0.013499201892682946
          ],
          [
            "semantic",
            0.012212244331723784
          ],
          [
            "brain",
            0.012057140470501437
          ],
          [
            "linguistic",
            0.01091576805800314
          ],
          [
            "dependency",
            0.010648150409443046
          ],
          [
            "word",
            0.010201831729762025
          ],
          [
            "neural",
            0.009781203585949114
          ],
          [
            "models",
            0.009756206520603529
          ]
        ],
        "count": 1389
      },
      "11": {
        "name": "11_summarization_summaries_Summarization_summary",
        "keywords": [
          [
            "summarization",
            0.052840840102397965
          ],
          [
            "summaries",
            0.030637684380461824
          ],
          [
            "Summarization",
            0.025802668201762087
          ],
          [
            "summary",
            0.022663254350406697
          ],
          [
            "abstractive",
            0.01666179262453296
          ],
          [
            "document",
            0.015192639987648155
          ],
          [
            "text summarization",
            0.009878033158856298
          ],
          [
            "Abstractive",
            0.009661558899468516
          ],
          [
            "text",
            0.009529909299855928
          ],
          [
            "news",
            0.009013936627099135
          ]
        ],
        "count": 1237
      },
      "12": {
        "name": "12_embeddings_word_semantic_sentence",
        "keywords": [
          [
            "embeddings",
            0.029528653962369412
          ],
          [
            "word",
            0.028843040389507957
          ],
          [
            "semantic",
            0.019029349747221747
          ],
          [
            "sentence",
            0.017845316223321455
          ],
          [
            "similarity",
            0.01574186110564403
          ],
          [
            "words",
            0.013312407137735726
          ],
          [
            "Word",
            0.01198283279168037
          ],
          [
            "word embeddings",
            0.011558924176054862
          ],
          [
            "sense",
            0.011530333860870398
          ],
          [
            "Embeddings",
            0.009834996411206259
          ]
        ],
        "count": 1137
      },
      "13": {
        "name": "13_hate_hate speech_speech_detection",
        "keywords": [
          [
            "hate",
            0.04718370491754819
          ],
          [
            "hate speech",
            0.039331448369895966
          ],
          [
            "speech",
            0.02699487291862998
          ],
          [
            "detection",
            0.022233400374093142
          ],
          [
            "Hate",
            0.01957035342847141
          ],
          [
            "social",
            0.017657134407885213
          ],
          [
            "content",
            0.016959298612744585
          ],
          [
            "offensive",
            0.016782799082061907
          ],
          [
            "speech detection",
            0.015828764090197484
          ],
          [
            "media",
            0.015519551514128637
          ]
        ],
        "count": 1056
      },
      "14": {
        "name": "14_code_Code_code generation_programming",
        "keywords": [
          [
            "code",
            0.06265533463442943
          ],
          [
            "Code",
            0.022210456206235896
          ],
          [
            "code generation",
            0.021467779189517755
          ],
          [
            "programming",
            0.015718624951459585
          ],
          [
            "generation",
            0.015080409742451964
          ],
          [
            "software",
            0.01275344038867355
          ],
          [
            "source",
            0.009047282951247551
          ],
          [
            "program",
            0.008311996673597773
          ],
          [
            "models",
            0.008140633259398924
          ],
          [
            "source code",
            0.00790977377614996
          ]
        ],
        "count": 1020
      },
      "15": {
        "name": "15_scientific_papers_citation_research",
        "keywords": [
          [
            "scientific",
            0.029569904867144677
          ],
          [
            "papers",
            0.02274011896555477
          ],
          [
            "citation",
            0.017584245032511162
          ],
          [
            "research",
            0.017484181917291633
          ],
          [
            "patent",
            0.013164951297513649
          ],
          [
            "review",
            0.010539383399046203
          ],
          [
            "Scientific",
            0.010413097515998736
          ],
          [
            "literature",
            0.010380798753515259
          ],
          [
            "academic",
            0.010033084649545774
          ],
          [
            "paper",
            0.009853418216068948
          ]
        ],
        "count": 942
      },
      "16": {
        "name": "16_sentiment_Sentiment_sentiment analysis_aspect",
        "keywords": [
          [
            "sentiment",
            0.054821738350549556
          ],
          [
            "Sentiment",
            0.03179810203780995
          ],
          [
            "sentiment analysis",
            0.027539783103009894
          ],
          [
            "aspect",
            0.023673271298519392
          ],
          [
            "analysis",
            0.022152224041028793
          ],
          [
            "Aspect",
            0.01984311330530004
          ],
          [
            "Analysis",
            0.0169885396182113
          ],
          [
            "reviews",
            0.015452006750146819
          ],
          [
            "opinion",
            0.012759351052568466
          ],
          [
            "classification",
            0.010060601886678084
          ]
        ],
        "count": 926
      },
      "17": {
        "name": "17_languages_lingual_multilingual_cross",
        "keywords": [
          [
            "languages",
            0.03266331393135955
          ],
          [
            "lingual",
            0.027168113573641326
          ],
          [
            "multilingual",
            0.021857632874582505
          ],
          [
            "cross",
            0.01933011674680236
          ],
          [
            "language",
            0.018156845602524608
          ],
          [
            "transfer",
            0.01617781425385592
          ],
          [
            "resource",
            0.0134633290300167
          ],
          [
            "models",
            0.011875390339490392
          ],
          [
            "lingual transfer",
            0.011861712599039698
          ],
          [
            "cross lingual transfer",
            0.011419218707830381
          ]
        ],
        "count": 823
      },
      "18": {
        "name": "18_generation_text_text generation_Generation",
        "keywords": [
          [
            "generation",
            0.03125600235056162
          ],
          [
            "text",
            0.02411780746347055
          ],
          [
            "text generation",
            0.02359148572749816
          ],
          [
            "Generation",
            0.015663440084052356
          ],
          [
            "NLG",
            0.014283784816271116
          ],
          [
            "evaluation",
            0.012371233565353652
          ],
          [
            "Text",
            0.011631291306783823
          ],
          [
            "metrics",
            0.011550483887216873
          ],
          [
            "human",
            0.009422169724537586
          ],
          [
            "language",
            0.009044874388781729
          ]
        ],
        "count": 812
      },
      "19": {
        "name": "19_preference_reward_RLHF_alignment",
        "keywords": [
          [
            "preference",
            0.037945113472598366
          ],
          [
            "reward",
            0.03402251167605839
          ],
          [
            "RLHF",
            0.026128163736616628
          ],
          [
            "alignment",
            0.025943749437133144
          ],
          [
            "preferences",
            0.025280264962758203
          ],
          [
            "DPO",
            0.024136393058838416
          ],
          [
            "Preference",
            0.020915348212763935
          ],
          [
            "human",
            0.01860841844699901
          ],
          [
            "human preferences",
            0.016533714028429333
          ],
          [
            "Optimization",
            0.015518856508496706
          ]
        ],
        "count": 721
      },
      "20": {
        "name": "20_NER_entity_Entity_Named",
        "keywords": [
          [
            "NER",
            0.06263124566557851
          ],
          [
            "entity",
            0.038760358423147276
          ],
          [
            "Entity",
            0.034726742724100826
          ],
          [
            "Named",
            0.0324179382686673
          ],
          [
            "Recognition",
            0.023085704789167102
          ],
          [
            "entities",
            0.02186106872448174
          ],
          [
            "entity recognition",
            0.015680712118711075
          ],
          [
            "recognition",
            0.012452642249966115
          ],
          [
            "domain",
            0.008257565942459593
          ],
          [
            "data",
            0.00811888070834447
          ]
        ],
        "count": 715
      },
      "21": {
        "name": "21_hallucination_hallucinations_LLMs_LLM",
        "keywords": [
          [
            "hallucination",
            0.028204565172429853
          ],
          [
            "hallucinations",
            0.02527398258600324
          ],
          [
            "LLMs",
            0.01699984201538703
          ],
          [
            "LLM",
            0.01627116325657596
          ],
          [
            "Hallucination",
            0.013371851832900636
          ],
          [
            "hallucination detection",
            0.013005878680605473
          ],
          [
            "evaluation",
            0.01289272732867487
          ],
          [
            "Large",
            0.010555197147891588
          ],
          [
            "detection",
            0.010064560297155332
          ],
          [
            "Language",
            0.009130028014980349
          ]
        ],
        "count": 650
      },
      "22": {
        "name": "22_legal_Legal_case_law",
        "keywords": [
          [
            "legal",
            0.09928062558894409
          ],
          [
            "Legal",
            0.038470654004162795
          ],
          [
            "case",
            0.01791448791160474
          ],
          [
            "law",
            0.014891917726408046
          ],
          [
            "domain",
            0.012813169147499484
          ],
          [
            "legal domain",
            0.012787786401361993
          ],
          [
            "cases",
            0.010697192930006866
          ],
          [
            "documents",
            0.009908051763206473
          ],
          [
            "retrieval",
            0.008871607582120243
          ],
          [
            "dataset",
            0.008108366163796071
          ]
        ],
        "count": 615
      },
      "23": {
        "name": "23_graph_Knowledge_knowledge_Graph",
        "keywords": [
          [
            "graph",
            0.04017392322376284
          ],
          [
            "Knowledge",
            0.026346321633843636
          ],
          [
            "knowledge",
            0.0260917385993654
          ],
          [
            "Graph",
            0.02345201679338576
          ],
          [
            "KG",
            0.023040250185154
          ],
          [
            "KGs",
            0.020464419518622354
          ],
          [
            "graphs",
            0.020008440957012482
          ],
          [
            "knowledge graph",
            0.019334954180177902
          ],
          [
            "entities",
            0.01855578846409478
          ],
          [
            "entity",
            0.016186532488976734
          ]
        ],
        "count": 585
      },
      "24": {
        "name": "24_emotion_Emotion_emotions_emotional",
        "keywords": [
          [
            "emotion",
            0.062013955548216024
          ],
          [
            "Emotion",
            0.034158006473271954
          ],
          [
            "emotions",
            0.027001946864091693
          ],
          [
            "emotional",
            0.015435552384918705
          ],
          [
            "emotion recognition",
            0.014937489397131888
          ],
          [
            "multimodal",
            0.014119811347540711
          ],
          [
            "Multimodal",
            0.012531249150175429
          ],
          [
            "recognition",
            0.011868323711433443
          ],
          [
            "modalities",
            0.010794773008372017
          ],
          [
            "sentiment",
            0.010232429062881425
          ]
        ],
        "count": 579
      },
      "25": {
        "name": "25_RAG_Retrieval_retrieval_Augmented",
        "keywords": [
          [
            "RAG",
            0.06543449156812362
          ],
          [
            "Retrieval",
            0.02968245204484921
          ],
          [
            "retrieval",
            0.02786447771190581
          ],
          [
            "Augmented",
            0.024059513031425037
          ],
          [
            "Generation",
            0.01875599956727341
          ],
          [
            "knowledge",
            0.014416214625954138
          ],
          [
            "generation",
            0.013287369780111366
          ],
          [
            "augmented",
            0.01311082010695632
          ],
          [
            "LLMs",
            0.011929310361082728
          ],
          [
            "context",
            0.011699759452325686
          ]
        ],
        "count": 566
      },
      "26": {
        "name": "26_financial_Financial_stock_market",
        "keywords": [
          [
            "financial",
            0.05568642875990264
          ],
          [
            "Financial",
            0.018995437509085968
          ],
          [
            "stock",
            0.017276207493608527
          ],
          [
            "market",
            0.016421148344723524
          ],
          [
            "sentiment",
            0.014025871655662004
          ],
          [
            "analysis",
            0.011929052170665951
          ],
          [
            "data",
            0.010780577074301185
          ],
          [
            "news",
            0.01072681595518436
          ],
          [
            "forecasting",
            0.010473214359263686
          ],
          [
            "finance",
            0.01026250931165494
          ]
        ],
        "count": 557
      },
      "27": {
        "name": "27_relation_extraction_Relation_RE",
        "keywords": [
          [
            "relation",
            0.049838673517029655
          ],
          [
            "extraction",
            0.037438098375851284
          ],
          [
            "Relation",
            0.033974333981663246
          ],
          [
            "RE",
            0.033089111708285684
          ],
          [
            "relation extraction",
            0.03175995722291481
          ],
          [
            "Extraction",
            0.030524324249412572
          ],
          [
            "relations",
            0.022113687957774287
          ],
          [
            "entity",
            0.01797529496972126
          ],
          [
            "entities",
            0.012128640155162263
          ],
          [
            "level",
            0.011309328225393657
          ]
        ],
        "count": 552
      },
      "28": {
        "name": "28_explanations_explanation_rationales_methods",
        "keywords": [
          [
            "explanations",
            0.04656390081651119
          ],
          [
            "explanation",
            0.02390416989343724
          ],
          [
            "rationales",
            0.014220752394441384
          ],
          [
            "methods",
            0.013161832692459081
          ],
          [
            "model",
            0.012867315510907985
          ],
          [
            "interpretability",
            0.011920196063032034
          ],
          [
            "Explanations",
            0.011287255457892224
          ],
          [
            "faithfulness",
            0.010686319401123369
          ],
          [
            "rationale",
            0.01063693907758798
          ],
          [
            "NLP",
            0.01059471497906011
          ]
        ],
        "count": 546
      },
      "29": {
        "name": "29_QA_question_answer_Question",
        "keywords": [
          [
            "QA",
            0.03693065659674907
          ],
          [
            "question",
            0.028530976163149237
          ],
          [
            "answer",
            0.0271816663578307
          ],
          [
            "Question",
            0.023622977451649475
          ],
          [
            "questions",
            0.019295086460206768
          ],
          [
            "Answering",
            0.01889201576162292
          ],
          [
            "Question Answering",
            0.01843713887063994
          ],
          [
            "answering",
            0.016681282619883164
          ],
          [
            "answers",
            0.016657876384729032
          ],
          [
            "MRC",
            0.01443719145081928
          ]
        ],
        "count": 506
      },
      "30": {
        "name": "30_Arabic_morphological_languages_NLP",
        "keywords": [
          [
            "Arabic",
            0.05152075643378907
          ],
          [
            "morphological",
            0.029922754467921538
          ],
          [
            "languages",
            0.016794137716674904
          ],
          [
            "NLP",
            0.013924340067884232
          ],
          [
            "language",
            0.01255141861464066
          ],
          [
            "dialects",
            0.011998409208937987
          ],
          [
            "dialect",
            0.010622259965614647
          ],
          [
            "word",
            0.010035364152557024
          ],
          [
            "segmentation",
            0.00923111656964795
          ],
          [
            "Morphological",
            0.009129382504964353
          ]
        ],
        "count": 485
      },
      "31": {
        "name": "31_students_student_educational_questions",
        "keywords": [
          [
            "students",
            0.026642660298564814
          ],
          [
            "student",
            0.024096491809873236
          ],
          [
            "educational",
            0.02203635550456414
          ],
          [
            "questions",
            0.016274148241900512
          ],
          [
            "education",
            0.014516791124080077
          ],
          [
            "feedback",
            0.01283301757215582
          ],
          [
            "learning",
            0.0125959525917693
          ],
          [
            "AI",
            0.012591883016002102
          ],
          [
            "grading",
            0.011314531014369305
          ],
          [
            "tutoring",
            0.011271800342857019
          ]
        ],
        "count": 485
      },
      "32": {
        "name": "32_political_media_social_pandemic",
        "keywords": [
          [
            "political",
            0.0277308161396539
          ],
          [
            "media",
            0.027536868478299965
          ],
          [
            "social",
            0.023669832361130547
          ],
          [
            "pandemic",
            0.01980667027303571
          ],
          [
            "public",
            0.018688501324710758
          ],
          [
            "tweets",
            0.018453476560983888
          ],
          [
            "Twitter",
            0.01805826371087005
          ],
          [
            "news",
            0.017649319411044778
          ],
          [
            "social media",
            0.016580156248045423
          ],
          [
            "analysis",
            0.014432110496256437
          ]
        ],
        "count": 481
      },
      "33": {
        "name": "33_prompt_prompts_Prompt_shot",
        "keywords": [
          [
            "prompt",
            0.048298538201134306
          ],
          [
            "prompts",
            0.030902885370025553
          ],
          [
            "Prompt",
            0.022700889965041757
          ],
          [
            "shot",
            0.020022533823452607
          ],
          [
            "tuning",
            0.015006883696820036
          ],
          [
            "tasks",
            0.013704857191669025
          ],
          [
            "task",
            0.012084695245699694
          ],
          [
            "prompt tuning",
            0.010244327972514244
          ],
          [
            "performance",
            0.010195942524808209
          ],
          [
            "learning",
            0.009233470408434315
          ]
        ],
        "count": 470
      },
      "34": {
        "name": "34_news_fake_fake news_Fake",
        "keywords": [
          [
            "news",
            0.08629164361603434
          ],
          [
            "fake",
            0.07098305630320371
          ],
          [
            "fake news",
            0.0681702469205853
          ],
          [
            "Fake",
            0.031658611694527904
          ],
          [
            "news detection",
            0.02966570154536683
          ],
          [
            "detection",
            0.028435841170678957
          ],
          [
            "fake news detection",
            0.027220866950344542
          ],
          [
            "News",
            0.026438794327594075
          ],
          [
            "Detection",
            0.0184864664477525
          ],
          [
            "misinformation",
            0.017787307022319142
          ]
        ],
        "count": 439
      },
      "35": {
        "name": "35_retrieval_ranking_query_dense",
        "keywords": [
          [
            "retrieval",
            0.04097143908957547
          ],
          [
            "ranking",
            0.024201914574644594
          ],
          [
            "query",
            0.019334599741154073
          ],
          [
            "dense",
            0.017808212059515664
          ],
          [
            "IR",
            0.016507942061305358
          ],
          [
            "Retrieval",
            0.01638803966043735
          ],
          [
            "document",
            0.016384891342064035
          ],
          [
            "documents",
            0.014536420258053639
          ],
          [
            "queries",
            0.01302483269139732
          ],
          [
            "dense retrieval",
            0.011083873582862086
          ]
        ],
        "count": 432
      },
      "36": {
        "name": "36_fact_checking_fact checking_claim",
        "keywords": [
          [
            "fact",
            0.058189480516614836
          ],
          [
            "checking",
            0.048667102321898144
          ],
          [
            "fact checking",
            0.046141694942971
          ],
          [
            "claim",
            0.04566537580724058
          ],
          [
            "claims",
            0.042132059375954906
          ],
          [
            "evidence",
            0.031885833396954624
          ],
          [
            "Fact",
            0.02763744011837024
          ],
          [
            "verification",
            0.026231153155877283
          ],
          [
            "Claim",
            0.01431955183709315
          ],
          [
            "misinformation",
            0.013079432719774828
          ]
        ],
        "count": 431
      },
      "37": {
        "name": "37_event_temporal_Event_events",
        "keywords": [
          [
            "event",
            0.07496108257200082
          ],
          [
            "temporal",
            0.03870626074434356
          ],
          [
            "Event",
            0.034308880520145704
          ],
          [
            "events",
            0.024452222400610048
          ],
          [
            "extraction",
            0.022489230668256303
          ],
          [
            "event extraction",
            0.016483173353230404
          ],
          [
            "Temporal",
            0.015079216956866334
          ],
          [
            "Extraction",
            0.01351080764115377
          ],
          [
            "argument",
            0.01239311219595213
          ],
          [
            "temporal reasoning",
            0.011881258798100947
          ]
        ],
        "count": 427
      },
      "38": {
        "name": "38_simplification_paraphrase_sentence_Simplification",
        "keywords": [
          [
            "simplification",
            0.03751282566883163
          ],
          [
            "paraphrase",
            0.03105610350050493
          ],
          [
            "sentence",
            0.01973346512771839
          ],
          [
            "Simplification",
            0.01888514863103697
          ],
          [
            "Paraphrase",
            0.01620477685351679
          ],
          [
            "text simplification",
            0.01581832702711843
          ],
          [
            "paraphrases",
            0.014617701722252952
          ],
          [
            "text",
            0.014529706540956934
          ],
          [
            "sentences",
            0.012682844483156572
          ],
          [
            "paraphrasing",
            0.012327110419587617
          ]
        ],
        "count": 354
      },
      "39": {
        "name": "39_detection_text_AI_detectors",
        "keywords": [
          [
            "detection",
            0.03076958037707109
          ],
          [
            "text",
            0.03076584530182486
          ],
          [
            "AI",
            0.02924150801838352
          ],
          [
            "detectors",
            0.026964575303457316
          ],
          [
            "texts",
            0.02103626601608516
          ],
          [
            "Generated",
            0.019408686841590576
          ],
          [
            "human",
            0.01663482639303028
          ],
          [
            "Text",
            0.014686326076621988
          ],
          [
            "text detection",
            0.014406600970027788
          ],
          [
            "Detection",
            0.014397471854217126
          ]
        ],
        "count": 346
      },
      "40": {
        "name": "40_SQL_Text_database_schema",
        "keywords": [
          [
            "SQL",
            0.1282277025253776
          ],
          [
            "Text",
            0.026871556586155886
          ],
          [
            "database",
            0.023433541930310775
          ],
          [
            "schema",
            0.02312398195961854
          ],
          [
            "queries",
            0.021568204273699248
          ],
          [
            "Spider",
            0.01832609715381549
          ],
          [
            "databases",
            0.016561212996636373
          ],
          [
            "query",
            0.01561977888532456
          ],
          [
            "natural language",
            0.013007850128326106
          ],
          [
            "natural",
            0.012622786051245689
          ]
        ],
        "count": 345
      },
      "41": {
        "name": "41_recommendation_user_item_recommender",
        "keywords": [
          [
            "recommendation",
            0.06314497440620165
          ],
          [
            "user",
            0.03593158128524297
          ],
          [
            "item",
            0.02812530127588264
          ],
          [
            "recommender",
            0.02488126624761309
          ],
          [
            "Recommendation",
            0.024859804634841793
          ],
          [
            "recommendations",
            0.02112085360314256
          ],
          [
            "recommender systems",
            0.01753588755668186
          ],
          [
            "items",
            0.01705297318230269
          ],
          [
            "users",
            0.014826978201292858
          ],
          [
            "Recommender",
            0.01420089363748986
          ]
        ],
        "count": 316
      },
      "42": {
        "name": "42_ICL_context_context learning_demonstrations",
        "keywords": [
          [
            "ICL",
            0.07508544211374318
          ],
          [
            "context",
            0.03948596102783599
          ],
          [
            "context learning",
            0.0354558640948656
          ],
          [
            "demonstrations",
            0.03310927162032242
          ],
          [
            "examples",
            0.024304477760545607
          ],
          [
            "learning",
            0.023218680779708865
          ],
          [
            "Context",
            0.02130428938030542
          ],
          [
            "In",
            0.017891157234670493
          ],
          [
            "Learning",
            0.01704044038294854
          ],
          [
            "demonstration",
            0.015678095180132294
          ]
        ],
        "count": 311
      },
      "43": {
        "name": "43_quantum_Quantum_classical_finite",
        "keywords": [
          [
            "quantum",
            0.08115505710916357
          ],
          [
            "Quantum",
            0.027912489555646364
          ],
          [
            "classical",
            0.019919337886883076
          ],
          [
            "finite",
            0.016359306189679523
          ],
          [
            "automata",
            0.016292909604391656
          ],
          [
            "theory",
            0.01410111330284434
          ],
          [
            "circuits",
            0.013279671063003652
          ],
          [
            "computing",
            0.013111745744796798
          ],
          [
            "calculus",
            0.012985880802858975
          ],
          [
            "logic",
            0.01158401301724952
          ]
        ],
        "count": 302
      },
      "44": {
        "name": "44_instruction_Instruction_tuning_instructions",
        "keywords": [
          [
            "instruction",
            0.05793672472829121
          ],
          [
            "Instruction",
            0.02832670547313311
          ],
          [
            "tuning",
            0.024081976897725486
          ],
          [
            "instructions",
            0.023451562676113914
          ],
          [
            "instruction tuning",
            0.02117486875320625
          ],
          [
            "data",
            0.018665633835710675
          ],
          [
            "following",
            0.016172188362007144
          ],
          [
            "LLMs",
            0.013057525798666886
          ],
          [
            "quality",
            0.012457532161415453
          ],
          [
            "Tuning",
            0.011589992730505214
          ]
        ],
        "count": 288
      },
      "45": {
        "name": "45_editing_knowledge_Editing_Knowledge",
        "keywords": [
          [
            "editing",
            0.06235489630908526
          ],
          [
            "knowledge",
            0.052657879153693965
          ],
          [
            "Editing",
            0.024573956269944578
          ],
          [
            "Knowledge",
            0.021309436330818197
          ],
          [
            "knowledge editing",
            0.01936060501698602
          ],
          [
            "model editing",
            0.01818860470375649
          ],
          [
            "factual",
            0.01702797262321343
          ],
          [
            "facts",
            0.015501593257280991
          ],
          [
            "LLMs",
            0.014411153376271921
          ],
          [
            "editing methods",
            0.01297266113415734
          ]
        ],
        "count": 287
      },
      "46": {
        "name": "46_story_stories_narrative_creativity",
        "keywords": [
          [
            "story",
            0.055505577288548824
          ],
          [
            "stories",
            0.03802628801160743
          ],
          [
            "narrative",
            0.03136654151524108
          ],
          [
            "creativity",
            0.02167118576255822
          ],
          [
            "story generation",
            0.019782689177918106
          ],
          [
            "creative",
            0.018396120790480038
          ],
          [
            "generation",
            0.017667157034538125
          ],
          [
            "Story",
            0.016913622297737206
          ],
          [
            "human",
            0.01656066663576352
          ],
          [
            "narratives",
            0.01361890083317401
          ]
        ],
        "count": 286
      }
    },
    "correlations": [
      [
        1.0,
        -0.747210068289762,
        -0.7390527197589423,
        -0.7279085938553332,
        -0.7396796483003016,
        -0.7510285585903059,
        -0.7399029084308959,
        -0.6883713331385235,
        -0.7299635139026381,
        -0.7561576915800825,
        -0.7274452485896974,
        -0.7565535506455625,
        -0.7065082117290908,
        -0.5061186976112029,
        -0.7257134728464457,
        -0.7489799689892314,
        -0.750652238872439,
        -0.6638234838042718,
        -0.729200135385695,
        -0.7431665571848862,
        -0.7381215917852176,
        -0.7409155881149556,
        -0.7626337428674153,
        -0.7477569467317536,
        -0.7071128068781354,
        -0.7606573218654031,
        -0.7620410428901423,
        -0.7527588536054313,
        -0.760746175941674,
        -0.751645513505812,
        -0.7172872722870808,
        -0.742963611507313,
        -0.742677178591544,
        -0.7414014393635384,
        -0.7534610851283527,
        -0.753306417975108,
        -0.7499757471920913,
        -0.7398249678335643,
        -0.7550345044275172,
        -0.7136578981694948,
        -0.7619855836801233,
        -0.7509027860821764,
        -0.7364075139712836,
        -0.7654348645043507,
        -0.7182195874350603,
        -0.7326143597967352,
        -0.7591945599346295
      ],
      [
        -0.747210068289762,
        1.0,
        -0.7191739268762072,
        -0.5518483112377004,
        -0.565974721978366,
        -0.5705297520301114,
        -0.7323394582593311,
        -0.7483560509773743,
        -0.5854906106631017,
        -0.7284222195994354,
        -0.7392855073398261,
        -0.716272503036727,
        -0.7364784235415114,
        -0.7355922286836947,
        -0.6982567030958111,
        -0.72508402656876,
        -0.7462485623192958,
        -0.7321014660434184,
        -0.7185287567254337,
        -0.6061354454199452,
        -0.7149252702460531,
        -0.533536533453304,
        -0.7556675601180363,
        -0.7221779703096003,
        -0.7540916230702638,
        -0.7168004260982324,
        -0.7503934378728099,
        -0.7102675609910196,
        -0.7365306652982166,
        -0.6951103654577968,
        -0.7538739475151726,
        -0.5771900973657307,
        -0.7144185391417945,
        -0.694794408998741,
        -0.7536457709038702,
        -0.7261711866445026,
        -0.7319497084635082,
        -0.7328143979843238,
        -0.7528587282721818,
        -0.7032219461277194,
        -0.7587903566204188,
        -0.7375049318883997,
        -0.7216201280133545,
        -0.7648485939026233,
        -0.5678060070841744,
        -0.5964632209304586,
        -0.7487979124716888
      ],
      [
        -0.7390527197589423,
        -0.7191739268762072,
        1.0,
        -0.7129826503064151,
        -0.7190677422335745,
        -0.6854151870440847,
        -0.7481286591608739,
        -0.7389018400540039,
        -0.7153400222502779,
        -0.7291603450494994,
        -0.7293794289375467,
        -0.7493999179554911,
        -0.722063238079806,
        -0.7428833335739102,
        -0.683267146155411,
        -0.7404015889319913,
        -0.7424382851066538,
        -0.7200967148807512,
        -0.6834516101987563,
        -0.7206317194515623,
        -0.7495063295019997,
        -0.7033837259034836,
        -0.7636387888975293,
        -0.730746114117853,
        -0.7558710863532592,
        -0.7458885047607644,
        -0.7602861776434662,
        -0.7331558588095435,
        -0.7421041088025617,
        -0.6602117484714658,
        -0.7574364051591582,
        -0.7397076117819912,
        -0.7432234953897094,
        -0.6857176195976014,
        -0.7505728474622528,
        -0.7021788450298834,
        -0.7463564405631016,
        -0.7269316667558494,
        -0.7560581873132353,
        -0.6894157211127291,
        -0.7597257034420781,
        -0.7474530763967713,
        -0.7277199097294414,
        -0.7650125196806121,
        -0.6877526930548065,
        -0.7056883073610815,
        -0.7425714671676418
      ],
      [
        -0.7279085938553332,
        -0.5518483112377004,
        -0.7129826503064151,
        1.0,
        -0.15434798701162508,
        -0.2439451261848429,
        -0.7263599165088978,
        -0.7201191427809335,
        -0.23791904432081756,
        -0.6940557694480705,
        -0.6905295653560295,
        -0.722117667323442,
        -0.7178256772266973,
        -0.7334531980973372,
        -0.5536362800462916,
        -0.7054590304660879,
        -0.7263358608020596,
        -0.6754916924083911,
        -0.6730777770458446,
        -0.5373294189939308,
        -0.7418332380105612,
        0.03936135138208546,
        -0.7433374990024699,
        -0.703899874224531,
        -0.748133227902593,
        -0.6986175590427612,
        -0.7371406920907194,
        -0.7303943599006657,
        -0.72921776602781,
        -0.6681008143491255,
        -0.7436822123437463,
        -0.5319708063338859,
        -0.7307047907341738,
        -0.6103452756023906,
        -0.7402303339750871,
        -0.6966175850580238,
        -0.7145104990412242,
        -0.7347345142116344,
        -0.7471175721279146,
        -0.6724308991713811,
        -0.7518676478504958,
        -0.7231930740530316,
        -0.6648819812258788,
        -0.761133692010928,
        -0.21566106190810314,
        -0.5452296014384508,
        -0.7361910997657322
      ],
      [
        -0.7396796483003016,
        -0.565974721978366,
        -0.7190677422335745,
        -0.15434798701162508,
        1.0,
        -0.1923003790324214,
        -0.7336904132224982,
        -0.7316032068074455,
        -0.1556573182372304,
        -0.681283201304081,
        -0.7214985231785509,
        -0.733071488138344,
        -0.7208239814412913,
        -0.7303123797686975,
        -0.5450039380255377,
        -0.7121574393924545,
        -0.7261054542764195,
        -0.7017637933261351,
        -0.6733044555337824,
        -0.5455927649328345,
        -0.7450687646252144,
        0.07221376416552011,
        -0.7409292015046752,
        -0.708986090765376,
        -0.7508113229314521,
        -0.6960270428164606,
        -0.7370070606641017,
        -0.7325148744965915,
        -0.7327234000039518,
        -0.67424019530956,
        -0.7463765332526586,
        -0.5556163387533603,
        -0.7330162195401725,
        -0.597479320879022,
        -0.7402891788920287,
        -0.7070723088700435,
        -0.7163097086290936,
        -0.7395266857769075,
        -0.7461661412987541,
        -0.6538811910951233,
        -0.752661542601156,
        -0.7187481584627518,
        -0.6771675008328355,
        -0.7623218843850709,
        -0.4602966441949009,
        -0.5610971958417266,
        -0.7406686798003013
      ],
      [
        -0.7510285585903059,
        -0.5705297520301114,
        -0.6854151870440847,
        -0.2439451261848429,
        -0.1923003790324214,
        1.0,
        -0.735038955738851,
        -0.7380688914983213,
        -0.2637577493410887,
        -0.6580832411307151,
        -0.717694981877554,
        -0.736126033144045,
        -0.7361776654023149,
        -0.7463645063106517,
        -0.5477750916036204,
        -0.705505469918264,
        -0.7380911027801834,
        -0.7025936214279774,
        -0.6837762095316957,
        -0.5313208791725652,
        -0.7473616497188624,
        -0.06113779887156589,
        -0.7430993703482833,
        -0.6804997285757588,
        -0.7535117440098181,
        -0.6953542278611321,
        -0.7356913205380474,
        -0.72948922932235,
        -0.7096877530552745,
        -0.6135672174793871,
        -0.7495828930463707,
        -0.5480518539896866,
        -0.738859345415295,
        -0.6230561000844211,
        -0.7494813887131229,
        -0.7028969132050258,
        -0.6996336794154006,
        -0.7285434419405714,
        -0.7520057421951659,
        -0.6778670072251649,
        -0.7506568599503973,
        -0.7277188184853468,
        -0.6700638592609276,
        -0.7615278931789611,
        -0.45808857772705086,
        -0.551561241073854,
        -0.7359655349756068
      ],
      [
        -0.7399029084308959,
        -0.7323394582593311,
        -0.7481286591608739,
        -0.7263599165088978,
        -0.7336904132224982,
        -0.735038955738851,
        1.0,
        -0.7546310796983972,
        -0.735994475656173,
        -0.698938983534815,
        -0.7440834209613362,
        -0.7327951185528823,
        -0.7489908835637225,
        -0.7538947929189723,
        -0.7338417355518251,
        -0.7534940346490874,
        -0.7520576128858585,
        -0.7409154163186593,
        -0.7099295913659158,
        -0.7299911552739559,
        -0.7545792972664987,
        -0.7235330010039691,
        -0.7639089549646756,
        -0.7327779777360905,
        -0.722612543600109,
        -0.7543898610193523,
        -0.7606426176627563,
        -0.7485324563721825,
        -0.7567395616558992,
        -0.7372471180990383,
        -0.7613198067831625,
        -0.7353754437875474,
        -0.7540613767031028,
        -0.7320475279182364,
        -0.7595817742860762,
        -0.7322351189064634,
        -0.7506587140541421,
        -0.7535689718859784,
        -0.7596020578220366,
        -0.7318840045393237,
        -0.7625294564872753,
        -0.6913603178391738,
        -0.7301348376633774,
        -0.7646315536915762,
        -0.724562407331808,
        -0.7168452922837414,
        -0.7543266163485677
      ],
      [
        -0.6883713331385235,
        -0.7483560509773743,
        -0.7389018400540039,
        -0.7201191427809335,
        -0.7316032068074455,
        -0.7380688914983213,
        -0.7546310796983972,
        1.0,
        -0.717365507951772,
        -0.7579904063168423,
        -0.7120580700652619,
        -0.7372440532169395,
        -0.7222951561124622,
        -0.7400132897430953,
        -0.7129866951942128,
        -0.746778864947105,
        -0.7430670473118937,
        -0.5665058128599253,
        -0.7144223760618302,
        -0.7351177038387356,
        -0.7478270355166307,
        -0.7233671592060306,
        -0.7591858000434047,
        -0.7444013884318466,
        -0.7598717833281184,
        -0.756538557644898,
        -0.7585827932086755,
        -0.7535468371497794,
        -0.7564293829889464,
        -0.7413156140861918,
        -0.6940176585725712,
        -0.739707136807694,
        -0.7505909457606099,
        -0.7409244841749156,
        -0.7465533556417987,
        -0.7412686750159958,
        -0.7477310642948753,
        -0.7580499859526703,
        -0.7311291270499329,
        -0.7255295083999886,
        -0.7572485137417955,
        -0.7561918263940841,
        -0.7304590788810841,
        -0.7607553210519736,
        -0.7173401498999651,
        -0.7187307424923572,
        -0.7584918187016252
      ],
      [
        -0.7299635139026381,
        -0.5854906106631017,
        -0.7153400222502779,
        -0.23791904432081756,
        -0.1556573182372304,
        -0.2637577493410887,
        -0.735994475656173,
        -0.717365507951772,
        1.0,
        -0.6996387998058109,
        -0.7123792142675427,
        -0.7330954402538497,
        -0.7053836882075613,
        -0.7157192876568905,
        -0.5779892170076912,
        -0.7104097042522322,
        -0.7216574906802504,
        -0.6922717515665723,
        -0.6846305501065452,
        -0.560212171501064,
        -0.7450167713949566,
        -0.0755092223207709,
        -0.7423995368722366,
        -0.7149237544662022,
        -0.7446850406421172,
        -0.7073457140934702,
        -0.738135921890928,
        -0.7345912619921704,
        -0.7312581454109361,
        -0.6785973629735118,
        -0.7373523855237003,
        -0.5733967413046288,
        -0.7087958090385551,
        -0.6292156341594031,
        -0.7311709880396431,
        -0.7127245991845668,
        -0.7106675739900655,
        -0.7371865429875465,
        -0.752192502718924,
        -0.6680355128148499,
        -0.7545179927728934,
        -0.7215164281640863,
        -0.6775283860985977,
        -0.7630869962400066,
        -0.4983268801799229,
        -0.5841802616898971,
        -0.7341888498207674
      ],
      [
        -0.7561576915800825,
        -0.7284222195994354,
        -0.7291603450494994,
        -0.6940557694480705,
        -0.681283201304081,
        -0.6580832411307151,
        -0.698938983534815,
        -0.7579904063168423,
        -0.6996387998058109,
        1.0,
        -0.7475675431971884,
        -0.7552970067304623,
        -0.7586169495897053,
        -0.7584430028783714,
        -0.7006358863111367,
        -0.7300854259857317,
        -0.7560172336058131,
        -0.74874622371275,
        -0.741946839529931,
        -0.7036315034980934,
        -0.7610468060828015,
        -0.6621516272706434,
        -0.7592446832413229,
        -0.7372696391307128,
        -0.756208334528373,
        -0.7372211000868165,
        -0.7476521665270659,
        -0.7564523680579822,
        -0.753068644162433,
        -0.7308005609387344,
        -0.7632394250912741,
        -0.7212086606069654,
        -0.7553382635694696,
        -0.7275496843466798,
        -0.7601066035209603,
        -0.7424183032420688,
        -0.7481652851364291,
        -0.7506244650291416,
        -0.7624749153350707,
        -0.7317755620249391,
        -0.7585642015244121,
        -0.724530690427694,
        -0.7247875247022795,
        -0.763951175926324,
        -0.6887039860667343,
        -0.7198717752453938,
        -0.7526773284238031
      ],
      [
        -0.7274452485896974,
        -0.7392855073398261,
        -0.7293794289375467,
        -0.6905295653560295,
        -0.7214985231785509,
        -0.717694981877554,
        -0.7440834209613362,
        -0.7120580700652619,
        -0.7123792142675427,
        -0.7475675431971884,
        1.0,
        -0.7508946442643383,
        -0.682094336158352,
        -0.7370642730818975,
        -0.7084816291728284,
        -0.7331137749744239,
        -0.7299252999679033,
        -0.36156142366781097,
        -0.7085519404844588,
        -0.7297982683199601,
        -0.735896037681957,
        -0.6996225322404259,
        -0.758541087408528,
        -0.7087977563245142,
        -0.757183966619607,
        -0.7542145139093359,
        -0.756503003551287,
        -0.7261893692495964,
        -0.7456227801152413,
        -0.7125893160826688,
        -0.7125172896776661,
        -0.739322652530106,
        -0.7439738478818083,
        -0.7250805418450836,
        -0.7496017084151234,
        -0.7358171820185578,
        -0.7291815520155107,
        -0.7418017088428429,
        -0.7317511577453256,
        -0.7140660965058783,
        -0.735695280265284,
        -0.7494106968741299,
        -0.7136597977643111,
        -0.7605661127632125,
        -0.6877151832308727,
        -0.7176790730147107,
        -0.7502730784788663
      ],
      [
        -0.7565535506455625,
        -0.716272503036727,
        -0.7493999179554911,
        -0.722117667323442,
        -0.733071488138344,
        -0.736126033144045,
        -0.7327951185528823,
        -0.7372440532169395,
        -0.7330954402538497,
        -0.7552970067304623,
        -0.7508946442643383,
        1.0,
        -0.7453555212675955,
        -0.7583776343431241,
        -0.7257725374914259,
        -0.7135491877618225,
        -0.7434293897103277,
        -0.7383849774144565,
        -0.7000833977200844,
        -0.7337887449830863,
        -0.749267135401023,
        -0.7130101862486087,
        -0.7362250461458446,
        -0.7391078718319846,
        -0.7633909769708007,
        -0.7459794842338263,
        -0.7570379732306931,
        -0.7461380389106519,
        -0.7528473812141283,
        -0.7242321491352417,
        -0.7576984303033598,
        -0.7435872851115779,
        -0.750400498147588,
        -0.7387115881832711,
        -0.7102623182138729,
        -0.7263647269478739,
        -0.7431037648176146,
        -0.746613272756717,
        -0.7409193448097102,
        -0.7239772022490005,
        -0.7618162230807026,
        -0.7483604956556607,
        -0.7462306002482973,
        -0.7650284064786226,
        -0.7297578092418808,
        -0.7370037485918024,
        -0.7417589588620986
      ],
      [
        -0.7065082117290908,
        -0.7364784235415114,
        -0.722063238079806,
        -0.7178256772266973,
        -0.7208239814412913,
        -0.7361776654023149,
        -0.7489908835637225,
        -0.7222951561124622,
        -0.7053836882075613,
        -0.7586169495897053,
        -0.682094336158352,
        -0.7453555212675955,
        1.0,
        -0.7265208579043263,
        -0.7179885647643873,
        -0.7341556546830954,
        -0.7171251998096864,
        -0.671099766009503,
        -0.7177293495482875,
        -0.7390412075200316,
        -0.727721505715317,
        -0.7251992122909677,
        -0.757882413194486,
        -0.7006083255515809,
        -0.7521864270612976,
        -0.7479272169682128,
        -0.7478894785274388,
        -0.7201043106886058,
        -0.75089043274604,
        -0.7293622696179993,
        -0.7247474967150529,
        -0.7393671834407042,
        -0.7360827052394121,
        -0.7312700188770709,
        -0.7420528814752876,
        -0.7029376523046915,
        -0.7340374977124788,
        -0.7414917143252796,
        -0.6590880824616687,
        -0.7054085394254335,
        -0.7600239381167797,
        -0.738337660405437,
        -0.7197742117597932,
        -0.7577865453218041,
        -0.7141018783045188,
        -0.7291777287520886,
        -0.7513850952961962
      ],
      [
        -0.5061186976112029,
        -0.7355922286836947,
        -0.7428833335739102,
        -0.7334531980973372,
        -0.7303123797686975,
        -0.7463645063106517,
        -0.7538947929189723,
        -0.7400132897430953,
        -0.7157192876568905,
        -0.7584430028783714,
        -0.7370642730818975,
        -0.7583776343431241,
        -0.7265208579043263,
        1.0,
        -0.732235642608569,
        -0.7417392994780987,
        -0.7004642719629797,
        -0.7052511420320411,
        -0.7334977516539503,
        -0.7482452644678096,
        -0.7455901743361245,
        -0.7341724029605259,
        -0.7571427316319381,
        -0.7441303295678374,
        -0.7243353387080014,
        -0.7604521944561035,
        -0.7495341457175585,
        -0.7502335428373167,
        -0.7433790480522579,
        -0.7548530792293309,
        -0.7217784002491856,
        -0.7511271567643347,
        -0.3352321672537405,
        -0.7435431067537994,
        -0.6597378114777691,
        -0.7553037600508605,
        -0.7354920587776005,
        -0.7333472485596249,
        -0.758235425300382,
        -0.6432205759351821,
        -0.7623679134509593,
        -0.7405750687780035,
        -0.7395736763939373,
        -0.7650980254795203,
        -0.731429239752271,
        -0.7399154891664019,
        -0.7525864318567815
      ],
      [
        -0.7257134728464457,
        -0.6982567030958111,
        -0.683267146155411,
        -0.5536362800462916,
        -0.5450039380255377,
        -0.5477750916036204,
        -0.7338417355518251,
        -0.7129866951942128,
        -0.5779892170076912,
        -0.7006358863111367,
        -0.7084816291728284,
        -0.7257725374914259,
        -0.7179885647643873,
        -0.732235642608569,
        1.0,
        -0.7133214228745728,
        -0.7260942132962057,
        -0.6835309825762715,
        -0.6772088661133295,
        -0.6832924701262075,
        -0.7361066168674041,
        -0.5170427138553628,
        -0.7498665353218968,
        -0.7092529998745097,
        -0.7491089214323121,
        -0.7229167072948557,
        -0.7488769389064811,
        -0.7260895812016526,
        -0.74050874078605,
        -0.6900617427543575,
        -0.7325505576873165,
        -0.7027381962403825,
        -0.732959809141299,
        -0.6763828400843693,
        -0.7464785114801484,
        -0.7001536750527666,
        -0.7264072714375014,
        -0.7344940130824111,
        -0.7475351376914359,
        -0.6876349379609399,
        -0.7479717416624502,
        -0.7308380842556097,
        -0.7062205856048573,
        -0.7609115174032148,
        -0.6297540986040693,
        -0.6800166412463997,
        -0.7496528017606394
      ],
      [
        -0.7489799689892314,
        -0.72508402656876,
        -0.7404015889319913,
        -0.7054590304660879,
        -0.7121574393924545,
        -0.705505469918264,
        -0.7534940346490874,
        -0.746778864947105,
        -0.7104097042522322,
        -0.7300854259857317,
        -0.7331137749744239,
        -0.7135491877618225,
        -0.7341556546830954,
        -0.7417392994780987,
        -0.7133214228745728,
        1.0,
        -0.731102430665578,
        -0.7312892120839376,
        -0.7226405704455092,
        -0.7371747856088247,
        -0.7361613594651659,
        -0.6944318237122109,
        -0.7493666093208122,
        -0.7220155987821524,
        -0.7572398195690566,
        -0.7418314473846229,
        -0.7498074592235677,
        -0.7208198070281301,
        -0.7467700474678592,
        -0.7215168946037318,
        -0.7528858493696755,
        -0.7239796027132406,
        -0.7212330409750011,
        -0.7386872158366232,
        -0.7385774464376633,
        -0.7227124550123694,
        -0.714557280526626,
        -0.7474337333308121,
        -0.750396703678767,
        -0.6996647032749668,
        -0.7618330676407004,
        -0.7273386600789397,
        -0.7330284725976235,
        -0.7615414498004607,
        -0.7153925000258781,
        -0.7206144797296999,
        -0.7527268032936564
      ],
      [
        -0.750652238872439,
        -0.7462485623192958,
        -0.7424382851066538,
        -0.7263358608020596,
        -0.7261054542764195,
        -0.7380911027801834,
        -0.7520576128858585,
        -0.7430670473118937,
        -0.7216574906802504,
        -0.7560172336058131,
        -0.7299252999679033,
        -0.7434293897103277,
        -0.7171251998096864,
        -0.7004642719629797,
        -0.7260942132962057,
        -0.731102430665578,
        1.0,
        -0.718808562155714,
        -0.7268485206817108,
        -0.7459127910081691,
        -0.7397459295768449,
        -0.7296032906738807,
        -0.7575982606081676,
        -0.7416787021242459,
        -0.6959290178633126,
        -0.7600548329829768,
        -0.5852243429113771,
        -0.7295182494175313,
        -0.7422903787722761,
        -0.7460761404260308,
        -0.7328301306418095,
        -0.7423957790606559,
        -0.6580801829977437,
        -0.7351666995591208,
        -0.7227768142552766,
        -0.747159776377412,
        -0.7450458304647556,
        -0.7439952436556427,
        -0.7445401671225789,
        -0.7221107600555006,
        -0.763374382775547,
        -0.7385239928104287,
        -0.7344984936086233,
        -0.7558387099422176,
        -0.7298817721296778,
        -0.7387517424810104,
        -0.7530820780629683
      ],
      [
        -0.6638234838042718,
        -0.7321014660434184,
        -0.7200967148807512,
        -0.6754916924083911,
        -0.7017637933261351,
        -0.7025936214279774,
        -0.7409154163186593,
        -0.5665058128599253,
        -0.6922717515665723,
        -0.74874622371275,
        -0.36156142366781097,
        -0.7383849774144565,
        -0.671099766009503,
        -0.7052511420320411,
        -0.6835309825762715,
        -0.7312892120839376,
        -0.718808562155714,
        1.0,
        -0.7076895826484564,
        -0.7207170260399519,
        -0.708560669886146,
        -0.6789817005242686,
        -0.7554461764978687,
        -0.7274408866413563,
        -0.7482810688545669,
        -0.7503321931128222,
        -0.7516125786437358,
        -0.7344645220794942,
        -0.7492610275899253,
        -0.7060946114750262,
        -0.47809784052854565,
        -0.7274546528411743,
        -0.7252619631647271,
        -0.7043475552334354,
        -0.7333916992594829,
        -0.7148024745419277,
        -0.7222504418591161,
        -0.7430112697083175,
        -0.7396185028999416,
        -0.7048126516627989,
        -0.7594630717663619,
        -0.750345720250041,
        -0.7094074192567218,
        -0.7608659514630682,
        -0.6592693813768328,
        -0.7048288953243769,
        -0.7525146032392076
      ],
      [
        -0.729200135385695,
        -0.7185287567254337,
        -0.6834516101987563,
        -0.6730777770458446,
        -0.6733044555337824,
        -0.6837762095316957,
        -0.7099295913659158,
        -0.7144223760618302,
        -0.6846305501065452,
        -0.741946839529931,
        -0.7085519404844588,
        -0.7000833977200844,
        -0.7177293495482875,
        -0.7334977516539503,
        -0.6772088661133295,
        -0.7226405704455092,
        -0.7268485206817108,
        -0.7076895826484564,
        1.0,
        -0.7040427114623184,
        -0.7429059745926974,
        -0.6542384122549801,
        -0.7570213950562635,
        -0.7137514979566448,
        -0.7499419541890515,
        -0.7315639431281453,
        -0.7553432703170028,
        -0.7336277984673678,
        -0.7386228801981691,
        -0.7061037360570559,
        -0.7465434765653003,
        -0.7143116010700499,
        -0.7444875233029202,
        -0.7017191351336511,
        -0.7394693968831709,
        -0.7189378933527988,
        -0.7327102700771574,
        -0.741063221210952,
        -0.7253341840899792,
        -0.43999976135110735,
        -0.745413096029337,
        -0.7331820603459038,
        -0.7193218610032285,
        -0.7628010634561944,
        -0.6780116608507726,
        -0.6994903378958044,
        -0.7091264009266925
      ],
      [
        -0.7431665571848862,
        -0.6061354454199452,
        -0.7206317194515623,
        -0.5373294189939308,
        -0.5455927649328345,
        -0.5313208791725652,
        -0.7299911552739559,
        -0.7351177038387356,
        -0.560212171501064,
        -0.7036315034980934,
        -0.7297982683199601,
        -0.7337887449830863,
        -0.7390412075200316,
        -0.7482452644678096,
        -0.6832924701262075,
        -0.7371747856088247,
        -0.7459127910081691,
        -0.7207170260399519,
        -0.7040427114623184,
        1.0,
        -0.7551710243130818,
        -0.5066147065819109,
        -0.7566562232872023,
        -0.7396303992761959,
        -0.7571166400994576,
        -0.7309429737245832,
        -0.7529001189650053,
        -0.7493269552753192,
        -0.7411866872847273,
        -0.7124728470374082,
        -0.7527979556304217,
        -0.5740978713341576,
        -0.7469817778847894,
        -0.6885794915049968,
        -0.7539778430558106,
        -0.7261805720237231,
        -0.7366953043562552,
        -0.7470074778408007,
        -0.7488377755476683,
        -0.7018389130522152,
        -0.7603420399765446,
        -0.711916674035399,
        -0.7167623840908331,
        -0.764229398188085,
        -0.5363846436483369,
        -0.6140908372553373,
        -0.7440284300769664
      ],
      [
        -0.7381215917852176,
        -0.7149252702460531,
        -0.7495063295019997,
        -0.7418332380105612,
        -0.7450687646252144,
        -0.7473616497188624,
        -0.7545792972664987,
        -0.7478270355166307,
        -0.7450167713949566,
        -0.7610468060828015,
        -0.735896037681957,
        -0.749267135401023,
        -0.727721505715317,
        -0.7455901743361245,
        -0.7361066168674041,
        -0.7361613594651659,
        -0.7397459295768449,
        -0.708560669886146,
        -0.7429059745926974,
        -0.7551710243130818,
        1.0,
        -0.7432713249965661,
        -0.7517030112527976,
        -0.6396302800458695,
        -0.7628405944367802,
        -0.7546381012614296,
        -0.7481183324970728,
        -0.6400141490598974,
        -0.7604977778267094,
        -0.7317496690719542,
        -0.7335372516424083,
        -0.7538362445503557,
        -0.7422343355638704,
        -0.7391087608206282,
        -0.7423243592013513,
        -0.7353451114176501,
        -0.7478591403805761,
        -0.7381387698569695,
        -0.7556335530521576,
        -0.7381373793242842,
        -0.760769378239976,
        -0.7486337095828878,
        -0.739083312137206,
        -0.7637700127237153,
        -0.7395563794953071,
        -0.7286461740879691,
        -0.758197692035568
      ],
      [
        -0.7409155881149556,
        -0.533536533453304,
        -0.7033837259034836,
        0.03936135138208546,
        0.07221376416552011,
        -0.06113779887156589,
        -0.7235330010039691,
        -0.7233671592060306,
        -0.0755092223207709,
        -0.6621516272706434,
        -0.6996225322404259,
        -0.7130101862486087,
        -0.7251992122909677,
        -0.7341724029605259,
        -0.5170427138553628,
        -0.6944318237122109,
        -0.7296032906738807,
        -0.6789817005242686,
        -0.6542384122549801,
        -0.5066147065819109,
        -0.7432713249965661,
        1.0,
        -0.7358900706432376,
        -0.6910990907046999,
        -0.7472215435229821,
        -0.6640451795835627,
        -0.7288867844310836,
        -0.7283167852215167,
        -0.72201770739759,
        -0.6406410808098915,
        -0.7415301770163054,
        -0.520097731613277,
        -0.7291474583134938,
        -0.5796615655859698,
        -0.7390443841378536,
        -0.6913464769698052,
        -0.6953250437493061,
        -0.7326829968318863,
        -0.7466710928384525,
        -0.6394983619408363,
        -0.7493599201591301,
        -0.7120950180470911,
        -0.6564130095718658,
        -0.7605452953362324,
        -0.33855664407580144,
        -0.5209139914783094,
        -0.7310578636702032
      ],
      [
        -0.7626337428674153,
        -0.7556675601180363,
        -0.7636387888975293,
        -0.7433374990024699,
        -0.7409292015046752,
        -0.7430993703482833,
        -0.7639089549646756,
        -0.7591858000434047,
        -0.7423995368722366,
        -0.7592446832413229,
        -0.758541087408528,
        -0.7362250461458446,
        -0.757882413194486,
        -0.7571427316319381,
        -0.7498665353218968,
        -0.7493666093208122,
        -0.7575982606081676,
        -0.7554461764978687,
        -0.7570213950562635,
        -0.7566562232872023,
        -0.7517030112527976,
        -0.7358900706432376,
        1.0,
        -0.7552137859256347,
        -0.7641747684428745,
        -0.7442461734879979,
        -0.7468501797875411,
        -0.754551431412219,
        -0.7535537077244034,
        -0.7503712137412503,
        -0.7605724060475826,
        -0.7528505387409142,
        -0.7597525161228074,
        -0.7532986079204484,
        -0.7571667672285101,
        -0.7380363984730144,
        -0.7475653546501513,
        -0.7558878159845566,
        -0.758930548522657,
        -0.7496356129225576,
        -0.764387452627902,
        -0.7553096060248913,
        -0.7559467922780234,
        -0.7655764556207767,
        -0.7495059181077732,
        -0.7503388631497184,
        -0.7632909542981876
      ],
      [
        -0.7477569467317536,
        -0.7221779703096003,
        -0.730746114117853,
        -0.703899874224531,
        -0.708986090765376,
        -0.6804997285757588,
        -0.7327779777360905,
        -0.7444013884318466,
        -0.7149237544662022,
        -0.7372696391307128,
        -0.7087977563245142,
        -0.7391078718319846,
        -0.7006083255515809,
        -0.7441303295678374,
        -0.7092529998745097,
        -0.7220155987821524,
        -0.7416787021242459,
        -0.7274408866413563,
        -0.7137514979566448,
        -0.7396303992761959,
        -0.6396302800458695,
        -0.6910990907046999,
        -0.7552137859256347,
        1.0,
        -0.7546022888746289,
        -0.7190995263495767,
        -0.7534140919631743,
        -0.629364117651273,
        -0.7441571003597903,
        -0.6682080968702535,
        -0.754681201931149,
        -0.7310336847729973,
        -0.7391400497403149,
        -0.726332403399315,
        -0.7443846939751363,
        -0.7162487783916144,
        -0.7233620866251658,
        -0.7150830647767428,
        -0.7507302625880723,
        -0.7202711404822426,
        -0.7573668305977764,
        -0.7204361534544403,
        -0.7338920723947504,
        -0.7626928144120217,
        -0.7138622826366564,
        -0.4668714609171031,
        -0.7497390105260786
      ],
      [
        -0.7071128068781354,
        -0.7540916230702638,
        -0.7558710863532592,
        -0.748133227902593,
        -0.7508113229314521,
        -0.7535117440098181,
        -0.722612543600109,
        -0.7598717833281184,
        -0.7446850406421172,
        -0.756208334528373,
        -0.757183966619607,
        -0.7633909769708007,
        -0.7521864270612976,
        -0.7243353387080014,
        -0.7491089214323121,
        -0.7572398195690566,
        -0.6959290178633126,
        -0.7482810688545669,
        -0.7499419541890515,
        -0.7571166400994576,
        -0.7628405944367802,
        -0.7472215435229821,
        -0.7641747684428745,
        -0.7546022888746289,
        1.0,
        -0.761266510821476,
        -0.7491078597478846,
        -0.7550133012594671,
        -0.7588013437723575,
        -0.7607320308487175,
        -0.7512460591065799,
        -0.7572968332361056,
        -0.7230605278701765,
        -0.7515399757485343,
        -0.7497475297370833,
        -0.7602283359515418,
        -0.7588937743524853,
        -0.7453116993788631,
        -0.7602933881434539,
        -0.7450265870349551,
        -0.7648553497289343,
        -0.7521651165127845,
        -0.7515658950015016,
        -0.7646615652104849,
        -0.7518477153351946,
        -0.754483048622486,
        -0.7468944612875683
      ],
      [
        -0.7606573218654031,
        -0.7168004260982324,
        -0.7458885047607644,
        -0.6986175590427612,
        -0.6960270428164606,
        -0.6953542278611321,
        -0.7543898610193523,
        -0.756538557644898,
        -0.7073457140934702,
        -0.7372211000868165,
        -0.7542145139093359,
        -0.7459794842338263,
        -0.7479272169682128,
        -0.7604521944561035,
        -0.7229167072948557,
        -0.7418314473846229,
        -0.7600548329829768,
        -0.7503321931128222,
        -0.7315639431281453,
        -0.7309429737245832,
        -0.7546381012614296,
        -0.6640451795835627,
        -0.7442461734879979,
        -0.7190995263495767,
        -0.761266510821476,
        1.0,
        -0.7463016714500295,
        -0.7517586774958677,
        -0.7575996958243404,
        -0.6683356970757088,
        -0.7600517745207428,
        -0.7301883536846749,
        -0.758717590537061,
        -0.7347561809333407,
        -0.7551932862618864,
        -0.524184707895947,
        -0.7327959161622369,
        -0.751937874991393,
        -0.7623770443593829,
        -0.7434389911929455,
        -0.7542023084465221,
        -0.7476054309709055,
        -0.7306980591780499,
        -0.7645219107661874,
        -0.7190713303832046,
        -0.7131563475739913,
        -0.757939414749933
      ],
      [
        -0.7620410428901423,
        -0.7503934378728099,
        -0.7602861776434662,
        -0.7371406920907194,
        -0.7370070606641017,
        -0.7356913205380474,
        -0.7606426176627563,
        -0.7585827932086755,
        -0.738135921890928,
        -0.7476521665270659,
        -0.756503003551287,
        -0.7570379732306931,
        -0.7478894785274388,
        -0.7495341457175585,
        -0.7488769389064811,
        -0.7498074592235677,
        -0.5852243429113771,
        -0.7516125786437358,
        -0.7553432703170028,
        -0.7529001189650053,
        -0.7481183324970728,
        -0.7288867844310836,
        -0.7468501797875411,
        -0.7534140919631743,
        -0.7491078597478846,
        -0.7463016714500295,
        1.0,
        -0.7438229378783672,
        -0.7579556873278384,
        -0.7474199406301401,
        -0.7576252954018022,
        -0.7535541114897631,
        -0.7326489816992288,
        -0.751878913100211,
        -0.7155238516628643,
        -0.7533841715244456,
        -0.7576741323354401,
        -0.7507666771994906,
        -0.7579444599323568,
        -0.7480341597111362,
        -0.7602604789937024,
        -0.7548027305890126,
        -0.7527488170158956,
        -0.7631354809026589,
        -0.7353902324347696,
        -0.751152235332692,
        -0.7590402617146036
      ],
      [
        -0.7527588536054313,
        -0.7102675609910196,
        -0.7331558588095435,
        -0.7303943599006657,
        -0.7325148744965915,
        -0.72948922932235,
        -0.7485324563721825,
        -0.7535468371497794,
        -0.7345912619921704,
        -0.7564523680579822,
        -0.7261893692495964,
        -0.7461380389106519,
        -0.7201043106886058,
        -0.7502335428373167,
        -0.7260895812016526,
        -0.7208198070281301,
        -0.7295182494175313,
        -0.7344645220794942,
        -0.7336277984673678,
        -0.7493269552753192,
        -0.6400141490598974,
        -0.7283167852215167,
        -0.754551431412219,
        -0.629364117651273,
        -0.7550133012594671,
        -0.7517586774958677,
        -0.7438229378783672,
        1.0,
        -0.7533855170839827,
        -0.7274768410286454,
        -0.7516446131730479,
        -0.75043706825833,
        -0.7413079861885976,
        -0.7241901106867303,
        -0.7467841341812611,
        -0.7322539030680587,
        -0.7363211914061021,
        -0.661026694695827,
        -0.745789252830187,
        -0.7316711349163458,
        -0.7620039456944203,
        -0.7494646023494201,
        -0.7329947728194874,
        -0.7630266289029415,
        -0.7282424731396869,
        -0.7188515569051622,
        -0.7547660060967074
      ],
      [
        -0.760746175941674,
        -0.7365306652982166,
        -0.7421041088025617,
        -0.72921776602781,
        -0.7327234000039518,
        -0.7096877530552745,
        -0.7567395616558992,
        -0.7564293829889464,
        -0.7312581454109361,
        -0.753068644162433,
        -0.7456227801152413,
        -0.7528473812141283,
        -0.75089043274604,
        -0.7433790480522579,
        -0.74050874078605,
        -0.7467700474678592,
        -0.7422903787722761,
        -0.7492610275899253,
        -0.7386228801981691,
        -0.7411866872847273,
        -0.7604977778267094,
        -0.72201770739759,
        -0.7535537077244034,
        -0.7441571003597903,
        -0.7588013437723575,
        -0.7575996958243404,
        -0.7579556873278384,
        -0.7533855170839827,
        1.0,
        -0.7258138607383047,
        -0.75975170938967,
        -0.7335553389503044,
        -0.7488511935138854,
        -0.7428539423714442,
        -0.7511526441371295,
        -0.7523605751805251,
        -0.727044281124545,
        -0.7534334172439087,
        -0.7574330634225441,
        -0.7221783373947521,
        -0.7602761421884721,
        -0.7329095771982413,
        -0.7413242623025168,
        -0.7637156050353118,
        -0.7381477524895801,
        -0.741340904650938,
        -0.7589330030044152
      ],
      [
        -0.751645513505812,
        -0.6951103654577968,
        -0.6602117484714658,
        -0.6681008143491255,
        -0.67424019530956,
        -0.6135672174793871,
        -0.7372471180990383,
        -0.7413156140861918,
        -0.6785973629735118,
        -0.7308005609387344,
        -0.7125893160826688,
        -0.7242321491352417,
        -0.7293622696179993,
        -0.7548530792293309,
        -0.6900617427543575,
        -0.7215168946037318,
        -0.7460761404260308,
        -0.7060946114750262,
        -0.7061037360570559,
        -0.7124728470374082,
        -0.7317496690719542,
        -0.6406410808098915,
        -0.7503712137412503,
        -0.6682080968702535,
        -0.7607320308487175,
        -0.6683356970757088,
        -0.7474199406301401,
        -0.7274768410286454,
        -0.7258138607383047,
        1.0,
        -0.7461722626580419,
        -0.6189095942441687,
        -0.7440454156413153,
        -0.7050464894871931,
        -0.7518238365963614,
        -0.6412194656497663,
        -0.7119392994454314,
        -0.7263831571515234,
        -0.7472214590531627,
        -0.7079372167530076,
        -0.7478073687097423,
        -0.7355363266585444,
        -0.7078774626470141,
        -0.7617101120797953,
        -0.6801403173200113,
        -0.6733925516537282,
        -0.7475777971988053
      ],
      [
        -0.7172872722870808,
        -0.7538739475151726,
        -0.7574364051591582,
        -0.7436822123437463,
        -0.7463765332526586,
        -0.7495828930463707,
        -0.7613198067831625,
        -0.6940176585725712,
        -0.7373523855237003,
        -0.7632394250912741,
        -0.7125172896776661,
        -0.7576984303033598,
        -0.7247474967150529,
        -0.7217784002491856,
        -0.7325505576873165,
        -0.7528858493696755,
        -0.7328301306418095,
        -0.47809784052854565,
        -0.7465434765653003,
        -0.7527979556304217,
        -0.7335372516424083,
        -0.7415301770163054,
        -0.7605724060475826,
        -0.754681201931149,
        -0.7512460591065799,
        -0.7600517745207428,
        -0.7576252954018022,
        -0.7516446131730479,
        -0.75975170938967,
        -0.7461722626580419,
        1.0,
        -0.7515152021294595,
        -0.7349235454928729,
        -0.7513397249439253,
        -0.7446423115803922,
        -0.7493839599485412,
        -0.7418491118129333,
        -0.7568861650858452,
        -0.7518937927739386,
        -0.7414674254539646,
        -0.762033746854546,
        -0.759927842008227,
        -0.7500572960746537,
        -0.764241518084734,
        -0.7359309996654455,
        -0.746283951843645,
        -0.758291540241692
      ],
      [
        -0.742963611507313,
        -0.5771900973657307,
        -0.7397076117819912,
        -0.5319708063338859,
        -0.5556163387533603,
        -0.5480518539896866,
        -0.7353754437875474,
        -0.739707136807694,
        -0.5733967413046288,
        -0.7212086606069654,
        -0.739322652530106,
        -0.7435872851115779,
        -0.7393671834407042,
        -0.7511271567643347,
        -0.7027381962403825,
        -0.7239796027132406,
        -0.7423957790606559,
        -0.7274546528411743,
        -0.7143116010700499,
        -0.5740978713341576,
        -0.7538362445503557,
        -0.520097731613277,
        -0.7528505387409142,
        -0.7310336847729973,
        -0.7572968332361056,
        -0.7301883536846749,
        -0.7535541114897631,
        -0.75043706825833,
        -0.7335553389503044,
        -0.6189095942441687,
        -0.7515152021294595,
        1.0,
        -0.7446391172018542,
        -0.6994097745851624,
        -0.7534856028751197,
        -0.7298254308973469,
        -0.7361522976073497,
        -0.750009682823357,
        -0.7499625919752926,
        -0.6928620465077068,
        -0.7560754149599518,
        -0.7290369313117557,
        -0.7114847216074389,
        -0.7641052960145281,
        -0.5449426269417539,
        -0.5708922784166737,
        -0.7427733526571157
      ],
      [
        -0.742677178591544,
        -0.7144185391417945,
        -0.7432234953897094,
        -0.7307047907341738,
        -0.7330162195401725,
        -0.738859345415295,
        -0.7540613767031028,
        -0.7505909457606099,
        -0.7087958090385551,
        -0.7553382635694696,
        -0.7439738478818083,
        -0.750400498147588,
        -0.7360827052394121,
        -0.3352321672537405,
        -0.732959809141299,
        -0.7212330409750011,
        -0.6580801829977437,
        -0.7252619631647271,
        -0.7444875233029202,
        -0.7469817778847894,
        -0.7422343355638704,
        -0.7291474583134938,
        -0.7597525161228074,
        -0.7391400497403149,
        -0.7230605278701765,
        -0.758717590537061,
        -0.7326489816992288,
        -0.7413079861885976,
        -0.7488511935138854,
        -0.7440454156413153,
        -0.7349235454928729,
        -0.7446391172018542,
        1.0,
        -0.7410859340243183,
        -0.6230132094261898,
        -0.7447117434300882,
        -0.7153210864914219,
        -0.7113889697090858,
        -0.7589413034696055,
        -0.7072511665436203,
        -0.7630837667037506,
        -0.7324573140956385,
        -0.7399102988428048,
        -0.7654868467762838,
        -0.7308486653732483,
        -0.7411367678230847,
        -0.7424898199742278
      ],
      [
        -0.7414014393635384,
        -0.694794408998741,
        -0.6857176195976014,
        -0.6103452756023906,
        -0.597479320879022,
        -0.6230561000844211,
        -0.7320475279182364,
        -0.7409244841749156,
        -0.6292156341594031,
        -0.7275496843466798,
        -0.7250805418450836,
        -0.7387115881832711,
        -0.7312700188770709,
        -0.7435431067537994,
        -0.6763828400843693,
        -0.7386872158366232,
        -0.7351666995591208,
        -0.7043475552334354,
        -0.7017191351336511,
        -0.6885794915049968,
        -0.7391087608206282,
        -0.5796615655859698,
        -0.7532986079204484,
        -0.726332403399315,
        -0.7515399757485343,
        -0.7347561809333407,
        -0.751878913100211,
        -0.7241901106867303,
        -0.7428539423714442,
        -0.7050464894871931,
        -0.7513397249439253,
        -0.6994097745851624,
        -0.7410859340243183,
        1.0,
        -0.7490591106245503,
        -0.7198693171339543,
        -0.7311525275322599,
        -0.7378648480620356,
        -0.7491351262720654,
        -0.69784591689969,
        -0.7547195522377199,
        -0.7259687871612344,
        -0.6656349609511619,
        -0.7644437211867094,
        -0.6217947048092931,
        -0.6826698129878443,
        -0.7443809502589904
      ],
      [
        -0.7534610851283527,
        -0.7536457709038702,
        -0.7505728474622528,
        -0.7402303339750871,
        -0.7402891788920287,
        -0.7494813887131229,
        -0.7595817742860762,
        -0.7465533556417987,
        -0.7311709880396431,
        -0.7601066035209603,
        -0.7496017084151234,
        -0.7102623182138729,
        -0.7420528814752876,
        -0.6597378114777691,
        -0.7464785114801484,
        -0.7385774464376633,
        -0.7227768142552766,
        -0.7333916992594829,
        -0.7394693968831709,
        -0.7539778430558106,
        -0.7423243592013513,
        -0.7390443841378536,
        -0.7571667672285101,
        -0.7443846939751363,
        -0.7497475297370833,
        -0.7551932862618864,
        -0.7155238516628643,
        -0.7467841341812611,
        -0.7511526441371295,
        -0.7518238365963614,
        -0.7446423115803922,
        -0.7534856028751197,
        -0.6230132094261898,
        -0.7490591106245503,
        1.0,
        -0.7513868788216036,
        -0.7111715565174945,
        -0.7084166712044765,
        -0.7556693600983334,
        -0.6686649572428017,
        -0.7638150468924932,
        -0.7357825900890438,
        -0.7466660184341023,
        -0.7650138560216386,
        -0.7429850397027947,
        -0.7420458414493222,
        -0.7264076787564071
      ],
      [
        -0.753306417975108,
        -0.7261711866445026,
        -0.7021788450298834,
        -0.6966175850580238,
        -0.7070723088700435,
        -0.7028969132050258,
        -0.7322351189064634,
        -0.7412686750159958,
        -0.7127245991845668,
        -0.7424183032420688,
        -0.7358171820185578,
        -0.7263647269478739,
        -0.7029376523046915,
        -0.7553037600508605,
        -0.7001536750527666,
        -0.7227124550123694,
        -0.747159776377412,
        -0.7148024745419277,
        -0.7189378933527988,
        -0.7261805720237231,
        -0.7353451114176501,
        -0.6913464769698052,
        -0.7380363984730144,
        -0.7162487783916144,
        -0.7602283359515418,
        -0.524184707895947,
        -0.7533841715244456,
        -0.7322539030680587,
        -0.7523605751805251,
        -0.6412194656497663,
        -0.7493839599485412,
        -0.7298254308973469,
        -0.7447117434300882,
        -0.7198693171339543,
        -0.7513868788216036,
        1.0,
        -0.7211300702879065,
        -0.7375677584119846,
        -0.7542570916210161,
        -0.7295654499443723,
        -0.7527496432042255,
        -0.7150498036449611,
        -0.718216473672523,
        -0.7619166067286389,
        -0.7076504557361701,
        -0.7107449122673732,
        -0.7551002388201995
      ],
      [
        -0.7499757471920913,
        -0.7319497084635082,
        -0.7463564405631016,
        -0.7145104990412242,
        -0.7163097086290936,
        -0.6996336794154006,
        -0.7506587140541421,
        -0.7477310642948753,
        -0.7106675739900655,
        -0.7481652851364291,
        -0.7291815520155107,
        -0.7431037648176146,
        -0.7340374977124788,
        -0.7354920587776005,
        -0.7264072714375014,
        -0.714557280526626,
        -0.7450458304647556,
        -0.7222504418591161,
        -0.7327102700771574,
        -0.7366953043562552,
        -0.7478591403805761,
        -0.6953250437493061,
        -0.7475653546501513,
        -0.7233620866251658,
        -0.7588937743524853,
        -0.7327959161622369,
        -0.7576741323354401,
        -0.7363211914061021,
        -0.727044281124545,
        -0.7119392994454314,
        -0.7418491118129333,
        -0.7361522976073497,
        -0.7153210864914219,
        -0.7311525275322599,
        -0.7111715565174945,
        -0.7211300702879065,
        1.0,
        -0.7444967363119898,
        -0.7490080678484545,
        -0.7169605972983197,
        -0.7624548930731867,
        -0.7455949008509517,
        -0.7300129817142886,
        -0.7612378816356373,
        -0.7208947131843466,
        -0.7112475970725495,
        -0.7478701579727598
      ],
      [
        -0.7398249678335643,
        -0.7328143979843238,
        -0.7269316667558494,
        -0.7347345142116344,
        -0.7395266857769075,
        -0.7285434419405714,
        -0.7535689718859784,
        -0.7580499859526703,
        -0.7371865429875465,
        -0.7506244650291416,
        -0.7418017088428429,
        -0.746613272756717,
        -0.7414917143252796,
        -0.7333472485596249,
        -0.7344940130824111,
        -0.7474337333308121,
        -0.7439952436556427,
        -0.7430112697083175,
        -0.741063221210952,
        -0.7470074778408007,
        -0.7381387698569695,
        -0.7326829968318863,
        -0.7558878159845566,
        -0.7150830647767428,
        -0.7453116993788631,
        -0.751937874991393,
        -0.7507666771994906,
        -0.661026694695827,
        -0.7534334172439087,
        -0.7263831571515234,
        -0.7568861650858452,
        -0.750009682823357,
        -0.7113889697090858,
        -0.7378648480620356,
        -0.7084166712044765,
        -0.7375677584119846,
        -0.7444967363119898,
        1.0,
        -0.7578850050111624,
        -0.739964780638666,
        -0.763371310237117,
        -0.7507450035893902,
        -0.7459754533055019,
        -0.7628772639426395,
        -0.7369639809535592,
        -0.7308883643395987,
        -0.7187435255952855
      ],
      [
        -0.7550345044275172,
        -0.7528587282721818,
        -0.7560581873132353,
        -0.7471175721279146,
        -0.7461661412987541,
        -0.7520057421951659,
        -0.7596020578220366,
        -0.7311291270499329,
        -0.752192502718924,
        -0.7624749153350707,
        -0.7317511577453256,
        -0.7409193448097102,
        -0.6590880824616687,
        -0.758235425300382,
        -0.7475351376914359,
        -0.750396703678767,
        -0.7445401671225789,
        -0.7396185028999416,
        -0.7253341840899792,
        -0.7488377755476683,
        -0.7556335530521576,
        -0.7466710928384525,
        -0.758930548522657,
        -0.7507302625880723,
        -0.7602933881434539,
        -0.7623770443593829,
        -0.7579444599323568,
        -0.745789252830187,
        -0.7574330634225441,
        -0.7472214590531627,
        -0.7518937927739386,
        -0.7499625919752926,
        -0.7589413034696055,
        -0.7491351262720654,
        -0.7556693600983334,
        -0.7542570916210161,
        -0.7490080678484545,
        -0.7578850050111624,
        1.0,
        -0.7318037539213127,
        -0.7621172270882604,
        -0.7590575299071766,
        -0.7469201823194993,
        -0.7604674852894009,
        -0.7488800353094132,
        -0.7476639767491995,
        -0.7538058614841725
      ],
      [
        -0.7136578981694948,
        -0.7032219461277194,
        -0.6894157211127291,
        -0.6724308991713811,
        -0.6538811910951233,
        -0.6778670072251649,
        -0.7318840045393237,
        -0.7255295083999886,
        -0.6680355128148499,
        -0.7317755620249391,
        -0.7140660965058783,
        -0.7239772022490005,
        -0.7054085394254335,
        -0.6432205759351821,
        -0.6876349379609399,
        -0.6996647032749668,
        -0.7221107600555006,
        -0.7048126516627989,
        -0.43999976135110735,
        -0.7018389130522152,
        -0.7381373793242842,
        -0.6394983619408363,
        -0.7496356129225576,
        -0.7202711404822426,
        -0.7450265870349551,
        -0.7434389911929455,
        -0.7480341597111362,
        -0.7316711349163458,
        -0.7221783373947521,
        -0.7079372167530076,
        -0.7414674254539646,
        -0.6928620465077068,
        -0.7072511665436203,
        -0.69784591689969,
        -0.6686649572428017,
        -0.7295654499443723,
        -0.7169605972983197,
        -0.739964780638666,
        -0.7318037539213127,
        1.0,
        -0.7492198060378323,
        -0.7255477165064912,
        -0.7186010747573655,
        -0.7614442910267605,
        -0.6750669457137776,
        -0.7026746008908784,
        -0.7143210974893861
      ],
      [
        -0.7619855836801233,
        -0.7587903566204188,
        -0.7597257034420781,
        -0.7518676478504958,
        -0.752661542601156,
        -0.7506568599503973,
        -0.7625294564872753,
        -0.7572485137417955,
        -0.7545179927728934,
        -0.7585642015244121,
        -0.735695280265284,
        -0.7618162230807026,
        -0.7600239381167797,
        -0.7623679134509593,
        -0.7479717416624502,
        -0.7618330676407004,
        -0.763374382775547,
        -0.7594630717663619,
        -0.745413096029337,
        -0.7603420399765446,
        -0.760769378239976,
        -0.7493599201591301,
        -0.764387452627902,
        -0.7573668305977764,
        -0.7648553497289343,
        -0.7542023084465221,
        -0.7602604789937024,
        -0.7620039456944203,
        -0.7602761421884721,
        -0.7478073687097423,
        -0.762033746854546,
        -0.7560754149599518,
        -0.7630837667037506,
        -0.7547195522377199,
        -0.7638150468924932,
        -0.7527496432042255,
        -0.7624548930731867,
        -0.763371310237117,
        -0.7621172270882604,
        -0.7492198060378323,
        1.0,
        -0.7566082645092448,
        -0.7510792646114867,
        -0.76511812440568,
        -0.7526049649598034,
        -0.7558316119350796,
        -0.7640446260930802
      ],
      [
        -0.7509027860821764,
        -0.7375049318883997,
        -0.7474530763967713,
        -0.7231930740530316,
        -0.7187481584627518,
        -0.7277188184853468,
        -0.6913603178391738,
        -0.7561918263940841,
        -0.7215164281640863,
        -0.724530690427694,
        -0.7494106968741299,
        -0.7483604956556607,
        -0.738337660405437,
        -0.7405750687780035,
        -0.7308380842556097,
        -0.7273386600789397,
        -0.7385239928104287,
        -0.750345720250041,
        -0.7331820603459038,
        -0.711916674035399,
        -0.7486337095828878,
        -0.7120950180470911,
        -0.7553096060248913,
        -0.7204361534544403,
        -0.7521651165127845,
        -0.7476054309709055,
        -0.7548027305890126,
        -0.7494646023494201,
        -0.7329095771982413,
        -0.7355363266585444,
        -0.759927842008227,
        -0.7290369313117557,
        -0.7324573140956385,
        -0.7259687871612344,
        -0.7357825900890438,
        -0.7150498036449611,
        -0.7455949008509517,
        -0.7507450035893902,
        -0.7590575299071766,
        -0.7255477165064912,
        -0.7566082645092448,
        1.0,
        -0.7395893217762841,
        -0.7652161015807312,
        -0.7239318042556564,
        -0.7301525555743846,
        -0.7545700901304129
      ],
      [
        -0.7364075139712836,
        -0.7216201280133545,
        -0.7277199097294414,
        -0.6648819812258788,
        -0.6771675008328355,
        -0.6700638592609276,
        -0.7301348376633774,
        -0.7304590788810841,
        -0.6775283860985977,
        -0.7247875247022795,
        -0.7136597977643111,
        -0.7462306002482973,
        -0.7197742117597932,
        -0.7395736763939373,
        -0.7062205856048573,
        -0.7330284725976235,
        -0.7344984936086233,
        -0.7094074192567218,
        -0.7193218610032285,
        -0.7167623840908331,
        -0.739083312137206,
        -0.6564130095718658,
        -0.7559467922780234,
        -0.7338920723947504,
        -0.7515658950015016,
        -0.7306980591780499,
        -0.7527488170158956,
        -0.7329947728194874,
        -0.7413242623025168,
        -0.7078774626470141,
        -0.7500572960746537,
        -0.7114847216074389,
        -0.7399102988428048,
        -0.6656349609511619,
        -0.7466660184341023,
        -0.718216473672523,
        -0.7300129817142886,
        -0.7459754533055019,
        -0.7469201823194993,
        -0.7186010747573655,
        -0.7510792646114867,
        -0.7395893217762841,
        1.0,
        -0.7622572171988847,
        -0.6663982147182242,
        -0.7071526266861528,
        -0.7535707124960456
      ],
      [
        -0.7654348645043507,
        -0.7648485939026233,
        -0.7650125196806121,
        -0.761133692010928,
        -0.7623218843850709,
        -0.7615278931789611,
        -0.7646315536915762,
        -0.7607553210519736,
        -0.7630869962400066,
        -0.763951175926324,
        -0.7605661127632125,
        -0.7650284064786226,
        -0.7577865453218041,
        -0.7650980254795203,
        -0.7609115174032148,
        -0.7615414498004607,
        -0.7558387099422176,
        -0.7608659514630682,
        -0.7628010634561944,
        -0.764229398188085,
        -0.7637700127237153,
        -0.7605452953362324,
        -0.7655764556207767,
        -0.7626928144120217,
        -0.7646615652104849,
        -0.7645219107661874,
        -0.7631354809026589,
        -0.7630266289029415,
        -0.7637156050353118,
        -0.7617101120797953,
        -0.764241518084734,
        -0.7641052960145281,
        -0.7654868467762838,
        -0.7644437211867094,
        -0.7650138560216386,
        -0.7619166067286389,
        -0.7612378816356373,
        -0.7628772639426395,
        -0.7604674852894009,
        -0.7614442910267605,
        -0.76511812440568,
        -0.7652161015807312,
        -0.7622572171988847,
        1.0,
        -0.7634539455784208,
        -0.7644458194947346,
        -0.761593186168938
      ],
      [
        -0.7182195874350603,
        -0.5678060070841744,
        -0.6877526930548065,
        -0.21566106190810314,
        -0.4602966441949009,
        -0.45808857772705086,
        -0.724562407331808,
        -0.7173401498999651,
        -0.4983268801799229,
        -0.6887039860667343,
        -0.6877151832308727,
        -0.7297578092418808,
        -0.7141018783045188,
        -0.731429239752271,
        -0.6297540986040693,
        -0.7153925000258781,
        -0.7298817721296778,
        -0.6592693813768328,
        -0.6780116608507726,
        -0.5363846436483369,
        -0.7395563794953071,
        -0.33855664407580144,
        -0.7495059181077732,
        -0.7138622826366564,
        -0.7518477153351946,
        -0.7190713303832046,
        -0.7353902324347696,
        -0.7282424731396869,
        -0.7381477524895801,
        -0.6801403173200113,
        -0.7359309996654455,
        -0.5449426269417539,
        -0.7308486653732483,
        -0.6217947048092931,
        -0.7429850397027947,
        -0.7076504557361701,
        -0.7208947131843466,
        -0.7369639809535592,
        -0.7488800353094132,
        -0.6750669457137776,
        -0.7526049649598034,
        -0.7239318042556564,
        -0.6663982147182242,
        -0.7634539455784208,
        1.0,
        -0.5621740473958617,
        -0.7410682962497547
      ],
      [
        -0.7326143597967352,
        -0.5964632209304586,
        -0.7056883073610815,
        -0.5452296014384508,
        -0.5610971958417266,
        -0.551561241073854,
        -0.7168452922837414,
        -0.7187307424923572,
        -0.5841802616898971,
        -0.7198717752453938,
        -0.7176790730147107,
        -0.7370037485918024,
        -0.7291777287520886,
        -0.7399154891664019,
        -0.6800166412463997,
        -0.7206144797296999,
        -0.7387517424810104,
        -0.7048288953243769,
        -0.6994903378958044,
        -0.6140908372553373,
        -0.7286461740879691,
        -0.5209139914783094,
        -0.7503388631497184,
        -0.4668714609171031,
        -0.754483048622486,
        -0.7131563475739913,
        -0.751152235332692,
        -0.7188515569051622,
        -0.741340904650938,
        -0.6733925516537282,
        -0.746283951843645,
        -0.5708922784166737,
        -0.7411367678230847,
        -0.6826698129878443,
        -0.7420458414493222,
        -0.7107449122673732,
        -0.7112475970725495,
        -0.7308883643395987,
        -0.7476639767491995,
        -0.7026746008908784,
        -0.7558316119350796,
        -0.7301525555743846,
        -0.7071526266861528,
        -0.7644458194947346,
        -0.5621740473958617,
        1.0,
        -0.7461093491071064
      ],
      [
        -0.7591945599346295,
        -0.7487979124716888,
        -0.7425714671676418,
        -0.7361910997657322,
        -0.7406686798003013,
        -0.7359655349756068,
        -0.7543266163485677,
        -0.7584918187016252,
        -0.7341888498207674,
        -0.7526773284238031,
        -0.7502730784788663,
        -0.7417589588620986,
        -0.7513850952961962,
        -0.7525864318567815,
        -0.7496528017606394,
        -0.7527268032936564,
        -0.7530820780629683,
        -0.7525146032392076,
        -0.7091264009266925,
        -0.7440284300769664,
        -0.758197692035568,
        -0.7310578636702032,
        -0.7632909542981876,
        -0.7497390105260786,
        -0.7468944612875683,
        -0.757939414749933,
        -0.7590402617146036,
        -0.7547660060967074,
        -0.7589330030044152,
        -0.7475777971988053,
        -0.758291540241692,
        -0.7427733526571157,
        -0.7424898199742278,
        -0.7443809502589904,
        -0.7264076787564071,
        -0.7551002388201995,
        -0.7478701579727598,
        -0.7187435255952855,
        -0.7538058614841725,
        -0.7143210974893861,
        -0.7640446260930802,
        -0.7545700901304129,
        -0.7535707124960456,
        -0.761593186168938,
        -0.7410682962497547,
        -0.7461093491071064,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        45,
        4,
        9,
        1,
        3,
        2,
        10,
        18,
        5,
        6,
        3,
        9,
        19,
        3,
        5,
        1,
        12,
        13,
        8,
        1,
        14,
        0,
        0,
        10,
        3,
        0,
        1,
        9,
        4,
        18,
        2,
        3,
        3,
        0,
        11,
        6,
        11,
        2,
        1,
        4,
        0,
        7,
        1,
        1,
        2,
        1,
        2
      ],
      "2020-02": [
        40,
        6,
        10,
        1,
        1,
        2,
        19,
        29,
        5,
        11,
        6,
        15,
        23,
        1,
        9,
        8,
        11,
        14,
        11,
        2,
        9,
        0,
        1,
        12,
        6,
        0,
        1,
        13,
        0,
        22,
        6,
        1,
        7,
        0,
        9,
        6,
        9,
        5,
        1,
        5,
        1,
        7,
        1,
        0,
        6,
        9,
        1
      ],
      "2020-03": [
        42,
        9,
        10,
        0,
        3,
        2,
        15,
        30,
        8,
        6,
        4,
        12,
        23,
        3,
        8,
        3,
        12,
        19,
        11,
        5,
        19,
        0,
        3,
        17,
        7,
        0,
        0,
        7,
        3,
        12,
        3,
        2,
        9,
        0,
        9,
        5,
        5,
        3,
        5,
        3,
        0,
        4,
        6,
        1,
        4,
        5,
        1
      ],
      "2020-04": [
        67,
        8,
        16,
        0,
        12,
        2,
        37,
        92,
        8,
        14,
        9,
        37,
        52,
        10,
        26,
        7,
        36,
        42,
        43,
        3,
        36,
        0,
        4,
        29,
        4,
        0,
        1,
        37,
        9,
        57,
        14,
        2,
        23,
        2,
        22,
        16,
        19,
        10,
        8,
        16,
        1,
        8,
        10,
        2,
        11,
        17,
        10
      ],
      "2020-05": [
        135,
        20,
        6,
        2,
        11,
        8,
        25,
        47,
        25,
        9,
        23,
        25,
        39,
        14,
        20,
        7,
        28,
        33,
        24,
        8,
        28,
        0,
        4,
        17,
        14,
        1,
        2,
        22,
        15,
        44,
        21,
        4,
        32,
        3,
        15,
        14,
        8,
        13,
        15,
        8,
        6,
        11,
        9,
        3,
        17,
        13,
        5
      ],
      "2020-06": [
        81,
        3,
        17,
        3,
        3,
        5,
        16,
        33,
        8,
        9,
        11,
        9,
        32,
        2,
        15,
        5,
        23,
        23,
        16,
        2,
        14,
        2,
        1,
        12,
        4,
        0,
        4,
        16,
        6,
        25,
        6,
        0,
        22,
        3,
        3,
        6,
        6,
        8,
        5,
        5,
        0,
        9,
        7,
        1,
        4,
        4,
        2
      ],
      "2020-07": [
        44,
        13,
        14,
        2,
        1,
        3,
        4,
        20,
        7,
        9,
        5,
        13,
        21,
        7,
        8,
        7,
        29,
        16,
        7,
        1,
        16,
        0,
        0,
        9,
        6,
        0,
        2,
        6,
        3,
        23,
        8,
        1,
        24,
        4,
        8,
        13,
        7,
        3,
        4,
        9,
        1,
        11,
        2,
        1,
        6,
        5,
        2
      ],
      "2020-08": [
        70,
        7,
        5,
        1,
        4,
        0,
        14,
        18,
        3,
        5,
        6,
        6,
        22,
        3,
        10,
        5,
        21,
        15,
        8,
        1,
        10,
        0,
        0,
        14,
        6,
        0,
        2,
        10,
        0,
        13,
        4,
        2,
        24,
        1,
        14,
        10,
        8,
        6,
        5,
        10,
        1,
        5,
        2,
        1,
        2,
        5,
        5
      ],
      "2020-09": [
        64,
        5,
        11,
        0,
        6,
        5,
        36,
        42,
        10,
        3,
        11,
        15,
        30,
        7,
        14,
        5,
        19,
        17,
        18,
        2,
        16,
        0,
        3,
        25,
        6,
        0,
        1,
        38,
        6,
        32,
        5,
        2,
        19,
        1,
        10,
        11,
        9,
        9,
        3,
        9,
        3,
        10,
        7,
        1,
        8,
        11,
        7
      ],
      "2020-10": [
        158,
        17,
        16,
        4,
        12,
        9,
        48,
        101,
        28,
        14,
        36,
        44,
        74,
        11,
        27,
        15,
        56,
        83,
        38,
        4,
        43,
        1,
        6,
        50,
        9,
        0,
        5,
        45,
        17,
        76,
        11,
        10,
        31,
        5,
        16,
        17,
        27,
        25,
        14,
        20,
        8,
        16,
        10,
        0,
        19,
        18,
        15
      ],
      "2020-11": [
        97,
        13,
        12,
        3,
        5,
        3,
        19,
        44,
        13,
        4,
        9,
        18,
        36,
        8,
        12,
        2,
        23,
        32,
        14,
        2,
        19,
        0,
        4,
        17,
        13,
        0,
        3,
        24,
        1,
        36,
        9,
        3,
        16,
        2,
        14,
        4,
        13,
        8,
        5,
        5,
        2,
        6,
        4,
        0,
        9,
        6,
        4
      ],
      "2020-12": [
        71,
        8,
        13,
        2,
        9,
        6,
        23,
        36,
        5,
        5,
        9,
        20,
        35,
        13,
        15,
        7,
        22,
        24,
        21,
        0,
        21,
        1,
        3,
        20,
        8,
        0,
        0,
        23,
        8,
        35,
        10,
        5,
        24,
        1,
        8,
        10,
        11,
        15,
        5,
        12,
        7,
        6,
        5,
        2,
        13,
        10,
        3
      ],
      "2021-01": [
        46,
        8,
        6,
        3,
        7,
        3,
        8,
        15,
        9,
        4,
        10,
        8,
        28,
        8,
        16,
        10,
        15,
        27,
        21,
        0,
        23,
        0,
        1,
        15,
        9,
        0,
        1,
        16,
        4,
        36,
        7,
        3,
        16,
        2,
        24,
        10,
        6,
        6,
        4,
        7,
        2,
        14,
        4,
        2,
        12,
        4,
        2
      ],
      "2021-02": [
        78,
        8,
        13,
        0,
        1,
        2,
        15,
        24,
        8,
        2,
        2,
        4,
        25,
        9,
        12,
        10,
        15,
        21,
        18,
        0,
        7,
        5,
        4,
        8,
        5,
        1,
        2,
        16,
        2,
        17,
        8,
        4,
        7,
        5,
        17,
        9,
        7,
        7,
        2,
        4,
        2,
        4,
        6,
        3,
        5,
        8,
        3
      ],
      "2021-03": [
        71,
        1,
        14,
        0,
        4,
        2,
        16,
        45,
        10,
        6,
        6,
        15,
        24,
        11,
        26,
        4,
        25,
        25,
        14,
        1,
        12,
        3,
        5,
        11,
        12,
        0,
        1,
        21,
        9,
        26,
        9,
        3,
        19,
        5,
        11,
        7,
        13,
        6,
        7,
        5,
        4,
        4,
        6,
        2,
        6,
        5,
        7
      ],
      "2021-04": [
        137,
        13,
        18,
        1,
        11,
        7,
        22,
        65,
        16,
        6,
        21,
        35,
        57,
        10,
        28,
        10,
        23,
        46,
        21,
        3,
        25,
        4,
        10,
        26,
        16,
        1,
        1,
        34,
        11,
        51,
        9,
        9,
        27,
        9,
        19,
        16,
        22,
        19,
        4,
        15,
        4,
        9,
        16,
        1,
        11,
        17,
        9
      ],
      "2021-05": [
        78,
        9,
        7,
        3,
        7,
        11,
        31,
        52,
        12,
        4,
        11,
        30,
        27,
        12,
        24,
        5,
        22,
        32,
        16,
        0,
        20,
        1,
        6,
        21,
        12,
        1,
        3,
        23,
        10,
        39,
        6,
        7,
        20,
        2,
        14,
        13,
        13,
        18,
        6,
        8,
        1,
        8,
        9,
        0,
        11,
        11,
        7
      ],
      "2021-06": [
        116,
        9,
        22,
        6,
        8,
        6,
        39,
        57,
        19,
        10,
        18,
        26,
        50,
        12,
        27,
        14,
        41,
        43,
        14,
        2,
        31,
        0,
        4,
        30,
        16,
        1,
        3,
        35,
        11,
        48,
        11,
        10,
        17,
        5,
        7,
        9,
        28,
        23,
        6,
        14,
        9,
        6,
        3,
        2,
        11,
        10,
        3
      ],
      "2021-07": [
        70,
        6,
        13,
        0,
        7,
        4,
        18,
        40,
        11,
        7,
        9,
        4,
        15,
        6,
        22,
        7,
        18,
        19,
        10,
        1,
        13,
        0,
        5,
        12,
        11,
        0,
        3,
        11,
        3,
        25,
        4,
        2,
        11,
        6,
        13,
        3,
        7,
        7,
        8,
        6,
        2,
        5,
        2,
        2,
        6,
        2,
        6
      ],
      "2021-08": [
        47,
        13,
        20,
        1,
        6,
        6,
        19,
        23,
        9,
        5,
        6,
        21,
        23,
        17,
        16,
        4,
        21,
        11,
        23,
        0,
        22,
        1,
        6,
        9,
        7,
        0,
        2,
        18,
        8,
        24,
        6,
        5,
        23,
        8,
        13,
        12,
        12,
        10,
        3,
        12,
        3,
        6,
        2,
        1,
        9,
        2,
        4
      ],
      "2021-09": [
        81,
        10,
        21,
        2,
        17,
        13,
        52,
        92,
        31,
        10,
        23,
        37,
        46,
        15,
        23,
        15,
        37,
        70,
        37,
        7,
        41,
        2,
        9,
        31,
        16,
        2,
        7,
        42,
        10,
        69,
        12,
        9,
        22,
        15,
        18,
        12,
        34,
        25,
        16,
        11,
        6,
        12,
        13,
        0,
        17,
        17,
        6
      ],
      "2021-10": [
        141,
        8,
        15,
        0,
        12,
        10,
        32,
        44,
        20,
        8,
        16,
        35,
        41,
        10,
        31,
        12,
        28,
        38,
        16,
        5,
        15,
        3,
        11,
        22,
        9,
        0,
        8,
        19,
        7,
        39,
        7,
        5,
        27,
        16,
        14,
        14,
        17,
        16,
        3,
        15,
        2,
        9,
        13,
        4,
        17,
        11,
        4
      ],
      "2021-11": [
        67,
        8,
        19,
        1,
        7,
        3,
        18,
        18,
        5,
        8,
        6,
        12,
        22,
        13,
        16,
        7,
        32,
        17,
        7,
        1,
        13,
        1,
        2,
        15,
        3,
        1,
        1,
        13,
        5,
        23,
        7,
        2,
        18,
        6,
        15,
        6,
        7,
        15,
        2,
        13,
        1,
        3,
        7,
        1,
        6,
        10,
        6
      ],
      "2021-12": [
        60,
        7,
        31,
        1,
        7,
        5,
        17,
        33,
        15,
        3,
        12,
        27,
        20,
        11,
        20,
        13,
        24,
        21,
        19,
        3,
        22,
        4,
        11,
        21,
        10,
        0,
        1,
        15,
        8,
        38,
        5,
        5,
        10,
        6,
        10,
        18,
        12,
        11,
        2,
        11,
        2,
        10,
        8,
        1,
        7,
        18,
        8
      ],
      "2022-01": [
        52,
        9,
        13,
        0,
        9,
        5,
        10,
        17,
        10,
        7,
        7,
        8,
        18,
        12,
        14,
        9,
        21,
        22,
        12,
        4,
        10,
        1,
        7,
        13,
        8,
        1,
        3,
        12,
        6,
        27,
        7,
        6,
        15,
        13,
        13,
        12,
        8,
        7,
        4,
        5,
        2,
        9,
        4,
        1,
        9,
        5,
        4
      ],
      "2022-02": [
        74,
        5,
        12,
        1,
        5,
        2,
        14,
        25,
        10,
        5,
        7,
        9,
        27,
        11,
        17,
        6,
        14,
        19,
        15,
        0,
        10,
        3,
        4,
        13,
        5,
        0,
        0,
        10,
        5,
        21,
        2,
        1,
        13,
        10,
        8,
        8,
        10,
        8,
        5,
        11,
        1,
        6,
        5,
        4,
        11,
        12,
        8
      ],
      "2022-03": [
        135,
        12,
        35,
        1,
        17,
        11,
        29,
        49,
        13,
        17,
        7,
        28,
        40,
        13,
        25,
        8,
        16,
        46,
        23,
        4,
        28,
        6,
        11,
        17,
        19,
        2,
        4,
        31,
        5,
        36,
        8,
        7,
        12,
        24,
        16,
        10,
        12,
        18,
        7,
        15,
        5,
        6,
        13,
        2,
        20,
        11,
        8
      ],
      "2022-04": [
        116,
        11,
        22,
        2,
        10,
        12,
        38,
        41,
        16,
        8,
        14,
        28,
        28,
        16,
        41,
        8,
        30,
        48,
        22,
        3,
        34,
        3,
        6,
        21,
        8,
        0,
        5,
        23,
        15,
        41,
        5,
        7,
        32,
        20,
        19,
        18,
        15,
        19,
        8,
        12,
        2,
        8,
        11,
        2,
        12,
        19,
        8
      ],
      "2022-05": [
        94,
        7,
        22,
        3,
        17,
        15,
        36,
        66,
        29,
        12,
        12,
        27,
        17,
        9,
        29,
        10,
        29,
        53,
        36,
        3,
        29,
        7,
        5,
        36,
        22,
        0,
        3,
        46,
        20,
        52,
        11,
        12,
        22,
        40,
        20,
        22,
        17,
        19,
        8,
        20,
        4,
        5,
        15,
        2,
        25,
        16,
        5
      ],
      "2022-06": [
        77,
        6,
        15,
        2,
        8,
        8,
        19,
        24,
        13,
        7,
        13,
        10,
        21,
        14,
        14,
        7,
        18,
        26,
        23,
        4,
        11,
        6,
        4,
        22,
        10,
        1,
        4,
        10,
        7,
        24,
        6,
        6,
        14,
        12,
        10,
        10,
        9,
        10,
        4,
        15,
        1,
        7,
        6,
        2,
        13,
        8,
        7
      ],
      "2022-07": [
        68,
        5,
        20,
        3,
        6,
        4,
        24,
        17,
        7,
        6,
        3,
        9,
        13,
        2,
        13,
        5,
        17,
        20,
        9,
        0,
        13,
        3,
        3,
        16,
        14,
        2,
        2,
        17,
        6,
        23,
        1,
        5,
        14,
        5,
        14,
        8,
        7,
        3,
        1,
        1,
        0,
        6,
        6,
        2,
        6,
        10,
        3
      ],
      "2022-08": [
        44,
        11,
        11,
        3,
        7,
        5,
        15,
        12,
        14,
        5,
        1,
        17,
        15,
        4,
        17,
        6,
        20,
        20,
        11,
        3,
        25,
        4,
        4,
        10,
        8,
        0,
        0,
        17,
        7,
        20,
        8,
        1,
        7,
        9,
        7,
        13,
        9,
        7,
        2,
        6,
        5,
        8,
        4,
        3,
        7,
        8,
        5
      ],
      "2022-09": [
        72,
        12,
        19,
        3,
        3,
        4,
        24,
        42,
        16,
        13,
        9,
        21,
        17,
        9,
        16,
        7,
        16,
        26,
        17,
        4,
        17,
        4,
        8,
        18,
        12,
        1,
        0,
        13,
        13,
        26,
        2,
        1,
        17,
        22,
        18,
        11,
        14,
        16,
        4,
        11,
        3,
        8,
        8,
        1,
        9,
        9,
        10
      ],
      "2022-10": [
        168,
        20,
        44,
        4,
        13,
        33,
        55,
        90,
        32,
        12,
        24,
        39,
        41,
        15,
        54,
        15,
        32,
        69,
        52,
        8,
        42,
        16,
        15,
        37,
        21,
        2,
        4,
        41,
        20,
        70,
        17,
        11,
        22,
        40,
        14,
        27,
        27,
        31,
        9,
        14,
        9,
        9,
        17,
        5,
        35,
        37,
        15
      ],
      "2022-11": [
        125,
        10,
        37,
        4,
        12,
        11,
        26,
        35,
        27,
        9,
        10,
        28,
        23,
        8,
        35,
        12,
        23,
        32,
        25,
        3,
        27,
        4,
        18,
        26,
        12,
        1,
        5,
        30,
        17,
        42,
        8,
        9,
        20,
        29,
        11,
        14,
        16,
        23,
        3,
        16,
        4,
        5,
        14,
        1,
        23,
        15,
        8
      ],
      "2022-12": [
        83,
        9,
        25,
        3,
        9,
        28,
        34,
        46,
        13,
        2,
        9,
        37,
        17,
        7,
        40,
        7,
        17,
        28,
        34,
        6,
        32,
        11,
        10,
        17,
        15,
        1,
        3,
        17,
        7,
        37,
        4,
        11,
        13,
        28,
        18,
        22,
        17,
        17,
        8,
        18,
        6,
        7,
        25,
        3,
        21,
        15,
        9
      ],
      "2023-01": [
        51,
        10,
        15,
        3,
        3,
        8,
        14,
        26,
        11,
        4,
        7,
        17,
        16,
        8,
        26,
        9,
        14,
        17,
        15,
        3,
        13,
        9,
        12,
        12,
        5,
        0,
        3,
        10,
        9,
        17,
        5,
        9,
        18,
        9,
        19,
        10,
        8,
        11,
        5,
        10,
        5,
        3,
        7,
        2,
        14,
        10,
        7
      ],
      "2023-02": [
        70,
        6,
        22,
        4,
        12,
        14,
        20,
        30,
        17,
        14,
        9,
        20,
        27,
        8,
        20,
        9,
        19,
        24,
        17,
        4,
        20,
        20,
        8,
        19,
        16,
        0,
        3,
        18,
        7,
        19,
        5,
        4,
        24,
        27,
        15,
        11,
        9,
        14,
        12,
        14,
        2,
        7,
        17,
        3,
        21,
        11,
        7
      ],
      "2023-03": [
        68,
        23,
        31,
        3,
        12,
        11,
        23,
        37,
        9,
        8,
        5,
        34,
        16,
        11,
        33,
        11,
        23,
        33,
        15,
        5,
        17,
        27,
        9,
        16,
        11,
        2,
        6,
        19,
        7,
        31,
        2,
        7,
        15,
        29,
        11,
        12,
        10,
        16,
        3,
        17,
        4,
        10,
        17,
        3,
        20,
        11,
        4
      ],
      "2023-04": [
        53,
        15,
        20,
        1,
        2,
        18,
        26,
        34,
        18,
        8,
        4,
        16,
        28,
        8,
        30,
        13,
        28,
        35,
        14,
        6,
        14,
        29,
        13,
        12,
        9,
        0,
        5,
        18,
        11,
        33,
        3,
        8,
        17,
        22,
        23,
        15,
        13,
        8,
        2,
        25,
        6,
        10,
        8,
        1,
        34,
        10,
        3
      ],
      "2023-05": [
        213,
        26,
        71,
        4,
        24,
        67,
        74,
        132,
        61,
        26,
        18,
        90,
        71,
        17,
        102,
        30,
        51,
        93,
        68,
        19,
        63,
        98,
        26,
        56,
        17,
        3,
        13,
        65,
        37,
        100,
        23,
        25,
        24,
        69,
        17,
        51,
        44,
        46,
        19,
        41,
        21,
        28,
        75,
        2,
        78,
        46,
        16
      ],
      "2023-06": [
        149,
        23,
        49,
        3,
        19,
        33,
        29,
        51,
        31,
        13,
        12,
        28,
        31,
        13,
        57,
        22,
        30,
        45,
        30,
        11,
        25,
        60,
        21,
        23,
        27,
        1,
        8,
        38,
        13,
        39,
        15,
        13,
        25,
        34,
        11,
        16,
        21,
        20,
        4,
        25,
        7,
        15,
        23,
        1,
        39,
        21,
        7
      ],
      "2023-07": [
        105,
        12,
        26,
        2,
        14,
        13,
        26,
        24,
        17,
        10,
        2,
        17,
        24,
        12,
        38,
        24,
        25,
        26,
        22,
        9,
        13,
        71,
        15,
        19,
        14,
        3,
        9,
        17,
        8,
        25,
        7,
        11,
        19,
        31,
        13,
        17,
        10,
        10,
        6,
        22,
        2,
        15,
        22,
        1,
        45,
        17,
        5
      ],
      "2023-08": [
        80,
        23,
        26,
        1,
        11,
        23,
        21,
        33,
        21,
        11,
        4,
        13,
        26,
        7,
        45,
        9,
        26,
        21,
        16,
        8,
        22,
        63,
        12,
        23,
        12,
        5,
        10,
        20,
        3,
        32,
        4,
        4,
        14,
        28,
        14,
        21,
        21,
        10,
        3,
        21,
        3,
        10,
        18,
        2,
        45,
        17,
        7
      ],
      "2023-09": [
        102,
        20,
        35,
        2,
        17,
        33,
        31,
        39,
        26,
        15,
        13,
        24,
        29,
        9,
        50,
        14,
        32,
        27,
        20,
        20,
        15,
        96,
        18,
        24,
        17,
        8,
        8,
        18,
        13,
        33,
        8,
        5,
        19,
        43,
        18,
        11,
        23,
        8,
        4,
        30,
        0,
        15,
        31,
        0,
        46,
        21,
        6
      ],
      "2023-10": [
        139,
        21,
        46,
        10,
        33,
        86,
        51,
        68,
        45,
        31,
        14,
        38,
        44,
        13,
        80,
        27,
        32,
        65,
        50,
        38,
        36,
        172,
        26,
        49,
        22,
        6,
        16,
        47,
        34,
        67,
        21,
        22,
        24,
        60,
        21,
        20,
        38,
        38,
        10,
        47,
        14,
        14,
        55,
        3,
        96,
        59,
        18
      ],
      "2023-11": [
        77,
        31,
        42,
        6,
        33,
        43,
        27,
        54,
        37,
        13,
        16,
        34,
        28,
        20,
        59,
        25,
        31,
        48,
        31,
        24,
        23,
        111,
        14,
        34,
        11,
        7,
        17,
        24,
        23,
        51,
        10,
        17,
        17,
        36,
        14,
        19,
        22,
        18,
        4,
        40,
        4,
        8,
        42,
        2,
        61,
        26,
        10
      ],
      "2023-12": [
        58,
        22,
        34,
        7,
        14,
        42,
        17,
        32,
        21,
        18,
        4,
        15,
        17,
        11,
        52,
        14,
        28,
        14,
        25,
        15,
        16,
        94,
        15,
        33,
        14,
        10,
        7,
        16,
        17,
        24,
        10,
        13,
        22,
        32,
        18,
        11,
        15,
        20,
        5,
        23,
        10,
        12,
        26,
        1,
        48,
        25,
        5
      ],
      "2024-01": [
        83,
        29,
        31,
        3,
        26,
        38,
        30,
        43,
        25,
        32,
        2,
        16,
        24,
        15,
        62,
        20,
        29,
        39,
        25,
        23,
        25,
        129,
        20,
        25,
        12,
        21,
        10,
        18,
        13,
        48,
        7,
        8,
        26,
        34,
        15,
        10,
        17,
        14,
        3,
        35,
        4,
        9,
        24,
        3,
        50,
        22,
        7
      ],
      "2024-02": [
        117,
        31,
        46,
        10,
        57,
        101,
        36,
        55,
        43,
        59,
        7,
        40,
        34,
        17,
        113,
        28,
        27,
        67,
        39,
        53,
        32,
        279,
        14,
        42,
        14,
        39,
        13,
        25,
        26,
        57,
        16,
        22,
        30,
        66,
        25,
        32,
        35,
        17,
        7,
        45,
        18,
        28,
        63,
        2,
        121,
        52,
        12
      ],
      "2024-03": [
        97,
        39,
        50,
        11,
        36,
        51,
        35,
        55,
        37,
        37,
        13,
        39,
        36,
        18,
        83,
        22,
        37,
        51,
        34,
        42,
        31,
        164,
        37,
        32,
        22,
        28,
        14,
        28,
        19,
        57,
        22,
        18,
        27,
        55,
        29,
        17,
        28,
        23,
        5,
        34,
        7,
        17,
        40,
        5,
        73,
        35,
        8
      ],
      "2024-04": [
        97,
        32,
        41,
        6,
        32,
        55,
        17,
        44,
        36,
        31,
        7,
        37,
        22,
        12,
        51,
        18,
        27,
        72,
        27,
        42,
        25,
        179,
        9,
        28,
        18,
        34,
        11,
        34,
        16,
        49,
        15,
        14,
        19,
        49,
        17,
        20,
        26,
        32,
        5,
        34,
        8,
        14,
        35,
        1,
        54,
        19,
        3
      ],
      "2024-05": [
        96,
        43,
        43,
        10,
        32,
        53,
        19,
        46,
        38,
        34,
        2,
        30,
        26,
        11,
        80,
        24,
        37,
        37,
        35,
        49,
        25,
        208,
        16,
        26,
        16,
        52,
        8,
        22,
        27,
        44,
        13,
        25,
        13,
        53,
        22,
        22,
        21,
        21,
        5,
        30,
        12,
        23,
        40,
        5,
        67,
        38,
        19
      ],
      "2024-06": [
        220,
        42,
        59,
        17,
        48,
        96,
        33,
        58,
        72,
        58,
        8,
        44,
        27,
        12,
        103,
        31,
        35,
        80,
        38,
        86,
        26,
        268,
        25,
        41,
        24,
        81,
        15,
        28,
        29,
        72,
        20,
        24,
        17,
        79,
        24,
        30,
        36,
        37,
        6,
        56,
        13,
        19,
        58,
        1,
        136,
        54,
        17
      ],
      "2024-07": [
        113,
        26,
        42,
        11,
        40,
        52,
        22,
        47,
        68,
        33,
        7,
        28,
        26,
        7,
        71,
        23,
        34,
        39,
        26,
        44,
        25,
        190,
        25,
        25,
        21,
        53,
        12,
        22,
        16,
        49,
        23,
        10,
        17,
        54,
        22,
        21,
        36,
        17,
        4,
        37,
        8,
        18,
        39,
        1,
        63,
        24,
        4
      ],
      "2024-08": [
        94,
        34,
        29,
        9,
        34,
        53,
        20,
        38,
        41,
        27,
        4,
        28,
        30,
        8,
        67,
        20,
        31,
        29,
        23,
        35,
        12,
        137,
        14,
        25,
        13,
        43,
        18,
        12,
        14,
        34,
        5,
        11,
        9,
        39,
        8,
        16,
        24,
        30,
        1,
        22,
        10,
        11,
        30,
        3,
        61,
        26,
        12
      ],
      "2024-09": [
        142,
        37,
        37,
        8,
        30,
        61,
        17,
        36,
        36,
        47,
        5,
        24,
        26,
        19,
        70,
        17,
        27,
        47,
        20,
        44,
        22,
        178,
        18,
        23,
        15,
        63,
        6,
        14,
        15,
        37,
        7,
        11,
        15,
        42,
        6,
        26,
        15,
        22,
        6,
        30,
        12,
        19,
        43,
        2,
        70,
        22,
        13
      ],
      "2024-10": [
        149,
        41,
        67,
        30,
        66,
        145,
        29,
        72,
        65,
        62,
        11,
        33,
        43,
        21,
        129,
        32,
        28,
        69,
        43,
        107,
        19,
        322,
        37,
        36,
        25,
        112,
        19,
        23,
        28,
        77,
        18,
        16,
        32,
        95,
        37,
        36,
        52,
        32,
        7,
        65,
        15,
        19,
        64,
        3,
        129,
        54,
        17
      ],
      "2024-11": [
        88,
        25,
        44,
        10,
        26,
        51,
        19,
        32,
        33,
        32,
        3,
        17,
        24,
        17,
        73,
        20,
        27,
        40,
        25,
        34,
        14,
        145,
        5,
        20,
        11,
        52,
        14,
        13,
        8,
        39,
        11,
        7,
        16,
        43,
        19,
        18,
        28,
        15,
        5,
        28,
        6,
        22,
        31,
        1,
        39,
        12,
        13
      ],
      "2024-12": [
        116,
        27,
        57,
        20,
        47,
        82,
        26,
        50,
        30,
        38,
        8,
        28,
        23,
        13,
        72,
        28,
        40,
        50,
        24,
        41,
        23,
        163,
        27,
        37,
        14,
        70,
        11,
        21,
        17,
        53,
        12,
        14,
        20,
        53,
        18,
        28,
        21,
        24,
        6,
        33,
        11,
        12,
        42,
        5,
        67,
        24,
        17
      ],
      "2025-01": [
        71,
        37,
        40,
        9,
        40,
        82,
        20,
        34,
        30,
        22,
        4,
        22,
        23,
        12,
        62,
        22,
        24,
        38,
        25,
        30,
        10,
        149,
        24,
        22,
        16,
        70,
        8,
        12,
        15,
        25,
        11,
        19,
        13,
        37,
        15,
        18,
        16,
        17,
        9,
        35,
        9,
        14,
        21,
        4,
        43,
        26,
        8
      ],
      "2025-02": [
        118,
        38,
        49,
        29,
        72,
        201,
        30,
        49,
        55,
        71,
        12,
        34,
        35,
        13,
        156,
        35,
        46,
        71,
        51,
        91,
        13,
        324,
        34,
        33,
        20,
        112,
        23,
        23,
        29,
        60,
        15,
        19,
        19,
        75,
        20,
        37,
        37,
        29,
        6,
        46,
        15,
        25,
        53,
        1,
        92,
        46,
        13
      ],
      "2025-03": [
        111,
        41,
        57,
        27,
        30,
        150,
        26,
        39,
        45,
        81,
        8,
        27,
        19,
        10,
        107,
        25,
        39,
        47,
        33,
        62,
        13,
        208,
        22,
        25,
        20,
        80,
        14,
        20,
        16,
        45,
        9,
        13,
        17,
        50,
        18,
        24,
        35,
        15,
        3,
        35,
        9,
        21,
        37,
        4,
        56,
        31,
        10
      ],
      "2025-04": [
        68,
        40,
        38,
        10,
        33,
        143,
        19,
        41,
        31,
        56,
        6,
        17,
        26,
        10,
        65,
        29,
        29,
        44,
        24,
        43,
        18,
        186,
        29,
        26,
        10,
        84,
        21,
        12,
        13,
        30,
        10,
        15,
        21,
        50,
        18,
        28,
        24,
        24,
        3,
        39,
        9,
        17,
        20,
        4,
        52,
        21,
        16
      ],
      "2025-05": [
        196,
        59,
        61,
        22,
        89,
        344,
        29,
        59,
        80,
        99,
        9,
        26,
        29,
        21,
        170,
        36,
        39,
        76,
        48,
        103,
        22,
        325,
        33,
        43,
        35,
        131,
        15,
        22,
        30,
        67,
        17,
        25,
        18,
        91,
        19,
        38,
        48,
        30,
        5,
        58,
        25,
        24,
        47,
        3,
        109,
        56,
        20
      ],
      "2025-06": [
        181,
        49,
        55,
        17,
        45,
        215,
        31,
        35,
        57,
        75,
        12,
        23,
        25,
        27,
        130,
        29,
        18,
        55,
        34,
        70,
        24,
        233,
        20,
        29,
        29,
        92,
        18,
        22,
        13,
        51,
        19,
        14,
        28,
        51,
        21,
        31,
        50,
        37,
        4,
        53,
        20,
        19,
        44,
        6,
        84,
        36,
        16
      ],
      "2025-07": [
        121,
        50,
        39,
        14,
        33,
        132,
        16,
        34,
        44,
        53,
        9,
        20,
        21,
        10,
        91,
        26,
        24,
        36,
        26,
        48,
        18,
        159,
        27,
        25,
        20,
        76,
        20,
        16,
        12,
        33,
        21,
        16,
        12,
        59,
        12,
        25,
        28,
        13,
        7,
        31,
        8,
        24,
        23,
        2,
        53,
        30,
        12
      ],
      "2025-08": [
        115,
        37,
        30,
        13,
        46,
        146,
        24,
        40,
        56,
        80,
        4,
        16,
        23,
        12,
        83,
        21,
        44,
        53,
        29,
        55,
        22,
        177,
        29,
        30,
        21,
        108,
        19,
        15,
        21,
        42,
        24,
        18,
        18,
        62,
        19,
        25,
        27,
        17,
        6,
        45,
        15,
        19,
        31,
        3,
        53,
        14,
        18
      ],
      "2025-09": [
        60,
        19,
        22,
        6,
        17,
        51,
        7,
        17,
        25,
        42,
        2,
        6,
        12,
        1,
        20,
        9,
        17,
        21,
        17,
        27,
        11,
        82,
        11,
        14,
        12,
        36,
        9,
        16,
        5,
        14,
        15,
        11,
        10,
        23,
        7,
        15,
        13,
        11,
        0,
        12,
        6,
        2,
        8,
        3,
        30,
        11,
        10
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Moshi: a speech-text foundation model for real-time dialogue",
          "year": "2024-09",
          "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken\ndialogue framework. Current systems for spoken dialogue rely on pipelines of\nindependent components, namely voice activity detection, speech recognition,\ntextual dialogue and text-to-speech. Such frameworks cannot emulate the\nexperience of real conversations. First, their complexity induces a latency of\nseveral seconds between interactions. Second, text being the intermediate\nmodality for dialogue, non-linguistic information that modifies meaning -- such\nas emotion or non-speech sounds -- is lost in the interaction. Finally, they\nrely on a segmentation into speaker turns, which does not take into account\noverlapping speech, interruptions and interjections. Moshi solves these\nindependent issues altogether by casting spoken dialogue as speech-to-speech\ngeneration. Starting from a text language model backbone, Moshi generates\nspeech as tokens from the residual quantizer of a neural audio codec, while\nmodeling separately its own speech and that of the user into parallel streams.\nThis allows for the removal of explicit speaker turns, and the modeling of\narbitrary conversational dynamics. We moreover extend the hierarchical\nsemantic-to-acoustic token generation of previous work to first predict\ntime-aligned text tokens as a prefix to audio tokens. Not only this \"Inner\nMonologue\" method significantly improves the linguistic quality of generated\nspeech, but we also illustrate how it can provide streaming speech recognition\nand text-to-speech. Our resulting model is the first real-time full-duplex\nspoken large language model, with a theoretical latency of 160ms, 200ms in\npractice, and is available at https://github.com/kyutai-labs/moshi.",
          "arxiv_id": "2410.00037v2"
        },
        {
          "title": "Exploring Machine Speech Chain for Domain Adaptation and Few-Shot Speaker Adaptation",
          "year": "2021-04",
          "abstract": "Machine Speech Chain, which integrates both end-to-end (E2E) automatic speech\nrecognition (ASR) and text-to-speech (TTS) into one circle for joint training,\nhas been proven to be effective in data augmentation by leveraging large\namounts of unpaired data. In this paper, we explore the TTS->ASR pipeline in\nspeech chain to do domain adaptation for both neural TTS and E2E ASR models,\nwith only text data from target domain. We conduct experiments by adapting from\naudiobook domain (LibriSpeech) to presentation domain (TED-LIUM), there is a\nrelative word error rate (WER) reduction of 10% for the E2E ASR model on the\nTED-LIUM test set, and a relative WER reduction of 51.5% in synthetic speech\ngenerated by neural TTS in the presentation domain. Further, we apply few-shot\nspeaker adaptation for the E2E ASR by using a few utterances from target\nspeakers in an unsupervised way, results in additional gains.",
          "arxiv_id": "2104.03815v1"
        },
        {
          "title": "Gated Recurrent Fusion with Joint Training Framework for Robust End-to-End Speech Recognition",
          "year": "2020-11",
          "abstract": "The joint training framework for speech enhancement and recognition methods\nhave obtained quite good performances for robust end-to-end automatic speech\nrecognition (ASR). However, these methods only utilize the enhanced feature as\nthe input of the speech recognition component, which are affected by the speech\ndistortion problem. In order to address this problem, this paper proposes a\ngated recurrent fusion (GRF) method with joint training framework for robust\nend-to-end ASR. The GRF algorithm is used to dynamically combine the noisy and\nenhanced features. Therefore, the GRF can not only remove the noise signals\nfrom the enhanced features, but also learn the raw fine structures from the\nnoisy features so that it can alleviate the speech distortion. The proposed\nmethod consists of speech enhancement, GRF and speech recognition. Firstly, the\nmask based speech enhancement network is applied to enhance the input speech.\nSecondly, the GRF is applied to address the speech distortion problem. Thirdly,\nto improve the performance of ASR, the state-of-the-art speech transformer\nalgorithm is used as the speech recognition component. Finally, the joint\ntraining framework is utilized to optimize these three components,\nsimultaneously. Our experiments are conducted on an open-source Mandarin speech\ncorpus called AISHELL-1. Experimental results show that the proposed method\nachieves the relative character error rate (CER) reduction of 10.04\\% over the\nconventional joint enhancement and transformer method only using the enhanced\nfeatures. Especially for the low signal-to-noise ratio (0 dB), our proposed\nmethod can achieves better performances with 12.67\\% CER reduction, which\nsuggests the potential of our proposed method.",
          "arxiv_id": "2011.04249v1"
        }
      ],
      "1": [
        {
          "title": "Large Language Model Benchmarks in Medical Tasks",
          "year": "2024-10",
          "abstract": "With the increasing application of large language models (LLMs) in the\nmedical domain, evaluating these models' performance using benchmark datasets\nhas become crucial. This paper presents a comprehensive survey of various\nbenchmark datasets employed in medical LLM tasks. These datasets span multiple\nmodalities including text, image, and multimodal benchmarks, focusing on\ndifferent aspects of medical knowledge such as electronic health records\n(EHRs), doctor-patient dialogues, medical question-answering, and medical image\ncaptioning. The survey categorizes the datasets by modality, discussing their\nsignificance, data structure, and impact on the development of LLMs for\nclinical tasks such as diagnosis, report generation, and predictive decision\nsupport. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and\nCheXpert, which have facilitated advancements in tasks like medical report\ngeneration, clinical summarization, and synthetic data generation. The paper\nsummarizes the challenges and opportunities in leveraging these benchmarks for\nadvancing multimodal medical intelligence, emphasizing the need for datasets\nwith a greater degree of language diversity, structured omics data, and\ninnovative approaches to synthesis. This work also provides a foundation for\nfuture research in the application of LLMs in medicine, contributing to the\nevolving field of medical artificial intelligence.",
          "arxiv_id": "2410.21348v2"
        },
        {
          "title": "MedCT: A Clinical Terminology Graph for Generative AI Applications in Healthcare",
          "year": "2025-01",
          "abstract": "We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development.",
          "arxiv_id": "2501.06465v3"
        },
        {
          "title": "Adapted Large Language Models Can Outperform Medical Experts in Clinical Text Summarization",
          "year": "2023-09",
          "abstract": "Analyzing vast textual data and summarizing key information from electronic\nhealth records imposes a substantial burden on how clinicians allocate their\ntime. Although large language models (LLMs) have shown promise in natural\nlanguage processing (NLP), their effectiveness on a diverse range of clinical\nsummarization tasks remains unproven. In this study, we apply adaptation\nmethods to eight LLMs, spanning four distinct clinical summarization tasks:\nradiology reports, patient questions, progress notes, and doctor-patient\ndialogue. Quantitative assessments with syntactic, semantic, and conceptual NLP\nmetrics reveal trade-offs between models and adaptation methods. A clinical\nreader study with ten physicians evaluates summary completeness, correctness,\nand conciseness; in a majority of cases, summaries from our best adapted LLMs\nare either equivalent (45%) or superior (36%) compared to summaries from\nmedical experts. The ensuing safety analysis highlights challenges faced by\nboth LLMs and medical experts, as we connect errors to potential medical harm\nand categorize types of fabricated information. Our research provides evidence\nof LLMs outperforming medical experts in clinical text summarization across\nmultiple tasks. This suggests that integrating LLMs into clinical workflows\ncould alleviate documentation burden, allowing clinicians to focus more on\npatient care.",
          "arxiv_id": "2309.07430v5"
        }
      ],
      "2": [
        {
          "title": "Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone",
          "year": "2022-06",
          "abstract": "Vision-language (VL) pre-training has recently received considerable\nattention. However, most existing end-to-end pre-training approaches either\nonly aim to tackle VL tasks such as image-text retrieval, visual question\nanswering (VQA) and image captioning that test high-level understanding of\nimages, or only target region-level understanding for tasks such as phrase\ngrounding and object detection. We present FIBER (Fusion-In-the-Backbone-based\ntransformER), a new VL model architecture that can seamlessly handle both these\ntypes of tasks. Instead of having dedicated transformer layers for fusion after\nthe uni-modal backbones, FIBER pushes multimodal fusion deep into the model by\ninserting cross-attention into the image and text backbones, bringing gains in\nterms of memory and performance. In addition, unlike previous work that is\neither only pre-trained on image-text data or on fine-grained data with\nbox-level annotations, we present a two-stage pre-training strategy that uses\nboth these kinds of data efficiently: (i) coarse-grained pre-training based on\nimage-text data; followed by (ii) fine-grained pre-training based on\nimage-text-box data. We conduct comprehensive experiments on a wide range of VL\ntasks, ranging from VQA, image captioning, and retrieval, to phrase grounding,\nreferring expression comprehension, and object detection. Using deep multimodal\nfusion coupled with the two-stage pre-training, FIBER provides consistent\nperformance improvements over strong baselines across all tasks, often\noutperforming methods using magnitudes more data. Code is available at\nhttps://github.com/microsoft/FIBER.",
          "arxiv_id": "2206.07643v2"
        },
        {
          "title": "Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner",
          "year": "2023-05",
          "abstract": "Large pre-trained multimodal models have demonstrated significant success in\na range of downstream tasks, including image captioning, image-text retrieval,\nvisual question answering (VQA), etc. However, many of these methods rely on\nimage-text pairs collected from the web as pre-training data and unfortunately\noverlook the need for fine-grained feature alignment between vision and\nlanguage modalities, which requires detailed understanding of images and\nlanguage expressions. While integrating VQA and dense captioning (DC) into\npre-training can address this issue, acquiring image-question-answer as well as\nimage-location-caption triplets is challenging and time-consuming.\nAdditionally, publicly available datasets for VQA and dense captioning are\ntypically limited in scale due to manual data collection and labeling efforts.\nIn this paper, we propose a novel method called Joint QA and DC GEneration\n(JADE), which utilizes a pre-trained multimodal model and easily-crawled\nimage-text pairs to automatically generate and filter large-scale VQA and dense\ncaptioning datasets. We apply this method to the Conceptual Caption (CC3M)\ndataset to generate a new dataset called CC3M-QA-DC. Experiments show that when\nused for pre-training in a multi-task manner, CC3M-QA-DC can improve the\nperformance with various backbones on various downstream tasks. Furthermore,\nour generated CC3M-QA-DC can be combined with larger image-text datasets (e.g.,\nCC15M) and achieve competitive results compared with models using much more\ndata. Code and dataset are available at\nhttps://github.com/johncaged/OPT_Questioner.",
          "arxiv_id": "2305.11769v2"
        },
        {
          "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection",
          "year": "2025-03",
          "abstract": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*.",
          "arxiv_id": "2503.11794v1"
        }
      ],
      "3": [
        {
          "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression",
          "year": "2025-03",
          "abstract": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
          "arxiv_id": "2503.11132v4"
        },
        {
          "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs",
          "year": "2025-08",
          "abstract": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
          "arxiv_id": "2508.04257v1"
        },
        {
          "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
          "year": "2024-10",
          "abstract": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
          "arxiv_id": "2410.10819v1"
        }
      ],
      "4": [
        {
          "title": "Representation Bending for Large Language Model Safety",
          "year": "2025-04",
          "abstract": "Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities.",
          "arxiv_id": "2504.01550v3"
        },
        {
          "title": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense",
          "year": "2025-01",
          "abstract": "As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nattacks to demonstrate the efficacy of our approach. Results indicate that our\nframework reduces the harmfulness and attack success rate of jailbreak attacks\nwithout compromising utility for benign queries compared to recent defense\nmethods. Our code is publicly available at:\nhttps://github.com/oyy2000/LayerAdvPatcher",
          "arxiv_id": "2501.02629v2"
        },
        {
          "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models",
          "year": "2025-02",
          "abstract": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.",
          "arxiv_id": "2502.13141v1"
        }
      ],
      "5": [
        {
          "title": "Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning",
          "year": "2025-08",
          "abstract": "This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved\napproach to standard CoT, for logical reasoning in large language models\n(LLMs). The key idea is to integrate lightweight symbolic representations into\nfew-shot prompts, structuring the inference steps with a consistent strategy to\nmake reasoning patterns more explicit within a non-iterative reasoning process.\nBy incorporating these symbolic structures, our method preserves the\ngeneralizability of standard prompting techniques while enhancing the\ntransparency, interpretability, and analyzability of LLM logical reasoning.\nExtensive experiments on four well-known logical reasoning benchmarks --\nProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse\nreasoning scenarios -- demonstrate the effectiveness of the proposed approach,\nparticularly in complex reasoning tasks that require navigating multiple\nconstraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'\nreasoning capabilities across various model sizes and significantly outperforms\nconventional CoT on three out of four datasets, ProofWriter, ProntoQA, and\nLogicalDeduction.",
          "arxiv_id": "2508.12425v1"
        },
        {
          "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
          "year": "2024-10",
          "abstract": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
          "arxiv_id": "2410.17635v2"
        },
        {
          "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models",
          "year": "2025-03",
          "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking. Project website:\nhttps://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs",
          "arxiv_id": "2503.16419v4"
        }
      ],
      "6": [
        {
          "title": "Dialog-to-Actions: Building Task-Oriented Dialogue System via Action-Level Generation",
          "year": "2023-04",
          "abstract": "End-to-end generation-based approaches have been investigated and applied in\ntask-oriented dialogue systems. However, in industrial scenarios, existing\nmethods face the bottlenecks of controllability (e.g., domain-inconsistent\nresponses, repetition problem, etc) and efficiency (e.g., long computation\ntime, etc). In this paper, we propose a task-oriented dialogue system via\naction-level generation. Specifically, we first construct dialogue actions from\nlarge-scale dialogues and represent each natural language (NL) response as a\nsequence of dialogue actions. Further, we train a Sequence-to-Sequence model\nwhich takes the dialogue history as input and outputs sequence of dialogue\nactions. The generated dialogue actions are transformed into verbal responses.\nExperimental results show that our light-weighted method achieves competitive\nperformance, and has the advantage of controllability and efficiency.",
          "arxiv_id": "2304.00884v1"
        },
        {
          "title": "JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset",
          "year": "2024-03",
          "abstract": "Dialogue datasets are crucial for deep learning-based task-oriented dialogue\nsystem research. While numerous English language multi-domain task-oriented\ndialogue datasets have been developed and contributed to significant\nadvancements in task-oriented dialogue systems, such a dataset does not exist\nin Japanese, and research in this area is limited compared to that in English.\nIn this study, towards the advancement of research and development of\ntask-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first\nJapanese language large-scale multi-domain task-oriented dialogue dataset.\nUsing JMultiWOZ, we evaluated the dialogue state tracking and response\ngeneration capabilities of the state-of-the-art methods on the existing major\nEnglish benchmark dataset MultiWOZ2.2 and the latest large language model\n(LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ\nprovides a benchmark that is on par with MultiWOZ2.2. In addition, through\nevaluation experiments of interactive dialogues with the models and human\nparticipants, we identified limitations in the task completion capabilities of\nLLMs in Japanese.",
          "arxiv_id": "2403.17319v1"
        },
        {
          "title": "CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation",
          "year": "2023-03",
          "abstract": "Dialogue structure discovery is essential in dialogue generation.\nWell-structured topic flow can leverage background information and predict\nfuture topics to help generate controllable and explainable responses. However,\nmost previous work focused on dialogue structure learning in task-oriented\ndialogue other than open-domain dialogue which is more complicated and\nchallenging. In this paper, we present a new framework CTRLStruct for dialogue\nstructure learning to effectively explore topic-level dialogue clusters as well\nas their transitions with unlabelled information. Precisely, dialogue\nutterances encoded by bi-directional Transformer are further trained through a\nspecial designed contrastive learning task to improve representation. Then we\nperform clustering to utterance-level representations and form topic-level\nclusters that can be considered as vertices in dialogue structure graph. The\nedges in the graph indicating transition probability between vertices are\ncalculated by mimicking expert behavior in datasets. Finally, dialogue\nstructure graph is integrated into dialogue model to perform controlled\nresponse generation. Experiments on two popular open-domain dialogue datasets\nshow our model can generate more coherent responses compared to some excellent\ndialogue models, as well as outperform some typical sentence embedding methods\nin dialogue utterance representation. Code is available in GitHub.",
          "arxiv_id": "2303.01094v1"
        }
      ],
      "7": [
        {
          "title": "A Hybrid Approach for Improved Low Resource Neural Machine Translation using Monolingual Data",
          "year": "2020-11",
          "abstract": "Many language pairs are low resource, meaning the amount and/or quality of\navailable parallel data is not sufficient to train a neural machine translation\n(NMT) model which can reach an acceptable standard of accuracy. Many works have\nexplored using the readily available monolingual data in either or both of the\nlanguages to improve the standard of translation models in low, and even high,\nresource languages. One of the most successful of such works is the\nback-translation that utilizes the translations of the target language\nmonolingual data to increase the amount of the training data. The quality of\nthe backward model which is trained on the available parallel data has been\nshown to determine the performance of the back-translation approach. Despite\nthis, only the forward model is improved on the monolingual target data in\nstandard back-translation. A previous study proposed an iterative\nback-translation approach for improving both models over several iterations.\nBut unlike in the traditional back-translation, it relied on both the target\nand source monolingual data. This work, therefore, proposes a novel approach\nthat enables both the backward and forward models to benefit from the\nmonolingual target data through a hybrid of self-learning and back-translation\nrespectively. Experimental results have shown the superiority of the proposed\napproach over the traditional back-translation method on English-German low\nresource neural machine translation. We also proposed an iterative\nself-learning approach that outperforms the iterative back-translation while\nalso relying only on the monolingual target data and require the training of\nless models.",
          "arxiv_id": "2011.07403v3"
        },
        {
          "title": "Extended Parallel Corpus for Amharic-English Machine Translation",
          "year": "2021-04",
          "abstract": "This paper describes the acquisition, preprocessing, segmentation, and\nalignment of an Amharic-English parallel corpus. It will be helpful for machine\ntranslation of a low-resource language, Amharic. We freely released the corpus\nfor research purposes. Furthermore, we developed baseline statistical and\nneural machine translation systems; we trained statistical and neural machine\ntranslation models using the corpus. In the experiments, we also used a large\nmonolingual corpus for the language model of statistical machine translation\nand back-translation of neural machine translation. In the automatic\nevaluation, neural machine translation models outperform statistical machine\ntranslation models by approximately six to seven Bilingual Evaluation\nUnderstudy (BLEU) points. Besides, among the neural machine translation models,\nthe subword models outperform the word-based models by three to four BLEU\npoints. Moreover, two other relevant automatic evaluation metrics, Translation\nEdit Rate on Character Level and Better Evaluation as Ranking, reflect\ncorresponding differences among the trained models.",
          "arxiv_id": "2104.03543v3"
        },
        {
          "title": "SJTU-NICT's Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task",
          "year": "2020-10",
          "abstract": "In this paper, we introduced our joint team SJTU-NICT 's participation in the\nWMT 2020 machine translation shared task. In this shared task, we participated\nin four translation directions of three language pairs: English-Chinese,\nEnglish-Polish on supervised machine translation track, German-Upper Sorbian on\nlow-resource and unsupervised machine translation tracks. Based on different\nconditions of language pairs, we have experimented with diverse neural machine\ntranslation (NMT) techniques: document-enhanced NMT, XLM pre-trained language\nmodel enhanced NMT, bidirectional translation as a pre-training, reference\nlanguage based UNMT, data-dependent gaussian prior objective, and BT-BLEU\ncollaborative filtering self-training. We also used the TF-IDF algorithm to\nfilter the training set to obtain a domain more similar set with the test set\nfor finetuning. In our submissions, the primary systems won the first place on\nEnglish to Chinese, Polish to English, and German to Upper Sorbian translation\ndirections.",
          "arxiv_id": "2010.05122v1"
        }
      ],
      "8": [
        {
          "title": "A Survey on Gender Bias in Natural Language Processing",
          "year": "2021-12",
          "abstract": "Language can be used as a means of reproducing and enforcing harmful\nstereotypes and biases and has been analysed as such in numerous research. In\nthis paper, we present a survey of 304 papers on gender bias in natural\nlanguage processing. We analyse definitions of gender and its categories within\nsocial sciences and connect them to formal definitions of gender bias in NLP\nresearch. We survey lexica and datasets applied in research on gender bias and\nthen compare and contrast approaches to detecting and mitigating gender bias.\nWe find that research on gender bias suffers from four core limitations. 1)\nMost research treats gender as a binary variable neglecting its fluidity and\ncontinuity. 2) Most of the work has been conducted in monolingual setups for\nEnglish or other high-resource languages. 3) Despite a myriad of papers on\ngender bias in NLP methods, we find that most of the newly developed algorithms\ndo not test their models for bias and disregard possible ethical considerations\nof their work. 4) Finally, methodologies developed in this line of research are\nfundamentally flawed covering very limited definitions of gender bias and\nlacking evaluation baselines and pipelines. We suggest recommendations towards\novercoming these limitations as a guide for future research.",
          "arxiv_id": "2112.14168v1"
        },
        {
          "title": "Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes",
          "year": "2025-08",
          "abstract": "As Large Language Models (LLMs) are increasingly used across different\napplications, concerns about their potential to amplify gender biases in\nvarious tasks are rising. Prior research has often probed gender bias using\nexplicit gender cues as counterfactual, or studied them in sentence completion\nand short question answering tasks. These formats might overlook more implicit\nforms of bias embedded in generative behavior of longer content. In this work,\nwe investigate gender bias in LLMs using gender stereotypes studied in\npsychology (e.g., aggressiveness or gossiping) in an open-ended task of\nnarrative generation. We introduce a novel dataset called StereoBias-Stories\ncontaining short stories either unconditioned or conditioned on (one, two, or\nsix) random attributes from 25 psychological stereotypes and three task-related\nstory endings. We analyze how the gender contribution in the overall story\nchanges in response to these attributes and present three key findings: (1)\nWhile models, on average, are highly biased towards male in unconditioned\nprompts, conditioning on attributes independent from gender stereotypes\nmitigates this bias. (2) Combining multiple attributes associated with the same\ngender stereotype intensifies model behavior, with male ones amplifying bias\nand female ones alleviating it. (3) Model biases align with psychological\nground-truth used for categorization, and alignment strength increases with\nmodel size. Together, these insights highlight the importance of\npsychology-grounded evaluation of LLMs.",
          "arxiv_id": "2508.03292v1"
        },
        {
          "title": "Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation",
          "year": "2023-11",
          "abstract": "Large Language Models (LLMs) can generate biased and toxic responses. Yet\nmost prior work on LLM gender bias evaluation requires predefined\ngender-related phrases or gender stereotypes, which are challenging to be\ncomprehensively collected and are limited to explicit bias evaluation. In\naddition, we believe that instances devoid of gender-related language or\nexplicit stereotypes in inputs can still induce gender bias in LLMs. Thus, in\nthis work, we propose a conditional text generation mechanism without the need\nfor predefined gender phrases and stereotypes. This approach employs three\ntypes of inputs generated through three distinct strategies to probe LLMs,\naiming to show evidence of explicit and implicit gender biases in LLMs. We also\nutilize explicit and implicit evaluation metrics to evaluate gender bias in\nLLMs under different strategies. Our experiments demonstrate that an increased\nmodel size does not consistently lead to enhanced fairness and all tested LLMs\nexhibit explicit and/or implicit gender bias, even when explicit gender\nstereotypes are absent in the inputs.",
          "arxiv_id": "2311.00306v1"
        }
      ],
      "9": [
        {
          "title": "CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation",
          "year": "2025-01",
          "abstract": "While much work on web agents emphasizes the promise of autonomously\nperforming tasks on behalf of users, in reality, agents often fall short on\ncomplex tasks in real-world contexts and modeling user preference. This\npresents an opportunity for humans to collaborate with the agent and leverage\nthe agent's capabilities effectively. We propose CowPilot, a framework\nsupporting autonomous as well as human-agent collaborative web navigation, and\nevaluation across task success and task efficiency. CowPilot reduces the number\nof steps humans need to perform by allowing agents to propose next steps, while\nusers are able to pause, reject, or take alternative actions. During execution,\nusers can interleave their actions with the agent by overriding suggestions or\nresuming agent control when needed. We conducted case studies on five common\nwebsites and found that the human-agent collaborative mode achieves the highest\nsuccess rate of 95% while requiring humans to perform only 15.2% of the total\nsteps. Even with human interventions during task execution, the agent\nsuccessfully drives up to half of task success on its own. CowPilot can serve\nas a useful tool for data collection and agent evaluation across websites,\nwhich we believe will enable research in how users and agents can work\ntogether. Video demonstrations are available at\nhttps://oaishi.github.io/cowpilot.html",
          "arxiv_id": "2501.16609v3"
        },
        {
          "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
          "year": "2024-10",
          "abstract": "Autonomy via agents using large language models (LLMs) for personalized,\nstandardized tasks boosts human efficiency. Automating web tasks (like booking\nhotels within a budget) is increasingly sought after. Fulfilling practical\nneeds, the web agent also serves as an important proof-of-concept example for\nvarious agent grounding scenarios, with its success promising advancements in\nmany future applications. Prior research often handcrafts web agent strategies\n(e.g., prompting templates, multi-agent systems, search methods, etc.) and the\ncorresponding in-context examples, which may not generalize well across all\nreal-world scenarios. On the other hand, there has been limited study on the\nmisalignment between a web agent's observation/action representation and the\npre-training data of the LLM it's based on. This discrepancy is especially\nnotable when LLMs are primarily trained for language completion rather than\ntasks involving embodied navigation actions and symbolic web elements. Our\nstudy enhances an LLM-based web agent by simply refining its observation and\naction space to better align with the LLM's capabilities. This approach enables\nour base agent to significantly outperform previous methods on a wide variety\nof web tasks. Specifically, on WebArena, a benchmark featuring general-purpose\nweb interaction tasks, our agent AgentOccam surpasses the previous\nstate-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute\npoints respectively, and boosts the success rate by 26.6 points (+161%) over\nsimilar plain web agents with its observation and action space alignment. We\nachieve this without using in-context examples, new agent roles, online\nfeedback or search strategies. AgentOccam's simple design highlights LLMs'\nimpressive zero-shot performance on web tasks, and underlines the critical role\nof carefully tuning observation and action spaces for LLM-based agents.",
          "arxiv_id": "2410.13825v2"
        },
        {
          "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL",
          "year": "2025-08",
          "abstract": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
          "arxiv_id": "2508.13167v1"
        }
      ],
      "10": [
        {
          "title": "Syntactic Substitutability as Unsupervised Dependency Syntax",
          "year": "2022-11",
          "abstract": "Syntax is a latent hierarchical structure which underpins the robust and\ncompositional nature of human language. In this work, we explore the hypothesis\nthat syntactic dependencies can be represented in language model attention\ndistributions and propose a new method to induce these structures\ntheory-agnostically. Instead of modeling syntactic relations as defined by\nannotation schemata, we model a more general property implicit in the\ndefinition of dependency relations, syntactic substitutability. This property\ncaptures the fact that words at either end of a dependency can be substituted\nwith words from the same category. Substitutions can be used to generate a set\nof syntactically invariant sentences whose representations are then used for\nparsing. We show that increasing the number of substitutions used improves\nparsing accuracy on natural data. On long-distance subject-verb agreement\nconstructions, our method achieves 79.5% recall compared to 8.9% using a\nprevious method. Our method also provides improvements when transferred to a\ndifferent parsing setup, demonstrating that it generalizes.",
          "arxiv_id": "2211.16031v3"
        },
        {
          "title": "Do Neural Language Models Show Preferences for Syntactic Formalisms?",
          "year": "2020-04",
          "abstract": "Recent work on the interpretability of deep neural language models has\nconcluded that many properties of natural language syntax are encoded in their\nrepresentational spaces. However, such studies often suffer from limited scope\nby focusing on a single language and a single linguistic formalism. In this\nstudy, we aim to investigate the extent to which the semblance of syntactic\nstructure captured by language models adheres to a surface-syntactic or deep\nsyntactic style of analysis, and whether the patterns are consistent across\ndifferent languages. We apply a probe for extracting directed dependency trees\nto BERT and ELMo models trained on 13 different languages, probing for two\ndifferent syntactic annotation styles: Universal Dependencies (UD),\nprioritizing deep syntactic relations, and Surface-Syntactic Universal\nDependencies (SUD), focusing on surface structure. We find that both models\nexhibit a preference for UD over SUD - with interesting variations across\nlanguages and layers - and that the strength of this preference is correlated\nwith differences in tree shape.",
          "arxiv_id": "2004.14096v1"
        },
        {
          "title": "Semantic Role Labeling as Syntactic Dependency Parsing",
          "year": "2020-10",
          "abstract": "We reduce the task of (span-based) PropBank-style semantic role labeling\n(SRL) to syntactic dependency parsing. Our approach is motivated by our\nempirical analysis that shows three common syntactic patterns account for over\n98% of the SRL annotations for both English and Chinese data. Based on this\nobservation, we present a conversion scheme that packs SRL annotations into\ndependency tree representations through joint labels that permit highly\naccurate recovery back to the original format. This representation allows us to\ntrain statistical dependency parsers to tackle SRL and achieve competitive\nperformance with the current state of the art. Our findings show the promise of\nsyntactic dependency trees in encoding semantic role relations within their\nsyntactic domain of locality, and point to potential further integration of\nsyntactic methods into semantic role labeling in the future.",
          "arxiv_id": "2010.11170v1"
        }
      ],
      "11": [
        {
          "title": "SummerTime: Text Summarization Toolkit for Non-experts",
          "year": "2021-08",
          "abstract": "Recent advances in summarization provide models that can generate summaries\nof higher quality. Such models now exist for a number of summarization tasks,\nincluding query-based summarization, dialogue summarization, and multi-document\nsummarization. While such models and tasks are rapidly growing in the research\nfield, it has also become challenging for non-experts to keep track of them. To\nmake summarization methods more accessible to a wider audience, we develop\nSummerTime by rethinking the summarization task from the perspective of an NLP\nnon-expert. SummerTime is a complete toolkit for text summarization, including\nvarious models, datasets and evaluation metrics, for a full spectrum of\nsummarization-related tasks. SummerTime integrates with libraries designed for\nNLP researchers, and enables users with easy-to-use APIs. With SummerTime,\nusers can locate pipeline solutions and search for the best model with their\nown data, and visualize the differences, all with a few lines of code. We also\nprovide explanations for models and evaluation metrics to help users understand\nthe model behaviors and select models that best suit their needs. Our library,\nalong with a notebook demo, is available at\nhttps://github.com/Yale-LILY/SummerTime.",
          "arxiv_id": "2108.12738v2"
        },
        {
          "title": "AgreeSum: Agreement-Oriented Multi-Document Summarization",
          "year": "2021-06",
          "abstract": "We aim to renew interest in a particular multi-document summarization (MDS)\ntask which we call AgreeSum: agreement-oriented multi-document summarization.\nGiven a cluster of articles, the goal is to provide abstractive summaries that\nrepresent information common and faithful to all input articles. Given the lack\nof existing datasets, we create a dataset for AgreeSum, and provide annotations\non article-summary entailment relations for a subset of the clusters in the\ndataset. We aim to create strong baselines for the task by applying the\ntop-performing pretrained single-document summarization model PEGASUS onto\nAgreeSum, leveraging both annotated clusters by supervised losses, and\nunannotated clusters by T5-based entailment-related and language-related\nlosses. Compared to other baselines, both automatic evaluation and human\nevaluation show better article-summary and cluster-summary entailment in\ngenerated summaries. On a separate note, we hope that our article-summary\nentailment annotations contribute to the community's effort in improving\nabstractive summarization faithfulness.",
          "arxiv_id": "2106.02278v1"
        },
        {
          "title": "CNewSum: A Large-scale Chinese News Summarization Dataset with Human-annotated Adequacy and Deducibility Level",
          "year": "2021-10",
          "abstract": "Automatic text summarization aims to produce a brief but crucial summary for\nthe input documents. Both extractive and abstractive methods have witnessed\ngreat success in English datasets in recent years. However, there has been a\nminimal exploration of text summarization in Chinese, limited by the lack of\nlarge-scale datasets. In this paper, we present a large-scale Chinese news\nsummarization dataset CNewSum, which consists of 304,307 documents and\nhuman-written summaries for the news feed. It has long documents with\nhigh-abstractive summaries, which can encourage document-level understanding\nand generation for current summarization models. An additional distinguishing\nfeature of CNewSum is that its test set contains adequacy and deducibility\nannotations for the summaries. The adequacy level measures the degree of\nsummary information covered by the document, and the deducibility indicates the\nreasoning ability the model needs to generate the summary. These annotations\ncan help researchers analyze and target their model performance bottleneck. We\nexamine recent methods on CNewSum and release our dataset to provide a solid\ntestbed for automatic Chinese summarization research.",
          "arxiv_id": "2110.10874v1"
        }
      ],
      "12": [
        {
          "title": "Interactive Re-Fitting as a Technique for Improving Word Embeddings",
          "year": "2020-09",
          "abstract": "Word embeddings are a fixed, distributional representation of the context of\nwords in a corpus learned from word co-occurrences. While word embeddings have\nproven to have many practical uses in natural language processing tasks, they\nreflect the attributes of the corpus upon which they are trained. Recent work\nhas demonstrated that post-processing of word embeddings to apply information\nfound in lexical dictionaries can improve their quality. We build on this\npost-processing technique by making it interactive. Our approach makes it\npossible for humans to adjust portions of a word embedding space by moving sets\nof words closer to one another. One motivating use case for this capability is\nto enable users to identify and reduce the presence of bias in word embeddings.\nOur approach allows users to trigger selective post-processing as they interact\nwith and assess potential bias in word embeddings.",
          "arxiv_id": "2010.00121v1"
        },
        {
          "title": "Learning Efficient Task-Specific Meta-Embeddings with Word Prisms",
          "year": "2020-11",
          "abstract": "Word embeddings are trained to predict word cooccurrence statistics, which\nleads them to possess different lexical properties (syntactic, semantic, etc.)\ndepending on the notion of context defined at training time. These properties\nmanifest when querying the embedding space for the most similar vectors, and\nwhen used at the input layer of deep neural networks trained to solve\ndownstream NLP problems. Meta-embeddings combine multiple sets of differently\ntrained word embeddings, and have been shown to successfully improve intrinsic\nand extrinsic performance over equivalent models which use just one set of\nsource embeddings. We introduce word prisms: a simple and efficient\nmeta-embedding method that learns to combine source embeddings according to the\ntask at hand. Word prisms learn orthogonal transformations to linearly combine\nthe input source embeddings, which allows them to be very efficient at\ninference time. We evaluate word prisms in comparison to other meta-embedding\nmethods on six extrinsic evaluations and observe that word prisms offer\nimprovements in performance on all tasks.",
          "arxiv_id": "2011.02944v1"
        },
        {
          "title": "Enhanced word embeddings using multi-semantic representation through lexical chains",
          "year": "2021-01",
          "abstract": "The relationship between words in a sentence often tells us more about the\nunderlying semantic content of a document than its actual words, individually.\nIn this work, we propose two novel algorithms, called Flexible Lexical Chain II\nand Fixed Lexical Chain II. These algorithms combine the semantic relations\nderived from lexical chains, prior knowledge from lexical databases, and the\nrobustness of the distributional hypothesis in word embeddings as building\nblocks forming a single system. In short, our approach has three main\ncontributions: (i) a set of techniques that fully integrate word embeddings and\nlexical chains; (ii) a more robust semantic representation that considers the\nlatent relation between words in a document; and (iii) lightweight word\nembeddings models that can be extended to any natural language task. We intend\nto assess the knowledge of pre-trained models to evaluate their robustness in\nthe document classification task. The proposed techniques are tested against\nseven word embeddings algorithms using five different machine learning\nclassifiers over six scenarios in the document classification task. Our results\nshow the integration between lexical chains and word embeddings representations\nsustain state-of-the-art results, even against more complex systems.",
          "arxiv_id": "2101.09023v2"
        }
      ],
      "13": [
        {
          "title": "Hate Speech Detection via Dual Contrastive Learning",
          "year": "2023-07",
          "abstract": "The fast spread of hate speech on social media impacts the Internet\nenvironment and our society by increasing prejudice and hurting people.\nDetecting hate speech has aroused broad attention in the field of natural\nlanguage processing. Although hate speech detection has been addressed in\nrecent work, this task still faces two inherent unsolved challenges. The first\nchallenge lies in the complex semantic information conveyed in hate speech,\nparticularly the interference of insulting words in hate speech detection. The\nsecond challenge is the imbalanced distribution of hate speech and non-hate\nspeech, which may significantly deteriorate the performance of models. To\ntackle these challenges, we propose a novel dual contrastive learning (DCL)\nframework for hate speech detection. Our framework jointly optimizes the\nself-supervised and the supervised contrastive learning loss for capturing\nspan-level information beyond the token-level emotional semantics used in\nexisting models, particularly detecting speech containing abusive and insulting\nwords. Moreover, we integrate the focal loss into the dual contrastive learning\nframework to alleviate the problem of data imbalance. We conduct experiments on\ntwo publicly available English datasets, and experimental results show that the\nproposed model outperforms the state-of-the-art models and precisely detects\nhate speeches.",
          "arxiv_id": "2307.05578v1"
        },
        {
          "title": "Hierarchical Sentiment Analysis Framework for Hate Speech Detection: Implementing Binary and Multiclass Classification Strategy",
          "year": "2024-11",
          "abstract": "A significant challenge in automating hate speech detection on social media\nis distinguishing hate speech from regular and offensive language. These\nidentify an essential category of content that web filters seek to remove. Only\nautomated methods can manage this volume of daily data. To solve this problem,\nthe community of Natural Language Processing is currently investigating\ndifferent ways of hate speech detection. In addition to those, previous\napproaches (e.g., Convolutional Neural Networks, multi-channel BERT models, and\nlexical detection) have always achieved low precision without carefully\ntreating other related tasks like sentiment analysis and emotion\nclassification. They still like to group all messages with specific words in\nthem as hate speech simply because those terms often appear alongside hateful\nrhetoric. In this research, our paper presented the hate speech text\nclassification system model drawn upon deep learning and machine learning. In\nthis paper, we propose a new multitask model integrated with shared emotional\nrepresentations to detect hate speech across the English language. The\nTransformer-based model we used from Hugging Face and sentiment analysis helped\nus prevent false positives. Conclusion. We conclude that utilizing sentiment\nanalysis and a Transformer-based trained model considerably improves hate\nspeech detection across multiple datasets.",
          "arxiv_id": "2411.05819v1"
        },
        {
          "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI",
          "year": "2025-05",
          "abstract": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text.",
          "arxiv_id": "2505.16263v1"
        }
      ],
      "14": [
        {
          "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding",
          "year": "2025-05",
          "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence.",
          "arxiv_id": "2505.07768v1"
        },
        {
          "title": "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback",
          "year": "2024-03",
          "abstract": "Large Language Models (LLMs) have shown remarkable progress in automated code\ngeneration. Yet, LLM-generated code may contain errors in API usage, class,\ndata structure, or missing project-specific information. As much of this\nproject-specific context cannot fit into the prompts of LLMs, we must find ways\nto allow the model to explore the project-level code context. We present\nCoCoGen, a new code generation approach that uses compiler feedback to improve\nthe LLM-generated code. CoCoGen first leverages static analysis to identify\nmismatches between the generated code and the project's context. It then\niteratively aligns and fixes the identified errors using information extracted\nfrom the code repository. We integrate CoCoGen with two representative LLMs,\ni.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code\ngeneration. Experimental results show that CoCoGen significantly improves the\nvanilla LLMs by over 80% in generating code dependent on the project context\nand consistently outperforms the existing retrieval-based code generation\nbaselines.",
          "arxiv_id": "2403.16792v3"
        },
        {
          "title": "Building A Coding Assistant via the Retrieval-Augmented Language Model",
          "year": "2024-10",
          "abstract": "Pretrained language models have shown strong effectiveness in code-related\ntasks, such as code retrieval, code generation, code summarization, and code\ncompletion tasks. In this paper, we propose COde assistaNt viA\nretrieval-augmeNted language model (CONAN), which aims to build a code\nassistant by mimicking the knowledge-seeking behaviors of humans during coding.\nSpecifically, it consists of a code structure aware retriever (CONAN-R) and a\ndual-view code representation-based retrieval-augmented generation model\n(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and\nMasked Entity Prediction tasks to make language models code structure-aware and\nlearn effective representations for code snippets and documentation. Then\nCONAN-G designs a dual-view code representation mechanism for implementing a\nretrieval-augmented code generation model. CONAN-G regards the code\ndocumentation descriptions as prompts, which help language models better\nunderstand the code semantics. Our experiments show that CONAN achieves\nconvincing performance on different code generation tasks and significantly\noutperforms previous retrieval augmented code generation models. Our further\nanalyses show that CONAN learns tailored representations for both code snippets\nand documentation by aligning code-documentation data pairs and capturing\nstructural semantics by masking and predicting entities in the code data.\nAdditionally, the retrieved code snippets and documentation provide necessary\ninformation from both program language and natural language to assist the code\ngeneration process. CONAN can also be used as an assistant for Large Language\nModels (LLMs), providing LLMs with external knowledge in shorter code document\nlengths to improve their effectiveness on various code tasks. It shows the\nability of CONAN to extract necessary information and help filter out the noise\nfrom retrieved code documents.",
          "arxiv_id": "2410.16229v2"
        }
      ],
      "15": [
        {
          "title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
          "year": "2024-04",
          "abstract": "The pace of scientific research, vital for improving human life, is complex,\nslow, and needs specialized expertise. Meanwhile, novel, impactful research\noften stems from both a deep understanding of prior work, and a\ncross-pollination of ideas across domains and fields. To enhance the\nproductivity of researchers, we propose ResearchAgent, which leverages the\nencyclopedic knowledge and linguistic reasoning capabilities of Large Language\nModels (LLMs) to assist them in their work. This system automatically defines\nnovel problems, proposes methods and designs experiments, while iteratively\nrefining them based on the feedback from collaborative LLM-powered reviewing\nagents. Specifically, starting with a core scientific paper, ResearchAgent is\naugmented not only with relevant publications by connecting information over an\nacademic graph but also entities retrieved from a knowledge store derived from\nshared underlying concepts mined across numerous papers. Then, mimicking a\nscientific approach to improving ideas with peer discussions, we leverage\nmultiple LLM-based ReviewingAgents that provide reviews and feedback via\niterative revision processes. These reviewing agents are instantiated with\nhuman preference-aligned LLMs whose criteria for evaluation are elicited from\nactual human judgments via LLM prompting. We experimentally validate our\nResearchAgent on scientific publications across multiple disciplines, showing\nits effectiveness in generating novel, clear, and valid ideas based on both\nhuman and model-based evaluation results. Our initial foray into AI-mediated\nscientific research has important implications for the development of future\nsystems aimed at supporting researchers in their ideation and\noperationalization of novel work.",
          "arxiv_id": "2404.07738v2"
        },
        {
          "title": "Context-Enhanced Language Models for Generating Multi-Paper Citations",
          "year": "2024-04",
          "abstract": "Citation text plays a pivotal role in elucidating the connection between\nscientific documents, demanding an in-depth comprehension of the cited paper.\nConstructing citations is often time-consuming, requiring researchers to delve\ninto extensive literature and grapple with articulating relevant content. To\naddress this challenge, the field of citation text generation (CTG) has\nemerged. However, while earlier methods have primarily centered on creating\nsingle-sentence citations, practical scenarios frequently necessitate citing\nmultiple papers within a single paragraph. To bridge this gap, we propose a\nmethod that leverages Large Language Models (LLMs) to generate multi-citation\nsentences. Our approach involves a single source paper and a collection of\ntarget papers, culminating in a coherent paragraph containing multi-sentence\ncitation text. Furthermore, we introduce a curated dataset named MCG-S2ORC,\ncomposed of English-language academic research papers in Computer Science,\nshowcasing multiple citation instances. In our experiments, we evaluate three\nLLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this\nendeavor. Additionally, we exhibit enhanced performance by integrating\nknowledge graphs from target papers into the prompts for generating citation\ntext. This research underscores the potential of harnessing LLMs for citation\ngeneration, opening a compelling avenue for exploring the intricate connections\nbetween scientific documents.",
          "arxiv_id": "2404.13865v1"
        },
        {
          "title": "Detecting and analyzing missing citations to published scientific entities",
          "year": "2022-10",
          "abstract": "Proper citation is of great importance in academic writing for it enables\nknowledge accumulation and maintains academic integrity. However, citing\nproperly is not an easy task. For published scientific entities, the\never-growing academic publications and over-familiarity of terms easily lead to\nmissing citations. To deal with this situation, we design a special method\nCitation Recommendation for Published Scientific Entity (CRPSE) based on the\ncooccurrences between published scientific entities and in-text citations in\nthe same sentences from previous researchers. Experimental outcomes show the\neffectiveness of our method in recommending the source papers for published\nscientific entities. We further conduct a statistical analysis on missing\ncitations among papers published in prestigious computer science conferences in\n2020. In the 12,278 papers collected, 475 published scientific entities of\ncomputer science and mathematics are found to have missing citations. Many\nentities mentioned without citations are found to be well-accepted research\nresults. On a median basis, the papers proposing these published scientific\nentities with missing citations were published 8 years ago, which can be\nconsidered the time frame for a published scientific entity to develop into a\nwell-accepted concept. For published scientific entities, we appeal for\naccurate and full citation of their source papers as required by academic\nstandards.",
          "arxiv_id": "2210.10073v1"
        }
      ],
      "16": [
        {
          "title": "Sentiment analysis and opinion mining on educational data: A survey",
          "year": "2023-02",
          "abstract": "Sentiment analysis AKA opinion mining is one of the most widely used NLP\napplications to identify human intentions from their reviews. In the education\nsector, opinion mining is used to listen to student opinions and enhance their\nlearning-teaching practices pedagogically. With advancements in sentiment\nannotation techniques and AI methodologies, student comments can be labelled\nwith their sentiment orientation without much human intervention. In this\nreview article, (1) we consider the role of emotional analysis in education\nfrom four levels: document level, sentence level, entity level, and aspect\nlevel, (2) sentiment annotation techniques including lexicon-based and\ncorpus-based approaches for unsupervised annotations are explored, (3) the role\nof AI in sentiment analysis with methodologies like machine learning, deep\nlearning, and transformers are discussed, (4) the impact of sentiment analysis\non educational procedures to enhance pedagogy, decision-making, and evaluation\nare presented. Educational institutions have been widely invested to build\nsentiment analysis tools and process their student feedback to draw their\nopinions and insights. Applications built on sentiment analysis of student\nfeedback are reviewed in this study. Challenges in sentiment analysis like\nmulti-polarity, polysemous, negation words, and opinion spam detection are\nexplored and their trends in the research space are discussed. The future\ndirections of sentiment analysis in education are discussed.",
          "arxiv_id": "2302.04359v1"
        },
        {
          "title": "Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction",
          "year": "2021-09",
          "abstract": "Aspect-level sentiment classification (ALSC) aims at identifying the\nsentiment polarity of a specified aspect in a sentence. ALSC is a practical\nsetting in aspect-based sentiment analysis due to no opinion term labeling\nneeded, but it fails to interpret why a sentiment polarity is derived for the\naspect. To address this problem, recent works fine-tune pre-trained Transformer\nencoders for ALSC to extract an aspect-centric dependency tree that can locate\nthe opinion words. However, the induced opinion words only provide an intuitive\ncue far below human-level interpretability. Besides, the pre-trained encoder\ntends to internalize an aspect's intrinsic sentiment, causing sentiment bias\nand thus affecting model performance. In this paper, we propose a span-based\nanti-bias aspect representation learning framework. It first eliminates the\nsentiment bias in the aspect embedding by adversarial learning against aspects'\nprior sentiment. Then, it aligns the distilled opinion candidates with the\naspect by span-based dependency modeling to highlight the interpretable opinion\nterms. Our method achieves new state-of-the-art performance on five benchmarks,\nwith the capability of unsupervised opinion extraction.",
          "arxiv_id": "2109.02403v2"
        },
        {
          "title": "Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa",
          "year": "2022-07",
          "abstract": "Text sentiment analysis, also known as opinion mining, is research on the\ncalculation of people's views, evaluations, attitude and emotions expressed by\nentities. Text sentiment analysis can be divided into text-level sentiment\nanalysis, sen-tence-level sentiment analysis and aspect-level sentiment\nanalysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the\nfield of sentiment analysis, which aims to predict the polarity of aspects. The\nresearch of pre-training neural model has significantly improved the\nperformance of many natural language processing tasks. In recent years, pre\ntraining model (PTM) has been applied in ABSA. Therefore, there has been a\nquestion, which is whether PTMs contain sufficient syntactic information for\nABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced\nBERT with disentangled attention) to solve Aspect-Based Sentiment Analysis\nproblem. DeBERTa is a kind of neural language model based on transformer, which\nuses self-supervised learning to pre-train on a large number of original text\ncorpora. Based on the Local Context Focus (LCF) mechanism, by integrating\nDeBERTa model, we purpose a multi-task learning model for aspect-based\nsentiment analysis. The experiments result on the most commonly used the laptop\nand restaurant datasets of SemEval-2014 and the ACL twitter dataset show that\nLCF mechanism with DeBERTa has significant improvement.",
          "arxiv_id": "2207.02424v2"
        }
      ],
      "17": [
        {
          "title": "Cross-Lingual Language Model Meta-Pretraining",
          "year": "2021-09",
          "abstract": "The success of pretrained cross-lingual language models relies on two\nessential abilities, i.e., generalization ability for learning downstream tasks\nin a source language, and cross-lingual transferability for transferring the\ntask knowledge to other languages. However, current methods jointly learn the\ntwo abilities in a single-phase cross-lingual pretraining process, resulting in\na trade-off between generalization and cross-lingual transfer. In this paper,\nwe propose cross-lingual language model meta-pretraining, which learns the two\nabilities in different training phases. Our method introduces an additional\nmeta-pretraining phase before cross-lingual pretraining, where the model learns\ngeneralization ability on a large-scale monolingual corpus. Then, the model\nfocuses on learning cross-lingual transfer on a multilingual corpus.\nExperimental results show that our method improves both generalization and\ncross-lingual transfer, and produces better-aligned representations across\ndifferent languages.",
          "arxiv_id": "2109.11129v1"
        },
        {
          "title": "Transfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese",
          "year": "2023-04",
          "abstract": "Multilingual language models have pushed state-of-the-art in cross-lingual\nNLP transfer. The majority of zero-shot cross-lingual transfer, however, use\none and the same massively multilingual transformer (e.g., mBERT or XLM-R) to\ntransfer to all target languages, irrespective of their typological,\netymological, and phylogenetic relations to other languages. In particular,\nreadily available data and models of resource-rich sibling languages are often\nignored. In this work, we empirically show, in a case study for Faroese -- a\nlow-resource language from a high-resource language family -- that by\nleveraging the phylogenetic information and departing from the\n'one-size-fits-all' paradigm, one can improve cross-lingual transfer to\nlow-resource languages. In particular, we leverage abundant resources of other\nScandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for\nthe benefit of Faroese. Our evaluation results show that we can substantially\nimprove the transfer performance to Faroese by exploiting data and models of\nclosely-related high-resource languages. Further, we release a new web corpus\nof Faroese and Faroese datasets for named entity recognition (NER), semantic\ntext similarity (STS), and new language models trained on all Scandinavian\nlanguages.",
          "arxiv_id": "2304.08823v1"
        },
        {
          "title": "Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers",
          "year": "2023-06",
          "abstract": "Without any explicit cross-lingual training data, multilingual language\nmodels can achieve cross-lingual transfer. One common way to improve this\ntransfer is to perform realignment steps before fine-tuning, i.e., to train the\nmodel to build similar representations for pairs of words from translated\nsentences. But such realignment methods were found to not always improve\nresults across languages and tasks, which raises the question of whether\naligned representations are truly beneficial for cross-lingual transfer. We\nprovide evidence that alignment is actually significantly correlated with\ncross-lingual transfer across languages, models and random seeds. We show that\nfine-tuning can have a significant impact on alignment, depending mainly on the\ndownstream task and the model. Finally, we show that realignment can, in some\ninstances, improve cross-lingual transfer, and we identify conditions in which\nrealignment methods provide significant improvements. Namely, we find that\nrealignment works better on tasks for which alignment is correlated with\ncross-lingual transfer when generalizing to a distant language and with smaller\nmodels, as well as when using a bilingual dictionary rather than FastAlign to\nextract realignment pairs. For example, for POS-tagging, between English and\nArabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even\noutperforming XLM-R Large by 1.7. We thus advocate for further research on\nrealignment methods for smaller multilingual models as an alternative to\nscaling.",
          "arxiv_id": "2306.02790v1"
        }
      ],
      "18": [
        {
          "title": "Harnessing the Plug-and-Play Controller by Prompting",
          "year": "2024-02",
          "abstract": "Controllable text generation is a growing field within natural language\ngeneration (NLG) that focuses on producing text that meets specific constraints\nin real-world applications. Previous approaches, such as plug-and-play\ncontrollers (PPCs), aimed to steer the properties of generated text in a\nflexible manner. However, these methods often compromised the integrity of the\nlanguage model's decoding process, resulting in less smooth text generation.\nAlternatively, other techniques utilized multiple attribute prompts to align\nthe generated text with desired attributes, but this approach required prompt\ndesign for each attribute and was dependent on the size of the language model.\nThis paper introduces a novel method for flexible attribute control in text\ngeneration using pre-trained language models (PLMs). The proposed approach aims\nto enhance the fluency of generated text by guiding the generation process with\nPPCs. The key idea is to dynamically adjust the distribution of generated text\nby modifying prompts, effectively constraining the output space of the language\nmodel and influencing the desired attribute. To enable smooth cooperation\nbetween the PLM and the PPC, our work innovatively proposes a new model\nfine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback\n(RLDAF).This fine-tuning process adapts a small subset of the language model's\nparameters based on the generating actions taken during the PPC control\nprocess. The resulting harmonious collaboration between the PLM and PPC leads\nto improved smoothness in text generation during inference. Extensive\nexperiments were conducted on the SST2 dataset, and the proposed method\noutperformed previous approaches in various evaluation metrics, including text\nfluency and attribute consistency.",
          "arxiv_id": "2402.04160v1"
        },
        {
          "title": "SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes",
          "year": "2022-12",
          "abstract": "Is it possible to train a general metric for evaluating text generation\nquality without human annotated ratings? Existing learned metrics either\nperform unsatisfactorily across text generation tasks or require human ratings\nfor training on specific tasks. In this paper, we propose SESCORE2, a\nself-supervised approach for training a model-based metric for text generation\nevaluation. The key concept is to synthesize realistic model mistakes by\nperturbing sentences retrieved from a corpus. The primary advantage of the\nSESCORE2 is its ease of extension to many other languages while providing\nreliable severity estimation. We evaluate SESCORE2 and previous methods on four\ntext generation tasks across three languages. SESCORE2 outperforms unsupervised\nmetric PRISM on four text generation evaluation benchmarks, with a Kendall\nimprovement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised\nBLEURT and COMET on multiple text generation tasks. The code and data are\navailable at https://github.com/xu1998hz/SEScore2.",
          "arxiv_id": "2212.09305v2"
        },
        {
          "title": "CoNT: Contrastive Neural Text Generation",
          "year": "2022-05",
          "abstract": "Recently, contrastive learning attracts increasing interests in neural text\ngeneration as a new solution to alleviate the exposure bias problem. It\nintroduces a sequence-level training signal which is crucial to generation\ntasks that always rely on auto-regressive decoding. However, previous methods\nusing contrastive learning in neural text generation usually lead to inferior\nperformance. In this paper, we analyse the underlying reasons and propose a new\nContrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks\nthat prevent contrastive learning from being widely adopted in generation tasks\nfrom three aspects -- the construction of contrastive examples, the choice of\nthe contrastive loss, and the strategy in decoding. We validate CoNT on five\ngeneration tasks with ten benchmarks, including machine translation,\nsummarization, code comment generation, data-to-text generation and commonsense\ngeneration. Experimental results show that CoNT clearly outperforms the\nconventional training framework on all the ten benchmarks with a convincing\nmargin. Especially, CoNT surpasses previous the most competitive contrastive\nlearning method for text generation, by 1.50 BLEU on machine translation and\n1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art\non summarization, code comment generation (without external data) and\ndata-to-text generation.",
          "arxiv_id": "2205.14690v4"
        }
      ],
      "19": [
        {
          "title": "TSO: Self-Training with Scaled Preference Optimization",
          "year": "2024-08",
          "abstract": "Enhancing the conformity of large language models (LLMs) to human preferences\nremains an ongoing research challenge. Recently, offline approaches such as\nDirect Preference Optimization (DPO) have gained prominence as attractive\noptions due to offering effective improvement in simple, efficient, and stable\nwithout interactions with reward models. However, these offline preference\noptimization methods highly rely on the quality of pairwise preference samples.\nMeanwhile, numerous iterative methods require additional training of reward\nmodels to select positive and negative samples from the model's own generated\nresponses for preference learning. Furthermore, as LLMs' capabilities advance,\nit is quite challenging to continuously construct high-quality positive and\nnegative preference instances from the model's outputs due to the lack of\ndiversity. To tackle these challenges, we propose TSO, or Self-Training with\nScaled Preference Optimization, a framework for preference optimization that\nconducts self-training preference learning without training an additional\nreward model. TSO enhances the diversity of responses by constructing a model\nmatrix and incorporating human preference responses. Furthermore, TSO\nintroduces corrections for model preference errors through human and AI\nfeedback. Finally, TSO adopts iterative and dual clip reward strategies to\nupdate the reference model and its responses, adaptively adjusting preference\ndata and balancing the optimization process. Experimental results demonstrate\nthat TSO outperforms existing mainstream methods on various alignment\nevaluation benchmarks, providing practical insight into preference data\nconstruction and model training strategies in the alignment domain.",
          "arxiv_id": "2409.02118v1"
        },
        {
          "title": "AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization",
          "year": "2024-10",
          "abstract": "Aligning large language models (LLMs) with human values and intentions is\ncrucial for their utility, honesty, and safety. Reinforcement learning from\nhuman feedback (RLHF) is a popular approach to achieve this alignment, but it\nfaces challenges in computational efficiency and training stability. Recent\nmethods like Direct Preference Optimization (DPO) and Simple Preference\nOptimization (SimPO) have proposed offline alternatives to RLHF, simplifying\nthe process by reparameterizing the reward function. However, DPO depends on a\npotentially suboptimal reference model, and SimPO's assumption of a fixed\ntarget reward margin may lead to suboptimal decisions in diverse data settings.\nIn this work, we propose $\\alpha$-DPO, an adaptive preference optimization\nalgorithm designed to address these limitations by introducing a dynamic reward\nmargin. Specifically, $\\alpha$-DPO employs an adaptive preference distribution,\nbalancing the policy model and the reference model to achieve personalized\nreward margins. We provide theoretical guarantees for $\\alpha$-DPO,\ndemonstrating its effectiveness as a surrogate optimization objective and its\nability to balance alignment and diversity through KL divergence control.\nEmpirical evaluations on AlpacaEval 2 and Arena-Hard show that $\\alpha$-DPO\nconsistently outperforms DPO and SimPO across various model settings,\nestablishing it as a robust approach for fine-tuning LLMs. Our method achieves\nsignificant improvements in win rates, highlighting its potential as a powerful\ntool for LLM alignment. The code is available at\nhttps://github.com/junkangwu/alpha-DPO",
          "arxiv_id": "2410.10148v4"
        },
        {
          "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization",
          "year": "2024-09",
          "abstract": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.",
          "arxiv_id": "2409.03650v2"
        }
      ],
      "20": [
        {
          "title": "Dynamic Named Entity Recognition",
          "year": "2023-02",
          "abstract": "Named Entity Recognition (NER) is a challenging and widely studied task that\ninvolves detecting and typing entities in text. So far,NER still approaches\nentity typing as a task of classification into universal classes (e.g. date,\nperson, or location). Recent advances innatural language processing focus on\narchitectures of increasing complexity that may lead to overfitting and\nmemorization, and thus, underuse of context. Our work targets situations where\nthe type of entities depends on the context and cannot be solved solely by\nmemorization. We hence introduce a new task: Dynamic Named Entity Recognition\n(DNER), providing a framework to better evaluate the ability of algorithms to\nextract entities by exploiting the context. The DNER benchmark is based on two\ndatasets, DNER-RotoWire and DNER-IMDb. We evaluate baseline models and present\nexperiments reflecting issues and research axes related to this novel task.",
          "arxiv_id": "2302.10314v1"
        },
        {
          "title": "A Unified Generative Framework for Various NER Subtasks",
          "year": "2021-06",
          "abstract": "Named Entity Recognition (NER) is the task of identifying spans that\nrepresent entities in sentences. Whether the entity spans are nested or\ndiscontinuous, the NER task can be categorized into the flat NER, nested NER,\nand discontinuous NER subtasks. These subtasks have been mainly solved by the\ntoken-level sequence labelling or span-level classification. However, these\nsolutions can hardly tackle the three kinds of NER subtasks concurrently. To\nthat end, we propose to formulate the NER subtasks as an entity span sequence\ngeneration task, which can be solved by a unified sequence-to-sequence\n(Seq2Seq) framework. Based on our unified framework, we can leverage the\npre-trained Seq2Seq model to solve all three kinds of NER subtasks without the\nspecial design of the tagging schema or ways to enumerate spans. We exploit\nthree types of entity representations to linearize entities into a sequence.\nOur proposed framework is easy-to-implement and achieves state-of-the-art\n(SoTA) or near SoTA performance on eight English NER datasets, including two\nflat NER datasets, three nested NER datasets, and three discontinuous NER\ndatasets.",
          "arxiv_id": "2106.01223v1"
        },
        {
          "title": "NaijaNER : Comprehensive Named Entity Recognition for 5 Nigerian Languages",
          "year": "2021-03",
          "abstract": "Most of the common applications of Named Entity Recognition (NER) is on\nEnglish and other highly available languages. In this work, we present our\nfindings on Named Entity Recognition for 5 Nigerian Languages (Nigerian\nEnglish, Nigerian Pidgin English, Igbo, Yoruba and Hausa). These languages are\nconsidered low-resourced, and very little openly available Natural Language\nProcessing work has been done in most of them. In this work, individual NER\nmodels were trained and metrics recorded for each of the languages. We also\nworked on a combined model that can handle Named Entity Recognition (NER) for\nany of the five languages. The combined model works well for Named Entity\nRecognition(NER) on each of the languages and with better performance compared\nto individual NER models trained specifically on annotated data for the\nspecific language. The aim of this work is to share our learning on how\ninformation extraction using Named Entity Recognition can be optimized for the\nlisted Nigerian Languages for inclusion, ease of deployment in production and\nreusability of models. Models developed during this project are available on\nGitHub https://git.io/JY0kk and an interactive web app\nhttps://nigner.herokuapp.com/.",
          "arxiv_id": "2105.00810v1"
        }
      ],
      "21": [
        {
          "title": "Halu-J: Critique-Based Hallucination Judge",
          "year": "2024-07",
          "abstract": "Large language models (LLMs) frequently generate non-factual content, known\nas hallucinations. Existing retrieval-augmented-based hallucination detection\napproaches typically address this by framing it as a classification task,\nevaluating hallucinations based on their consistency with retrieved evidence.\nHowever, this approach usually lacks detailed explanations for these\nevaluations and does not assess the reliability of these explanations.\nFurthermore, deficiencies in retrieval systems can lead to irrelevant or\npartially relevant evidence retrieval, impairing the detection process.\nMoreover, while real-world hallucination detection requires analyzing multiple\npieces of evidence, current systems usually treat all evidence uniformly\nwithout considering its relevance to the content. To address these challenges,\nwe introduce Halu-J, a critique-based hallucination judge with 7 billion\nparameters. Halu-J enhances hallucination detection by selecting pertinent\nevidence and providing detailed critiques. Our experiments indicate that Halu-J\noutperforms GPT-4o in multiple-evidence hallucination detection and matches its\ncapability in critique generation and evidence selection. We also introduce\nME-FEVER, a new dataset designed for multiple-evidence hallucination detection.\nOur code and dataset can be found in https://github.com/GAIR-NLP/factool .",
          "arxiv_id": "2407.12943v1"
        },
        {
          "title": "Theoretical Foundations and Mitigation of Hallucination in Large Language Models",
          "year": "2025-07",
          "abstract": "Hallucination in Large Language Models (LLMs) refers to the generation of\ncontent that is not faithful to the input or the real-world facts. This paper\nprovides a rigorous treatment of hallucination in LLMs, including formal\ndefinitions and theoretical analyses. We distinguish between intrinsic and\nextrinsic hallucinations, and define a \\textit{hallucination risk} for models.\nWe derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes\nand Rademacher complexity). We then survey detection strategies for\nhallucinations, such as token-level uncertainty estimation, confidence\ncalibration, and attention alignment checks. On the mitigation side, we discuss\napproaches including retrieval-augmented generation, hallucination-aware\nfine-tuning, logit calibration, and the incorporation of fact-verification\nmodules. We propose a unified detection and mitigation workflow, illustrated\nwith a diagram, to integrate these strategies. Finally, we outline evaluation\nprotocols for hallucination, recommending datasets, metrics, and experimental\nsetups to quantify and reduce hallucinations. Our work lays a theoretical\nfoundation and practical guidelines for addressing the crucial challenge of\nhallucination in LLMs.",
          "arxiv_id": "2507.22915v1"
        },
        {
          "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
          "year": "2024-07",
          "abstract": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.",
          "arxiv_id": "2407.04693v2"
        }
      ],
      "22": [
        {
          "title": "SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval",
          "year": "2023-04",
          "abstract": "Legal case retrieval, which aims to find relevant cases for a query case,\nplays a core role in the intelligent legal system. Despite the success that\npre-training has achieved in ad-hoc retrieval tasks, effective pre-training\nstrategies for legal case retrieval remain to be explored. Compared with\ngeneral documents, legal case documents are typically long text sequences with\nintrinsic logical structures. However, most existing language models have\ndifficulty understanding the long-distance dependencies between different\nstructures. Moreover, in contrast to the general retrieval, the relevance in\nthe legal domain is sensitive to key legal elements. Even subtle differences in\nkey legal elements can significantly affect the judgement of relevance.\nHowever, existing pre-trained language models designed for general purposes\nhave not been equipped to handle legal elements.\n  To address these issues, in this paper, we propose SAILER, a new\nStructure-Aware pre-traIned language model for LEgal case Retrieval. It is\nhighlighted in the following three aspects: (1) SAILER fully utilizes the\nstructural information contained in legal case documents and pays more\nattention to key legal elements, similar to how legal experts browse legal case\ndocuments. (2) SAILER employs an asymmetric encoder-decoder architecture to\nintegrate several different pre-training objectives. In this way, rich semantic\ninformation across tasks is encoded into dense vectors. (3) SAILER has powerful\ndiscriminative ability, even without any legal annotation data. It can\ndistinguish legal cases with different charges accurately. Extensive\nexperiments over publicly available legal benchmarks demonstrate that our\napproach can significantly outperform previous state-of-the-art methods in\nlegal case retrieval.",
          "arxiv_id": "2304.11370v1"
        },
        {
          "title": "LAiW: A Chinese Legal Large Language Models Benchmark",
          "year": "2023-10",
          "abstract": "General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.",
          "arxiv_id": "2310.05620v2"
        },
        {
          "title": "DeliLaw: A Chinese Legal Counselling System Based on a Large Language Model",
          "year": "2024-08",
          "abstract": "Traditional legal retrieval systems designed to retrieve legal documents,\nstatutes, precedents, and other legal information are unable to give\nsatisfactory answers due to lack of semantic understanding of specific\nquestions. Large Language Models (LLMs) have achieved excellent results in a\nvariety of natural language processing tasks, which inspired us that we train a\nLLM in the legal domain to help legal retrieval. However, in the Chinese legal\ndomain, due to the complexity of legal questions and the rigour of legal\narticles, there is no legal large model with satisfactory practical application\nyet. In this paper, we present DeliLaw, a Chinese legal counselling system\nbased on a large language model. DeliLaw integrates a legal retrieval module\nand a case retrieval module to overcome the model hallucination. Users can\nconsult professional legal questions, search for legal articles and relevant\njudgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,\nDeliLaw supports the use of English for counseling. we provide the address of\nthe system: https://data.delilegal.com/lawQuestion.",
          "arxiv_id": "2408.00357v1"
        }
      ],
      "23": [
        {
          "title": "Efficient Relational Context Perception for Knowledge Graph Completion",
          "year": "2024-12",
          "abstract": "Knowledge Graphs (KGs) provide a structured representation of knowledge but\noften suffer from challenges of incompleteness. To address this, link\nprediction or knowledge graph completion (KGC) aims to infer missing new facts\nbased on existing facts in KGs. Previous knowledge graph embedding models are\nlimited in their ability to capture expressive features, especially when\ncompared to deeper, multi-layer models. These approaches also assign a single\nstatic embedding to each entity and relation, disregarding the fact that\nentities and relations can exhibit different behaviors in varying graph\ncontexts. Due to complex context over a fact triple of a KG, existing methods\nhave to leverage complex non-linear context encoder, like transformer, to\nproject entity and relation into low dimensional representations, resulting in\nhigh computation cost. To overcome these limitations, we propose Triple\nReceptance Perception (TRP) architecture to model sequential information,\nenabling the learning of dynamic context of entities and relations. Then we use\ntensor decomposition to calculate triple scores, providing robust relational\ndecoding capabilities. This integration allows for more expressive\nrepresentations. Experiments on benchmark datasets such as YAGO3-10, UMLS,\nFB15k, and FB13 in link prediction and triple classification tasks demonstrate\nthat our method performs better than several state-of-the-art models, proving\nthe effectiveness of the integration.",
          "arxiv_id": "2501.00397v1"
        },
        {
          "title": "Farspredict: A benchmark dataset for link prediction",
          "year": "2023-03",
          "abstract": "Link prediction with knowledge graph embedding (KGE) is a popular method for\nknowledge graph completion. Furthermore, training KGEs on non-English knowledge\ngraph promote knowledge extraction and knowledge graph reasoning in the context\nof these languages. However, many challenges in non-English KGEs pose to\nlearning a low-dimensional representation of a knowledge graph's entities and\nrelations. This paper proposes \"Farspredict\" a Persian knowledge graph based on\nFarsbase (the most comprehensive knowledge graph in Persian). It also explains\nhow the knowledge graph structure affects link prediction accuracy in KGE. To\nevaluate Farspredict, we implemented the popular models of KGE on it and\ncompared the results with Freebase. Given the analysis results, some\noptimizations on the knowledge graph are carried out to improve its\nfunctionality in the KGE. As a result, a new Persian knowledge graph is\nachieved. Implementation results in the KGE models on Farspredict outperforming\nFreebases in many cases. At last, we discuss what improvements could be\neffective in enhancing the quality of Farspredict and how much it improves.",
          "arxiv_id": "2303.14647v1"
        },
        {
          "title": "FedE: Embedding Knowledge Graphs in Federated Setting",
          "year": "2020-10",
          "abstract": "Knowledge graphs (KGs) consisting of triples are always incomplete, so it's\nimportant to do Knowledge Graph Completion (KGC) by predicting missing triples.\nMulti-Source KG is a common situation in real KG applications which can be\nviewed as a set of related individual KGs where different KGs contains\nrelations of different aspects of entities. It's intuitive that, for each\nindividual KG, its completion could be greatly contributed by the triples\ndefined and labeled in other ones. However, because of the data privacy and\nsensitivity, a set of relevant knowledge graphs cannot complement each other's\nKGC by just collecting data from different knowledge graphs together.\nTherefore, in this paper, we introduce federated setting to keep their privacy\nwithout triple transferring between KGs and apply it in embedding knowledge\ngraph, a typical method which have proven effective for KGC in the past decade.\nWe propose a Federated Knowledge Graph Embedding framework FedE, focusing on\nlearning knowledge graph embeddings by aggregating locally-computed updates.\nFinally, we conduct extensive experiments on datasets derived from KGE\nbenchmark datasets and results show the effectiveness of our proposed FedE.",
          "arxiv_id": "2010.12882v1"
        }
      ],
      "24": [
        {
          "title": "Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling and Appraisal-based Approaches",
          "year": "2023-09",
          "abstract": "The term emotion analysis in text subsumes various natural language\nprocessing tasks which have in common the goal to enable computers to\nunderstand emotions. Most popular is emotion classification in which one or\nmultiple emotions are assigned to a predefined textual unit. While such setting\nis appropriate for identifying the reader's or author's emotion, emotion role\nlabeling adds the perspective of mentioned entities and extracts text spans\nthat correspond to the emotion cause. The underlying emotion theories agree on\none important point; that an emotion is caused by some internal or external\nevent and comprises several subcomponents, including the subjective feeling and\na cognitive evaluation. We therefore argue that emotions and events are related\nin two ways. (1) Emotions are events; and this perspective is the fundament in\nnatural language processing for emotion role labeling. (2) Emotions are caused\nby events; a perspective that is made explicit with research how to incorporate\npsychological appraisal theories in NLP models to interpret events. These two\nresearch directions, role labeling and (event-focused) emotion classification,\nhave by and large been tackled separately. In this paper, we contextualize both\nperspectives and discuss open research questions.",
          "arxiv_id": "2309.02092v3"
        },
        {
          "title": "Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations",
          "year": "2024-04",
          "abstract": "In human-computer interaction, it is crucial for agents to respond to human\nby understanding their emotions. Unraveling the causes of emotions is more\nchallenging. A new task named Multimodal Emotion-Cause Pair Extraction in\nConversations is responsible for recognizing emotion and identifying causal\nexpressions. In this study, we propose a multi-stage framework to generate\nemotion and extract the emotion causal pairs given the target emotion. In the\nfirst stage, Llama-2-based InstructERC is utilized to extract the emotion\ncategory of each utterance in a conversation. After emotion recognition, a\ntwo-stream attention model is employed to extract the emotion causal pairs\ngiven the target emotion for subtask 2 while MuTEC is employed to extract\ncausal span for subtask 1. Our approach achieved first place for both of the\ntwo subtasks in the competition.",
          "arxiv_id": "2404.16905v1"
        },
        {
          "title": "EmoVerse: Exploring Multimodal Large Language Models for Sentiment and Emotion Understanding",
          "year": "2024-12",
          "abstract": "Sentiment and emotion understanding are essential to applications such as\nhuman-computer interaction and depression detection. While Multimodal Large\nLanguage Models (MLLMs) demonstrate robust general capabilities, they face\nconsiderable challenges in the field of affective computing, particularly in\ndetecting subtle facial expressions and handling complex emotion-related tasks,\nsuch as emotion reason inference and understanding emotions in long-context\nscenarios. Furthermore, there is a lack of a unified MLLM that can effectively\nhandle both sentiment and emotion-related tasks. To address these challenges,\nwe explore multi-task training strategies for MLLMs in affective computing and\nintroduce Emotion Universe (EmoVerse), an MLLM designed to handle a broad\nspectrum of sentiment and emotion-related tasks. In addition, EmoVerse is\ncapable of deeply analyzing the underlying causes of emotional states. We also\nintroduce the Affective Multitask (AMT) Dataset, which supports multimodal\nsentiment analysis, multimodal emotion recognition, facial expression\nrecognition, emotion reason inference, and emotion cause-pair extraction tasks.\nExtensive experiments demonstrate that EmoVerse outperforms existing methods,\nachieving state-of-the-art results in sentiment and emotion-related tasks. The\ncode is available at https://github.com/liaolea/EmoVerse.",
          "arxiv_id": "2412.08049v3"
        }
      ],
      "25": [
        {
          "title": "MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation",
          "year": "2024-12",
          "abstract": "Large Language Models (LLMs) are becoming essential tools for various natural\nlanguage processing tasks but often suffer from generating outdated or\nincorrect information. Retrieval-Augmented Generation (RAG) addresses this\nissue by incorporating external, real-time information retrieval to ground LLM\nresponses. However, the existing RAG systems frequently struggle with the\nquality of retrieval documents, as irrelevant or noisy documents degrade\nperformance, increase computational overhead, and undermine response\nreliability. To tackle this problem, we propose Multi-Agent Filtering\nRetrieval-Augmented Generation (MAIN-RAG), a training-free RAG framework that\nleverages multiple LLM agents to collaboratively filter and score retrieved\ndocuments. Specifically, MAIN-RAG introduces an adaptive filtering mechanism\nthat dynamically adjusts the relevance filtering threshold based on score\ndistributions, effectively minimizing noise while maintaining high recall of\nrelevant documents. The proposed approach leverages inter-agent consensus to\nensure robust document selection without requiring additional training data or\nfine-tuning. Experimental results across four QA benchmarks demonstrate that\nMAIN-RAG consistently outperforms traditional RAG approaches, achieving a 2-11%\nimprovement in answer accuracy while reducing the number of irrelevant\nretrieved documents. Quantitative analysis further reveals that our approach\nachieves superior response consistency and answer accuracy over baseline\nmethods, offering a competitive and practical alternative to training-based\nsolutions.",
          "arxiv_id": "2501.00332v1"
        },
        {
          "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
          "year": "2024-01",
          "abstract": "Retrieval-Augmented Generation (RAG) is a technique that enhances the\ncapabilities of large language models (LLMs) by incorporating external\nknowledge sources. This method addresses common LLM limitations, including\noutdated information and the tendency to produce inaccurate \"hallucinated\"\ncontent. However, the evaluation of RAG systems is challenging, as existing\nbenchmarks are limited in scope and diversity. Most of the current benchmarks\npredominantly assess question-answering applications, overlooking the broader\nspectrum of situations where RAG could prove advantageous. Moreover, they only\nevaluate the performance of the LLM component of the RAG pipeline in the\nexperiments, and neglect the influence of the retrieval component and the\nexternal knowledge database. To address these issues, this paper constructs a\nlarge-scale and more comprehensive benchmark, and evaluates all the components\nof RAG systems in various RAG application scenarios. Specifically, we have\ncategorized the range of RAG applications into four distinct types-Create,\nRead, Update, and Delete (CRUD), each representing a unique use case. \"Create\"\nrefers to scenarios requiring the generation of original, varied content.\n\"Read\" involves responding to intricate questions in knowledge-intensive\nsituations. \"Update\" focuses on revising and rectifying inaccuracies or\ninconsistencies in pre-existing texts. \"Delete\" pertains to the task of\nsummarizing extensive texts into more concise forms. For each of these CRUD\ncategories, we have developed comprehensive datasets to evaluate the\nperformance of RAG systems. We also analyze the effects of various components\nof the RAG system, such as the retriever, the context length, the knowledge\nbase construction, and the LLM. Finally, we provide useful insights for\noptimizing the RAG technology for different scenarios.",
          "arxiv_id": "2401.17043v3"
        },
        {
          "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
          "year": "2024-10",
          "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in\nmitigating hallucinations in Large Language Models (LLMs) by retrieving\nknowledge from external resources. To adapt LLMs for the RAG systems, current\napproaches use instruction tuning to optimize LLMs, improving their ability to\nutilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses\non equipping LLMs to handle diverse RAG tasks using different instructions.\nHowever, it trains RAG modules to overfit training signals and overlooks the\nvarying data preferences among agents within the RAG system. In this paper, we\npropose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG\nsystems by aligning data preferences between different RAG modules. DDR works\nby collecting the rewards to optimize each agent in the RAG system with the\nrollout method, which prompts agents to sample some potential responses as\nperturbations, evaluates the impact of these perturbations on the whole RAG\nsystem, and subsequently optimizes the agent to produce outputs that improve\nthe performance of the RAG system. Our experiments on various\nknowledge-intensive tasks demonstrate that DDR significantly outperforms the\nSFT method, particularly for LLMs with smaller-scale parameters that depend\nmore on the retrieved knowledge. Additionally, DDR exhibits a stronger\ncapability to align the data preference between RAG modules. The DDR method\nmakes the generation module more effective in extracting key information from\ndocuments and mitigating conflicts between parametric memory and external\nknowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
          "arxiv_id": "2410.13509v2"
        }
      ],
      "26": [
        {
          "title": "AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework",
          "year": "2024-03",
          "abstract": "The task of financial analysis primarily encompasses two key areas: stock\ntrend prediction and the corresponding financial question answering. Currently,\nmachine learning and deep learning algorithms (ML&DL) have been widely applied\nfor stock trend predictions, leading to significant progress. However, these\nmethods fail to provide reasons for predictions, lacking interpretability and\nreasoning processes. Also, they can not integrate textual information such as\nfinancial news or reports. Meanwhile, large language models (LLMs) have\nremarkable textual understanding and generation ability. But due to the\nscarcity of financial training datasets and limited integration with real-time\nknowledge, LLMs still suffer from hallucinations and are unable to keep up with\nthe latest information. To tackle these challenges, we first release AlphaFin\ndatasets, combining traditional research datasets, real-time financial data,\nand handwritten chain-of-thought (CoT) data. It has a positive impact on\ntraining LLMs for completing financial analysis. We then use AlphaFin datasets\nto benchmark a state-of-the-art method, called Stock-Chain, for effectively\ntackling the financial analysis task, which integrates retrieval-augmented\ngeneration (RAG) techniques. Extensive experiments are conducted to demonstrate\nthe effectiveness of our framework on financial analysis.",
          "arxiv_id": "2403.12582v1"
        },
        {
          "title": "Tracking Turbulence Through Financial News During COVID-19",
          "year": "2021-09",
          "abstract": "Grave human toll notwithstanding, the COVID-19 pandemic created uniquely\nunstable conditions in financial markets. In this work we uncover and discuss\nrelationships involving sentiment in financial publications during the 2020\npandemic-motivated U.S. financial crash. First, we introduce a set of expert\nannotations of financial sentiment for articles from major American financial\nnews publishers. After an exploratory data analysis, we then describe a\nCNN-based architecture to address the task of predicting financial sentiment in\nthis anomalous, tumultuous setting. Our best performing model achieves a\nmaximum weighted F1 score of 0.746, establishing a strong performance\nbenchmark. Using predictions from our top performing model, we close by\nconducting a statistical correlation study with real stock market data, finding\ninteresting and strong relationships between financial news and the S\\&P 500\nindex, trading volume, market volatility, and different single-factor ETFs.",
          "arxiv_id": "2109.04369v1"
        },
        {
          "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance",
          "year": "2023-06",
          "abstract": "Although large language models (LLMs) has shown great performance on natural\nlanguage processing (NLP) in the financial domain, there are no publicly\navailable financial tailtored LLMs, instruction tuning datasets, and evaluation\nbenchmarks, which is critical for continually pushing forward the open-source\ndevelopment of financial artificial intelligence (AI). This paper introduces\nPIXIU, a comprehensive framework including the first financial LLM based on\nfine-tuning LLaMA with instruction data, the first instruction data with 136K\ndata samples to support the fine-tuning, and an evaluation benchmark with 5\ntasks and 9 datasets. We first construct the large-scale multi-task instruction\ndata considering a variety of financial tasks, financial document types, and\nfinancial data modalities. We then propose a financial LLM called FinMA by\nfine-tuning LLaMA with the constructed dataset to be able to follow\ninstructions for various financial tasks. To support the evaluation of\nfinancial LLMs, we propose a standardized benchmark that covers a set of\ncritical financial tasks, including five financial NLP tasks and one financial\nprediction task. With this benchmark, we conduct a detailed analysis of FinMA\nand several existing LLMs, uncovering their strengths and weaknesses in\nhandling critical financial tasks. The model, datasets, benchmark, and\nexperimental results are open-sourced to facilitate future research in\nfinancial AI.",
          "arxiv_id": "2306.05443v1"
        }
      ],
      "27": [
        {
          "title": "Relation Extraction with Contextualized Relation Embedding (CRE)",
          "year": "2020-11",
          "abstract": "Relation extraction is the task of identifying relation instance between two\nentities given a corpus whereas Knowledge base modeling is the task of\nrepresenting a knowledge base, in terms of relations between entities. This\npaper proposes an architecture for the relation extraction task that integrates\nsemantic information with knowledge base modeling in a novel manner. Existing\napproaches for relation extraction either do not utilize knowledge base\nmodelling or use separately trained KB models for the RE task. We present a\nmodel architecture that internalizes KB modeling in relation extraction. This\nmodel applies a novel approach to encode sentences into contextualized relation\nembeddings, which can then be used together with parameterized entity\nembeddings to score relation instances. The proposed CRE model achieves state\nof the art performance on datasets derived from The New York Times Annotated\nCorpus and FreeBase. The source code has been made available.",
          "arxiv_id": "2011.09658v1"
        },
        {
          "title": "Multi-Attribute Relation Extraction (MARE) -- Simplifying the Application of Relation Extraction",
          "year": "2021-11",
          "abstract": "Natural language understanding's relation extraction makes innovative and\nencouraging novel business concepts possible and facilitates new digitilized\ndecision-making processes. Current approaches allow the extraction of relations\nwith a fixed number of entities as attributes. Extracting relations with an\narbitrary amount of attributes requires complex systems and costly\nrelation-trigger annotations to assist these systems. We introduce\nmulti-attribute relation extraction (MARE) as an assumption-less problem\nformulation with two approaches, facilitating an explicit mapping from business\nuse cases to the data annotations. Avoiding elaborated annotation constraints\nsimplifies the application of relation extraction approaches. The evaluation\ncompares our models to current state-of-the-art event extraction and binary\nrelation extraction methods. Our approaches show improvement compared to these\non the extraction of general multi-attribute relations.",
          "arxiv_id": "2111.09035v1"
        },
        {
          "title": "A Trigger-Sense Memory Flow Framework for Joint Entity and Relation Extraction",
          "year": "2021-01",
          "abstract": "Joint entity and relation extraction framework constructs a unified model to\nperform entity recognition and relation extraction simultaneously, which can\nexploit the dependency between the two tasks to mitigate the error propagation\nproblem suffered by the pipeline model. Current efforts on joint entity and\nrelation extraction focus on enhancing the interaction between entity\nrecognition and relation extraction through parameter sharing, joint decoding,\nor other ad-hoc tricks (e.g., modeled as a semi-Markov decision process, cast\nas a multi-round reading comprehension task). However, there are still two\nissues on the table. First, the interaction utilized by most methods is still\nweak and uni-directional, which is unable to model the mutual dependency\nbetween the two tasks. Second, relation triggers are ignored by most methods,\nwhich can help explain why humans would extract a relation in the sentence.\nThey're essential for relation extraction but overlooked. To this end, we\npresent a Trigger-Sense Memory Flow Framework (TriMF) for joint entity and\nrelation extraction. We build a memory module to remember category\nrepresentations learned in entity recognition and relation extraction tasks.\nAnd based on it, we design a multi-level memory flow attention mechanism to\nenhance the bi-directional interaction between entity recognition and relation\nextraction. Moreover, without any human annotations, our model can enhance\nrelation trigger information in a sentence through a trigger sensor module,\nwhich improves the model performance and makes model predictions with better\ninterpretation. Experiment results show that our proposed framework achieves\nstate-of-the-art results by improves the relation F1 to 52.44% (+3.2%) on\nSciERC, 66.49% (+4.9%) on ACE05, 72.35% (+0.6%) on CoNLL04 and 80.66% (+2.3%)\non ADE.",
          "arxiv_id": "2101.10213v3"
        }
      ],
      "28": [
        {
          "title": "New Faithfulness-Centric Interpretability Paradigms for Natural Language Processing",
          "year": "2024-11",
          "abstract": "As machine learning becomes more widespread and is used in more critical\napplications, it's important to provide explanations for these models, to\nprevent unintended behavior. Unfortunately, many current interpretability\nmethods struggle with faithfulness. Therefore, this Ph.D. thesis investigates\nthe question \"How to provide and ensure faithful explanations for complex\ngeneral-purpose neural NLP models?\" The main thesis is that we should develop\nnew paradigms in interpretability. This is achieved by first developing solid\nfaithfulness metrics and then applying the lessons learned from this\ninvestigation to develop new paradigms. The two new paradigms explored are\nfaithfulness measurable models (FMMs) and self-explanations. The idea in\nself-explanations is to have large language models explain themselves, we\nidentify that current models are not capable of doing this consistently.\nHowever, we suggest how this could be achieved. The idea of FMMs is to create\nmodels that are designed such that measuring faithfulness is cheap and precise.\nThis makes it possible to optimize an explanation towards maximum faithfulness,\nwhich makes FMMs designed to be explained. We find that FMMs yield explanations\nthat are near theoretical optimal in terms of faithfulness. Overall, from all\ninvestigations of faithfulness, results show that post-hoc and intrinsic\nexplanations are by default model and task-dependent. However, this was not the\ncase when using FMMs, even with the same post-hoc explanation methods. This\nshows, that even simple modifications to the model, such as randomly masking\nthe training dataset, as was done in FMMs, can drastically change the situation\nand result in consistently faithful explanations. This answers the question of\nhow to provide and ensure faithful explanations.",
          "arxiv_id": "2411.17992v1"
        },
        {
          "title": "MaNtLE: Model-agnostic Natural Language Explainer",
          "year": "2023-05",
          "abstract": "Understanding the internal reasoning behind the predictions of machine\nlearning systems is increasingly vital, given their rising adoption and\nacceptance. While previous approaches, such as LIME, generate algorithmic\nexplanations by attributing importance to input features for individual\nexamples, recent research indicates that practitioners prefer examining\nlanguage explanations that explain sub-groups of examples. In this paper, we\nintroduce MaNtLE, a model-agnostic natural language explainer that analyzes\nmultiple classifier predictions and generates faithful natural language\nexplanations of classifier rationale for structured classification tasks.\nMaNtLE uses multi-task training on thousands of synthetic classification tasks\nto generate faithful explanations. Simulated user studies indicate that, on\naverage, MaNtLE-generated explanations are at least 11% more faithful compared\nto LIME and Anchors explanations across three tasks. Human evaluations\ndemonstrate that users can better predict model behavior using explanations\nfrom MaNtLE compared to other techniques",
          "arxiv_id": "2305.12995v1"
        },
        {
          "title": "NILE : Natural Language Inference with Faithful Natural Language Explanations",
          "year": "2020-05",
          "abstract": "The recent growth in the popularity and success of deep learning models on\nNLP classification tasks has accompanied the need for generating some form of\nnatural language explanation of the predicted labels. Such generated natural\nlanguage (NL) explanations are expected to be faithful, i.e., they should\ncorrelate well with the model's internal decision making. In this work, we\nfocus on the task of natural language inference (NLI) and address the following\nquestion: can we build NLI systems which produce labels with high accuracy,\nwhile also generating faithful explanations of its decisions? We propose\nNatural-language Inference over Label-specific Explanations (NILE), a novel NLI\nmethod which utilizes auto-generated label-specific NL explanations to produce\nlabels along with its faithful explanation. We demonstrate NILE's effectiveness\nover previously reported methods through automated and human evaluation of the\nproduced labels and explanations. Our evaluation of NILE also supports the\nclaim that accurate systems capable of providing testable explanations of their\ndecisions can be designed. We discuss the faithfulness of NILE's explanations\nin terms of sensitivity of the decisions to the corresponding explanations. We\nargue that explicit evaluation of faithfulness, in addition to label and\nexplanation accuracy, is an important step in evaluating model's explanations.\nFurther, we demonstrate that task-specific probes are necessary to establish\nsuch sensitivity.",
          "arxiv_id": "2005.12116v1"
        }
      ],
      "29": [
        {
          "title": "XOR QA: Cross-lingual Open-Retrieval Question Answering",
          "year": "2020-10",
          "abstract": "Multilingual question answering tasks typically assume answers exist in the\nsame language as the question. Yet in practice, many languages face both\ninformation scarcity -- where languages have few reference articles -- and\ninformation asymmetry -- where questions reference concepts from other\ncultures. This work extends open-retrieval question answering to a\ncross-lingual setting enabling questions from one language to be answered via\nanswer content from another language. We construct a large-scale dataset built\non questions from TyDi QA lacking same-language answers. Our task formulation,\ncalled Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k\ninformation-seeking questions from across 7 diverse non-English languages.\nBased on this dataset, we introduce three new tasks that involve cross-lingual\ndocument retrieval using multi-lingual and English resources. We establish\nbaselines with state-of-the-art machine translation systems and cross-lingual\npretrained models. Experimental results suggest that XOR QA is a challenging\ntask that will facilitate the development of novel techniques for multilingual\nquestion answering. Our data and code are available at\nhttps://nlp.cs.washington.edu/xorqa.",
          "arxiv_id": "2010.11856v3"
        },
        {
          "title": "Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering",
          "year": "2021-09",
          "abstract": "In this paper we propose a novel approach towards improving the efficiency of\nQuestion Answering (QA) systems by filtering out questions that will not be\nanswered by them. This is based on an interesting new finding: the answer\nconfidence scores of state-of-the-art QA systems can be approximated well by\nmodels solely using the input question text. This enables preemptive filtering\nof questions that are not answered by the system due to their answer confidence\nscores being lower than the system threshold. Specifically, we learn\nTransformer-based question models by distilling Transformer-based answering\nmodels. Our experiments on three popular QA datasets and one industrial QA\nbenchmark demonstrate the ability of our question models to approximate the\nPrecision/Recall curves of the target QA system well. These question models,\nwhen used as filters, can effectively trade off lower computation cost of QA\nsystems for lower Recall, e.g., reducing computation by ~60%, while only losing\n~3-4% of Recall.",
          "arxiv_id": "2109.07009v1"
        },
        {
          "title": "XLMRQA: Open-Domain Question Answering on Vietnamese Wikipedia-based Textual Knowledge Source",
          "year": "2022-04",
          "abstract": "Question answering (QA) is a natural language understanding task within the\nfields of information retrieval and information extraction that has attracted\nmuch attention from the computational linguistics and artificial intelligence\nresearch community in recent years because of the strong development of machine\nreading comprehension-based models. A reader-based QA system is a high-level\nsearch engine that can find correct answers to queries or questions in\nopen-domain or domain-specific texts using machine reading comprehension (MRC)\ntechniques. The majority of advancements in data resources and machine-learning\napproaches in the MRC and QA systems especially are developed significantly in\ntwo resource-rich languages such as English and Chinese. A low-resource\nlanguage like Vietnamese has witnessed a scarcity of research on QA systems.\nThis paper presents XLMRQA, the first Vietnamese QA system using a supervised\ntransformer-based reader on the Wikipedia-based textual knowledge source (using\nthe UIT-ViQuAD corpus), outperforming the two robust QA systems using deep\nneural network models: DrQA and BERTserini with 24.46% and 6.28%, respectively.\nFrom the results obtained on the three systems, we analyze the influence of\nquestion types on the performance of the QA systems.",
          "arxiv_id": "2204.07002v2"
        }
      ],
      "30": [
        {
          "title": "Interpreting Arabic Transformer Models",
          "year": "2022-01",
          "abstract": "Arabic is a Semitic language which is widely spoken with many dialects. Given\nthe success of pre-trained language models, many transformer models trained on\nArabic and its dialects have surfaced. While these models have been compared\nwith respect to downstream NLP tasks, no evaluation has been carried out to\ndirectly compare the internal representations. We probe how linguistic\ninformation is encoded in Arabic pretrained models, trained on different\nvarieties of Arabic language. We perform a layer and neuron analysis on the\nmodels using three intrinsic tasks: two morphological tagging tasks based on\nMSA (modern standard Arabic) and dialectal POS-tagging and a dialectal\nidentification task. Our analysis enlightens interesting findings such as: i)\nword morphology is learned at the lower and middle layers ii) dialectal\nidentification necessitate more knowledge and hence preserved even in the final\nlayers, iii) despite a large overlap in their vocabulary, the MSA-based models\nfail to capture the nuances of Arabic dialects, iv) we found that neurons in\nembedding layers are polysemous in nature, while the neurons in middle layers\nare exclusive to specific properties.",
          "arxiv_id": "2201.07434v2"
        },
        {
          "title": "Automatic Arabic Dialect Identification Systems for Written Texts: A Survey",
          "year": "2020-09",
          "abstract": "Arabic dialect identification is a specific task of natural language\nprocessing, aiming to automatically predict the Arabic dialect of a given text.\nArabic dialect identification is the first step in various natural language\nprocessing applications such as machine translation, multilingual\ntext-to-speech synthesis, and cross-language text generation. Therefore, in the\nlast decade, interest has increased in addressing the problem of Arabic dialect\nidentification. In this paper, we present a comprehensive survey of Arabic\ndialect identification research in written texts. We first define the problem\nand its challenges. Then, the survey extensively discusses in a critical manner\nmany aspects related to Arabic dialect identification task. So, we review the\ntraditional machine learning methods, deep learning architectures, and complex\nlearning approaches to Arabic dialect identification. We also detail the\nfeatures and techniques for feature representations used to train the proposed\nsystems. Moreover, we illustrate the taxonomy of Arabic dialects studied in the\nliterature, the various levels of text processing at which Arabic dialect\nidentification are conducted (e.g., token, sentence, and document level), as\nwell as the available annotated resources, including evaluation benchmark\ncorpora. Open challenges and issues are discussed at the end of the survey.",
          "arxiv_id": "2009.12622v1"
        },
        {
          "title": "A Tale of Two Scripts: Transliteration and Post-Correction for Judeo-Arabic",
          "year": "2025-07",
          "abstract": "Judeo-Arabic refers to Arabic variants historically spoken by Jewish\ncommunities across the Arab world, primarily during the Middle Ages. Unlike\nstandard Arabic, it is written in Hebrew script by Jewish writers and for\nJewish audiences. Transliterating Judeo-Arabic into Arabic script is\nchallenging due to ambiguous letter mappings, inconsistent orthographic\nconventions, and frequent code-switching into Hebrew and Aramaic. In this\npaper, we introduce a two-step approach to automatically transliterate\nJudeo-Arabic into Arabic script: simple character-level mapping followed by\npost-correction to address grammatical and orthographic errors. We also present\nthe first benchmark evaluation of LLMs on this task. Finally, we show that\ntransliteration enables Arabic NLP tools to perform morphosyntactic tagging and\nmachine translation, which would have not been feasible on the original texts.",
          "arxiv_id": "2507.04746v1"
        }
      ],
      "31": [
        {
          "title": "The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances",
          "year": "2024-04",
          "abstract": "Large language models (LLMs) are quickly being adopted in a wide range of\nlearning experiences, especially via ubiquitous and broadly accessible chat\ninterfaces like ChatGPT and Copilot. This type of interface is readily\navailable to students and teachers around the world, yet relatively little\nresearch has been done to assess the impact of such generic tools on student\nlearning. Coding education is an interesting test case, both because LLMs have\nstrong performance on coding tasks, and because LLM-powered support tools are\nrapidly becoming part of the workflow of professional software engineers. To\nhelp understand the impact of generic LLM use on coding education, we conducted\na large-scale randomized control trial with 5,831 students from 146 countries\nin an online coding class in which we provided some students with access to a\nchat interface with GPT-4. We estimate positive benefits on exam performance\nfor adopters, the students who used the tool, but over all students, the\nadvertisement of GPT-4 led to a significant average decrease in exam\nparticipation. We observe similar decreases in other forms of course\nengagement. However, this decrease is modulated by the student's country of\norigin. Offering access to LLMs to students from low human development index\ncountries increased their exam participation rate on average. Our results\nsuggest there may be promising benefits to using LLMs in an introductory coding\nclass, but also potential harms for engagement, which makes their longer term\nimpact on student success unclear. Our work highlights the need for additional\ninvestigations to help understand the potential impact of future adoption and\nintegration of LLMs into classrooms.",
          "arxiv_id": "2407.09975v2"
        },
        {
          "title": "RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education",
          "year": "2024-03",
          "abstract": "The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale and real-world interactions between students and AI\nsystems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE\nfor University), a dataset sourced from a semester-long experiment with 212\ncollege students in English as Foreign Language (EFL) writing courses. During\nthe study, students engaged in dialogues with ChatGPT to revise their essays.\nRECIPE4U includes comprehensive records of these interactions, including\nconversation logs, students' intent, students' self-rated satisfaction, and\nstudents' essay edit histories. In particular, we annotate the students'\nutterances in RECIPE4U with 13 intention labels based on our coding schemes. We\nestablish baseline results for two subtasks in task-oriented dialogue systems\nwithin educational contexts: intent detection and satisfaction estimation. As a\nfoundational step, we explore student-ChatGPT interaction patterns through\nRECIPE4U and analyze them by focusing on students' dialogue, essay data\nstatistics, and students' essay edits. We further illustrate potential\napplications of RECIPE4U dataset for enhancing the incorporation of LLMs in\neducational frameworks. RECIPE4U is publicly available at\nhttps://zeunie.github.io/RECIPE4U/.",
          "arxiv_id": "2403.08272v1"
        },
        {
          "title": "Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between Language Models and Human Error Patterns",
          "year": "2025-02",
          "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious educational tasks, yet their alignment with human learning patterns,\nparticularly in predicting which incorrect options students are most likely to\nselect in multiple-choice questions (MCQs), remains underexplored. Our work\ninvestigates the relationship between LLM generation likelihood and student\nresponse distributions in MCQs with a specific focus on distractor selections.\nWe collect a comprehensive dataset of MCQs with real-world student response\ndistributions to explore two fundamental research questions: (1). RQ1 - Do the\ndistractors that students more frequently select correspond to those that LLMs\nassign higher generation likelihood to? (2). RQ2 - When an LLM selects a\nincorrect choice, does it choose the same distractor that most students pick?\nOur experiments reveals moderate correlations between LLM-assigned\nprobabilities and student selection patterns for distractors in MCQs.\nAdditionally, when LLMs make mistakes, they are more likley to select the same\nincorrect answers that commonly mislead students, which is a pattern consistent\nacross both small and large language models. Our work provides empirical\nevidence that despite LLMs' strong performance on generating educational\ncontent, there remains a gap between LLM's underlying reasoning process and\nhuman cognitive processes in identifying confusing distractors. Our findings\nalso have significant implications for educational assessment development. The\nsmaller language models could be efficiently utilized for automated distractor\ngeneration as they demonstrate similar patterns in identifying confusing answer\nchoices as larger language models. This observed alignment between LLMs and\nstudent misconception patterns opens new opportunities for generating\nhigh-quality distractors that complement traditional human-designed\ndistractors.",
          "arxiv_id": "2502.15140v1"
        }
      ],
      "32": [
        {
          "title": "Sentiment Analysis and Topic Modeling for COVID-19 Vaccine Discussions",
          "year": "2021-10",
          "abstract": "The outbreak of the novel Coronavirus Disease 2019 (COVID-19) has lasted for\nnearly two years and caused unprecedented impacts on people's daily life around\nthe world. Even worse, the emergence of the COVID-19 Delta variant once again\nputs the world in danger. Fortunately, many countries and companies have\nstarted to develop coronavirus vaccines since the beginning of this disaster.\nTill now, more than 20 vaccines have been approved by the World Health\nOrganization (WHO), bringing light to people besieged by the pandemic. The\npromotion of COVID-19 vaccination around the world also brings a lot of\ndiscussions on social media about different aspects of vaccines, such as\nefficacy and security. However, there does not exist much research work to\nsystematically analyze public opinion towards COVID-19 vaccines. In this study,\nwe conduct an in-depth analysis of tweets related to the coronavirus vaccine on\nTwitter to understand the trending topics and their corresponding sentimental\npolarities regarding the country and vaccine levels. The results show that a\nmajority of people are confident in the effectiveness of vaccines and are\nwilling to get vaccinated. In contrast, the negative tweets are often\nassociated with the complaints of vaccine shortages, side effects after\ninjections and possible death after being vaccinated. Overall, this study\nexploits popular NLP and topic modeling methods to mine people's opinions on\nthe COVID-19 vaccines on social media and to analyse and visualise them\nobjectively. Our findings can improve the readability of the noisy information\non social media and provide effective data support for the government and\npolicy makers.",
          "arxiv_id": "2111.04415v1"
        },
        {
          "title": "COVID-19: Social Media Sentiment Analysis on Reopening",
          "year": "2020-06",
          "abstract": "The novel coronavirus (COVID-19) pandemic is the most talked topic in social\nmedia platforms in 2020. People are using social media such as Twitter to\nexpress their opinion and share information on a number of issues related to\nthe COVID-19 in this stay at home order. In this paper, we investigate the\nsentiment and emotion of peoples in the United States on the subject of\nreopening. We choose the social media platform Twitter for our analysis and\nstudy the Tweets to discover the sentimental perspective, emotional\nperspective, and triggering words towards the reopening. During this COVID-19\npandemic, researchers have made some analysis on various social media dataset\nregarding lockdown and stay at home. However, in our analysis, we are\nparticularly interested to analyse public sentiment on reopening. Our major\nfinding is that when all states resorted to lockdown in March, people showed\ndominant emotion of fear, but as reopening starts people have less fear. While\nthis may be true, due to this reopening phase daily positive cases are rising\ncompared to the lockdown situation. Overall, people have a less negative\nsentiment towards the situation of reopening.",
          "arxiv_id": "2006.00804v1"
        },
        {
          "title": "Twitter discussions and emotions about COVID-19 pandemic: a machine learning approach",
          "year": "2020-05",
          "abstract": "The objective of the study is to examine coronavirus disease (COVID-19)\nrelated discussions, concerns, and sentiments that emerged from tweets posted\nby Twitter users. We analyze 4 million Twitter messages related to the COVID-19\npandemic using a list of 25 hashtags such as \"coronavirus,\" \"COVID-19,\"\n\"quarantine\" from March 1 to April 21 in 2020. We use a machine learning\napproach, Latent Dirichlet Allocation (LDA), to identify popular unigram,\nbigrams, salient topics and themes, and sentiments in the collected Tweets.\nPopular unigrams include \"virus,\" \"lockdown,\" and \"quarantine.\" Popular bigrams\ninclude \"COVID-19,\" \"stay home,\" \"corona virus,\" \"social distancing,\" and \"new\ncases.\" We identify 13 discussion topics and categorize them into five\ndifferent themes, such as \"public health measures to slow the spread of\nCOVID-19,\" \"social stigma associated with COVID-19,\" \"coronavirus news cases\nand deaths,\" \"COVID-19 in the United States,\" and \"coronavirus cases in the\nrest of the world\". Across all identified topics, the dominant sentiments for\nthe spread of coronavirus are anticipation that measures that can be taken,\nfollowed by a mixed feeling of trust, anger, and fear for different topics. The\npublic reveals a significant feeling of fear when they discuss the coronavirus\nnew cases and deaths than other topics. The study shows that Twitter data and\nmachine learning approaches can be leveraged for infodemiology study by\nstudying the evolving public discussions and sentiments during the COVID-19.\nReal-time monitoring and assessment of the Twitter discussion and concerns can\nbe promising for public health emergency responses and planning. Already\nemerged pandemic fear, stigma, and mental health concerns may continue to\ninfluence public trust when there occurs a second wave of COVID-19 or a new\nsurge of the imminent pandemic.",
          "arxiv_id": "2005.12830v2"
        }
      ],
      "33": [
        {
          "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning",
          "year": "2022-10",
          "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a\ndownstream task conditioning on frozen pre-trained models, have attracted\ngrowing interest due to its parameter efficiency. With large language models\nand sufficient training data, prompt tuning performs comparably to full-model\ntuning. However, with limited training samples in few-shot settings, prompt\ntuning fails to match the performance of full-model fine-tuning. In this work,\nwe focus on improving the few-shot performance of prompt tuning by transferring\nknowledge from soft prompts of source tasks. Recognizing the good\ngeneralization capabilities of ensemble methods in low-data regime, we first\nexperiment and show that a simple ensemble of model predictions based on\ndifferent source prompts, outperforms existing multi-prompt knowledge transfer\napproaches such as source prompt fusion in the few-shot setting. Motivated by\nthis observation, we further investigate model ensembles and propose\nSample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the\ncontribution of each source model for each target sample separately when\nensembling source model outputs. Through this way, SESoM inherits the superior\ngeneralization of model ensemble approaches and simultaneously captures the\nsample-specific competence of each source prompt. We conduct experiments across\na diverse set of eight NLP tasks using models of different scales (T5-{base,\nlarge, XL}) and find that SESoM consistently outperforms the existing models of\nthe same as well as larger parametric scale by a large margin.",
          "arxiv_id": "2210.12587v3"
        },
        {
          "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning",
          "year": "2021-09",
          "abstract": "Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.",
          "arxiv_id": "2109.04332v3"
        },
        {
          "title": "Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt Tuning through Modular Prompt Composition",
          "year": "2024-08",
          "abstract": "In recent years, multi-task prompt tuning has garnered considerable attention\nfor its inherent modularity and potential to enhance parameter-efficient\ntransfer learning across diverse tasks. This paper aims to analyze and improve\nthe performance of multiple tasks by facilitating the transfer of knowledge\nbetween their corresponding prompts in a multi-task setting. Our proposed\napproach decomposes the prompt for each target task into a combination of\nshared prompts (source prompts) and a task-specific prompt (private prompt).\nDuring training, the source prompts undergo fine-tuning and are integrated with\nthe private prompt to drive the target prompt for each task. We present and\ncompare multiple methods for combining source prompts to construct the target\nprompt, analyzing the roles of both source and private prompts within each\nmethod. We investigate their contributions to task performance and offer\nflexible, adjustable configurations based on these insights to optimize\nperformance. Our empirical findings clearly showcase improvements in accuracy\nand robustness compared to the conventional practice of prompt tuning and\nrelated works. Notably, our results substantially outperform other methods in\nthe field in few-shot settings, demonstrating superior performance in various\ntasks across GLUE benchmark, among other tasks. This achievement is attained\nwith a significantly reduced amount of training data, making our method a\npromising one for few-shot settings.",
          "arxiv_id": "2408.13227v2"
        }
      ],
      "34": [
        {
          "title": "Annotation-Scheme Reconstruction for \"Fake News\" and Japanese Fake News Dataset",
          "year": "2022-04",
          "abstract": "Fake news provokes many societal problems; therefore, there has been\nextensive research on fake news detection tasks to counter it. Many fake news\ndatasets were constructed as resources to facilitate this task. Contemporary\nresearch focuses almost exclusively on the factuality aspect of the news.\nHowever, this aspect alone is insufficient to explain \"fake news,\" which is a\ncomplex phenomenon that involves a wide range of issues. To fully understand\nthe nature of each instance of fake news, it is important to observe it from\nvarious perspectives, such as the intention of the false news disseminator, the\nharmfulness of the news to our society, and the target of the news. We propose\na novel annotation scheme with fine-grained labeling based on detailed\ninvestigations of existing fake news datasets to capture these various aspects\nof fake news. Using the annotation scheme, we construct and publish the first\nJapanese fake news dataset. The annotation scheme is expected to provide an\nin-depth understanding of fake news. We plan to build datasets for both\nJapanese and other languages using our scheme. Our Japanese dataset is\npublished at https://hkefka385.github.io/dataset/fakenews-japanese/.",
          "arxiv_id": "2204.02718v1"
        },
        {
          "title": "Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity Benchmark for Multimodal Fake News Detection",
          "year": "2024-12",
          "abstract": "Social platforms, while facilitating access to information, have also become\nsaturated with a plethora of fake news, resulting in negative consequences.\nAutomatic multimodal fake news detection is a worthwhile pursuit. Existing\nmultimodal fake news datasets only provide binary labels of real or fake.\nHowever, real news is alike, while each fake news is fake in its own way. These\ndatasets fail to reflect the mixed nature of various types of multimodal fake\nnews. To bridge the gap, we construct an attributing multi-granularity\nmultimodal fake news detection dataset \\amg, revealing the inherent fake\npattern. Furthermore, we propose a multi-granularity clue alignment model \\our\nto achieve multimodal fake news detection and attribution. Experimental results\ndemonstrate that \\amg is a challenging dataset, and its attribution setting\nopens up new avenues for future research.",
          "arxiv_id": "2412.14686v1"
        },
        {
          "title": "Dataset of Fake News Detection and Fact Verification: A Survey",
          "year": "2021-11",
          "abstract": "The rapid increase in fake news, which causes significant damage to society,\ntriggers many fake news related studies, including the development of fake news\ndetection and fact verification techniques. The resources for these studies are\nmainly available as public datasets taken from Web data. We surveyed 118\ndatasets related to fake news research on a large scale from three\nperspectives: (1) fake news detection, (2) fact verification, and (3) other\ntasks; for example, the analysis of fake news and satire detection. We also\ndescribe in detail their utilization tasks and their characteristics. Finally,\nwe highlight the challenges in the fake news dataset construction and some\nresearch opportunities that address these challenges. Our survey facilitates\nfake news research by helping researchers find suitable datasets without\nreinventing the wheel, and thereby, improves fake news studies in depth.",
          "arxiv_id": "2111.03299v1"
        }
      ],
      "35": [
        {
          "title": "CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion",
          "year": "2022-12",
          "abstract": "The dual-encoder has become the de facto architecture for dense retrieval.\nTypically, it computes the latent representations of the query and document\nindependently, thus failing to fully capture the interactions between the query\nand document. To alleviate this, recent research has focused on obtaining\nquery-informed document representations. During training, it expands the\ndocument with a real query, but during inference, it replaces the real query\nwith a generated one. This inconsistency between training and inference causes\nthe dense retrieval model to prioritize query information while disregarding\nthe document when computing the document representation. Consequently, it\nperforms even worse than the vanilla dense retrieval model because its\nperformance heavily relies on the relevance between the generated queries and\nthe real query.In this paper, we propose a curriculum sampling strategy that\nutilizes pseudo queries during training and progressively enhances the\nrelevance between the generated query and the real query. By doing so, the\nretrieval model learns to extend its attention from the document alone to both\nthe document and query, resulting in high-quality query-informed document\nrepresentations. Experimental results on both in-domain and out-of-domain\ndatasets demonstrate that our approach outperforms previous dense retrieval\nmodels.",
          "arxiv_id": "2212.09114v2"
        },
        {
          "title": "Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models",
          "year": "2022-01",
          "abstract": "The advent of transformer-based models such as BERT has led to the rise of\nneural ranking models. These models have improved the effectiveness of\nretrieval systems well beyond that of lexical term matching models such as\nBM25. While monolingual retrieval tasks have benefited from large-scale\ntraining collections such as MS MARCO and advances in neural architectures,\ncross-language retrieval tasks have fallen behind these advancements. This\npaper introduces ColBERT-X, a generalization of the ColBERT\nmulti-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R)\nencoder to support cross-language information retrieval (CLIR). ColBERT-X can\nbe trained in two ways. In zero-shot training, the system is trained on the\nEnglish MS MARCO collection, relying on the XLM-R encoder for cross-language\nmappings. In translate-train, the system is trained on the MS MARCO English\nqueries coupled with machine translations of the associated MS MARCO passages.\nResults on ad hoc document ranking tasks in several languages demonstrate\nsubstantial and statistically significant improvements of these trained dense\nretrieval models over traditional lexical CLIR baselines.",
          "arxiv_id": "2201.08471v1"
        },
        {
          "title": "Lexically-Accelerated Dense Retrieval",
          "year": "2023-07",
          "abstract": "Retrieval approaches that score documents based on learned dense vectors\n(i.e., dense retrieval) rather than lexical signals (i.e., conventional\nretrieval) are increasingly popular. Their ability to identify related\ndocuments that do not necessarily contain the same terms as those appearing in\nthe user's query (thereby improving recall) is one of their key advantages.\nHowever, to actually achieve these gains, dense retrieval approaches typically\nrequire an exhaustive search over the document collection, making them\nconsiderably more expensive at query-time than conventional lexical approaches.\nSeveral techniques aim to reduce this computational overhead by approximating\nthe results of a full dense retriever. Although these approaches reasonably\napproximate the top results, they suffer in terms of recall -- one of the key\nadvantages of dense retrieval. We introduce 'LADR' (Lexically-Accelerated Dense\nRetrieval), a simple-yet-effective approach that improves the efficiency of\nexisting dense retrieval models without compromising on retrieval\neffectiveness. LADR uses lexical retrieval techniques to seed a dense retrieval\nexploration that uses a document proximity graph. We explore two variants of\nLADR: a proactive approach that expands the search space to the neighbors of\nall seed documents, and an adaptive approach that selectively searches the\ndocuments with the highest estimated relevance in an iterative fashion. Through\nextensive experiments across a variety of dense retrieval models, we find that\nLADR establishes a new dense retrieval effectiveness-efficiency Pareto frontier\namong approximate k nearest neighbor techniques. Further, we find that when\ntuned to take around 8ms per query in retrieval latency on our hardware, LADR\nconsistently achieves both precision and recall that are on par with an\nexhaustive search on standard benchmarks.",
          "arxiv_id": "2307.16779v1"
        }
      ],
      "36": [
        {
          "title": "RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict",
          "year": "2024-03",
          "abstract": "Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.",
          "arxiv_id": "2403.16662v2"
        },
        {
          "title": "Evidence-based Interpretable Open-domain Fact-checking with Large Language Models",
          "year": "2023-12",
          "abstract": "Universal fact-checking systems for real-world claims face significant\nchallenges in gathering valid and sufficient real-time evidence and making\nreasoned decisions. In this work, we introduce the Open-domain Explainable\nFact-checking (OE-Fact) system for claim-checking in real-world scenarios. The\nOE-Fact system can leverage the powerful understanding and reasoning\ncapabilities of large language models (LLMs) to validate claims and generate\ncausal explanations for fact-checking decisions. To adapt the traditional\nthree-module fact-checking framework to the open domain setting, we first\nretrieve claim-related information as relevant evidence from open websites.\nAfter that, we retain the evidence relevant to the claim through LLM and\nsimilarity calculation for subsequent verification. We evaluate the performance\nof our adapted three-module OE-Fact system on the Fact Extraction and\nVerification (FEVER) dataset. Experimental results show that our OE-Fact system\noutperforms general fact-checking baseline systems in both closed- and\nopen-domain scenarios, ensuring stable and accurate verdicts while providing\nconcise and convincing real-time explanations for fact-checking decisions.",
          "arxiv_id": "2312.05834v1"
        },
        {
          "title": "Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation",
          "year": "2022-10",
          "abstract": "Misinformation emerges in times of uncertainty when credible information is\nlimited. This is challenging for NLP-based fact-checking as it relies on\ncounter-evidence, which may not yet be available. Despite increasing interest\nin automatic fact-checking, it is still unclear if automated approaches can\nrealistically refute harmful real-world misinformation. Here, we contrast and\ncompare NLP fact-checking with how professional fact-checkers combat\nmisinformation in the absence of counter-evidence. In our analysis, we show\nthat, by design, existing NLP task definitions for fact-checking cannot refute\nmisinformation as professional fact-checkers do for the majority of claims. We\nthen define two requirements that the evidence in datasets must fulfill for\nrealistic fact-checking: It must be (1) sufficient to refute the claim and (2)\nnot leaked from existing fact-checking articles. We survey existing\nfact-checking datasets and find that all of them fail to satisfy both criteria.\nFinally, we perform experiments to demonstrate that models trained on a\nlarge-scale fact-checking dataset rely on leaked evidence, which makes them\nunsuitable in real-world scenarios. Taken together, we show that current NLP\nfact-checking cannot realistically combat real-world misinformation because it\ndepends on unrealistic assumptions about counter-evidence in the data.",
          "arxiv_id": "2210.13865v1"
        }
      ],
      "37": [
        {
          "title": "Improving Event Definition Following For Zero-Shot Event Detection",
          "year": "2024-03",
          "abstract": "Existing approaches on zero-shot event detection usually train models on\ndatasets annotated with known event types, and prompt them with unseen event\ndefinitions. These approaches yield sporadic successes, yet generally fall\nshort of expectations. In this work, we aim to improve zero-shot event\ndetection by training models to better follow event definitions. We hypothesize\nthat a diverse set of event types and definitions are the key for models to\nlearn to follow event definitions while existing event extraction datasets\nfocus on annotating many high-quality examples for a few event types. To verify\nour hypothesis, we construct an automatically generated Diverse Event\nDefinition (DivED) dataset and conduct comparative studies. Our experiments\nreveal that a large number of event types (200) and diverse event definitions\ncan significantly boost event extraction performance; on the other hand, the\nperformance does not scale with over ten examples per event type. Beyond\nscaling, we incorporate event ontology information and hard-negative samples\nduring training, further boosting the performance. Based on these findings, we\nfine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that\nsurpasses SOTA large language models like GPT-3.5 across three open benchmarks\non zero-shot event detection.",
          "arxiv_id": "2403.02586v1"
        },
        {
          "title": "MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation",
          "year": "2023-11",
          "abstract": "Understanding events in texts is a core objective of natural language\nunderstanding, which requires detecting event occurrences, extracting event\narguments, and analyzing inter-event relationships. However, due to the\nannotation challenges brought by task complexity, a large-scale dataset\ncovering the full process of event understanding has long been absent. In this\npaper, we introduce MAVEN-Arg, which augments MAVEN datasets with event\nargument annotations, making the first all-in-one dataset supporting event\ndetection, event argument extraction (EAE), and event relation extraction. As\nan EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive\nschema covering 162 event types and 612 argument roles, all with expert-written\ndefinitions and examples; (2) a large data scale, containing 98,591 events and\n290,613 arguments obtained with laborious human annotation; (3) the exhaustive\nannotation supporting all task variants of EAE, which annotates both entity and\nnon-entity event arguments in document level. Experiments indicate that\nMAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary\nlarge language models (LLMs). Furthermore, to demonstrate the benefits of an\nall-in-one dataset, we preliminarily explore a potential application, future\nevent prediction, with LLMs. MAVEN-Arg and codes can be obtained from\nhttps://github.com/THU-KEG/MAVEN-Argument.",
          "arxiv_id": "2311.09105v2"
        },
        {
          "title": "EventPlus: A Temporal Event Understanding Pipeline",
          "year": "2021-01",
          "abstract": "We present EventPlus, a temporal event understanding pipeline that integrates\nvarious state-of-the-art event understanding components including event trigger\nand type detection, event argument detection, event duration and temporal\nrelation extraction. Event information, especially event temporal knowledge, is\na type of common sense knowledge that helps people understand how stories\nevolve and provides predictive hints for future events. EventPlus as the first\ncomprehensive temporal event understanding pipeline provides a convenient tool\nfor users to quickly obtain annotations about events and their temporal\ninformation for any user-provided document. Furthermore, we show EventPlus can\nbe easily adapted to other domains (e.g., biomedical domain). We make EventPlus\npublicly available to facilitate event-related information extraction and\ndownstream applications.",
          "arxiv_id": "2101.04922v2"
        }
      ],
      "38": [
        {
          "title": "Readability-guided Idiom-aware Sentence Simplification (RISS) for Chinese",
          "year": "2024-06",
          "abstract": "Chinese sentence simplification faces challenges due to the lack of\nlarge-scale labeled parallel corpora and the prevalence of idioms. To address\nthese challenges, we propose Readability-guided Idiom-aware Sentence\nSimplification (RISS), a novel framework that combines data augmentation\ntechniques with lexcial simplification. RISS introduces two key components: (1)\nReadability-guided Paraphrase Selection (RPS), a method for mining high-quality\nsentence pairs, and (2) Idiom-aware Simplification (IAS), a model that enhances\nthe comprehension and simplification of idiomatic expressions. By integrating\nRPS and IAS using multi-stage and multi-task learning strategies, RISS\noutperforms previous state-of-the-art methods on two Chinese sentence\nsimplification datasets. Furthermore, RISS achieves additional improvements\nwhen fine-tuned on a small labeled dataset. Our approach demonstrates the\npotential for more effective and accessible Chinese text simplification.",
          "arxiv_id": "2406.02974v1"
        },
        {
          "title": "Elaborative Simplification: Content Addition and Explanation Generation in Text Simplification",
          "year": "2020-10",
          "abstract": "Much of modern-day text simplification research focuses on sentence-level\nsimplification, transforming original, more complex sentences into simplified\nversions. However, adding content can often be useful when difficult concepts\nand reasoning need to be explained. In this work, we present the first\ndata-driven study of content addition in text simplification, which we call\nelaborative simplification. We introduce a new annotated dataset of 1.3K\ninstances of elaborative simplification in the Newsela corpus, and analyze how\nentities, ideas, and concepts are elaborated through the lens of contextual\nspecificity. We establish baselines for elaboration generation using\nlarge-scale pre-trained language models, and demonstrate that considering\ncontextual specificity during generation can improve performance. Our results\nillustrate the complexities of elaborative simplification, suggesting many\ninteresting directions for future work.",
          "arxiv_id": "2010.10035v3"
        },
        {
          "title": "MUSS: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases",
          "year": "2020-05",
          "abstract": "Progress in sentence simplification has been hindered by a lack of labeled\nparallel simplification data, particularly in languages other than English. We\nintroduce MUSS, a Multilingual Unsupervised Sentence Simplification system that\ndoes not require labeled simplification data. MUSS uses a novel approach to\nsentence simplification that trains strong models using sentence-level\nparaphrase data instead of proper simplification data. These models leverage\nunsupervised pretraining and controllable generation mechanisms to flexibly\nadjust attributes such as length and lexical complexity at inference time. We\nfurther present a method to mine such paraphrase data in any language from\nCommon Crawl using semantic sentence embeddings, thus removing the need for\nlabeled data. We evaluate our approach on English, French, and Spanish\nsimplification benchmarks and closely match or outperform the previous best\nsupervised results, despite not using any labeled simplification data. We push\nthe state of the art further by incorporating labeled simplification data.",
          "arxiv_id": "2005.00352v2"
        }
      ],
      "39": [
        {
          "title": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive Learning",
          "year": "2024-10",
          "abstract": "Current techniques for detecting AI-generated text are largely confined to\nmanual feature crafting and supervised binary classification paradigms. These\nmethodologies typically lead to performance bottlenecks and unsatisfactory\ngeneralizability. Consequently, these methods are often inapplicable for\nout-of-distribution (OOD) data and newly emerged large language models (LLMs).\nIn this paper, we revisit the task of AI-generated text detection. We argue\nthat the key to accomplishing this task lies in distinguishing writing styles\nof different authors, rather than simply classifying the text into\nhuman-written or AI-generated text. To this end, we propose DeTeCtive, a\nmulti-task auxiliary, multi-level contrastive learning framework. DeTeCtive is\ndesigned to facilitate the learning of distinct writing styles, combined with a\ndense information retrieval pipeline for AI-generated text detection. Our\nmethod is compatible with a range of text encoders. Extensive experiments\ndemonstrate that our method enhances the ability of various text encoders in\ndetecting AI-generated text across multiple benchmarks and achieves\nstate-of-the-art results. Notably, in OOD zero-shot evaluation, our method\noutperforms existing approaches by a large margin. Moreover, we find our method\nboasts a Training-Free Incremental Adaptation (TFIA) capability towards OOD\ndata, further enhancing its efficacy in OOD detection scenarios. We will\nopen-source our code and models in hopes that our work will spark new thoughts\nin the field of AI-generated text detection, ensuring safe application of LLMs\nand enhancing compliance. Our code is available at\nhttps://github.com/heyongxin233/DeTeCtive.",
          "arxiv_id": "2410.20964v1"
        },
        {
          "title": "Who Said That? Benchmarking Social Media AI Detection",
          "year": "2023-10",
          "abstract": "AI-generated text has proliferated across various online platforms, offering\nboth transformative prospects and posing significant risks related to\nmisinformation and manipulation. Addressing these challenges, this paper\nintroduces SAID (Social media AI Detection), a novel benchmark developed to\nassess AI-text detection models' capabilities in real social media platforms.\nIt incorporates real AI-generate text from popular social media platforms like\nZhihu and Quora. Unlike existing benchmarks, SAID deals with content that\nreflects the sophisticated strategies employed by real AI users on the Internet\nwhich may evade detection or gain visibility, providing a more realistic and\nchallenging evaluation landscape. A notable finding of our study, based on the\nZhihu dataset, reveals that annotators can distinguish between AI-generated and\nhuman-generated texts with an average accuracy rate of 96.5%. This finding\nnecessitates a re-evaluation of human capability in recognizing AI-generated\ntext in today's widely AI-influenced environment. Furthermore, we present a new\nuser-oriented AI-text detection challenge focusing on the practicality and\neffectiveness of identifying AI-generated text based on user information and\nmultiple responses. The experimental results demonstrate that conducting\ndetection tasks on actual social media platforms proves to be more challenging\ncompared to traditional simulated AI-text detection, resulting in a decreased\naccuracy. On the other hand, user-oriented AI-generated text detection\nsignificantly improve the accuracy of detection.",
          "arxiv_id": "2310.08240v1"
        },
        {
          "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
          "year": "2023-10",
          "abstract": "Large Language Models (LLMs) have revolutionized the domain of natural\nlanguage processing (NLP) with remarkable capabilities of generating human-like\ntext responses. However, despite these advancements, several works in the\nexisting literature have raised serious concerns about the potential misuse of\nLLMs such as spreading misinformation, generating fake news, plagiarism in\nacademia, and contaminating the web. To address these concerns, a consensus\namong the research community is to develop algorithmic solutions to detect\nAI-generated text. The basic idea is that whenever we can tell if the given\ntext is either written by a human or an AI, we can utilize this information to\naddress the above-mentioned concerns. To that end, a plethora of detection\nframeworks have been proposed, highlighting the possibilities of AI-generated\ntext detection. But in parallel to the development of detection frameworks,\nresearchers have also concentrated on designing strategies to elude detection,\ni.e., focusing on the impossibilities of AI-generated text detection. This is a\ncrucial step in order to make sure the detection frameworks are robust enough\nand it is not too easy to fool a detector. Despite the huge interest and the\nflurry of research in this domain, the community currently lacks a\ncomprehensive analysis of recent developments. In this survey, we aim to\nprovide a concise categorization and overview of current work encompassing both\nthe prospects and the limitations of AI-generated text detection. To enrich the\ncollective knowledge, we engage in an exhaustive discussion on critical and\nchallenging open questions related to ongoing research on AI-generated text\ndetection.",
          "arxiv_id": "2310.15264v1"
        }
      ],
      "40": [
        {
          "title": "Natural SQL: Making SQL Easier to Infer from Natural Language Specifications",
          "year": "2021-09",
          "abstract": "Addressing the mismatch between natural language descriptions and the\ncorresponding SQL queries is a key challenge for text-to-SQL translation. To\nbridge this gap, we propose an SQL intermediate representation (IR) called\nNatural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities\nof SQL, while it simplifies the queries as follows: (1) dispensing with\noperators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are\nusually hard to find counterparts for in the text descriptions; (2) removing\nthe need for nested subqueries and set operators; and (3) making schema linking\neasier by reducing the required number of schema items. On Spider, a\nchallenging text-to-SQL benchmark that contains complex and nested SQL queries,\nwe demonstrate that NatSQL outperforms other IRs, and significantly improves\nthe performance of several previous SOTA models. Furthermore, for existing\nmodels that do not support executable SQL generation, NatSQL easily enables\nthem to generate executable SQL queries, and achieves the new state-of-the-art\nexecution accuracy.",
          "arxiv_id": "2109.05153v1"
        },
        {
          "title": "Abacus-SQL: A Text-to-SQL System Empowering Cross-Domain and Open-Domain Database Retrieval",
          "year": "2025-04",
          "abstract": "The existing text-to-SQL systems have made significant progress in SQL query\ngeneration, but they still face numerous challenges. Existing systems often\nlack retrieval capabilities for open-domain databases, requiring users to\nmanually filter relevant databases. Additionally, their cross-domain\ntransferability is limited, making it challenging to accommodate diverse query\nrequirements. To address these issues, we propose Abacus-SQL. Abacus-SQL\nutilizes database retrieval technology to accurately locate the required\ndatabases in an open-domain database environment. It also enhances the system\ncross-domain transfer ability through data augmentation methods. Moreover,\nAbacus-SQL employs Pre-SQL and Self-debug methods, thereby enhancing the\naccuracy of SQL queries. Experimental results demonstrate that Abacus-SQL\nperforms excellently in multi-turn text-to-SQL tasks, effectively validating\nthe approach's effectiveness. Abacus-SQL is publicly accessible at\nhttps://huozi.8wss.com/abacus-sql/.",
          "arxiv_id": "2504.09824v1"
        },
        {
          "title": "CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers",
          "year": "2022-05",
          "abstract": "Context-dependent text-to-SQL is the task of translating multi-turn questions\ninto database-related SQL queries. Existing methods typically focus on making\nfull use of history context or previously predicted SQL for currently SQL\nparsing, while neglecting to explicitly comprehend the schema and\nconversational dependency, such as co-reference, ellipsis and user focus\nchange. In this paper, we propose CQR-SQL, which uses auxiliary Conversational\nQuestion Reformulation (CQR) learning to explicitly exploit schema and decouple\ncontextual dependency for SQL parsing. Specifically, we first present a schema\nenhanced recursive CQR method to produce domain-relevant self-contained\nquestions. Secondly, we train CQR-SQL models to map the semantics of multi-turn\nquestions and auxiliary self-contained questions into the same latent space\nthrough schema grounding consistency task and tree-structured SQL parsing\nconsistency task, which enhances the abilities of SQL parsing by adequately\ncontextual understanding. At the time of writing, our CQR-SQL achieves new\nstate-of-the-art results on two context-dependent text-to-SQL benchmarks SParC\nand CoSQL.",
          "arxiv_id": "2205.07686v3"
        }
      ],
      "41": [
        {
          "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)",
          "year": "2022-03",
          "abstract": "For a long time, different recommendation tasks typically require designing\ntask-specific architectures and training objectives. As a result, it is hard to\ntransfer the learned knowledge and representations from one task to another,\nthus restricting the generalization ability of existing recommendation\napproaches, e.g., a sequential recommendation model can hardly be applied or\ntransferred to a review generation method. To deal with such issues,\nconsidering that language can describe almost anything and language grounding\nis a powerful medium to represent various problems or tasks, we present a\nflexible and unified text-to-text paradigm called \"Pretrain, Personalized\nPrompt, and Predict Paradigm\" (P5) for recommendation, which unifies various\nrecommendation tasks in a shared framework. In P5, all data such as user-item\ninteractions, user descriptions, item metadata, and user reviews are converted\nto a common format -- natural language sequences. The rich information from\nnatural language assists P5 to capture deeper semantics for personalization and\nrecommendation. Specifically, P5 learns different tasks with the same language\nmodeling objective during pretraining. Thus, it serves as the foundation model\nfor various downstream recommendation tasks, allows easy integration with other\nmodalities, and enables instruction-based recommendation based on prompts. P5\nadvances recommender systems from shallow model to deep model to big model, and\nwill revolutionize the technical form of recommender systems towards universal\nrecommendation engine. With adaptive personalized prompt for different users,\nP5 is able to make predictions in a zero-shot or few-shot manner and largely\nreduces the necessity for extensive fine-tuning. On several recommendation\nbenchmarks, we conduct experiments to show the effectiveness of P5. We release\nthe source code at https://github.com/jeykigung/P5.",
          "arxiv_id": "2203.13366v7"
        },
        {
          "title": "GenRec: Large Language Model for Generative Recommendation",
          "year": "2023-07",
          "abstract": "In recent years, large language models (LLM) have emerged as powerful tools\nfor diverse natural language processing tasks. However, their potential for\nrecommender systems under the generative recommendation paradigm remains\nrelatively unexplored. This paper presents an innovative approach to\nrecommendation systems using large language models (LLMs) based on text data.\nIn this paper, we present a novel LLM for generative recommendation (GenRec)\nthat utilized the expressive power of LLM to directly generate the target item\nto recommend, rather than calculating ranking score for each candidate item one\nby one as in traditional discriminative recommendation. GenRec uses LLM's\nunderstanding ability to interpret context, learn user preferences, and\ngenerate relevant recommendation. Our proposed approach leverages the vast\nknowledge encoded in large language models to accomplish recommendation tasks.\nWe first we formulate specialized prompts to enhance the ability of LLM to\ncomprehend recommendation tasks. Subsequently, we use these prompts to\nfine-tune the LLaMA backbone LLM on a dataset of user-item interactions,\nrepresented by textual data, to capture user preferences and item\ncharacteristics. Our research underscores the potential of LLM-based generative\nrecommendation in revolutionizing the domain of recommendation systems and\noffers a foundational framework for future explorations in this field. We\nconduct extensive experiments on benchmark datasets, and the experiments shows\nthat our GenRec has significant better results on large dataset.",
          "arxiv_id": "2307.00457v2"
        },
        {
          "title": "Aligning Recommendation and Conversation via Dual Imitation",
          "year": "2022-11",
          "abstract": "Human conversations of recommendation naturally involve the shift of\ninterests which can align the recommendation actions and conversation process\nto make accurate recommendations with rich explanations. However, existing\nconversational recommendation systems (CRS) ignore the advantage of user\ninterest shift in connecting recommendation and conversation, which leads to an\nineffective loose coupling structure of CRS. To address this issue, by modeling\nthe recommendation actions as recommendation paths in a knowledge graph (KG),\nwe propose DICR (Dual Imitation for Conversational Recommendation), which\ndesigns a dual imitation to explicitly align the recommendation paths and user\ninterest shift paths in a recommendation module and a conversation module,\nrespectively. By exchanging alignment signals, DICR achieves bidirectional\npromotion between recommendation and conversation modules and generates\nhigh-quality responses with accurate recommendations and coherent explanations.\nExperiments demonstrate that DICR outperforms the state-of-the-art models on\nrecommendation and conversation performance with automatic, human, and novel\nexplainability metrics.",
          "arxiv_id": "2211.02848v1"
        }
      ],
      "42": [
        {
          "title": "Refract ICL: Rethinking Example Selection in the Era of Million-Token Models",
          "year": "2025-06",
          "abstract": "The emergence of long-context large language models (LLMs) has enabled the\nuse of hundreds, or even thousands, of demonstrations for in-context learning\n(ICL) - a previously impractical regime. This paper investigates whether\ntraditional ICL selection strategies, which balance the similarity of ICL\nexamples to the test input (using a text retriever) with diversity within the\nICL set, remain effective when utilizing a large number of demonstrations. Our\nexperiments demonstrate that, while longer contexts can accommodate more\nexamples, simply increasing the number of demonstrations does not guarantee\nimproved performance. Smart ICL selection remains crucial, even with thousands\nof demonstrations. To further enhance ICL in this setting, we introduce Refract\nICL, a novel ICL selection algorithm specifically designed to focus LLM\nattention on challenging examples by strategically repeating them within the\ncontext and incorporating zero-shot predictions as error signals. Our results\nshow that Refract ICL significantly improves the performance of extremely\nlong-context models such as Gemini 1.5 Pro, particularly on tasks with a\nsmaller number of output classes.",
          "arxiv_id": "2506.12346v1"
        },
        {
          "title": "Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes",
          "year": "2023-11",
          "abstract": "In-context learning (ICL) refers to the ability of a model to condition on a\nfew in-context demonstrations (input-output examples of the underlying task) to\ngenerate the answer for a new query input, without updating parameters. Despite\nthe impressive ICL ability of LLMs, it has also been found that ICL in LLMs is\nsensitive to input demonstrations and limited to short context lengths. To\nunderstand the limitations and principles for successful ICL, we conduct an\ninvestigation with ICL linear regression of transformers. We characterize\nseveral Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL\nfailures and compare transformers with DeepSet, a simple yet powerful\narchitecture for ICL. Surprisingly, DeepSet outperforms transformers across a\nvariety of distribution shifts, implying that preserving permutation invariance\nsymmetry to input demonstrations is crucial for OOD ICL. The phenomenon\nspecifies a fundamental requirement by ICL, which we termed as ICL invariance.\nNevertheless, the positional encodings in LLMs will break ICL invariance. To\nthis end, we further evaluate transformers with identical positional encodings\nand find preserving ICL invariance in transformers achieves state-of-the-art\nperformance across various ICL distribution shifts",
          "arxiv_id": "2311.18194v1"
        },
        {
          "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
          "year": "2023-07",
          "abstract": "The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.",
          "arxiv_id": "2307.12375v4"
        }
      ],
      "43": [
        {
          "title": "Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer",
          "year": "2022-07",
          "abstract": "The increasing capabilities of quantum computing hardware and the challenge\nof realizing deep quantum circuits require fully automated and efficient tools\nfor compiling quantum circuits. To express arbitrary circuits in a sequence of\nnative gates specific to the quantum computer architecture, it is necessary to\nmake algorithms portable across the landscape of quantum hardware providers. In\nthis work, we present a compiler capable of transforming and optimizing a\nquantum circuit targeting a shuttling-based trapped-ion quantum processor. It\nconsists of custom algorithms set on top of the quantum circuit framework\nPytket. The performance was evaluated for a wide range of quantum circuits and\nthe results show that the gate counts can be reduced by factors up to 5.1\ncompared to standard Pytket and up to 2.2 compared to standard Qiskit\ncompilation.",
          "arxiv_id": "2207.01964v4"
        },
        {
          "title": "Grammar-aware sentence classification on quantum computers",
          "year": "2020-12",
          "abstract": "Natural language processing (NLP) is at the forefront of great advances in\ncontemporary AI, and it is arguably one of the most challenging areas of the\nfield. At the same time, in the area of Quantum Computing (QC), with the steady\ngrowth of quantum hardware and notable improvements towards implementations of\nquantum algorithms, we are approaching an era when quantum computers perform\ntasks that cannot be done on classical computers with a reasonable amount of\nresources. This provides a new range of opportunities for AI, and for NLP\nspecifically. In this work, we work with the Categorical Distributional\nCompositional (DisCoCat) model of natural language meaning, whose underlying\nmathematical underpinnings make it amenable to quantum instantiations. Earlier\nwork on fault-tolerant quantum algorithms has already demonstrated potential\nquantum advantage for NLP, notably employing DisCoCat. In this work, we focus\non the capabilities of noisy intermediate-scale quantum (NISQ) hardware and\nperform the first implementation of an NLP task on a NISQ processor, using the\nDisCoCat framework. Sentences are instantiated as parameterised quantum\ncircuits; word-meanings are embedded in quantum states using parameterised\nquantum-circuits and the sentence's grammatical structure faithfully manifests\nas a pattern of entangling operations which compose the word-circuits into a\nsentence-circuit. The circuits' parameters are trained using a classical\noptimiser in a supervised NLP task of binary classification. Our novel QNLP\nmodel shows concrete promise for scalability as the quality of the quantum\nhardware improves in the near future and solidifies a novel branch of\nexperimental research at the intersection of QC and AI.",
          "arxiv_id": "2012.03756v2"
        },
        {
          "title": "Foundations for Near-Term Quantum Natural Language Processing",
          "year": "2020-12",
          "abstract": "We provide conceptual and mathematical foundations for near-term quantum\nnatural language processing (QNLP), and do so in quantum computer scientist\nfriendly terms. We opted for an expository presentation style, and provide\nreferences for supporting empirical evidence and formal statements concerning\nmathematical generality.\n  We recall how the quantum model for natural language that we employ\ncanonically combines linguistic meanings with rich linguistic structure, most\nnotably grammar. In particular, the fact that it takes a quantum-like model to\ncombine meaning and structure, establishes QNLP as quantum-native, on par with\nsimulation of quantum systems. Moreover, the now leading Noisy\nIntermediate-Scale Quantum (NISQ) paradigm for encoding classical data on\nquantum hardware, variational quantum circuits, makes NISQ exceptionally\nQNLP-friendly: linguistic structure can be encoded as a free lunch, in contrast\nto the apparently exponentially expensive classical encoding of grammar.\n  Quantum speed-up for QNLP tasks has already been established in previous work\nwith Will Zeng. Here we provide a broader range of tasks which all enjoy the\nsame advantage.\n  Diagrammatic reasoning is at the heart of QNLP. Firstly, the quantum model\ninterprets language as quantum processes via the diagrammatic formalism of\ncategorical quantum mechanics. Secondly, these diagrams are via ZX-calculus\ntranslated into quantum circuits. Parameterisations of meanings then become the\ncircuit variables to be learned.\n  Our encoding of linguistic structure within quantum circuits also embodies a\nnovel approach for establishing word-meanings that goes beyond the current\nstandards in mainstream AI, by placing linguistic structure at the heart of\nWittgenstein's meaning-is-context.",
          "arxiv_id": "2012.03755v1"
        }
      ],
      "44": [
        {
          "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
          "year": "2023-05",
          "abstract": "Instruction tuning has emerged to enhance the capabilities of large language\nmodels (LLMs) to comprehend instructions and generate appropriate responses.\nExisting methods either manually annotate or employ LLM (e.g., GPT-series) to\ngenerate data for instruction tuning. However, they often overlook associating\ninstructions with existing annotated datasets. In this paper, we propose\nDynosaur, a dynamic growth paradigm for the automatic curation of\ninstruction-tuning data. Based on the metadata of existing datasets, we use\nLLMs to automatically construct instruction-tuning data by identifying relevant\ndata fields and generating appropriate instructions.\n  By leveraging the existing annotated datasets, Dynosaur offers several\nadvantages: 1) it reduces the API cost for generating instructions (e.g., it\ncosts less than $12 USD by calling GPT-3.5-turbo for generating 800K\ninstruction tuning samples; 2) it provides high-quality data for instruction\ntuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform\nwith comparable data sizes); and 3) it supports the continuous improvement of\nmodels by generating instruction-tuning data when a new annotated dataset\nbecomes available. We further investigate a continual learning scheme for\nlearning with the ever-growing instruction-tuning dataset, and demonstrate that\nreplaying tasks with diverse instruction embeddings not only helps mitigate\nforgetting issues but generalizes to unseen tasks better.\n  Code and data are available at https://github.com/WadeYin9712/Dynosaur.",
          "arxiv_id": "2305.14327v2"
        },
        {
          "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
          "year": "2023-11",
          "abstract": "Instruction tuning has become the de facto method to equip large language\nmodels (LLMs) with the ability of following user instructions. Usually,\nhundreds of thousands or millions of instruction-following pairs are employed\nto fine-tune the foundation LLMs. Recently, some studies show that a small\nnumber of high-quality instruction data is enough. However, how to select\nappropriate instruction data for a given LLM is still an open problem. To\naddress this problem, in this paper we present a model-oriented data selection\n(MoDS) approach, which selects instruction data based on a new criteria\nconsidering three aspects: quality, coverage and necessity. First, our approach\nutilizes a quality evaluation model to filter out the high-quality subset from\nthe original instruction dataset, and then designs an algorithm to further\nselect from the high-quality subset a seed instruction dataset with good\ncoverage. The seed dataset is applied to fine-tune the foundation LLM to obtain\nan initial instruction-following LLM. Finally, we develop a necessity\nevaluation model to find out the instruction data which are performed badly in\nthe initial instruction-following LLM and consider them necessary instructions\nto further improve the LLMs. In this way, we can get a small high-quality,\nbroad-coverage and high-necessity subset from the original instruction\ndatasets. Experimental results show that, the model fine-tuned with 4,000\ninstruction pairs selected by our approach could perform better than the model\nfine-tuned with the full original dataset which includes 214k instruction data.",
          "arxiv_id": "2311.15653v1"
        },
        {
          "title": "Instruction Following without Instruction Tuning",
          "year": "2024-09",
          "abstract": "Instruction tuning commonly means finetuning a language model on\ninstruction-response pairs. We discover two forms of adaptation (tuning) that\nare deficient compared to instruction tuning, yet still yield instruction\nfollowing; we call this implicit instruction tuning. We first find that\ninstruction-response pairs are not necessary: training solely on responses,\nwithout any corresponding instructions, yields instruction following. This\nsuggests pretrained models have an instruction-response mapping which is\nrevealed by teaching the model the desired distribution of responses. However,\nwe then find it's not necessary to teach the desired distribution of responses:\ninstruction-response training on narrow-domain data like poetry still leads to\nbroad instruction-following behavior like recipe generation. In particular,\nwhen instructions are very different from those in the narrow finetuning\ndomain, models' responses do not adhere to the style of the finetuning domain.\nTo begin to explain implicit instruction tuning, we hypothesize that very\nsimple changes to a language model's distribution yield instruction following.\nWe support this by hand-writing a rule-based language model which yields\ninstruction following in a product-of-experts with a pretrained model. The\nrules are to slowly increase the probability of ending the sequence, penalize\nrepetition, and uniformly change 15 words' probabilities. In summary,\nadaptations made without being designed to yield instruction following can do\nso implicitly.",
          "arxiv_id": "2409.14254v1"
        }
      ],
      "45": [
        {
          "title": "Knowledge Graph Enhanced Large Language Model Editing",
          "year": "2024-02",
          "abstract": "Large language models (LLMs) are pivotal in advancing natural language\nprocessing (NLP) tasks, yet their efficacy is hampered by inaccuracies and\noutdated knowledge. Model editing emerges as a promising solution to address\nthese challenges. However, existing editing methods struggle to track and\nincorporate changes in knowledge associated with edits, which limits the\ngeneralization ability of postedit LLMs in processing edited knowledge. To\ntackle these problems, we propose a novel model editing method that leverages\nknowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we\nfirst utilize a knowledge graph augmentation module to uncover associated\nknowledge that has changed due to editing, obtaining its internal\nrepresentations within LLMs. This approach allows knowledge alterations within\nLLMs to be reflected through an external graph structure. Subsequently, we\ndesign a graph-based knowledge edit module to integrate structured knowledge\ninto the model editing. This ensures that the updated parameters reflect not\nonly the modifications of the edited knowledge but also the changes in other\nassociated knowledge resulting from the editing process. Comprehensive\nexperiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME\nsignificantly improves the generalization capabilities of post-edit LLMs in\nemploying edited knowledge.",
          "arxiv_id": "2402.13593v1"
        },
        {
          "title": "Event-level Knowledge Editing",
          "year": "2024-02",
          "abstract": "Knowledge editing aims at updating knowledge of large language models (LLMs)\nto prevent them from becoming outdated. Existing work edits LLMs at the level\nof factual knowledge triplets. However, natural knowledge updates in the real\nworld come from the occurrences of new events rather than direct changes in\nfactual triplets. In this paper, we propose a new task setting: event-level\nknowledge editing, which directly edits new events into LLMs and improves over\nconventional triplet-level editing on (1) Efficiency. A single event edit leads\nto updates in multiple entailed knowledge triplets. (2) Completeness. Beyond\nupdating factual knowledge, event-level editing also requires considering the\nevent influences and updating LLMs' knowledge about future trends. We construct\na high-quality event-level editing benchmark ELKEN, consisting of 1,515 event\nedits, 6,449 questions about factual knowledge, and 10,150 questions about\nfuture tendencies. We systematically evaluate the performance of various\nknowledge editing methods and LLMs on this benchmark. We find that ELKEN poses\nsignificant challenges to existing knowledge editing approaches. Our codes and\ndataset are publicly released to facilitate further research.",
          "arxiv_id": "2402.13093v2"
        },
        {
          "title": "WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing",
          "year": "2024-02",
          "abstract": "Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.",
          "arxiv_id": "2402.10987v2"
        }
      ],
      "46": [
        {
          "title": "Evaluating Creative Short Story Generation in Humans and Large Language Models",
          "year": "2024-11",
          "abstract": "Story-writing is a fundamental aspect of human imagination, relying heavily\non creativity to produce narratives that are novel, effective, and surprising.\nWhile large language models (LLMs) have demonstrated the ability to generate\nhigh-quality stories, their creative story-writing capabilities remain\nunder-explored. In this work, we conduct a systematic analysis of creativity in\nshort story generation across 60 LLMs and 60 people using a five-sentence\ncue-word-based creative story-writing task. We use measures to automatically\nevaluate model- and human-generated stories across several dimensions of\ncreativity, including novelty, surprise, diversity, and linguistic complexity.\nWe also collect creativity ratings and Turing Test classifications from\nnon-expert and expert human raters and LLMs. Automated metrics show that LLMs\ngenerate stylistically complex stories, but tend to fall short in terms of\nnovelty, surprise and diversity when compared to average human writers. Expert\nratings generally coincide with automated metrics. However, LLMs and\nnon-experts rate LLM stories to be more creative than human-generated stories.\nWe discuss why and how these differences in ratings occur, and their\nimplications for both human and artificial creativity.",
          "arxiv_id": "2411.02316v5"
        },
        {
          "title": "Whose story is it? Personalizing story generation by inferring author styles",
          "year": "2025-02",
          "abstract": "Personalization is critical for improving user experience in interactive\nwriting and educational applications, yet remains understudied in story\ngeneration. We study the task of personalizing story generation, where our goal\nis to mimic an author's writing style, given other stories written by them. We\ncollect Mythos, a dataset of 3.6k stories from 112 authors, with an average of\n16 stories per author, across five distinct sources reflecting diverse\nstory-writing settings. We propose a two-stage pipeline for personalized story\ngeneration: first, we infer authors' implicit writing characteristics and\norganize them into an Author Writing Sheet, which is validated by humans to be\nof high quality; second, we simulate the author's persona using tailored\npersona descriptions and personalized story rules. We find that stories\npersonalized using the Author Writing Sheet outperform a non-personalized\nbaseline, achieving a 78% win-rate in capturing authors' past style and 59% in\nsimilarity to ground-truth author stories. Human evaluation supports these\nfindings and further highlights trends, such as Reddit stories being easier to\npersonalize, and the Creativity and Language Use aspects of stories being\neasier to personalize than the Plot.",
          "arxiv_id": "2502.13028v2"
        },
        {
          "title": "A Character-Centric Creative Story Generation via Imagination",
          "year": "2024-09",
          "abstract": "Creative story generation has long been a goal of NLP research. While\nexisting methodologies have aimed to generate long and coherent stories, they\nfall significantly short of human capabilities in terms of diversity and\ncharacter depth. To address this, we introduce a novel story generation\nframework called CCI (Character-centric Creative story generation via\nImagination). CCI features two modules for creative story generation: IG\n(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we\nutilize a text-to-image model to create visual representations of key story\nelements, such as characters, backgrounds, and main plots, in a more novel and\nconcrete manner than text-only approaches. The MW module uses these story\nelements to generate multiple persona-description candidates and selects the\nbest one to insert into the story, thereby enhancing the richness and depth of\nthe narrative. We compared the stories generated by CCI and baseline models\nthrough statistical analysis, as well as human and LLM evaluations. The results\nshowed that the IG and MW modules significantly improve various aspects of the\nstories' creativity. Furthermore, our framework enables interactive multi-modal\nstory generation with users, opening up new possibilities for human-LLM\nintegration in cultural development. Project page : https://www.2024cci.p-e.kr/",
          "arxiv_id": "2409.16667v3"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:23:37Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}