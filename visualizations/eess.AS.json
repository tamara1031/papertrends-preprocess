{
  "topics": {
    "data": {
      "0": {
        "name": "0_music_generation_musical_audio",
        "keywords": [
          [
            "music",
            0.03946158165377838
          ],
          [
            "generation",
            0.018298041268572932
          ],
          [
            "musical",
            0.01619213509483555
          ],
          [
            "audio",
            0.015615982427515258
          ],
          [
            "Music",
            0.015097499951422696
          ],
          [
            "model",
            0.010847586888329765
          ],
          [
            "models",
            0.009881635435400311
          ],
          [
            "music generation",
            0.008385696753486226
          ],
          [
            "melody",
            0.00824390463273352
          ],
          [
            "text",
            0.007885041226664589
          ]
        ],
        "count": 1322
      },
      "1": {
        "name": "1_attacks_detection_adversarial_audio",
        "keywords": [
          [
            "attacks",
            0.021618902797114575
          ],
          [
            "detection",
            0.016061396389037943
          ],
          [
            "adversarial",
            0.01561131029788385
          ],
          [
            "audio",
            0.015559693190325912
          ],
          [
            "spoofing",
            0.01487850057187302
          ],
          [
            "deepfake",
            0.013384159081015676
          ],
          [
            "privacy",
            0.012302152995087249
          ],
          [
            "attack",
            0.012288516204731646
          ],
          [
            "speech",
            0.011896723572545615
          ],
          [
            "speaker",
            0.011693506054114046
          ]
        ],
        "count": 973
      },
      "2": {
        "name": "2_sound_acoustic_room_method",
        "keywords": [
          [
            "sound",
            0.02384137384880846
          ],
          [
            "acoustic",
            0.016591469784707203
          ],
          [
            "room",
            0.014960782535901676
          ],
          [
            "method",
            0.013714189261298573
          ],
          [
            "source",
            0.012913924244592496
          ],
          [
            "field",
            0.012260103841405827
          ],
          [
            "microphone",
            0.01211341373145432
          ],
          [
            "spatial",
            0.01189650038273658
          ],
          [
            "estimation",
            0.01135571998580211
          ],
          [
            "localization",
            0.01044480162847734
          ]
        ],
        "count": 843
      },
      "3": {
        "name": "3_enhancement_speech enhancement_speech_channel",
        "keywords": [
          [
            "enhancement",
            0.02538750310354819
          ],
          [
            "speech enhancement",
            0.022130403761147527
          ],
          [
            "speech",
            0.02115116798781154
          ],
          [
            "channel",
            0.01421170864706964
          ],
          [
            "noise",
            0.012402817057663738
          ],
          [
            "network",
            0.012364515140252542
          ],
          [
            "SE",
            0.01148948194961418
          ],
          [
            "Enhancement",
            0.011363446133261217
          ],
          [
            "Speech",
            0.010807059767093273
          ],
          [
            "proposed",
            0.010646653112849921
          ]
        ],
        "count": 805
      },
      "4": {
        "name": "4_TTS_speech_style_speaker",
        "keywords": [
          [
            "TTS",
            0.033993823116799174
          ],
          [
            "speech",
            0.023010117019381455
          ],
          [
            "style",
            0.019503892326316708
          ],
          [
            "speaker",
            0.018194576992283534
          ],
          [
            "prosody",
            0.017000542333968575
          ],
          [
            "text",
            0.016166059702373514
          ],
          [
            "synthesis",
            0.013812380654837995
          ],
          [
            "model",
            0.01287572891070965
          ],
          [
            "emotion",
            0.011937267099771382
          ],
          [
            "Speech",
            0.01100621027821456
          ]
        ],
        "count": 755
      },
      "5": {
        "name": "5_emotion_emotion recognition_Emotion_recognition",
        "keywords": [
          [
            "emotion",
            0.03182525282338383
          ],
          [
            "emotion recognition",
            0.02071135945636234
          ],
          [
            "Emotion",
            0.020205683473732323
          ],
          [
            "recognition",
            0.015369915336065597
          ],
          [
            "Recognition",
            0.012396502218751696
          ],
          [
            "features",
            0.011557316081288727
          ],
          [
            "emotional",
            0.011526817402569235
          ],
          [
            "emotions",
            0.01129593168007374
          ],
          [
            "speech emotion",
            0.011291923680664633
          ],
          [
            "speech",
            0.011189492280287795
          ]
        ],
        "count": 676
      },
      "6": {
        "name": "6_ASR_CTC_streaming_model",
        "keywords": [
          [
            "ASR",
            0.02238157289051604
          ],
          [
            "CTC",
            0.019933958029580458
          ],
          [
            "streaming",
            0.017258611979161103
          ],
          [
            "model",
            0.016006254961725485
          ],
          [
            "recognition",
            0.015565471178246628
          ],
          [
            "speech recognition",
            0.01520372009680814
          ],
          [
            "models",
            0.015139380500287762
          ],
          [
            "attention",
            0.014258757883643655
          ],
          [
            "end",
            0.014039526720060393
          ],
          [
            "RNN",
            0.013775543436493667
          ]
        ],
        "count": 644
      },
      "7": {
        "name": "7_event_sound_audio_classification",
        "keywords": [
          [
            "event",
            0.020059424782873297
          ],
          [
            "sound",
            0.0182890950219005
          ],
          [
            "audio",
            0.01705551643161939
          ],
          [
            "classification",
            0.014221485301858384
          ],
          [
            "SED",
            0.01267351841945298
          ],
          [
            "sound event",
            0.012414427461784169
          ],
          [
            "events",
            0.012063712872422463
          ],
          [
            "Sound",
            0.011633497204448461
          ],
          [
            "Event",
            0.010734221364730992
          ],
          [
            "data",
            0.010464966147543137
          ]
        ],
        "count": 633
      },
      "8": {
        "name": "8_audio_Audio_visual_text",
        "keywords": [
          [
            "audio",
            0.040604465512020384
          ],
          [
            "Audio",
            0.021209047033743652
          ],
          [
            "visual",
            0.02083891950727227
          ],
          [
            "text",
            0.014267882806360934
          ],
          [
            "captioning",
            0.012635317160543627
          ],
          [
            "language",
            0.01259402723088915
          ],
          [
            "video",
            0.011358922744774498
          ],
          [
            "modal",
            0.010785753748462456
          ],
          [
            "models",
            0.010421228004615203
          ],
          [
            "tasks",
            0.010007141516281598
          ]
        ],
        "count": 539
      },
      "9": {
        "name": "9_speech_dysarthric_articulatory_features",
        "keywords": [
          [
            "speech",
            0.023704973933045412
          ],
          [
            "dysarthric",
            0.014754047995094983
          ],
          [
            "articulatory",
            0.014592213185322018
          ],
          [
            "features",
            0.012745320627849157
          ],
          [
            "Speech",
            0.011064851131676852
          ],
          [
            "data",
            0.010276282389041526
          ],
          [
            "disease",
            0.01000414259067377
          ],
          [
            "dysarthric speech",
            0.009495817230427401
          ],
          [
            "detection",
            0.009005459892263928
          ],
          [
            "AD",
            0.0088625393191185
          ]
        ],
        "count": 532
      },
      "10": {
        "name": "10_speaker_verification_speaker verification_Speaker",
        "keywords": [
          [
            "speaker",
            0.04112017229131047
          ],
          [
            "verification",
            0.026116439211498605
          ],
          [
            "speaker verification",
            0.023458776656022143
          ],
          [
            "Speaker",
            0.022111036606629126
          ],
          [
            "speaker recognition",
            0.01492489923590218
          ],
          [
            "Verification",
            0.01239942322905078
          ],
          [
            "embeddings",
            0.01057833735305975
          ],
          [
            "VoxCeleb",
            0.010374452023743395
          ],
          [
            "performance",
            0.010260536319343167
          ],
          [
            "recognition",
            0.009912536349436
          ]
        ],
        "count": 491
      },
      "11": {
        "name": "11_speaker_diarization_speaker diarization_Speaker",
        "keywords": [
          [
            "speaker",
            0.0427997478778291
          ],
          [
            "diarization",
            0.03937537359282891
          ],
          [
            "speaker diarization",
            0.017928180500498356
          ],
          [
            "Speaker",
            0.01701151290847432
          ],
          [
            "speakers",
            0.014351593939590374
          ],
          [
            "clustering",
            0.013901657849625998
          ],
          [
            "multi",
            0.013621865933413616
          ],
          [
            "Diarization",
            0.013547517870573986
          ],
          [
            "speech",
            0.012740418426519148
          ],
          [
            "EEND",
            0.012271131514218315
          ]
        ],
        "count": 451
      },
      "12": {
        "name": "12_SLU_spoken_speech_Spoken",
        "keywords": [
          [
            "SLU",
            0.023846113750250593
          ],
          [
            "spoken",
            0.02098357943031237
          ],
          [
            "speech",
            0.018759789627142365
          ],
          [
            "Spoken",
            0.016686093567291695
          ],
          [
            "language",
            0.01655111668024953
          ],
          [
            "dialogue",
            0.015953392268674732
          ],
          [
            "models",
            0.01437861813257158
          ],
          [
            "text",
            0.014369859301691339
          ],
          [
            "understanding",
            0.012908796000078937
          ],
          [
            "end",
            0.012447953084876132
          ]
        ],
        "count": 368
      },
      "13": {
        "name": "13_supervised_speech_training_data",
        "keywords": [
          [
            "supervised",
            0.020827492202965134
          ],
          [
            "speech",
            0.01613386941885593
          ],
          [
            "training",
            0.01517537198811564
          ],
          [
            "data",
            0.014687837672744007
          ],
          [
            "self",
            0.014662880155082795
          ],
          [
            "pre",
            0.01455276506630964
          ],
          [
            "Self",
            0.01447523466388267
          ],
          [
            "representations",
            0.013587611733448461
          ],
          [
            "models",
            0.013505674407039587
          ],
          [
            "ASR",
            0.013182026092621946
          ]
        ],
        "count": 362
      },
      "14": {
        "name": "14_COVID_respiratory_cough_heart",
        "keywords": [
          [
            "COVID",
            0.036338364668166014
          ],
          [
            "respiratory",
            0.0263844048925479
          ],
          [
            "cough",
            0.024585593435595847
          ],
          [
            "heart",
            0.019338415369404355
          ],
          [
            "sounds",
            0.019084175724576765
          ],
          [
            "sound",
            0.015769701321458166
          ],
          [
            "detection",
            0.013355999885035153
          ],
          [
            "classification",
            0.012991694501722453
          ],
          [
            "data",
            0.01078512504124468
          ],
          [
            "patients",
            0.010700042325790884
          ]
        ],
        "count": 343
      },
      "15": {
        "name": "15_diffusion_speech_quality_neural",
        "keywords": [
          [
            "diffusion",
            0.019528667123577873
          ],
          [
            "speech",
            0.017819440531700537
          ],
          [
            "quality",
            0.015936357351584374
          ],
          [
            "neural",
            0.014248872423812235
          ],
          [
            "vocoder",
            0.013885498201167334
          ],
          [
            "codecs",
            0.011775727397189946
          ],
          [
            "audio",
            0.011500772078067482
          ],
          [
            "high",
            0.011436943575768563
          ],
          [
            "model",
            0.011393145324519191
          ],
          [
            "codec",
            0.011354912203154146
          ]
        ],
        "count": 343
      },
      "16": {
        "name": "16_VC_conversion_voice conversion_voice",
        "keywords": [
          [
            "VC",
            0.05100495816894143
          ],
          [
            "conversion",
            0.046975561011055114
          ],
          [
            "voice conversion",
            0.035859779570101113
          ],
          [
            "voice",
            0.03289234210525824
          ],
          [
            "speaker",
            0.025589132630190265
          ],
          [
            "Conversion",
            0.023011176438515964
          ],
          [
            "Voice",
            0.022097988399468596
          ],
          [
            "content",
            0.017800235792617428
          ],
          [
            "speech",
            0.015507613533438937
          ],
          [
            "shot",
            0.012816299789113446
          ]
        ],
        "count": 289
      },
      "17": {
        "name": "17_KWS_keyword_spotting_Keyword",
        "keywords": [
          [
            "KWS",
            0.04879384000504368
          ],
          [
            "keyword",
            0.0417500288130915
          ],
          [
            "spotting",
            0.02847462626865995
          ],
          [
            "Keyword",
            0.02744710999690241
          ],
          [
            "keyword spotting",
            0.024475598095778255
          ],
          [
            "Spotting",
            0.021101254541724
          ],
          [
            "keywords",
            0.015772549434265808
          ],
          [
            "model",
            0.013467374030284408
          ],
          [
            "learning",
            0.009802751403142126
          ],
          [
            "accuracy",
            0.009417040181948201
          ]
        ],
        "count": 211
      },
      "18": {
        "name": "18_ASR_languages_speech_language",
        "keywords": [
          [
            "ASR",
            0.030544003854366498
          ],
          [
            "languages",
            0.025775309157373978
          ],
          [
            "speech",
            0.018691199050453636
          ],
          [
            "language",
            0.01757860431593112
          ],
          [
            "Speech",
            0.016643401997105872
          ],
          [
            "speech recognition",
            0.01549641202768859
          ],
          [
            "recognition",
            0.015335912994141823
          ],
          [
            "data",
            0.014460226443480955
          ],
          [
            "Recognition",
            0.013280483594611758
          ],
          [
            "models",
            0.01287233282722191
          ]
        ],
        "count": 205
      },
      "19": {
        "name": "19_singing_singing voice_voice_Singing",
        "keywords": [
          [
            "singing",
            0.07743942320229019
          ],
          [
            "singing voice",
            0.04372505276291297
          ],
          [
            "voice",
            0.03844777591377311
          ],
          [
            "Singing",
            0.0314807310167443
          ],
          [
            "SVS",
            0.025198437701030172
          ],
          [
            "singer",
            0.01990073430863237
          ],
          [
            "Voice",
            0.017442137785900837
          ],
          [
            "pitch",
            0.016997167440807676
          ],
          [
            "voice synthesis",
            0.016804046197633305
          ],
          [
            "singing voice synthesis",
            0.014645563197539969
          ]
        ],
        "count": 203
      },
      "20": {
        "name": "20_translation_speech translation_Translation_ST",
        "keywords": [
          [
            "translation",
            0.07074318453431189
          ],
          [
            "speech translation",
            0.035646808359885845
          ],
          [
            "Translation",
            0.031363062818841325
          ],
          [
            "ST",
            0.029720235718806783
          ],
          [
            "speech",
            0.028336803039494336
          ],
          [
            "text",
            0.023702825181357832
          ],
          [
            "Speech",
            0.019738276480468876
          ],
          [
            "end",
            0.01927363490692252
          ],
          [
            "BLEU",
            0.017404425745312316
          ],
          [
            "language",
            0.01522463160933739
          ]
        ],
        "count": 184
      },
      "21": {
        "name": "21_species_bird_classification_monitoring",
        "keywords": [
          [
            "species",
            0.03256033000814255
          ],
          [
            "bird",
            0.02431181763503277
          ],
          [
            "classification",
            0.018708888792805117
          ],
          [
            "monitoring",
            0.01755476342450617
          ],
          [
            "bioacoustic",
            0.01718617989292827
          ],
          [
            "data",
            0.016878018500648156
          ],
          [
            "learning",
            0.016532906801424826
          ],
          [
            "animal",
            0.01372142214453514
          ],
          [
            "vocalizations",
            0.013301792990782914
          ],
          [
            "recordings",
            0.012724478796012486
          ]
        ],
        "count": 178
      },
      "22": {
        "name": "22_visual_audio_AVSR_Visual",
        "keywords": [
          [
            "visual",
            0.05141038842525886
          ],
          [
            "audio",
            0.0227430379993657
          ],
          [
            "AVSR",
            0.02085049663428871
          ],
          [
            "Visual",
            0.020230755934427726
          ],
          [
            "lip",
            0.019962385251623277
          ],
          [
            "visual speech",
            0.01898824851116326
          ],
          [
            "recognition",
            0.01627006382760535
          ],
          [
            "AV",
            0.01585647780433711
          ],
          [
            "speech recognition",
            0.01480342880629372
          ],
          [
            "speech",
            0.014624093764060382
          ]
        ],
        "count": 166
      },
      "23": {
        "name": "23_separation_source separation_source_music",
        "keywords": [
          [
            "separation",
            0.05238221148741084
          ],
          [
            "source separation",
            0.04125740916616738
          ],
          [
            "source",
            0.038386387246707454
          ],
          [
            "music",
            0.022037784931127218
          ],
          [
            "Separation",
            0.018687991788062255
          ],
          [
            "Source",
            0.016623360279443358
          ],
          [
            "music source",
            0.016093933369681852
          ],
          [
            "music source separation",
            0.015579243232992061
          ],
          [
            "Music",
            0.012962103793603649
          ],
          [
            "sources",
            0.01231553431935793
          ]
        ],
        "count": 164
      },
      "24": {
        "name": "24_face_lip_facial_speech",
        "keywords": [
          [
            "face",
            0.027691509729775358
          ],
          [
            "lip",
            0.02467865509015729
          ],
          [
            "facial",
            0.022044777332619937
          ],
          [
            "speech",
            0.01935572886581342
          ],
          [
            "video",
            0.018939239199393738
          ],
          [
            "head",
            0.015011998477730604
          ],
          [
            "audio",
            0.014580936130239343
          ],
          [
            "3D",
            0.013587353047658866
          ],
          [
            "generation",
            0.011703328818776709
          ],
          [
            "visual",
            0.011352599805434188
          ]
        ],
        "count": 164
      },
      "25": {
        "name": "25_MOS_quality_speech_speech quality",
        "keywords": [
          [
            "MOS",
            0.04370534594221722
          ],
          [
            "quality",
            0.029167988908739676
          ],
          [
            "speech",
            0.021820632068055228
          ],
          [
            "speech quality",
            0.021685002248552434
          ],
          [
            "prediction",
            0.02074053664141912
          ],
          [
            "assessment",
            0.017445455881231545
          ],
          [
            "evaluation",
            0.014016712085911952
          ],
          [
            "subjective",
            0.013021937732269168
          ],
          [
            "Quality",
            0.012688224130322188
          ],
          [
            "Speech",
            0.012455842597131877
          ]
        ],
        "count": 154
      },
      "26": {
        "name": "26_EEG_brain_decoding_auditory",
        "keywords": [
          [
            "EEG",
            0.0753712323797411
          ],
          [
            "brain",
            0.03773213323415979
          ],
          [
            "decoding",
            0.02468515790447396
          ],
          [
            "auditory",
            0.02289629291685684
          ],
          [
            "speech",
            0.02108398156953489
          ],
          [
            "signals",
            0.019875605683893592
          ],
          [
            "AAD",
            0.016918981934009744
          ],
          [
            "attention",
            0.01599089975444367
          ],
          [
            "neural",
            0.014875970947055193
          ],
          [
            "auditory attention",
            0.01316630885228397
          ]
        ],
        "count": 143
      },
      "27": {
        "name": "27_anomalous_detection_machine_anomaly",
        "keywords": [
          [
            "anomalous",
            0.0334213075267356
          ],
          [
            "detection",
            0.027672112405620702
          ],
          [
            "machine",
            0.025219923893385753
          ],
          [
            "anomaly",
            0.021360822214183263
          ],
          [
            "ASD",
            0.020670342033314183
          ],
          [
            "sound",
            0.019213778316955126
          ],
          [
            "anomalous sound",
            0.017878033701169548
          ],
          [
            "sounds",
            0.01704727137136137
          ],
          [
            "Anomalous",
            0.017023158998116704
          ],
          [
            "sound detection",
            0.016854518669577795
          ]
        ],
        "count": 140
      },
      "28": {
        "name": "28_audio_effects_sound_neural",
        "keywords": [
          [
            "audio",
            0.021175041381402318
          ],
          [
            "effects",
            0.020766421638847853
          ],
          [
            "sound",
            0.014304340950542631
          ],
          [
            "neural",
            0.014217984467506491
          ],
          [
            "synthesizer",
            0.014162417152925388
          ],
          [
            "differentiable",
            0.013862125111255348
          ],
          [
            "synthesis",
            0.01365428109900614
          ],
          [
            "time",
            0.013185688026718146
          ],
          [
            "audio effects",
            0.012247789248809017
          ],
          [
            "signal",
            0.01218945008449041
          ]
        ],
        "count": 130
      }
    },
    "correlations": [
      [
        1.0,
        -0.6972401839905578,
        -0.7471011978448123,
        -0.7525788117918365,
        -0.7391206249036265,
        -0.741469185051695,
        -0.7109042364700542,
        -0.7030856347275873,
        -0.6348928165119208,
        -0.7562579836425598,
        -0.7610818307953527,
        -0.760970201949924,
        -0.7329640987553605,
        -0.7175303506491635,
        -0.7586580449514095,
        -0.6998806309104733,
        -0.7546335853939673,
        -0.7631717291484907,
        -0.7530607322405709,
        -0.73966728093454,
        -0.7408446634435106,
        -0.7283879169414711,
        -0.6815353839059602,
        -0.6517021630473693,
        -0.7411744536484932,
        -0.7317178711416117,
        -0.7545195971237683,
        -0.7453804424449475,
        -0.6672332178725218
      ],
      [
        -0.6972401839905578,
        1.0,
        -0.7532888198385004,
        -0.7495189002344305,
        -0.7247501381637791,
        -0.746780932237548,
        -0.7272020679939624,
        -0.6657115349714753,
        -0.5967053338234548,
        -0.7359559892595346,
        -0.6554740154300549,
        -0.741224893939804,
        -0.7406521089704083,
        -0.7208645076558282,
        -0.7474002798088313,
        -0.7319106173178859,
        -0.7175046624817656,
        -0.7547726109704287,
        -0.7275254358348,
        -0.747491848109419,
        -0.7401132173773771,
        -0.7305888506212996,
        -0.6525148821550054,
        -0.7525504935275448,
        -0.7284757079308088,
        -0.7445424860450713,
        -0.7597091074869058,
        -0.6099412440860489,
        -0.6084312531758604
      ],
      [
        -0.7471011978448123,
        -0.7532888198385004,
        1.0,
        -0.7263418677439926,
        -0.755367950771416,
        -0.7593255741854819,
        -0.7335488785689444,
        -0.6497025531617613,
        -0.7300413336564329,
        -0.7490671073221922,
        -0.7554805557117932,
        -0.754216863262426,
        -0.7554932815472608,
        -0.745123340498127,
        -0.7391665644310716,
        -0.7413015221783812,
        -0.7564182248418464,
        -0.7639855270324301,
        -0.752772812708459,
        -0.7569728439940907,
        -0.751684166671057,
        -0.7271763769706131,
        -0.7212180114603823,
        -0.6803216801255006,
        -0.753791058079492,
        -0.7506893037510778,
        -0.7594623089845594,
        -0.7413607285516192,
        -0.7312385686080507
      ],
      [
        -0.7525788117918365,
        -0.7495189002344305,
        -0.7263418677439926,
        1.0,
        -0.6797125419541301,
        -0.7505802663123127,
        -0.6899067368981617,
        -0.7504989795461905,
        -0.7414972344896102,
        -0.6207668480664041,
        -0.7274466895983329,
        -0.7209059322141331,
        -0.6805461218712628,
        -0.6663764547352238,
        -0.7573765256793263,
        -0.4863239963458902,
        -0.73693296292043,
        -0.7557927176776892,
        -0.677713379115783,
        -0.7571221158415287,
        -0.6748789950961094,
        -0.7342075828958483,
        -0.7280537577341174,
        -0.6961599403913987,
        -0.6391776636910701,
        -0.5841458305401943,
        -0.7554601671251184,
        -0.7564457678400135,
        -0.7392304514512607
      ],
      [
        -0.7391206249036265,
        -0.7247501381637791,
        -0.755367950771416,
        -0.6797125419541301,
        1.0,
        -0.719801881925932,
        -0.6788285415344669,
        -0.7542767186581143,
        -0.7248800706169161,
        -0.6061795963553982,
        -0.6728869998274234,
        -0.6473758303050539,
        -0.6158138534157952,
        -0.654031348092314,
        -0.7614579780550405,
        -0.5742155941229392,
        -0.6162641855930825,
        -0.7578115518108816,
        -0.6522581519767898,
        -0.7320773699729327,
        -0.49219729704846715,
        -0.7300257976647264,
        -0.7350093228129082,
        -0.7462641910895165,
        -0.6148809386555322,
        -0.5647323044843798,
        -0.7551219131480421,
        -0.758464441163861,
        -0.7421802264331495
      ],
      [
        -0.741469185051695,
        -0.746780932237548,
        -0.7593255741854819,
        -0.7505802663123127,
        -0.719801881925932,
        1.0,
        -0.7126386548211208,
        -0.7529530612841115,
        -0.731811519040089,
        -0.7355141389656015,
        -0.7388086748868673,
        -0.7441643512253286,
        -0.7292831592109847,
        -0.7079205451177966,
        -0.7618799141706718,
        -0.7447781302448071,
        -0.7336051720683521,
        -0.7615732591019317,
        -0.7278169932270915,
        -0.7584721736110465,
        -0.7344773070988126,
        -0.7491044359987054,
        -0.7246941177555895,
        -0.7563991911849135,
        -0.7285303191085273,
        -0.7426315970784559,
        -0.759269640309925,
        -0.7544854186458414,
        -0.7493681769433946
      ],
      [
        -0.7109042364700542,
        -0.7272020679939624,
        -0.7335488785689444,
        -0.6899067368981617,
        -0.6788285415344669,
        -0.7126386548211208,
        1.0,
        -0.7328566803264205,
        -0.7077859984713344,
        -0.6454153432314368,
        -0.7186268722410931,
        -0.6970573287007777,
        -0.6484103416993643,
        -0.44606976518711283,
        -0.7475479892267276,
        -0.6921331986584406,
        -0.7251434400951728,
        -0.746588179487261,
        -0.06388514448230642,
        -0.7464845942773893,
        -0.6436610508282807,
        -0.6971060107182807,
        -0.653479867417326,
        -0.7188450732153366,
        -0.7209555910000274,
        -0.6971443012706731,
        -0.7436001853095046,
        -0.7487505889303903,
        -0.7311215365869586
      ],
      [
        -0.7030856347275873,
        -0.6657115349714753,
        -0.6497025531617613,
        -0.7504989795461905,
        -0.7542767186581143,
        -0.7529530612841115,
        -0.7328566803264205,
        1.0,
        -0.5865153321427592,
        -0.7567808517125366,
        -0.7593733416729511,
        -0.759103896642997,
        -0.75070097610422,
        -0.7267865222938001,
        -0.7316364068618126,
        -0.7449283498986162,
        -0.7601627328738312,
        -0.7574585949233604,
        -0.7530720817879406,
        -0.7613464354100166,
        -0.7516950393908407,
        -0.7010505947445815,
        -0.6579283874073283,
        -0.735310177281653,
        -0.7455161578600047,
        -0.74792772740367,
        -0.7576991242431517,
        -0.7145604756186175,
        -0.6156734551566446
      ],
      [
        -0.6348928165119208,
        -0.5967053338234548,
        -0.7300413336564329,
        -0.7414972344896102,
        -0.7248800706169161,
        -0.731811519040089,
        -0.7077859984713344,
        -0.5865153321427592,
        1.0,
        -0.7432047835081567,
        -0.7474277865179351,
        -0.741495467590559,
        -0.7069229524932867,
        -0.7041492302516448,
        -0.7485299362213409,
        -0.7155887006849879,
        -0.7506602816127514,
        -0.753270901748951,
        -0.7251460062723076,
        -0.7557580437241094,
        -0.7162594955245174,
        -0.7237318000280525,
        -0.22819852371456406,
        -0.7318787057487919,
        -0.6778528708006739,
        -0.7355607756409213,
        -0.7504320692237141,
        -0.741343125879307,
        -0.5060466471733763
      ],
      [
        -0.7562579836425598,
        -0.7359559892595346,
        -0.7490671073221922,
        -0.6207668480664041,
        -0.6061795963553982,
        -0.7355141389656015,
        -0.6454153432314368,
        -0.7567808517125366,
        -0.7432047835081567,
        1.0,
        -0.7281113148850409,
        -0.7251978784146211,
        -0.5819470907174566,
        -0.5857492985374195,
        -0.7579831279542661,
        -0.5564189491016793,
        -0.715395142154994,
        -0.7561943085926262,
        -0.5555865098144865,
        -0.7519470557558225,
        -0.573223270368294,
        -0.7426568166381708,
        -0.6712610576753886,
        -0.7457048264481232,
        -0.5041972111259222,
        -0.5904830921624027,
        -0.7490875644799433,
        -0.7511024514474176,
        -0.7496983428148232
      ],
      [
        -0.7610818307953527,
        -0.6554740154300549,
        -0.7554805557117932,
        -0.7274466895983329,
        -0.6728869998274234,
        -0.7388086748868673,
        -0.7186268722410931,
        -0.7593733416729511,
        -0.7474277865179351,
        -0.7281113148850409,
        1.0,
        -0.3451528131035716,
        -0.7391459461084138,
        -0.6910581203143396,
        -0.7567350125181598,
        -0.727874178321452,
        -0.6015736909852989,
        -0.7524258902347887,
        -0.7306333741343956,
        -0.7330756846323969,
        -0.7411267846089141,
        -0.7396922404903398,
        -0.735797784992613,
        -0.7432997916060322,
        -0.7211222230961414,
        -0.7323595154406772,
        -0.7585482665435459,
        -0.7537144264384974,
        -0.7543170164609597
      ],
      [
        -0.760970201949924,
        -0.741224893939804,
        -0.754216863262426,
        -0.7209059322141331,
        -0.6473758303050539,
        -0.7441643512253286,
        -0.6970573287007777,
        -0.759103896642997,
        -0.741495467590559,
        -0.7251978784146211,
        -0.3451528131035716,
        1.0,
        -0.7389239658227382,
        -0.709650408464956,
        -0.756223559844901,
        -0.7251414040611225,
        -0.5349524228015511,
        -0.761658388803645,
        -0.7037479710566306,
        -0.7306582469226521,
        -0.737664952464852,
        -0.7432610091564955,
        -0.7241395879493655,
        -0.7119958532941318,
        -0.7196173437089421,
        -0.7303184026753831,
        -0.7528360087655914,
        -0.757105220572805,
        -0.75427713810471
      ],
      [
        -0.7329640987553605,
        -0.7406521089704083,
        -0.7554932815472608,
        -0.6805461218712628,
        -0.6158138534157952,
        -0.7292831592109847,
        -0.6484103416993643,
        -0.75070097610422,
        -0.7069229524932867,
        -0.5819470907174566,
        -0.7391459461084138,
        -0.7389239658227382,
        1.0,
        -0.6270364135824698,
        -0.7561447520689586,
        -0.6228665728818037,
        -0.7392083266588748,
        -0.7557695895361136,
        -0.562575007306542,
        -0.7564425407956732,
        -0.5519206852067087,
        -0.7379723025191052,
        -0.72700808092662,
        -0.750146365120806,
        -0.6088891445442081,
        -0.6544446270510953,
        -0.7535460763037607,
        -0.7557223058998551,
        -0.7516637884398547
      ],
      [
        -0.7175303506491635,
        -0.7208645076558282,
        -0.745123340498127,
        -0.6663764547352238,
        -0.654031348092314,
        -0.7079205451177966,
        -0.44606976518711283,
        -0.7267865222938001,
        -0.7041492302516448,
        -0.5857492985374195,
        -0.6910581203143396,
        -0.709650408464956,
        -0.6270364135824698,
        1.0,
        -0.7490263446621781,
        -0.6622786831011893,
        -0.7198905147235681,
        -0.744765850643335,
        -0.3978188299549209,
        -0.7436800456760515,
        -0.644115351764444,
        -0.7082389019011592,
        -0.6713072739602325,
        -0.7290663295345663,
        -0.6603334073834171,
        -0.6523586239330098,
        -0.7490252704221597,
        -0.7411892193560754,
        -0.7455433449187834
      ],
      [
        -0.7586580449514095,
        -0.7474002798088313,
        -0.7391665644310716,
        -0.7573765256793263,
        -0.7614579780550405,
        -0.7618799141706718,
        -0.7475479892267276,
        -0.7316364068618126,
        -0.7485299362213409,
        -0.7579831279542661,
        -0.7567350125181598,
        -0.756223559844901,
        -0.7561447520689586,
        -0.7490263446621781,
        1.0,
        -0.7533788941800798,
        -0.7580282733282706,
        -0.7584161657223667,
        -0.7553817596609945,
        -0.7602268450107219,
        -0.7615522696999063,
        -0.7241244312841529,
        -0.7541524286762776,
        -0.7530521424095842,
        -0.7529470000235166,
        -0.7581556716166906,
        -0.7606722028962061,
        -0.7169756425931075,
        -0.7568524438705286
      ],
      [
        -0.6998806309104733,
        -0.7319106173178859,
        -0.7413015221783812,
        -0.4863239963458902,
        -0.5742155941229392,
        -0.7447781302448071,
        -0.6921331986584406,
        -0.7449283498986162,
        -0.7155887006849879,
        -0.5564189491016793,
        -0.727874178321452,
        -0.7251414040611225,
        -0.6228665728818037,
        -0.6622786831011893,
        -0.7533788941800798,
        1.0,
        -0.7061451778060508,
        -0.7592082489438552,
        -0.6711912673392935,
        -0.7327620176073628,
        -0.6195721142040804,
        -0.7382149949084446,
        -0.7232610971666054,
        -0.7244983653005542,
        -0.5673042675449408,
        -0.3364439986255192,
        -0.7537756457324298,
        -0.7526347385811577,
        -0.7329573140861361
      ],
      [
        -0.7546335853939673,
        -0.7175046624817656,
        -0.7564182248418464,
        -0.73693296292043,
        -0.6162641855930825,
        -0.7336051720683521,
        -0.7251434400951728,
        -0.7601627328738312,
        -0.7506602816127514,
        -0.715395142154994,
        -0.6015736909852989,
        -0.5349524228015511,
        -0.7392083266588748,
        -0.7198905147235681,
        -0.7580282733282706,
        -0.7061451778060508,
        1.0,
        -0.7623731498607097,
        -0.7319726318374089,
        -0.5331517083347559,
        -0.720792881023764,
        -0.7503570492208875,
        -0.7416600452685707,
        -0.7389094242747921,
        -0.7192454195410705,
        -0.6935726444811549,
        -0.756087461610621,
        -0.7566688787843805,
        -0.7503110433262927
      ],
      [
        -0.7631717291484907,
        -0.7547726109704287,
        -0.7639855270324301,
        -0.7557927176776892,
        -0.7578115518108816,
        -0.7615732591019317,
        -0.746588179487261,
        -0.7574585949233604,
        -0.753270901748951,
        -0.7561943085926262,
        -0.7524258902347887,
        -0.761658388803645,
        -0.7557695895361136,
        -0.744765850643335,
        -0.7584161657223667,
        -0.7592082489438552,
        -0.7623731498607097,
        1.0,
        -0.7491070906380906,
        -0.7629835698821559,
        -0.7581804246037451,
        -0.7545256680674519,
        -0.7558764552599999,
        -0.7642662731146977,
        -0.7576954216576255,
        -0.7610479755001861,
        -0.763591408800697,
        -0.7579857397001211,
        -0.7585603003342662
      ],
      [
        -0.7530607322405709,
        -0.7275254358348,
        -0.752772812708459,
        -0.677713379115783,
        -0.6522581519767898,
        -0.7278169932270915,
        -0.06388514448230642,
        -0.7530720817879406,
        -0.7251460062723076,
        -0.5555865098144865,
        -0.7306333741343956,
        -0.7037479710566306,
        -0.562575007306542,
        -0.3978188299549209,
        -0.7553817596609945,
        -0.6711912673392935,
        -0.7319726318374089,
        -0.7491070906380906,
        1.0,
        -0.7528580841347332,
        -0.6107116262621928,
        -0.7114739730841904,
        -0.6485089165593803,
        -0.7419969795337822,
        -0.6625155221892665,
        -0.6770277202368241,
        -0.7516150340747685,
        -0.7564268391153808,
        -0.7492414831000375
      ],
      [
        -0.73966728093454,
        -0.747491848109419,
        -0.7569728439940907,
        -0.7571221158415287,
        -0.7320773699729327,
        -0.7584721736110465,
        -0.7464845942773893,
        -0.7613464354100166,
        -0.7557580437241094,
        -0.7519470557558225,
        -0.7330756846323969,
        -0.7306582469226521,
        -0.7564425407956732,
        -0.7436800456760515,
        -0.7602268450107219,
        -0.7327620176073628,
        -0.5331517083347559,
        -0.7629835698821559,
        -0.7528580841347332,
        1.0,
        -0.7520983657379009,
        -0.7556300383282853,
        -0.7518226393621819,
        -0.7272358106986205,
        -0.7433861610828232,
        -0.732848612418322,
        -0.7633674013898648,
        -0.7616813924763671,
        -0.7494073094145124
      ],
      [
        -0.7408446634435106,
        -0.7401132173773771,
        -0.751684166671057,
        -0.6748789950961094,
        -0.49219729704846715,
        -0.7344773070988126,
        -0.6436610508282807,
        -0.7516950393908407,
        -0.7162594955245174,
        -0.573223270368294,
        -0.7411267846089141,
        -0.737664952464852,
        -0.5519206852067087,
        -0.644115351764444,
        -0.7615522696999063,
        -0.6195721142040804,
        -0.720792881023764,
        -0.7581804246037451,
        -0.6107116262621928,
        -0.7520983657379009,
        1.0,
        -0.7393699870018924,
        -0.7205091149004357,
        -0.7425347068351995,
        -0.6042184487344757,
        -0.6427130015811306,
        -0.7537101062936378,
        -0.7560674707419913,
        -0.7476747322995219
      ],
      [
        -0.7283879169414711,
        -0.7305888506212996,
        -0.7271763769706131,
        -0.7342075828958483,
        -0.7300257976647264,
        -0.7491044359987054,
        -0.6971060107182807,
        -0.7010505947445815,
        -0.7237318000280525,
        -0.7426568166381708,
        -0.7396922404903398,
        -0.7432610091564955,
        -0.7379723025191052,
        -0.7082389019011592,
        -0.7241244312841529,
        -0.7382149949084446,
        -0.7503570492208875,
        -0.7545256680674519,
        -0.7114739730841904,
        -0.7556300383282853,
        -0.7393699870018924,
        1.0,
        -0.7303414144438494,
        -0.7353899647791013,
        -0.7469619737013311,
        -0.7325670282406703,
        -0.7533394456035822,
        -0.7098795794589142,
        -0.7385804546167387
      ],
      [
        -0.6815353839059602,
        -0.6525148821550054,
        -0.7212180114603823,
        -0.7280537577341174,
        -0.7350093228129082,
        -0.7246941177555895,
        -0.653479867417326,
        -0.6579283874073283,
        -0.22819852371456406,
        -0.6712610576753886,
        -0.735797784992613,
        -0.7241395879493655,
        -0.72700808092662,
        -0.6713072739602325,
        -0.7541524286762776,
        -0.7232610971666054,
        -0.7416600452685707,
        -0.7558764552599999,
        -0.6485089165593803,
        -0.7518226393621819,
        -0.7205091149004357,
        -0.7303414144438494,
        1.0,
        -0.72129802487704,
        -0.617365202987371,
        -0.7347324181806125,
        -0.7405117564376439,
        -0.7494546782437082,
        -0.6011677782248193
      ],
      [
        -0.6517021630473693,
        -0.7525504935275448,
        -0.6803216801255006,
        -0.6961599403913987,
        -0.7462641910895165,
        -0.7563991911849135,
        -0.7188450732153366,
        -0.735310177281653,
        -0.7318787057487919,
        -0.7457048264481232,
        -0.7432997916060322,
        -0.7119958532941318,
        -0.750146365120806,
        -0.7290663295345663,
        -0.7530521424095842,
        -0.7244983653005542,
        -0.7389094242747921,
        -0.7642662731146977,
        -0.7419969795337822,
        -0.7272358106986205,
        -0.7425347068351995,
        -0.7353899647791013,
        -0.72129802487704,
        1.0,
        -0.7453655046613809,
        -0.7399156494809609,
        -0.7606501852649357,
        -0.7481804500262921,
        -0.7402212777593415
      ],
      [
        -0.7411744536484932,
        -0.7284757079308088,
        -0.753791058079492,
        -0.6391776636910701,
        -0.6148809386555322,
        -0.7285303191085273,
        -0.7209555910000274,
        -0.7455161578600047,
        -0.6778528708006739,
        -0.5041972111259222,
        -0.7211222230961414,
        -0.7196173437089421,
        -0.6088891445442081,
        -0.6603334073834171,
        -0.7529470000235166,
        -0.5673042675449408,
        -0.7192454195410705,
        -0.7576954216576255,
        -0.6625155221892665,
        -0.7433861610828232,
        -0.6042184487344757,
        -0.7469619737013311,
        -0.617365202987371,
        -0.7453655046613809,
        1.0,
        -0.6162073356074775,
        -0.7480903989699758,
        -0.7498532890926728,
        -0.7289658831338388
      ],
      [
        -0.7317178711416117,
        -0.7445424860450713,
        -0.7506893037510778,
        -0.5841458305401943,
        -0.5647323044843798,
        -0.7426315970784559,
        -0.6971443012706731,
        -0.74792772740367,
        -0.7355607756409213,
        -0.5904830921624027,
        -0.7323595154406772,
        -0.7303184026753831,
        -0.6544446270510953,
        -0.6523586239330098,
        -0.7581556716166906,
        -0.3364439986255192,
        -0.6935726444811549,
        -0.7610479755001861,
        -0.6770277202368241,
        -0.732848612418322,
        -0.6427130015811306,
        -0.7325670282406703,
        -0.7347324181806125,
        -0.7399156494809609,
        -0.6162073356074775,
        1.0,
        -0.7567741131028521,
        -0.7480549253882905,
        -0.7415795578519793
      ],
      [
        -0.7545195971237683,
        -0.7597091074869058,
        -0.7594623089845594,
        -0.7554601671251184,
        -0.7551219131480421,
        -0.759269640309925,
        -0.7436001853095046,
        -0.7576991242431517,
        -0.7504320692237141,
        -0.7490875644799433,
        -0.7585482665435459,
        -0.7528360087655914,
        -0.7535460763037607,
        -0.7490252704221597,
        -0.7606722028962061,
        -0.7537756457324298,
        -0.756087461610621,
        -0.763591408800697,
        -0.7516150340747685,
        -0.7633674013898648,
        -0.7537101062936378,
        -0.7533394456035822,
        -0.7405117564376439,
        -0.7606501852649357,
        -0.7480903989699758,
        -0.7567741131028521,
        1.0,
        -0.7583596214868018,
        -0.7563855361957386
      ],
      [
        -0.7453804424449475,
        -0.6099412440860489,
        -0.7413607285516192,
        -0.7564457678400135,
        -0.758464441163861,
        -0.7544854186458414,
        -0.7487505889303903,
        -0.7145604756186175,
        -0.741343125879307,
        -0.7511024514474176,
        -0.7537144264384974,
        -0.757105220572805,
        -0.7557223058998551,
        -0.7411892193560754,
        -0.7169756425931075,
        -0.7526347385811577,
        -0.7566688787843805,
        -0.7579857397001211,
        -0.7564268391153808,
        -0.7616813924763671,
        -0.7560674707419913,
        -0.7098795794589142,
        -0.7494546782437082,
        -0.7481804500262921,
        -0.7498532890926728,
        -0.7480549253882905,
        -0.7583596214868018,
        1.0,
        -0.749079487625187
      ],
      [
        -0.6672332178725218,
        -0.6084312531758604,
        -0.7312385686080507,
        -0.7392304514512607,
        -0.7421802264331495,
        -0.7493681769433946,
        -0.7311215365869586,
        -0.6156734551566446,
        -0.5060466471733763,
        -0.7496983428148232,
        -0.7543170164609597,
        -0.75427713810471,
        -0.7516637884398547,
        -0.7455433449187834,
        -0.7568524438705286,
        -0.7329573140861361,
        -0.7503110433262927,
        -0.7585603003342662,
        -0.7492414831000375,
        -0.7494073094145124,
        -0.7476747322995219,
        -0.7385804546167387,
        -0.6011677782248193,
        -0.7402212777593415,
        -0.7289658831338388,
        -0.7415795578519793,
        -0.7563855361957386,
        -0.749079487625187,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        20,
        3,
        2,
        9,
        2,
        3,
        16,
        5,
        3,
        2,
        10,
        2,
        3,
        2,
        3,
        0,
        1,
        3,
        7,
        1,
        1,
        4,
        4,
        2,
        0,
        1,
        0,
        0,
        0
      ],
      "2020-02": [
        14,
        3,
        2,
        13,
        5,
        10,
        17,
        11,
        5,
        1,
        14,
        6,
        4,
        2,
        0,
        0,
        5,
        0,
        8,
        3,
        2,
        4,
        7,
        13,
        2,
        2,
        6,
        3,
        2
      ],
      "2020-03": [
        19,
        4,
        3,
        8,
        3,
        6,
        7,
        4,
        0,
        1,
        7,
        2,
        2,
        0,
        1,
        0,
        3,
        0,
        10,
        4,
        4,
        1,
        6,
        7,
        1,
        4,
        1,
        2,
        0
      ],
      "2020-04": [
        26,
        4,
        1,
        4,
        3,
        2,
        10,
        1,
        2,
        3,
        15,
        6,
        3,
        1,
        6,
        1,
        5,
        1,
        8,
        4,
        7,
        0,
        7,
        9,
        1,
        3,
        2,
        1,
        0
      ],
      "2020-05": [
        34,
        2,
        3,
        15,
        13,
        6,
        39,
        5,
        7,
        4,
        13,
        7,
        4,
        9,
        5,
        1,
        12,
        9,
        26,
        6,
        9,
        4,
        10,
        10,
        3,
        9,
        5,
        5,
        0
      ],
      "2020-06": [
        30,
        3,
        5,
        13,
        7,
        3,
        15,
        6,
        2,
        5,
        6,
        7,
        0,
        6,
        4,
        0,
        3,
        1,
        13,
        6,
        9,
        4,
        4,
        7,
        2,
        2,
        1,
        5,
        1
      ],
      "2020-07": [
        27,
        4,
        6,
        7,
        3,
        4,
        10,
        14,
        9,
        3,
        20,
        2,
        4,
        5,
        1,
        0,
        2,
        2,
        12,
        4,
        4,
        5,
        9,
        8,
        0,
        8,
        0,
        3,
        0
      ],
      "2020-08": [
        42,
        3,
        3,
        13,
        14,
        10,
        18,
        1,
        6,
        3,
        32,
        11,
        10,
        3,
        3,
        1,
        9,
        2,
        12,
        8,
        3,
        1,
        8,
        16,
        4,
        8,
        2,
        2,
        2
      ],
      "2020-09": [
        19,
        0,
        1,
        3,
        5,
        6,
        9,
        4,
        2,
        3,
        8,
        2,
        2,
        1,
        2,
        0,
        7,
        4,
        5,
        3,
        3,
        1,
        2,
        7,
        0,
        3,
        0,
        2,
        0
      ],
      "2020-10": [
        67,
        6,
        7,
        17,
        14,
        9,
        28,
        7,
        8,
        4,
        28,
        13,
        6,
        8,
        6,
        0,
        21,
        3,
        18,
        3,
        6,
        6,
        6,
        18,
        3,
        9,
        3,
        1,
        3
      ],
      "2020-11": [
        28,
        2,
        5,
        20,
        11,
        4,
        28,
        8,
        2,
        3,
        16,
        10,
        7,
        5,
        7,
        0,
        8,
        3,
        9,
        5,
        5,
        1,
        7,
        17,
        1,
        6,
        1,
        1,
        0
      ],
      "2020-12": [
        25,
        1,
        6,
        5,
        7,
        1,
        11,
        1,
        2,
        3,
        14,
        4,
        1,
        2,
        9,
        2,
        2,
        3,
        7,
        3,
        3,
        1,
        5,
        6,
        0,
        5,
        2,
        4,
        1
      ],
      "2021-01": [
        18,
        2,
        2,
        5,
        3,
        1,
        9,
        4,
        3,
        0,
        3,
        5,
        0,
        3,
        5,
        0,
        3,
        3,
        5,
        2,
        1,
        2,
        3,
        9,
        0,
        2,
        0,
        1,
        0
      ],
      "2021-02": [
        27,
        4,
        3,
        11,
        7,
        5,
        19,
        11,
        2,
        4,
        7,
        3,
        6,
        5,
        5,
        0,
        4,
        3,
        15,
        9,
        1,
        7,
        8,
        12,
        2,
        7,
        6,
        6,
        3
      ],
      "2021-03": [
        25,
        6,
        9,
        3,
        5,
        4,
        18,
        6,
        0,
        0,
        6,
        4,
        3,
        7,
        6,
        1,
        4,
        1,
        17,
        2,
        0,
        5,
        5,
        11,
        0,
        6,
        4,
        1,
        2
      ],
      "2021-04": [
        32,
        6,
        7,
        13,
        13,
        5,
        35,
        5,
        4,
        6,
        14,
        14,
        8,
        9,
        5,
        3,
        11,
        6,
        21,
        2,
        7,
        5,
        11,
        5,
        3,
        12,
        0,
        0,
        1
      ],
      "2021-05": [
        29,
        0,
        3,
        12,
        4,
        5,
        17,
        8,
        3,
        1,
        4,
        7,
        6,
        4,
        3,
        1,
        6,
        0,
        8,
        4,
        5,
        5,
        6,
        7,
        1,
        7,
        3,
        2,
        1
      ],
      "2021-06": [
        29,
        2,
        7,
        12,
        24,
        9,
        31,
        13,
        7,
        5,
        9,
        11,
        10,
        8,
        9,
        6,
        7,
        5,
        17,
        4,
        7,
        3,
        7,
        15,
        1,
        8,
        2,
        4,
        0
      ],
      "2021-07": [
        38,
        7,
        4,
        12,
        6,
        5,
        17,
        15,
        5,
        1,
        10,
        8,
        3,
        8,
        9,
        0,
        9,
        2,
        17,
        3,
        8,
        10,
        4,
        10,
        2,
        7,
        2,
        5,
        0
      ],
      "2021-08": [
        32,
        1,
        0,
        6,
        9,
        2,
        22,
        0,
        5,
        6,
        8,
        2,
        3,
        2,
        3,
        1,
        1,
        4,
        12,
        3,
        2,
        3,
        6,
        7,
        1,
        5,
        0,
        3,
        0
      ],
      "2021-09": [
        24,
        9,
        8,
        7,
        5,
        6,
        11,
        2,
        3,
        1,
        17,
        8,
        4,
        7,
        5,
        2,
        3,
        4,
        9,
        1,
        8,
        3,
        7,
        8,
        1,
        6,
        1,
        1,
        1
      ],
      "2021-10": [
        49,
        6,
        7,
        17,
        19,
        10,
        35,
        16,
        8,
        4,
        21,
        7,
        7,
        27,
        7,
        1,
        15,
        7,
        22,
        16,
        8,
        11,
        6,
        21,
        4,
        9,
        4,
        1,
        3
      ],
      "2021-11": [
        28,
        2,
        4,
        15,
        7,
        8,
        23,
        7,
        3,
        3,
        9,
        4,
        5,
        7,
        6,
        1,
        8,
        3,
        9,
        4,
        2,
        0,
        5,
        14,
        1,
        8,
        1,
        5,
        0
      ],
      "2021-12": [
        21,
        4,
        3,
        9,
        0,
        10,
        11,
        1,
        7,
        3,
        10,
        1,
        0,
        5,
        3,
        2,
        3,
        2,
        9,
        1,
        5,
        5,
        5,
        12,
        1,
        3,
        2,
        3,
        2
      ],
      "2022-01": [
        19,
        4,
        2,
        7,
        7,
        9,
        17,
        2,
        5,
        3,
        6,
        1,
        1,
        4,
        11,
        0,
        6,
        5,
        10,
        3,
        3,
        5,
        7,
        6,
        0,
        2,
        1,
        4,
        0
      ],
      "2022-02": [
        41,
        9,
        6,
        17,
        12,
        4,
        16,
        10,
        1,
        4,
        15,
        10,
        1,
        7,
        3,
        1,
        7,
        1,
        21,
        4,
        6,
        5,
        16,
        8,
        0,
        8,
        1,
        7,
        1
      ],
      "2022-03": [
        42,
        8,
        9,
        19,
        15,
        19,
        21,
        10,
        8,
        6,
        32,
        16,
        5,
        23,
        7,
        5,
        15,
        4,
        33,
        11,
        11,
        4,
        10,
        16,
        6,
        9,
        2,
        7,
        1
      ],
      "2022-04": [
        39,
        6,
        5,
        17,
        16,
        9,
        34,
        5,
        10,
        2,
        22,
        12,
        10,
        25,
        6,
        1,
        9,
        5,
        13,
        8,
        12,
        2,
        11,
        11,
        4,
        13,
        2,
        4,
        2
      ],
      "2022-05": [
        23,
        4,
        2,
        10,
        7,
        3,
        12,
        2,
        6,
        5,
        5,
        4,
        5,
        9,
        3,
        1,
        5,
        5,
        17,
        4,
        13,
        8,
        8,
        8,
        1,
        5,
        1,
        3,
        0
      ],
      "2022-06": [
        34,
        4,
        4,
        18,
        15,
        9,
        23,
        9,
        8,
        2,
        11,
        10,
        5,
        16,
        6,
        3,
        8,
        8,
        18,
        4,
        5,
        7,
        16,
        10,
        3,
        7,
        2,
        7,
        0
      ],
      "2022-07": [
        24,
        4,
        7,
        7,
        15,
        14,
        17,
        7,
        5,
        0,
        12,
        4,
        4,
        6,
        2,
        3,
        7,
        5,
        21,
        4,
        3,
        1,
        13,
        12,
        1,
        7,
        4,
        1,
        3
      ],
      "2022-08": [
        34,
        5,
        4,
        4,
        5,
        6,
        9,
        2,
        7,
        2,
        8,
        4,
        3,
        7,
        7,
        0,
        3,
        3,
        8,
        4,
        0,
        1,
        5,
        5,
        3,
        0,
        2,
        7,
        3
      ],
      "2022-09": [
        44,
        3,
        5,
        4,
        7,
        4,
        15,
        4,
        5,
        4,
        17,
        3,
        1,
        12,
        5,
        0,
        6,
        4,
        12,
        2,
        5,
        2,
        5,
        8,
        3,
        4,
        1,
        1,
        0
      ],
      "2022-10": [
        39,
        7,
        7,
        13,
        18,
        13,
        45,
        9,
        11,
        9,
        20,
        15,
        9,
        33,
        7,
        3,
        12,
        8,
        34,
        9,
        19,
        6,
        17,
        11,
        1,
        7,
        3,
        5,
        1
      ],
      "2022-11": [
        54,
        6,
        1,
        25,
        23,
        17,
        34,
        4,
        9,
        3,
        14,
        10,
        8,
        30,
        6,
        4,
        9,
        8,
        22,
        8,
        11,
        5,
        11,
        13,
        1,
        11,
        0,
        2,
        3
      ],
      "2022-12": [
        29,
        5,
        3,
        9,
        10,
        4,
        11,
        1,
        2,
        0,
        3,
        4,
        7,
        14,
        3,
        1,
        5,
        1,
        8,
        3,
        10,
        3,
        9,
        7,
        2,
        5,
        1,
        1,
        1
      ],
      "2023-01": [
        35,
        2,
        5,
        5,
        10,
        3,
        3,
        2,
        4,
        1,
        4,
        2,
        1,
        3,
        3,
        3,
        2,
        1,
        6,
        2,
        2,
        2,
        6,
        3,
        1,
        2,
        1,
        2,
        1
      ],
      "2023-02": [
        19,
        0,
        2,
        21,
        8,
        9,
        18,
        6,
        3,
        3,
        12,
        3,
        0,
        13,
        0,
        2,
        8,
        1,
        13,
        0,
        5,
        5,
        12,
        10,
        1,
        0,
        2,
        1,
        0
      ],
      "2023-03": [
        43,
        2,
        5,
        12,
        17,
        7,
        23,
        11,
        11,
        4,
        12,
        12,
        5,
        15,
        4,
        5,
        3,
        4,
        14,
        6,
        7,
        1,
        18,
        11,
        4,
        7,
        4,
        3,
        0
      ],
      "2023-04": [
        29,
        2,
        9,
        4,
        7,
        7,
        10,
        3,
        11,
        6,
        3,
        1,
        5,
        5,
        6,
        0,
        3,
        5,
        8,
        4,
        6,
        2,
        3,
        7,
        1,
        2,
        1,
        4,
        0
      ],
      "2023-05": [
        49,
        7,
        6,
        12,
        27,
        15,
        28,
        8,
        16,
        11,
        18,
        8,
        22,
        36,
        6,
        4,
        11,
        10,
        38,
        7,
        21,
        5,
        20,
        15,
        2,
        10,
        5,
        7,
        1
      ],
      "2023-06": [
        56,
        4,
        12,
        16,
        17,
        15,
        36,
        11,
        7,
        5,
        9,
        10,
        10,
        27,
        3,
        3,
        13,
        1,
        39,
        7,
        11,
        6,
        20,
        14,
        5,
        7,
        5,
        1,
        3
      ],
      "2023-07": [
        52,
        4,
        9,
        9,
        11,
        5,
        17,
        2,
        6,
        5,
        10,
        2,
        8,
        12,
        4,
        2,
        7,
        2,
        22,
        0,
        7,
        6,
        16,
        5,
        2,
        5,
        6,
        2,
        3
      ],
      "2023-08": [
        36,
        7,
        8,
        9,
        13,
        10,
        14,
        5,
        7,
        4,
        12,
        6,
        4,
        9,
        3,
        1,
        8,
        5,
        13,
        6,
        5,
        8,
        12,
        5,
        1,
        9,
        3,
        3,
        3
      ],
      "2023-09": [
        63,
        14,
        10,
        27,
        25,
        15,
        39,
        16,
        27,
        8,
        16,
        16,
        12,
        27,
        8,
        5,
        19,
        6,
        33,
        11,
        18,
        6,
        16,
        8,
        4,
        13,
        1,
        4,
        4
      ],
      "2023-10": [
        48,
        6,
        9,
        21,
        13,
        11,
        24,
        6,
        6,
        2,
        10,
        16,
        10,
        21,
        4,
        2,
        5,
        1,
        20,
        8,
        12,
        2,
        16,
        8,
        3,
        9,
        4,
        5,
        2
      ],
      "2023-11": [
        35,
        2,
        13,
        6,
        10,
        5,
        16,
        4,
        8,
        3,
        4,
        6,
        7,
        9,
        6,
        2,
        7,
        2,
        6,
        3,
        2,
        1,
        8,
        4,
        3,
        6,
        4,
        5,
        3
      ],
      "2023-12": [
        38,
        5,
        8,
        13,
        7,
        16,
        17,
        6,
        6,
        1,
        10,
        13,
        4,
        15,
        3,
        1,
        6,
        4,
        12,
        4,
        7,
        6,
        14,
        8,
        3,
        3,
        4,
        5,
        2
      ],
      "2024-01": [
        41,
        4,
        12,
        12,
        11,
        11,
        23,
        8,
        13,
        5,
        5,
        8,
        6,
        13,
        2,
        5,
        9,
        4,
        20,
        10,
        5,
        3,
        16,
        10,
        2,
        8,
        5,
        3,
        1
      ],
      "2024-02": [
        46,
        3,
        13,
        9,
        15,
        10,
        11,
        2,
        13,
        7,
        6,
        7,
        6,
        17,
        5,
        3,
        2,
        0,
        11,
        6,
        7,
        6,
        8,
        8,
        0,
        3,
        3,
        3,
        0
      ],
      "2024-03": [
        34,
        3,
        15,
        12,
        12,
        17,
        12,
        10,
        5,
        1,
        5,
        4,
        6,
        9,
        1,
        4,
        3,
        4,
        16,
        5,
        8,
        5,
        17,
        4,
        0,
        3,
        2,
        5,
        2
      ],
      "2024-04": [
        41,
        6,
        3,
        4,
        7,
        8,
        10,
        4,
        5,
        1,
        7,
        2,
        1,
        9,
        3,
        0,
        7,
        2,
        13,
        6,
        5,
        4,
        11,
        4,
        0,
        5,
        1,
        0,
        3
      ],
      "2024-05": [
        44,
        5,
        4,
        8,
        7,
        10,
        10,
        1,
        9,
        1,
        7,
        4,
        2,
        10,
        7,
        3,
        6,
        3,
        18,
        7,
        8,
        4,
        8,
        4,
        0,
        2,
        4,
        3,
        0
      ],
      "2024-06": [
        73,
        14,
        8,
        25,
        51,
        18,
        33,
        15,
        18,
        8,
        14,
        16,
        23,
        34,
        6,
        4,
        12,
        11,
        47,
        15,
        23,
        11,
        22,
        13,
        2,
        8,
        5,
        5,
        3
      ],
      "2024-07": [
        83,
        8,
        6,
        9,
        19,
        13,
        18,
        6,
        12,
        6,
        20,
        9,
        8,
        13,
        7,
        2,
        7,
        6,
        21,
        5,
        11,
        14,
        18,
        10,
        1,
        1,
        5,
        1,
        3
      ],
      "2024-08": [
        53,
        11,
        13,
        12,
        19,
        8,
        7,
        6,
        11,
        5,
        11,
        7,
        8,
        18,
        3,
        3,
        13,
        6,
        19,
        4,
        1,
        4,
        17,
        12,
        5,
        6,
        2,
        5,
        3
      ],
      "2024-09": [
        79,
        15,
        14,
        23,
        40,
        21,
        24,
        12,
        26,
        6,
        22,
        23,
        14,
        28,
        4,
        9,
        21,
        6,
        40,
        17,
        6,
        5,
        20,
        14,
        5,
        17,
        6,
        15,
        2
      ],
      "2024-10": [
        86,
        9,
        5,
        8,
        28,
        16,
        17,
        7,
        17,
        2,
        13,
        7,
        7,
        9,
        5,
        3,
        10,
        2,
        26,
        7,
        15,
        5,
        14,
        7,
        1,
        9,
        7,
        11,
        3
      ],
      "2024-11": [
        49,
        8,
        9,
        5,
        8,
        7,
        10,
        5,
        12,
        2,
        5,
        6,
        8,
        6,
        4,
        0,
        4,
        3,
        14,
        3,
        4,
        4,
        8,
        8,
        6,
        7,
        6,
        4,
        2
      ],
      "2024-12": [
        54,
        7,
        7,
        12,
        20,
        10,
        8,
        1,
        20,
        6,
        12,
        5,
        8,
        11,
        5,
        6,
        2,
        4,
        21,
        4,
        10,
        8,
        20,
        1,
        3,
        3,
        3,
        1,
        4
      ],
      "2025-01": [
        55,
        7,
        6,
        9,
        12,
        13,
        13,
        4,
        10,
        8,
        6,
        6,
        13,
        11,
        4,
        4,
        13,
        2,
        21,
        9,
        5,
        3,
        17,
        9,
        1,
        6,
        10,
        5,
        1
      ],
      "2025-02": [
        36,
        8,
        4,
        13,
        12,
        7,
        9,
        6,
        15,
        2,
        3,
        2,
        12,
        9,
        2,
        1,
        7,
        1,
        15,
        1,
        8,
        6,
        9,
        5,
        3,
        6,
        2,
        2,
        2
      ],
      "2025-03": [
        49,
        2,
        6,
        12,
        7,
        7,
        7,
        6,
        12,
        1,
        4,
        3,
        9,
        13,
        1,
        1,
        3,
        0,
        10,
        2,
        7,
        3,
        16,
        8,
        4,
        3,
        4,
        5,
        3
      ],
      "2025-04": [
        31,
        3,
        4,
        3,
        8,
        4,
        7,
        4,
        7,
        2,
        4,
        3,
        5,
        7,
        2,
        2,
        6,
        1,
        7,
        3,
        7,
        5,
        10,
        6,
        0,
        4,
        2,
        0,
        2
      ],
      "2025-05": [
        91,
        18,
        9,
        21,
        31,
        18,
        23,
        6,
        27,
        10,
        13,
        10,
        20,
        21,
        6,
        6,
        20,
        8,
        39,
        8,
        13,
        10,
        23,
        15,
        7,
        16,
        14,
        6,
        4
      ],
      "2025-06": [
        104,
        7,
        10,
        18,
        29,
        16,
        25,
        5,
        16,
        6,
        11,
        12,
        20,
        34,
        3,
        2,
        15,
        9,
        33,
        6,
        16,
        9,
        17,
        10,
        3,
        22,
        3,
        3,
        0
      ],
      "2025-07": [
        91,
        7,
        10,
        22,
        22,
        7,
        15,
        14,
        16,
        4,
        6,
        10,
        18,
        20,
        4,
        3,
        10,
        4,
        25,
        4,
        7,
        4,
        10,
        13,
        2,
        12,
        2,
        7,
        2
      ],
      "2025-08": [
        68,
        7,
        9,
        17,
        20,
        20,
        15,
        4,
        14,
        2,
        11,
        4,
        10,
        19,
        4,
        4,
        7,
        7,
        23,
        3,
        11,
        8,
        16,
        12,
        7,
        8,
        5,
        3,
        1
      ],
      "2025-09": [
        29,
        8,
        7,
        4,
        8,
        7,
        11,
        4,
        6,
        0,
        6,
        2,
        3,
        5,
        4,
        1,
        1,
        0,
        10,
        2,
        2,
        4,
        2,
        3,
        1,
        2,
        0,
        2,
        2
      ]
    },
    "papers": {
      "0": [
        {
          "title": "MMT-BERT: Chord-aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT",
          "year": "2024-09",
          "abstract": "We propose a novel symbolic music representation and Generative Adversarial\nNetwork (GAN) framework specially designed for symbolic multitrack music\ngeneration. The main theme of symbolic music generation primarily encompasses\nthe preprocessing of music data and the implementation of a deep learning\nframework. Current techniques dedicated to symbolic music generation generally\nencounter two significant challenges: training data's lack of information about\nchords and scales and the requirement of specially designed model architecture\nadapted to the unique format of symbolic music representation. In this paper,\nwe solve the above problems by introducing new symbolic music representation\nwith MusicLang chord analysis model. We propose our MMT-BERT architecture\nadapting to the representation. To build a robust multitrack music generator,\nwe fine-tune a pre-trained MusicBERT model to serve as the discriminator, and\nincorporate relativistic standard loss. This approach, supported by the\nin-depth understanding of symbolic music encoded within MusicBERT, fortifies\nthe consonance and humanity of music generated by our method. Experimental\nresults demonstrate the effectiveness of our approach which strictly follows\nthe state-of-the-art methods.",
          "arxiv_id": "2409.00919v1"
        },
        {
          "title": "What is missing in deep music generation? A study of repetition and structure in popular music",
          "year": "2022-09",
          "abstract": "Structure is one of the most essential aspects of music, and music structure\nis commonly indicated through repetition. However, the nature of repetition and\nstructure in music is still not well understood, especially in the context of\nmusic generation, and much remains to be explored with Music Information\nRetrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and\nAmerican) illustrate important music construction principles: (1) structure\nexists at multiple hierarchical levels, (2) songs use repetition and limited\nvocabulary so that individual songs do not follow general statistics of song\ncollections, (3) structure interacts with rhythm, melody, harmony, and\npredictability, and (4) over the course of a song, repetition is not random,\nbut follows a general trend as revealed by cross-entropy. These and other\nfindings offer challenges as well as opportunities for deep-learning music\ngeneration and suggest new formal music criteria and evaluation methods. Music\nfrom recent music generation systems is analyzed and compared to human-composed\nmusic in our datasets, often revealing striking differences from a structural\nperspective.",
          "arxiv_id": "2209.00182v1"
        },
        {
          "title": "A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions",
          "year": "2020-11",
          "abstract": "The utilization of deep learning techniques in generating various contents\n(such as image, text, etc.) has become a trend. Especially music, the topic of\nthis paper, has attracted widespread attention of countless researchers.The\nwhole process of producing music can be divided into three stages,\ncorresponding to the three levels of music generation: score generation\nproduces scores, performance generation adds performance characteristics to the\nscores, and audio generation converts scores with performance characteristics\ninto audio by assigning timbre or generates music in audio format directly.\nPrevious surveys have explored the network models employed in the field of\nautomatic music generation. However, the development history, the model\nevolution, as well as the pros and cons of same music generation task have not\nbeen clearly illustrated. This paper attempts to provide an overview of various\ncomposition tasks under different music generation levels, covering most of the\ncurrently popular music generation tasks using deep learning. In addition, we\nsummarize the datasets suitable for diverse tasks, discuss the music\nrepresentations, the evaluation methods as well as the challenges under\ndifferent levels, and finally point out several future directions.",
          "arxiv_id": "2011.06801v1"
        }
      ],
      "1": [
        {
          "title": "Multi-task Learning Based Spoofing-Robust Automatic Speaker Verification System",
          "year": "2020-12",
          "abstract": "Spoofing attacks posed by generating artificial speech can severely degrade\nthe performance of a speaker verification system. Recently, many anti-spoofing\ncountermeasures have been proposed for detecting varying types of attacks from\nsynthetic speech to replay presentations. While there are numerous effective\ndefenses reported on standalone anti-spoofing solutions, the integration for\nspeaker verification and spoofing detection systems has obvious benefits. In\nthis paper, we propose a spoofing-robust automatic speaker verification\n(SR-ASV) system for diverse attacks based on a multi-task learning\narchitecture. This deep learning based model is jointly trained with\ntime-frequency representations from utterances to provide recognition decisions\nfor both tasks simultaneously. Compared with other state-of-the-art systems on\nthe ASVspoof 2017 and 2019 corpora, a substantial improvement of the combined\nsystem under different spoofing conditions can be obtained.",
          "arxiv_id": "2012.03154v1"
        },
        {
          "title": "Voice Spoofing Countermeasures: Taxonomy, State-of-the-art, experimental analysis of generalizability, open challenges, and the way forward",
          "year": "2022-10",
          "abstract": "Malicious actors may seek to use different voice-spoofing attacks to fool ASV\nsystems and even use them for spreading misinformation. Various countermeasures\nhave been proposed to detect these spoofing attacks. Due to the extensive work\ndone on spoofing detection in automated speaker verification (ASV) systems in\nthe last 6-7 years, there is a need to classify the research and perform\nqualitative and quantitative comparisons on state-of-the-art countermeasures.\nAdditionally, no existing survey paper has reviewed integrated solutions to\nvoice spoofing evaluation and speaker verification, adversarial/antiforensics\nattacks on spoofing countermeasures, and ASV itself, or unified solutions to\ndetect multiple attacks using a single model. Further, no work has been done to\nprovide an apples-to-apples comparison of published countermeasures in order to\nassess their generalizability by evaluating them across corpora. In this work,\nwe conduct a review of the literature on spoofing detection using hand-crafted\nfeatures, deep learning, end-to-end, and universal spoofing countermeasure\nsolutions to detect speech synthesis (SS), voice conversion (VC), and replay\nattacks. Additionally, we also review integrated solutions to voice spoofing\nevaluation and speaker verification, adversarial and anti-forensics attacks on\nvoice countermeasures, and ASV. The limitations and challenges of the existing\nspoofing countermeasures are also presented. We report the performance of these\ncountermeasures on several datasets and evaluate them across corpora. For the\nexperiments, we employ the ASVspoof2019 and VSDC datasets along with GMM, SVM,\nCNN, and CNN-GRU classifiers. (For reproduceability of the results, the code of\nthe test bed can be found in our GitHub Repository.",
          "arxiv_id": "2210.00417v2"
        },
        {
          "title": "One-class Learning Towards Synthetic Voice Spoofing Detection",
          "year": "2020-10",
          "abstract": "Human voices can be used to authenticate the identity of the speaker, but the\nautomatic speaker verification (ASV) systems are vulnerable to voice spoofing\nattacks, such as impersonation, replay, text-to-speech, and voice conversion.\nRecently, researchers developed anti-spoofing techniques to improve the\nreliability of ASV systems against spoofing attacks. However, most methods\nencounter difficulties in detecting unknown attacks in practical use, which\noften have different statistical distributions from known attacks. Especially,\nthe fast development of synthetic voice spoofing algorithms is generating\nincreasingly powerful attacks, putting the ASV systems at risk of unseen\nattacks. In this work, we propose an anti-spoofing system to detect unknown\nsynthetic voice spoofing attacks (i.e., text-to-speech or voice conversion)\nusing one-class learning. The key idea is to compact the bona fide speech\nrepresentation and inject an angular margin to separate the spoofing attacks in\nthe embedding space. Without resorting to any data augmentation methods, our\nproposed system achieves an equal error rate (EER) of 2.19% on the evaluation\nset of ASVspoof 2019 Challenge logical access scenario, outperforming all\nexisting single systems (i.e., those without model ensemble).",
          "arxiv_id": "2010.13995v2"
        }
      ],
      "2": [
        {
          "title": "Blind Identification of Binaural Room Impulse Responses from Smart Glasses",
          "year": "2024-03",
          "abstract": "Smart glasses are increasingly recognized as a key medium for augmented\nreality, offering a hands-free platform with integrated microphones and\nnon-ear-occluding loudspeakers to seamlessly mix virtual sound sources into the\nreal-world acoustic scene. To convincingly integrate virtual sound sources, the\nroom acoustic rendering of the virtual sources must match the real-world\nacoustics. Information about a user's acoustic environment however is typically\nnot available. This work uses a microphone array in a pair of smart glasses to\nblindly identify binaural room impulse responses (BRIRs) from a few seconds of\nspeech in the real-world environment. The proposed method uses dereverberation\nand beamforming to generate a pseudo reference signal that is used by a\nmultichannel Wiener filter to estimate room impulse responses which are then\nconverted to BRIRs. The multichannel room impulse responses can be used to\nestimate room acoustic parameters which is shown to outperform baseline\nalgorithms in the estimation of reverberation time and direct-to-reverberant\nenergy ratio. Results from a listening experiment further indicate that the\nestimated BRIRs often reproduce the real-world room acoustics perceptually more\nconvincingly than measured BRIRs from other rooms of similar size.",
          "arxiv_id": "2403.19217v2"
        },
        {
          "title": "In situ sound absorption estimation with the discrete complex image source method",
          "year": "2024-04",
          "abstract": "Estimating the sound absorption in situ relies on accurately describing the\nmeasured sound field. Evidence suggests that modeling the reflection of\nimpinging spherical waves is important, especially for compact measurement\nsystems. This article proposes a method for estimating the sound absorption\ncoefficient of a material sample by mapping the sound pressure, measured by a\nmicrophone array, to a distribution of monopoles along a line in the complex\nplane. The proposed method is compared to modeling the sound field as a\nsuperposition of two sources (a monopole and an image source). The obtained\ninverse problems are solved with Tikhonov regularization, with automatic choice\nof the regularization parameter by the L-curve criterion. The sound absorption\nmeasurement is tested with simulations of the sound field above infinite and\nfinite porous absorbers. The approaches are compared to the plane-wave\nabsorption coefficient and the one obtained by spherical wave incidence.\nExperimental analysis of two porous samples and one resonant absorber is also\ncarried out in situ. Four arrays were tested with an increasing aperture and\nnumber of sensors. It was demonstrated that measurements are feasible even with\nan array with only a few microphones. The discretization of the integral\nequation led to a more accurate reconstruction of the sound pressure and\nparticle velocity at the sample's surface. The resulting absorption coefficient\nagrees with the one obtained for spherical wave incidence, indicating that\nincluding more monopoles along the complex line is an essential feature of the\nsound field.",
          "arxiv_id": "2404.11399v1"
        },
        {
          "title": "Room geometry blind inference based on the localization of real sound source and first order reflections",
          "year": "2022-07",
          "abstract": "The conventional room geometry blind inference techniques with acoustic\nsignals are conducted based on the prior knowledge of the environment, such as\nthe room impulse response (RIR) or the sound source position, which will limit\nits application under unknown scenarios. To solve this problem, we have\nproposed a room geometry reconstruction method in this paper by using the\ngeometric relation between the direct signal and first-order reflections. In\naddition to the information of the compact microphone array itself, this method\ndoes not need any precognition of the environmental parameters. Besides, the\nlearning-based DNN models are designed and used to improve the accuracy and\nintegrity of the localization results of the direct source and first-order\nreflections. The direction of arrival (DOA) and time difference of arrival\n(TDOA) information of the direct and reflected signals are firstly estimated\nusing the proposed DCNN and TD-CNN models, which have higher sensitivity and\naccuracy than the conventional methods. Then the position of the sound source\nis inferred by integrating the DOA, TDOA and array height using the proposed\nDNN model. After that, the positions of image sources and corresponding\nboundaries are derived based on the geometric relation. Experimental results of\nboth simulations and real measurements verify the effectiveness and accuracy of\nthe proposed techniques compared with the conventional methods under different\nreverberant environments.",
          "arxiv_id": "2207.10478v2"
        }
      ],
      "3": [
        {
          "title": "FB-MSTCN: A Full-Band Single-Channel Speech Enhancement Method Based on Multi-Scale Temporal Convolutional Network",
          "year": "2022-03",
          "abstract": "In recent years, deep learning-based approaches have significantly improved\nthe performance of single-channel speech enhancement. However, due to the\nlimitation of training data and computational complexity, real-time enhancement\nof full-band (48 kHz) speech signals is still very challenging. Because of the\nlow energy of spectral information in the high-frequency part, it is more\ndifficult to directly model and enhance the full-band spectrum using neural\nnetworks. To solve this problem, this paper proposes a two-stage real-time\nspeech enhancement model with extraction-interpolation mechanism for a\nfull-band signal. The 48 kHz full-band time-domain signal is divided into three\nsub-channels by extracting, and a two-stage processing scheme of `masking +\ncompensation' is proposed to enhance the signal in the complex domain. After\nthe two-stage enhancement, the enhanced full-band speech signal is restored by\ninterval interpolation. In the subjective listening and word accuracy test, our\nproposed model achieves superior performance and outperforms the baseline model\noverall by 0.59 MOS and 4.0% WAcc for the non-personalized speech denoising\ntask.",
          "arxiv_id": "2203.07684v1"
        },
        {
          "title": "TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation",
          "year": "2022-11",
          "abstract": "We propose TF-GridNet for speech separation. The model is a novel deep neural\nnetwork (DNN) integrating full- and sub-band modeling in the time-frequency\n(T-F) domain. It stacks several blocks, each consisting of an intra-frame\nfull-band module, a sub-band temporal module, and a cross-frame self-attention\nmodule. It is trained to perform complex spectral mapping, where the real and\nimaginary (RI) components of input signals are stacked as features to predict\ntarget RI components. We first evaluate it on monaural anechoic speaker\nseparation. Without using data augmentation and dynamic mixing, it obtains a\nstate-of-the-art 23.5 dB improvement in scale-invariant signal-to-distortion\nratio (SI-SDR) on WSJ0-2mix, a standard dataset for two-speaker separation. To\nshow its robustness to noise and reverberation, we evaluate it on monaural\nreverberant speaker separation using the SMS-WSJ dataset and on\nnoisy-reverberant speaker separation using WHAMR!, and obtain state-of-the-art\nperformance on both datasets. We then extend TF-GridNet to multi-microphone\nconditions through multi-microphone complex spectral mapping, and integrate it\ninto a two-DNN system with a beamformer in between (named as MISO-BF-MISO in\nearlier studies), where the beamformer proposed in this paper is a novel\nmulti-frame Wiener filter computed based on the outputs of the first DNN.\nState-of-the-art performance is obtained on the multi-channel tasks of SMS-WSJ\nand WHAMR!. Besides speaker separation, we apply the proposed algorithms to\nspeech dereverberation and noisy-reverberant speech enhancement.\nState-of-the-art performance is obtained on a dereverberation dataset and on\nthe dataset of the recent L3DAS22 multi-channel speech enhancement challenge.",
          "arxiv_id": "2211.12433v2"
        },
        {
          "title": "Plugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network",
          "year": "2024-02",
          "abstract": "The expectation to deploy a universal neural network for speech enhancement,\nwith the aim of improving noise robustness across diverse speech processing\ntasks, faces challenges due to the existing lack of awareness within static\nspeech enhancement frameworks regarding the expected speech in downstream\nmodules. These limitations impede the effectiveness of static speech\nenhancement approaches in achieving optimal performance for a range of speech\nprocessing tasks, thereby challenging the notion of universal applicability.\nThe fundamental issue in achieving universal speech enhancement lies in\neffectively informing the speech enhancement module about the features of\ndownstream modules. In this study, we present a novel weighting prediction\napproach, which explicitly learns the task relationships from downstream\ntraining information to address the core challenge of universal speech\nenhancement. We found the role of deciding whether to employ data augmentation\ntechniques as crucial downstream training information. This decision\nsignificantly impacts the expected speech and the performance of the speech\nenhancement module. Moreover, we introduce a novel speech enhancement network,\nthe Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural\nnetwork that includes the speech enhancement module, gate module, and weight\nprediction module. Experimental results demonstrate that the proposed Plugin-SE\napproach is competitive or superior to other joint training methods across\nvarious downstream tasks.",
          "arxiv_id": "2402.12746v1"
        }
      ],
      "4": [
        {
          "title": "Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis",
          "year": "2023-07",
          "abstract": "Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech\nprompts, which significantly reduces the data and computation requirements for\nvoice cloning by skipping the fine-tuning process. However, the prompting\nmechanisms of zero-shot TTS still face challenges in the following aspects: 1)\nprevious works of zero-shot TTS are typically trained with single-sentence\nprompts, which significantly restricts their performance when the data is\nrelatively sufficient during the inference stage. 2) The prosodic information\nin prompts is highly coupled with timbre, making it untransferable to each\nother. This paper introduces Mega-TTS 2, a generic prompting mechanism for\nzero-shot TTS, to tackle the aforementioned challenges. Specifically, we design\na powerful acoustic autoencoder that separately encodes the prosody and timbre\ninformation into the compressed latent space while providing high-quality\nreconstructions. Then, we propose a multi-reference timbre encoder and a\nprosody latent language model (P-LLM) to extract useful information from\nmulti-sentence prompts. We further leverage the probabilities derived from\nmultiple P-LLM outputs to produce transferable and controllable prosody.\nExperimental results demonstrate that Mega-TTS 2 could not only synthesize\nidentity-preserving speech with a short prompt of an unseen speaker from\narbitrary sources but consistently outperform the fine-tuning method when the\nvolume of data ranges from 10 seconds to 5 minutes. Furthermore, our method\nenables to transfer various speaking styles to the target timbre in a\nfine-grained and controlled manner. Audio samples can be found in\nhttps://boostprompt.github.io/boostprompt/.",
          "arxiv_id": "2307.07218v4"
        },
        {
          "title": "MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech",
          "year": "2024-10",
          "abstract": "Text-to-speech (TTS) systems that scale up the amount of training data have\nachieved significant improvements in zero-shot speech synthesis. However, these\nsystems have certain limitations: they require a large amount of training data,\nwhich increases costs, and often overlook prosody similarity. To address these\nissues, we propose MultiVerse, a zero-shot multi-task TTS system that is able\nto perform TTS or speech style transfer in zero-shot and cross-lingual\nconditions. MultiVerse requires much less training data than traditional\ndata-driven approaches. To ensure zero-shot performance even with limited data,\nwe leverage source-filter theory-based disentanglement, utilizing the prompt\nfor modeling filter-related and source-related representations. Additionally,\nto further enhance prosody similarity, we adopt a prosody modeling approach\ncombining prompt-based autoregressive and non-autoregressive methods.\nEvaluations demonstrate the remarkable zero-shot multi-task TTS performance of\nMultiVerse and show that MultiVerse not only achieves zero-shot TTS performance\ncomparable to data-driven TTS systems with much less data, but also\nsignificantly outperforms other zero-shot TTS systems trained with the same\nsmall amount of data. In particular, our novel prosody modeling technique\nsignificantly contributes to MultiVerse's ability to generate speech with high\nprosody similarity to the given prompts. Our samples are available at\nhttps://nc-ai.github.io/speech/publications/multiverse/index.html",
          "arxiv_id": "2410.03192v1"
        },
        {
          "title": "Expressive Text-to-Speech using Style Tag",
          "year": "2021-04",
          "abstract": "As recent text-to-speech (TTS) systems have been rapidly improved in speech\nquality and generation speed, many researchers now focus on a more challenging\nissue: expressive TTS. To control speaking styles, existing expressive TTS\nmodels use categorical style index or reference speech as style input. In this\nwork, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that\nutilizes a style tag written in natural language. Using a style-tagged TTS\ndataset and a pre-trained language model, we modeled the relationship between\nlinguistic embedding and speaking style domain, which enables our model to work\neven with style tags unseen during training. As style tag is written in natural\nlanguage, it can control speaking style in a more intuitive, interpretable, and\nscalable way compared with style index or reference speech. In addition, in\nterms of model architecture, we propose an efficient non-autoregressive (NAR)\nTTS architecture with single-stage training. The experimental result shows that\nST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech\nquality and expressiveness.",
          "arxiv_id": "2104.00436v2"
        }
      ],
      "5": [
        {
          "title": "MSAC: Multiple Speech Attribute Control Method for Reliable Speech Emotion Recognition",
          "year": "2023-08",
          "abstract": "Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.",
          "arxiv_id": "2308.04025v3"
        },
        {
          "title": "emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation",
          "year": "2023-12",
          "abstract": "We propose emotion2vec, a universal speech emotion representation model.\nemotion2vec is pre-trained on open-source unlabeled emotion data through\nself-supervised online distillation, combining utterance-level loss and\nframe-level loss during pre-training. emotion2vec outperforms state-of-the-art\npre-trained universal models and emotion specialist models by only training\nlinear layers for the speech emotion recognition task on the mainstream IEMOCAP\ndataset. In addition, emotion2vec shows consistent improvements among 10\ndifferent languages of speech emotion recognition datasets. emotion2vec also\nshows excellent results on other emotion tasks, such as song emotion\nrecognition, emotion prediction in conversation, and sentiment analysis.\nComparison experiments, ablation experiments, and visualization comprehensively\ndemonstrate the universal capability of the proposed emotion2vec. To the best\nof our knowledge, emotion2vec is the first universal representation model in\nvarious emotion-related tasks, filling a gap in the field.",
          "arxiv_id": "2312.15185v1"
        },
        {
          "title": "Multimodal Speech Emotion Recognition Using Modality-specific Self-Supervised Frameworks",
          "year": "2023-12",
          "abstract": "Emotion recognition is a topic of significant interest in assistive robotics\ndue to the need to equip robots with the ability to comprehend human behavior,\nfacilitating their effective interaction in our society. Consequently,\nefficient and dependable emotion recognition systems supporting optimal\nhuman-machine communication are required. Multi-modality (including speech,\naudio, text, images, and videos) is typically exploited in emotion recognition\ntasks. Much relevant research is based on merging multiple data modalities and\ntraining deep learning models utilizing low-level data representations.\nHowever, most existing emotion databases are not large (or complex) enough to\nallow machine learning approaches to learn detailed representations. This paper\nexplores modalityspecific pre-trained transformer frameworks for\nself-supervised learning of speech and text representations for data-efficient\nemotion recognition while achieving state-of-the-art performance in recognizing\nemotions. This model applies feature-level fusion using nonverbal cue data\npoints from motion capture to provide multimodal speech emotion recognition.\nThe model was trained using the publicly available IEMOCAP dataset, achieving\nan overall accuracy of 77.58% for four emotions, outperforming state-of-the-art\napproaches",
          "arxiv_id": "2312.01568v1"
        }
      ],
      "6": [
        {
          "title": "A CTC Alignment-based Non-autoregressive Transformer for End-to-end Automatic Speech Recognition",
          "year": "2023-04",
          "abstract": "Recently, end-to-end models have been widely used in automatic speech\nrecognition (ASR) systems. Two of the most representative approaches are\nconnectionist temporal classification (CTC) and attention-based encoder-decoder\n(AED) models. Autoregressive transformers, variants of AED, adopt an\nautoregressive mechanism for token generation and thus are relatively slow\nduring inference. In this paper, we present a comprehensive study of a CTC\nAlignment-based Single-Step Non-Autoregressive Transformer (CASS-NAT) for\nend-to-end ASR. In CASS-NAT, word embeddings in the autoregressive transformer\n(AT) are substituted with token-level acoustic embeddings (TAE) that are\nextracted from encoder outputs with the acoustical boundary information offered\nby the CTC alignment. TAE can be obtained in parallel, resulting in a parallel\ngeneration of output tokens. During training, Viterbi-alignment is used for TAE\ngeneration, and multiple training strategies are further explored to improve\nthe word error rate (WER) performance. During inference, an error-based\nalignment sampling method is investigated in depth to reduce the alignment\nmismatch in the training and testing processes. Experimental results show that\nthe CASS-NAT has a WER that is close to AT on various ASR tasks, while\nproviding a ~24x inference speedup. With and without self-supervised learning,\nwe achieve new state-of-the-art results for non-autoregressive models on\nseveral datasets. We also analyze the behavior of the CASS-NAT decoder to\nexplain why it can perform similarly to AT. We find that TAEs have similar\nfunctionality to word embeddings for grammatical structures, which might\nindicate the possibility of learning some semantic information from TAEs\nwithout a language model.",
          "arxiv_id": "2304.07611v1"
        },
        {
          "title": "Mask-CTC-based Encoder Pre-training for Streaming End-to-End Speech Recognition",
          "year": "2023-09",
          "abstract": "Achieving high accuracy with low latency has always been a challenge in\nstreaming end-to-end automatic speech recognition (ASR) systems. By attending\nto more future contexts, a streaming ASR model achieves higher accuracy but\nresults in larger latency, which hurts the streaming performance. In the\nMask-CTC framework, an encoder network is trained to learn the feature\nrepresentation that anticipates long-term contexts, which is desirable for\nstreaming ASR. Mask-CTC-based encoder pre-training has been shown beneficial in\nachieving low latency and high accuracy for triggered attention-based ASR.\nHowever, the effectiveness of this method has not been demonstrated for various\nmodel architectures, nor has it been verified that the encoder has the expected\nlook-ahead capability to reduce latency. This study, therefore, examines the\neffectiveness of Mask-CTCbased pre-training for models with different\narchitectures, such as Transformer-Transducer and contextual block streaming\nASR. We also discuss the effect of the proposed pre-training method on\nobtaining accurate output spike timing.",
          "arxiv_id": "2309.04654v1"
        },
        {
          "title": "Alignment Knowledge Distillation for Online Streaming Attention-based Speech Recognition",
          "year": "2021-02",
          "abstract": "This article describes an efficient training method for online streaming\nattention-based encoder-decoder (AED) automatic speech recognition (ASR)\nsystems. AED models have achieved competitive performance in offline scenarios\nby jointly optimizing all components. They have recently been extended to an\nonline streaming framework via models such as monotonic chunkwise attention\n(MoChA). However, the elaborate attention calculation process is not robust for\nlong-form speech utterances. Moreover, the sequence-level training objective\nand time-restricted streaming encoder cause a nonnegligible delay in token\nemission during inference. To address these problems, we propose CTC\nsynchronous training (CTC-ST), in which CTC alignments are leveraged as a\nreference for token boundaries to enable a MoChA model to learn optimal\nmonotonic input-output alignments. We formulate a purely end-to-end training\nobjective to synchronize the boundaries of MoChA to those of CTC. The CTC model\nshares an encoder with the MoChA model to enhance the encoder representation.\nMoreover, the proposed method provides alignment information learned in the CTC\nbranch to the attention-based decoder. Therefore, CTC-ST can be regarded as\nself-distillation of alignment knowledge from CTC to MoChA. Experimental\nevaluations on a variety of benchmark datasets show that the proposed method\nsignificantly reduces recognition errors and emission latency simultaneously.\nThe robustness to long-form and noisy speech is also demonstrated. We compare\nCTC-ST with several methods that distill alignment knowledge from a hybrid ASR\nsystem and show that the CTC-ST can achieve a comparable tradeoff of accuracy\nand latency without relying on external alignment information. The best MoChA\nsystem shows recognition accuracy comparable to that of RNN-transducer (RNN-T)\nwhile achieving lower emission latency.",
          "arxiv_id": "2103.00422v2"
        }
      ],
      "7": [
        {
          "title": "Joint Analysis of Acoustic Scenes and Sound Events with Weakly labeled Data",
          "year": "2022-07",
          "abstract": "Considering that acoustic scenes and sound events are closely related to each\nother, in some previous papers, a joint analysis of acoustic scenes and sound\nevents utilizing multitask learning (MTL)-based neural networks was proposed.\nIn conventional methods, a strongly supervised scheme is applied to sound event\ndetection in MTL models, which requires strong labels of sound events in model\ntraining; however, annotating strong event labels is quite time-consuming. In\nthis paper, we thus propose a method for the joint analysis of acoustic scenes\nand sound events based on the MTL framework with weak labels of sound events.\nIn particular, in the proposed method, we introduce the multiple-instance\nlearning scheme for weakly supervised training of sound event detection and\nevaluate four pooling functions, namely, max pooling, average pooling,\nexponential softmax pooling, and attention pooling. Experimental results\nobtained using parts of the TUT Acoustic Scenes 2016/2017 and TUT Sound Events\n2016/2017 datasets show that the proposed MTL-based method with weak labels\noutperforms the conventional single-task-based scene classification and event\ndetection models with weak labels in terms of both the scene classification and\nevent detection performances.",
          "arxiv_id": "2207.04357v1"
        },
        {
          "title": "Cross-Referencing Self-Training Network for Sound Event Detection in Audio Mixtures",
          "year": "2021-05",
          "abstract": "Sound event detection is an important facet of audio tagging that aims to\nidentify sounds of interest and define both the sound category and time\nboundaries for each sound event in a continuous recording. With advances in\ndeep neural networks, there has been tremendous improvement in the performance\nof sound event detection systems, although at the expense of costly data\ncollection and labeling efforts. In fact, current state-of-the-art methods\nemploy supervised training methods that leverage large amounts of data samples\nand corresponding labels in order to facilitate identification of sound\ncategory and time stamps of events. As an alternative, the current study\nproposes a semi-supervised method for generating pseudo-labels from\nunsupervised data using a student-teacher scheme that balances self-training\nand cross-training. Additionally, this paper explores post-processing which\nextracts sound intervals from network prediction, for further improvement in\nsound event detection performance. The proposed approach is evaluated on sound\nevent detection task for the DCASE2020 challenge. The results of these methods\non both \"validation\" and \"public evaluation\" sets of DESED database show\nsignificant improvement compared to the state-of-the art systems in\nsemi-supervised learning.",
          "arxiv_id": "2105.13392v1"
        },
        {
          "title": "Sound Event Detection Using Duration Robust Loss Function",
          "year": "2020-06",
          "abstract": "Many methods of sound event detection (SED) based on machine learning regard\na segmented time frame as one data sample to model training. However, the sound\ndurations of sound events vary greatly depending on the sound event class,\ne.g., the sound event ``fan'' has a long time duration, while the sound event\n``mouse clicking'' is instantaneous. The difference in the time duration\nbetween sound event classes thus causes a serious data imbalance problem in\nSED. In this paper, we propose a method for SED using a duration robust loss\nfunction, which can focus model training on sound events of short duration. In\nthe proposed method, we focus on a relationship between the duration of the\nsound event and the ease/difficulty of model training. In particular, many\nsound events of long duration (e.g., sound event ``fan'') are stationary\nsounds, which have less variation in their acoustic features and their model\ntraining is easy. Meanwhile, some sound events of short duration (e.g., sound\nevent ``object impact'') have more than one audio pattern, such as attack,\ndecay, and release parts. We thus apply a class-wise reweighting to the\nbinary-cross entropy loss function depending on the ease/difficulty of model\ntraining. Evaluation experiments conducted using TUT Sound Events 2016/2017 and\nTUT Acoustic Scenes 2016 datasets show that the proposed method respectively\nimproves the detection performance of sound events by 3.15 and 4.37 percentage\npoints in macro- and micro-Fscores compared with a conventional method using\nthe binary-cross entropy loss function.",
          "arxiv_id": "2006.15253v1"
        }
      ],
      "8": [
        {
          "title": "Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models",
          "year": "2023-10",
          "abstract": "Audio-visual large language models (LLM) have drawn significant attention,\nyet the fine-grained combination of both input streams is rather\nunder-explored, which is challenging but necessary for LLMs to understand\ngeneral video inputs. To this end, a fine-grained audio-visual joint\nrepresentation (FAVOR) learning framework for multimodal LLMs is proposed in\nthis paper, which extends a text-based LLM to simultaneously perceive speech\nand audio events in the audio input stream and images or videos in the visual\ninput stream, at the frame level. To fuse the audio and visual feature streams\ninto joint representations and to align the joint space with the LLM input\nembedding space, we propose a causal Q-Former structure with a causal attention\nmodule to enhance the capture of causal relations of the audio-visual frames\nacross time. An audio-visual evaluation benchmark (AVEB) is also proposed which\ncomprises six representative single-modal tasks with five cross-modal tasks\nreflecting audio-visual co-reasoning abilities. While achieving competitive\nsingle-modal performance on audio, speech and image tasks in AVEB, FAVOR\nachieved over 20% accuracy improvements on the video question-answering task\nwhen fine-grained information or temporal causal reasoning is required. FAVOR,\nin addition, demonstrated remarkable video comprehension and reasoning\nabilities on tasks that are unprecedented by other multimodal LLMs. An\ninteractive demo of FAVOR is available at\nhttps://github.com/BriansIDP/AudioVisualLLM.git, and the training code and\nmodel checkpoints will be released soon.",
          "arxiv_id": "2310.05863v2"
        },
        {
          "title": "CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model for Multimodal Processing",
          "year": "2024-01",
          "abstract": "There has been a long-standing quest for a unified audio-visual-text model to\nenable various multimodal understanding tasks, which mimics the listening,\nseeing and reading process of human beings. Humans tends to represent knowledge\nusing two separate systems: one for representing verbal (textual) information\nand one for representing non-verbal (visual and auditory) information. These\ntwo systems can operate independently but can also interact with each other.\nMotivated by this understanding of human cognition, in this paper, we introduce\nCoAVT -- a novel cognition-inspired Correlated Audio-Visual-Text pre-training\nmodel to connect the three modalities. It contains a joint audio-visual encoder\nthat learns to encode audio-visual synchronization information together with\nthe audio and visual content for non-verbal information, and a text encoder to\nhandle textual input for verbal information. To bridge the gap between\nmodalities, CoAVT employs a query encoder, which contains a set of learnable\nquery embeddings, and extracts the most informative audiovisual features of the\ncorresponding text. Additionally, to leverage the correspondences between audio\nand vision with language respectively, we also establish the audio-text and\nvisual-text bi-modal alignments upon the foundational audiovisual-text\ntri-modal alignment to enhance the multimodal representation learning. Finally,\nwe jointly optimize CoAVT model with three multimodal objectives: contrastive\nloss, matching loss and language modeling loss. Extensive experiments show that\nCoAVT can learn strong multimodal correlations and be generalized to various\ndownstream tasks. CoAVT establishes new state-of-the-art performance on\ntext-video retrieval task on AudioCaps for both zero-shot and fine-tuning\nsettings, audio-visual event classification and audio-visual retrieval tasks on\nAudioSet and VGGSound.",
          "arxiv_id": "2401.12264v2"
        },
        {
          "title": "AudioSetCaps: An Enriched Audio-Caption Dataset using Automated Generation Pipeline with Large Audio and Language Models",
          "year": "2024-11",
          "abstract": "With the emergence of audio-language models, constructing large-scale paired\naudio-language datasets has become essential yet challenging for model\ndevelopment, primarily due to the time-intensive and labour-heavy demands\ninvolved. While large language models (LLMs) have improved the efficiency of\nsynthetic audio caption generation, current approaches struggle to effectively\nextract and incorporate detailed audio information. In this paper, we propose\nan automated pipeline that integrates audio-language models for fine-grained\ncontent extraction, LLMs for synthetic caption generation, and a contrastive\nlanguage-audio pretraining (CLAP) model-based refinement process to improve the\nquality of captions. Specifically, we employ prompt chaining techniques in the\ncontent extraction stage to obtain accurate and fine-grained audio information,\nwhile we use the refinement process to mitigate potential hallucinations in the\ngenerated captions. Leveraging the AudioSet dataset and the proposed approach,\nwe create AudioSetCaps, a dataset comprising 1.9 million audio-caption pairs,\nthe largest audio-caption dataset at the time of writing. The models trained\nwith AudioSetCaps achieve state-of-the-art performance on audio-text retrieval\nwith R@1 scores of 46.3% for text-to-audio and 59.7% for audio-to-text\nretrieval and automated audio captioning with the CIDEr score of 84.8. As our\napproach has shown promising results with AudioSetCaps, we create another\ndataset containing 4.1 million synthetic audio-language pairs based on the\nYoutube-8M and VGGSound datasets. To facilitate research in audio-language\nlearning, we have made our pipeline, datasets with 6 million audio-language\npairs, and pre-trained models publicly available at\nhttps://github.com/JishengBai/AudioSetCaps.",
          "arxiv_id": "2411.18953v1"
        }
      ],
      "9": [
        {
          "title": "Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech",
          "year": "2025-06",
          "abstract": "Background: Alzheimer's disease and related dementias (ADRD) are progressive\nneurodegenerative conditions where early detection is vital for timely\nintervention and care. Spontaneous speech contains rich acoustic and linguistic\nmarkers that may serve as non-invasive biomarkers for cognitive decline.\nFoundation models, pre-trained on large-scale audio or text data, produce\nhigh-dimensional embeddings encoding contextual and acoustic features.\n  Methods: We used the PREPARE Challenge dataset, which includes audio\nrecordings from over 1,600 participants with three cognitive statuses: healthy\ncontrol (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We\nexcluded non-English, non-spontaneous, or poor-quality recordings. The final\ndataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We\nbenchmarked a range of open-source foundation speech and language models to\nclassify cognitive status into the three categories.\n  Results: The Whisper-medium model achieved the highest performance among\nspeech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with\npause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection\nusing state-of-the-art automatic speech recognition (ASR) model-generated audio\nembeddings outperformed others. Including non-semantic features like pause\npatterns consistently improved text-based classification.\n  Conclusion: This study introduces a benchmarking framework using foundation\nmodels and a clinically relevant dataset. Acoustic-based approaches --\nparticularly ASR-derived embeddings -- demonstrate strong potential for\nscalable, non-invasive, and cost-effective early detection of ADRD.",
          "arxiv_id": "2506.11119v1"
        },
        {
          "title": "Speaker Adaptation Using Spectro-Temporal Deep Features for Dysarthric and Elderly Speech Recognition",
          "year": "2022-02",
          "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\ntargeting normal speech in recent decades, accurate recognition of dysarthric\nand elderly speech remains highly challenging tasks to date. Sources of\nheterogeneity commonly found in normal speech including accent or gender, when\nfurther compounded with the variability over age and speech pathology severity\nlevel, create large diversity among speakers. To this end, speaker adaptation\ntechniques play a key role in personalization of ASR systems for such users.\nMotivated by the spectro-temporal level differences between dysarthric, elderly\nand normal speech that systematically manifest in articulatory imprecision,\ndecreased volume and clarity, slower speaking rates and increased dysfluencies,\nnovel spectrotemporal subspace basis deep embedding features derived using SVD\nspeech spectrum decomposition are proposed in this paper to facilitate\nauxiliary feature based speaker adaptation of state-of-the-art hybrid DNN/TDNN\nand end-to-end Conformer speech recognition systems. Experiments were conducted\non four tasks: the English UASpeech and TORGO dysarthric speech corpora; the\nEnglish DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets.\nThe proposed spectro-temporal deep feature adapted systems outperformed\nbaseline i-Vector and xVector adaptation by up to 2.63% absolute (8.63%\nrelative) reduction in word error rate (WER). Consistent performance\nimprovements were retained after model based speaker adaptation using learning\nhidden unit contributions (LHUC) was further applied. The best speaker adapted\nsystem using the proposed spectral basis embedding features produced the lowest\npublished WER of 25.05% on the UASpeech test set of 16 dysarthric speakers.",
          "arxiv_id": "2202.10290v3"
        },
        {
          "title": "Recent Progress in the CUHK Dysarthric Speech Recognition System",
          "year": "2022-01",
          "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\nin the past few decades, recognition of disordered speech remains a highly\nchallenging task to date. Disordered speech presents a wide spectrum of\nchallenges to current data intensive deep neural networks (DNNs) based ASR\ntechnologies that predominantly target normal speech. This paper presents\nrecent research efforts at the Chinese University of Hong Kong (CUHK) to\nimprove the performance of disordered speech recognition systems on the largest\npublicly available UASpeech dysarthric speech corpus. A set of novel modelling\ntechniques including neural architectural search, data augmentation using\nspectra-temporal perturbation, model based speaker adaptation and cross-domain\ngeneration of visual features within an audio-visual speech recognition (AVSR)\nsystem framework were employed to address the above challenges. The combination\nof these techniques produced the lowest published word error rate (WER) of\n25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER\nreduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric\nspeech recognition system featuring a 6-way DNN system combination and cross\nadaptation of out-of-domain normal speech data trained systems. Bayesian model\nadaptation further allows rapid adaptation to individual dysarthric speakers to\nbe performed using as little as 3.06 seconds of speech. The efficacy of these\ntechniques were further demonstrated on a CUDYS Cantonese dysarthric speech\nrecognition task.",
          "arxiv_id": "2201.05845v2"
        }
      ],
      "10": [
        {
          "title": "Self-Distillation Prototypes Network: Learning Robust Speaker Representations without Supervision",
          "year": "2023-08",
          "abstract": "Training speaker-discriminative and robust speaker verification systems\nwithout explicit speaker labels remains a persistent challenge. In this paper,\nwe propose a novel self-supervised speaker verification approach,\nSelf-Distillation Prototypes Network (SDPN), which effectively facilitates\nself-supervised speaker representation learning. SDPN assigns the\nrepresentation of the augmented views of an utterance to the same prototypes as\nthe representation of the original view, thereby enabling effective knowledge\ntransfer between the augmented and original views. Due to lack of negative\npairs in the SDPN training process, the network tends to align positive pairs\nquite closely in the embedding space, a phenomenon known as model collapse. To\nmitigate this problem, we introduce a diversity regularization term to\nembeddings in SDPN. Comprehensive experiments on the VoxCeleb datasets\ndemonstrate the superiority of SDPN among self-supervised speaker verification\napproaches. SDPN sets a new state-of-the-art on the VoxCeleb1 speaker\nverification evaluation benchmark, achieving Equal Error Rate 1.80%, 1.99%, and\n3.62% for trial VoxCeleb1-O, VoxCeleb1-E and VoxCeleb1-H, without using any\nspeaker labels in training. Ablation studies show that both proposed learnable\nprototypes in self-distillation network and diversity regularization contribute\nto the verification performance.",
          "arxiv_id": "2308.02774v6"
        },
        {
          "title": "The IDLAB VoxSRC-20 Submission: Large Margin Fine-Tuning and Quality-Aware Score Calibration in DNN Based Speaker Verification",
          "year": "2020-10",
          "abstract": "In this paper we propose and analyse a large margin fine-tuning strategy and\na quality-aware score calibration in text-independent speaker verification.\nLarge margin fine-tuning is a secondary training stage for DNN based speaker\nverification systems trained with margin-based loss functions. It enables the\nnetwork to create more robust speaker embeddings by enabling the use of longer\ntraining utterances in combination with a more aggressive margin penalty. Score\ncalibration is a common practice in speaker verification systems to map output\nscores to well-calibrated log-likelihood-ratios, which can be converted to\ninterpretable probabilities. By including quality features in the calibration\nsystem, the decision thresholds of the evaluation metrics become\nquality-dependent and more consistent across varying trial conditions. Applying\nboth enhancements on the ECAPA-TDNN architecture leads to state-of-the-art\nresults on all publicly available VoxCeleb1 test sets and contributed to our\nwinning submissions in the supervised verification tracks of the VoxCeleb\nSpeaker Recognition Challenge 2020.",
          "arxiv_id": "2010.11255v2"
        },
        {
          "title": "Margin-Mixup: A Method for Robust Speaker Verification in Multi-Speaker Audio",
          "year": "2023-04",
          "abstract": "This paper is concerned with the task of speaker verification on audio with\nmultiple overlapping speakers. Most speaker verification systems are designed\nwith the assumption of a single speaker being present in a given audio segment.\nHowever, in a real-world setting this assumption does not always hold. In this\npaper, we demonstrate that current speaker verification systems are not robust\nagainst audio with noticeable speaker overlap. To alleviate this issue, we\npropose margin-mixup, a simple training strategy that can easily be adopted by\nexisting speaker verification pipelines to make the resulting speaker\nembeddings robust against multi-speaker audio. In contrast to other methods,\nmargin-mixup requires no alterations to regular speaker verification\narchitectures, while attaining better results. On our multi-speaker test set\nbased on VoxCeleb1, the proposed margin-mixup strategy improves the EER on\naverage with 44.4% relative to our state-of-the-art speaker verification\nbaseline systems.",
          "arxiv_id": "2304.03515v1"
        }
      ],
      "11": [
        {
          "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
          "year": "2023-10",
          "abstract": "This paper proposes an online target speaker voice activity detection system\nfor speaker diarization tasks, which does not require a priori knowledge from\nthe clustering-based diarization system to obtain the target speaker\nembeddings. By adapting the conventional target speaker voice activity\ndetection for real-time operation, this framework can identify speaker\nactivities using self-generated embeddings, resulting in consistent performance\nwithout permutation inconsistencies in the inference phase. During the\ninference process, we employ a front-end model to extract the frame-level\nspeaker embeddings for each coming block of a signal. Next, we predict the\ndetection state of each speaker based on these frame-level speaker embeddings\nand the previously estimated target speaker embedding. Then, the target speaker\nembeddings are updated by aggregating these frame-level speaker embeddings\naccording to the predictions in the current block. Our model predicts the\nresults for each block and updates the target speakers' embeddings until\nreaching the end of the signal. Experimental results show that the proposed\nmethod outperforms the offline clustering-based diarization system on the\nDIHARD III and AliMeeting datasets. The proposed method is further extended to\nmulti-channel data, which achieves similar performance with the\nstate-of-the-art offline diarization systems.",
          "arxiv_id": "2310.08696v1"
        },
        {
          "title": "End-to-End Neural Diarization: Reformulating Speaker Diarization as Simple Multi-label Classification",
          "year": "2020-02",
          "abstract": "The most common approach to speaker diarization is clustering of speaker\nembeddings. However, the clustering-based approach has a number of problems;\ni.e., (i) it is not optimized to minimize diarization errors directly, (ii) it\ncannot handle speaker overlaps correctly, and (iii) it has trouble adapting\ntheir speaker embedding models to real audio recordings with speaker overlaps.\nTo solve these problems, we propose the End-to-End Neural Diarization (EEND),\nin which a neural network directly outputs speaker diarization results given a\nmulti-speaker recording. To realize such an end-to-end model, we formulate the\nspeaker diarization problem as a multi-label classification problem and\nintroduce a permutation-free objective function to directly minimize\ndiarization errors. Besides its end-to-end simplicity, the EEND method can\nexplicitly handle speaker overlaps during training and inference. Just by\nfeeding multi-speaker recordings with corresponding speaker segment labels, our\nmodel can be easily adapted to real conversations. We evaluated our method on\nsimulated speech mixtures and real conversation datasets. The results showed\nthat the EEND method outperformed the state-of-the-art x-vector\nclustering-based method, while it correctly handled speaker overlaps. We\nexplored the neural network architecture for the EEND method, and found that\nthe self-attention-based neural network was the key to achieving excellent\nperformance. In contrast to conditioning the network only on its previous and\nnext hidden states, as is done using bidirectional long short-term memory\n(BLSTM), self-attention is directly conditioned on all the frames. By\nvisualizing the attention weights, we show that self-attention captures global\nspeaker characteristics in addition to local speech activity dynamics, making\nit especially suitable for dealing with the speaker diarization problem.",
          "arxiv_id": "2003.02966v1"
        },
        {
          "title": "Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR",
          "year": "2021-10",
          "abstract": "This paper presents Transcribe-to-Diarize, a new approach for neural speaker\ndiarization that uses an end-to-end (E2E) speaker-attributed automatic speech\nrecognition (SA-ASR). The E2E SA-ASR is a joint model that was recently\nproposed for speaker counting, multi-talker speech recognition, and speaker\nidentification from monaural audio that contains overlapping speech. Although\nthe E2E SA-ASR model originally does not estimate any time-related information,\nwe show that the start and end times of each word can be estimated with\nsufficient accuracy from the internal state of the E2E SA-ASR by adding a small\nnumber of learnable parameters. Similar to the target-speaker voice activity\ndetection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to\nestimate speech activity of each speaker while it has the advantages of (i)\nhandling unlimited number of speakers, (ii) leveraging linguistic information\nfor speaker diarization, and (iii) simultaneously generating speaker-attributed\ntranscriptions. Experimental results on the LibriCSS and AMI corpora show that\nthe proposed method achieves significantly better diarization error rate than\nvarious existing speaker diarization methods when the number of speakers is\nunknown, and achieves a comparable performance to TS-VAD when the number of\nspeakers is given in advance. The proposed method simultaneously generates\nspeaker-attributed transcription with state-of-the-art accuracy.",
          "arxiv_id": "2110.03151v2"
        }
      ],
      "12": [
        {
          "title": "Deliberation Model for On-Device Spoken Language Understanding",
          "year": "2022-04",
          "abstract": "We propose a novel deliberation-based approach to end-to-end (E2E) spoken\nlanguage understanding (SLU), where a streaming automatic speech recognition\n(ASR) model produces the first-pass hypothesis and a second-pass natural\nlanguage understanding (NLU) component generates the semantic parse by\nconditioning on both ASR's text and audio embeddings. By formulating E2E SLU as\na generalized decoder, our system is able to support complex compositional\nsemantic structures. Furthermore, the sharing of parameters between ASR and NLU\nmakes the system especially suitable for resource-constrained (on-device)\nenvironments; our proposed approach consistently outperforms strong pipeline\nNLU baselines by 0.60% to 0.65% on the spoken version of the TOPv2 dataset\n(STOP). We demonstrate that the fusion of text and audio features, coupled with\nthe system's ability to rewrite the first-pass hypothesis, makes our approach\nmore robust to ASR errors. Finally, we show that our approach can significantly\nreduce the degradation when moving from natural speech to synthetic speech\ntraining, but more work is required to make text-to-speech (TTS) a viable\nsolution for scaling up E2E SLU.",
          "arxiv_id": "2204.01893v3"
        },
        {
          "title": "Do as I mean, not as I say: Sequence Loss Training for Spoken Language Understanding",
          "year": "2021-02",
          "abstract": "Spoken language understanding (SLU) systems extract transcriptions, as well\nas semantics of intent or named entities from speech, and are essential\ncomponents of voice activated systems. SLU models, which either directly\nextract semantics from audio or are composed of pipelined automatic speech\nrecognition (ASR) and natural language understanding (NLU) models, are\ntypically trained via differentiable cross-entropy losses, even when the\nrelevant performance metrics of interest are word or semantic error rates. In\nthis work, we propose non-differentiable sequence losses based on SLU metrics\nas a proxy for semantic error and use the REINFORCE trick to train ASR and SLU\nmodels with this loss. We show that custom sequence loss training is the\nstate-of-the-art on open SLU datasets and leads to 6% relative improvement in\nboth ASR and NLU performance metrics on large proprietary datasets. We also\ndemonstrate how the semantic sequence loss training paradigm can be used to\nupdate ASR and SLU models without transcripts, using semantic feedback alone.",
          "arxiv_id": "2102.06750v1"
        },
        {
          "title": "Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks",
          "year": "2024-01",
          "abstract": "In the realm of spoken language understanding (SLU), numerous natural\nlanguage understanding (NLU) methodologies have been adapted by supplying large\nlanguage models (LLMs) with transcribed speech instead of conventional written\ntext. In real-world scenarios, prior to input into an LLM, an automated speech\nrecognition (ASR) system generates an output transcript hypothesis, where\ninherent errors can degrade subsequent SLU tasks. Here we introduce a method\nthat utilizes the ASR system's lattice output instead of relying solely on the\ntop hypothesis, aiming to encapsulate speech ambiguities and enhance SLU\noutcomes. Our in-context learning experiments, covering spoken question\nanswering and intent classification, underline the LLM's resilience to noisy\nspeech transcripts with the help of word confusion networks from lattices,\nbridging the SLU performance gap between using the top ASR hypothesis and an\noracle upper bound. Additionally, we delve into the LLM's robustness to varying\nASR performance conditions and scrutinize the aspects of in-context learning\nwhich prove the most influential.",
          "arxiv_id": "2401.02921v1"
        }
      ],
      "13": [
        {
          "title": "Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks",
          "year": "2021-10",
          "abstract": "Representation learning from unlabeled data has been of major interest in\nartificial intelligence research. While self-supervised speech representation\nlearning has been popular in the speech research community, very few works have\ncomprehensively analyzed audio representation learning for non-speech audio\ntasks. In this paper, we propose a self-supervised audio representation\nlearning method and apply it to a variety of downstream non-speech audio tasks.\nWe combine the well-known wav2vec 2.0 framework, which has shown success in\nself-supervised learning for speech tasks, with parameter-efficient conformer\narchitectures. Our self-supervised pre-training can reduce the need for labeled\ndata by two-thirds. On the AudioSet benchmark, we achieve a mean average\nprecision (mAP) score of 0.415, which is a new state-of-the-art on this dataset\nthrough audio-only self-supervised learning. Our fine-tuned conformers also\nsurpass or match the performance of previous systems pre-trained in a\nsupervised way on several downstream tasks. We further discuss the important\ndesign considerations for both pre-training and fine-tuning.",
          "arxiv_id": "2110.07313v3"
        },
        {
          "title": "Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition",
          "year": "2022-03",
          "abstract": "Self-supervised learning (SSL) to learn high-level speech representations has\nbeen a popular approach to building Automatic Speech Recognition (ASR) systems\nin low-resource settings. However, the common assumption made in literature is\nthat a considerable amount of unlabeled data is available for the same domain\nor language that can be leveraged for SSL pre-training, which we acknowledge is\nnot feasible in a real-world setting. In this paper, as part of the Interspeech\nGram Vaani ASR challenge, we try to study the effect of domain, language,\ndataset size, and other aspects of our upstream pre-training SSL data on the\nfinal performance low-resource downstream ASR task. We also build on the\ncontinued pre-training paradigm to study the effect of prior knowledge\npossessed by models trained using SSL. Extensive experiments and studies reveal\nthat the performance of ASR systems is susceptible to the data used for SSL\npre-training. Their performance improves with an increase in similarity and\nvolume of pre-training data. We believe our work will be helpful to the speech\ncommunity in building better ASR systems in low-resource settings and steer\nresearch towards improving generalization in SSL-based pre-training for speech\nsystems.",
          "arxiv_id": "2203.16973v4"
        },
        {
          "title": "Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR",
          "year": "2021-10",
          "abstract": "Self-supervised pre-training could effectively improve the performance of\nlow-resource automatic speech recognition (ASR). However, existing\nself-supervised pre-training are task-agnostic, i.e., could be applied to\nvarious downstream tasks. Although it enlarges the scope of its application,\nthe capacity of the pre-trained model is not fully utilized for the ASR task,\nand the learned representations may not be optimal for ASR. In this work, in\norder to build a better pre-trained model for low-resource ASR, we propose a\npre-training approach called wav2vec-S, where we use task-specific\nsemi-supervised pre-training to refine the self-supervised pre-trained model\nfor the ASR task thus more effectively utilize the capacity of the pre-trained\nmodel to generate task-specific representations for ASR. Experiments show that\ncompared to wav2vec 2.0, wav2vec-S only requires a marginal increment of\npre-training time but could significantly improve ASR performance on in-domain,\ncross-domain and cross-lingual datasets. Average relative WER reductions are\n24.5% and 6.6% for 1h and 10h fine-tuning, respectively. Furthermore, we show\nthat semi-supervised pre-training could close the representation gap between\nthe self-supervised pre-trained model and the corresponding fine-tuned model\nthrough canonical correlation analysis.",
          "arxiv_id": "2110.04484v2"
        }
      ],
      "14": [
        {
          "title": "QUCoughScope: An Artificially Intelligent Mobile Application to Detect Asymptomatic COVID-19 Patients using Cough and Breathing Sounds",
          "year": "2021-03",
          "abstract": "In the break of COVID-19 pandemic, mass testing has become essential to\nreduce the spread of the virus. Several recent studies suggest that a\nsignificant number of COVID-19 patients display no physical symptoms\nwhatsoever. Therefore, it is unlikely that these patients will undergo COVID-19\ntest, which increases their chances of unintentionally spreading the virus.\nCurrently, the primary diagnostic tool to detect COVID-19 is RT-PCR test on\ncollected respiratory specimens from the suspected case. This requires patients\nto travel to a laboratory facility to be tested, thereby potentially infecting\nothers along the way.It is evident from recent researches that asymptomatic\nCOVID-19 patients cough and breath in a different way than the healthy people.\nSeveral research groups have created mobile and web-platform for crowdsourcing\nthe symptoms, cough and breathing sounds from healthy, COVID-19 and Non-COVID\npatients. Some of these data repositories were made public. We have received\nsuch a repository from Cambridge University team under data-sharing agreement,\nwhere we have cough and breathing sound samples for 582 and 141 healthy and\nCOVID-19 patients, respectively. 87 COVID-19 patients were asymptomatic, while\nrest of them have cough. We have developed an Android application to\nautomatically screen COVID-19 from the comfort of people homes. Test subjects\ncan simply download a mobile application, enter their symptoms, record an audio\nclip of their cough and breath, and upload the data anonymously to our servers.\nOur backend server converts the audio clip to spectrogram and then apply our\nstate-of-the-art machine learning model to classify between cough sounds\nproduced by COVID-19 patients, as opposed to healthy subjects or those with\nother respiratory conditions. The system can detect asymptomatic COVID-19\npatients with a sensitivity more than 91%.",
          "arxiv_id": "2103.12063v1"
        },
        {
          "title": "AI4COVID-19: AI Enabled Preliminary Diagnosis for COVID-19 from Cough Samples via an App",
          "year": "2020-04",
          "abstract": "Background: The inability to test at scale has become humanity's Achille's\nheel in the ongoing war against the COVID-19 pandemic. A scalable screening\ntool would be a game changer. Building on the prior work on cough-based\ndiagnosis of respiratory diseases, we propose, develop and test an Artificial\nIntelligence (AI)-powered screening solution for COVID-19 infection that is\ndeployable via a smartphone app. The app, named AI4COVID-19 records and sends\nthree 3-second cough sounds to an AI engine running in the cloud, and returns a\nresult within two minutes. Methods: Cough is a symptom of over thirty\nnon-COVID-19 related medical conditions. This makes the diagnosis of a COVID-19\ninfection by cough alone an extremely challenging multidisciplinary problem. We\naddress this problem by investigating the distinctness of pathomorphological\nalterations in the respiratory system induced by COVID-19 infection when\ncompared to other respiratory infections. To overcome the COVID-19 cough\ntraining data shortage we exploit transfer learning. To reduce the misdiagnosis\nrisk stemming from the complex dimensionality of the problem, we leverage a\nmulti-pronged mediator centered risk-averse AI architecture. Results: Results\nshow AI4COVID-19 can distinguish among COVID-19 coughs and several types of\nnon-COVID-19 coughs. The accuracy is promising enough to encourage a\nlarge-scale collection of labeled cough data to gauge the generalization\ncapability of AI4COVID-19. AI4COVID-19 is not a clinical grade testing tool.\nInstead, it offers a screening tool deployable anytime, anywhere, by anyone. It\ncan also be a clinical decision assistance tool used to channel\nclinical-testing and treatment to those who need it the most, thereby saving\nmore lives.",
          "arxiv_id": "2004.01275v6"
        },
        {
          "title": "Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory Sound Data",
          "year": "2020-06",
          "abstract": "Audio signals generated by the human body (e.g., sighs, breathing, heart,\ndigestion, vibration sounds) have routinely been used by clinicians as\nindicators to diagnose disease or assess disease progression. Until recently,\nsuch signals were usually collected through manual auscultation at scheduled\nvisits. Research has now started to use digital technology to gather bodily\nsounds (e.g., from digital stethoscopes) for cardiovascular or respiratory\nexamination, which could then be used for automatic analysis. Some initial work\nshows promise in detecting diagnostic signals of COVID-19 from voice and\ncoughs. In this paper we describe our data analysis over a large-scale\ncrowdsourced dataset of respiratory sounds collected to aid diagnosis of\nCOVID-19. We use coughs and breathing to understand how discernible COVID-19\nsounds are from those in asthma or healthy controls. Our results show that even\na simple binary machine learning classifier is able to classify correctly\nhealthy and COVID-19 sounds. We also show how we distinguish a user who tested\npositive for COVID-19 and has a cough from a healthy user with a cough, and\nusers who tested positive for COVID-19 and have a cough from users with asthma\nand a cough. Our models achieve an AUC of above 80% across all tasks. These\nresults are preliminary and only scratch the surface of the potential of this\ntype of data and audio-based machine learning. This work opens the door to\nfurther investigation of how automatically analysed respiratory patterns could\nbe used as pre-screening signals to aid COVID-19 diagnosis.",
          "arxiv_id": "2006.05919v3"
        }
      ],
      "15": [
        {
          "title": "Is GAN Necessary for Mel-Spectrogram-based Neural Vocoder?",
          "year": "2025-08",
          "abstract": "Recently, mainstream mel-spectrogram-based neural vocoders rely on generative\nadversarial network (GAN) for high-fidelity speech generation, e.g., HiFi-GAN\nand BigVGAN. However, the use of GAN restricts training efficiency and model\ncomplexity. Therefore, this paper proposes a novel FreeGAN vocoder, aiming to\nanswer the question of whether GAN is necessary for mel-spectrogram-based\nneural vocoders. The FreeGAN employs an amplitude-phase serial prediction\nframework, eliminating the need for GAN training. It incorporates amplitude\nprior input, SNAKE-ConvNeXt v2 backbone and frequency-weighted anti-wrapping\nphase loss to compensate for the performance loss caused by the absence of GAN.\nExperimental results confirm that the speech quality of FreeGAN is comparable\nto that of advanced GAN-based vocoders, while significantly improving training\nefficiency and complexity. Other explicit-phase-prediction-based neural\nvocoders can also work without GAN, leveraging our proposed methods.",
          "arxiv_id": "2508.07711v1"
        },
        {
          "title": "CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model",
          "year": "2023-05",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have shown promising\nperformance for speech synthesis. However, a large number of iterative steps\nare required to achieve high sample quality, which restricts the inference\nspeed. Maintaining sample quality while increasing sampling speed has become a\nchallenging task. In this paper, we propose a \"Co\"nsistency \"Mo\"del-based\n\"Speech\" synthesis method, CoMoSpeech, which achieve speech synthesis through a\nsingle diffusion sampling step while achieving high audio quality. The\nconsistency constraint is applied to distill a consistency model from a\nwell-designed diffusion-based teacher model, which ultimately yields superior\nperformances in the distilled CoMoSpeech. Our experiments show that by\ngenerating audio recordings by a single sampling step, the CoMoSpeech achieves\nan inference speed more than 150 times faster than real-time on a single NVIDIA\nA100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based\nspeech synthesis truly practical. Meanwhile, objective and subjective\nevaluations on text-to-speech and singing voice synthesis show that the\nproposed teacher models yield the best audio quality, and the one-step sampling\nbased CoMoSpeech achieves the best inference speed with better or comparable\naudio quality to other conventional multi-step diffusion model baselines. Audio\nsamples are available at https://comospeech.github.io/.",
          "arxiv_id": "2305.06908v4"
        },
        {
          "title": "FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis",
          "year": "2022-04",
          "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved\nleading performances in many generative tasks. However, the inherited iterative\nsampling process costs hindered their applications to speech synthesis. This\npaper proposes FastDiff, a fast conditional diffusion model for high-quality\nspeech synthesis. FastDiff employs a stack of time-aware location-variable\nconvolutions of diverse receptive field patterns to efficiently model long-term\ntime dependencies with adaptive conditions. A noise schedule predictor is also\nadopted to reduce the sampling steps without sacrificing the generation\nquality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer,\nFastDiff-TTS, which generates high-fidelity speech waveforms without any\nintermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff\ndemonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech\nsamples. Also, FastDiff enables a sampling speed of 58x faster than real-time\non a V100 GPU, making diffusion models practically applicable to speech\nsynthesis deployment for the first time. We further show that FastDiff\ngeneralized well to the mel-spectrogram inversion of unseen speakers, and\nFastDiff-TTS outperformed other competing methods in end-to-end text-to-speech\nsynthesis. Audio samples are available at \\url{https://FastDiff.github.io/}.",
          "arxiv_id": "2204.09934v1"
        }
      ],
      "16": [
        {
          "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
          "year": "2023-12",
          "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to\narbitrary unseen target speaker timbre, while keeping the linguistic content\nunchanged. Although the voice of generated speech can be controlled by\nproviding the speaker embedding of the target speaker, the speaker similarity\nstill lags behind the ground truth recordings. In this paper, we propose\nSEF-VC, a speaker embedding free voice conversion model, which is designed to\nlearn and incorporate speaker timbre from reference speech via a powerful\nposition-agnostic cross-attention mechanism, and then reconstruct waveform from\nHuBERT semantic tokens in a non-autoregressive manner. The concise design of\nSEF-VC enhances its training stability and voice conversion performance.\nObjective and subjective evaluations demonstrate the superiority of SEF-VC to\ngenerate high-quality speech with better similarity to target reference than\nstrong zero-shot VC baselines, even for very short reference speeches.",
          "arxiv_id": "2312.08676v2"
        },
        {
          "title": "Subband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion",
          "year": "2022-07",
          "abstract": "Voice conversion is to generate a new speech with the source content and a\ntarget voice style. In this paper, we focus on one general setting, i.e.,\nnon-parallel many-to-many voice conversion, which is close to the real-world\nscenario. As the name implies, non-parallel many-to-many voice conversion does\nnot require the paired source and reference speeches and can be applied to\narbitrary voice transfer. In recent years, Generative Adversarial Networks\n(GANs) and other techniques such as Conditional Variational Autoencoders\n(CVAEs) have made considerable progress in this field. However, due to the\nsophistication of voice conversion, the style similarity of the converted\nspeech is still unsatisfactory. Inspired by the inherent structure of\nmel-spectrogram, we propose a new voice conversion framework, i.e.,\nSubband-based Generative Adversarial Network for Voice Conversion (SGAN-VC).\nSGAN-VC converts each subband content of the source speech separately by\nexplicitly utilizing the spatial characteristics between different subbands.\nSGAN-VC contains one style encoder, one content encoder, and one decoder. In\nparticular, the style encoder network is designed to learn style codes for\ndifferent subbands of the target speaker. The content encoder network can\ncapture the content information on the source speech. Finally, the decoder\ngenerates particular subband content. In addition, we propose a pitch-shift\nmodule to fine-tune the pitch of the source speaker, making the converted tone\nmore accurate and explainable. Extensive experiments demonstrate that the\nproposed approach achieves state-of-the-art performance on VCTK Corpus and\nAISHELL3 datasets both qualitatively and quantitatively, whether on seen or\nunseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data\neven exceeds that of StarGANv2-VC with ASR network assistance.",
          "arxiv_id": "2207.06057v2"
        },
        {
          "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
          "year": "2023-10",
          "abstract": "Voice conversion aims to convert source speech into a target voice using\nrecordings of the target speaker as a reference. Newer models are producing\nincreasingly realistic output. But what happens when models are fed with\nnon-standard data, such as speech from a user with a speech impairment? We\ninvestigate how a recent voice conversion model performs on non-standard\ndownstream voice conversion tasks. We use a simple but robust approach called\nk-nearest neighbors voice conversion (kNN-VC). We look at four non-standard\napplications: stuttered voice conversion, cross-lingual voice conversion,\nmusical instrument conversion, and text-to-voice conversion. The latter\ninvolves converting to a target voice specified through a text description,\ne.g. \"a young man with a high-pitched voice\". Compared to an established\nbaseline, we find that kNN-VC retains high performance in stuttered and\ncross-lingual voice conversion. Results are more mixed for the musical\ninstrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some\ninstruments like drums but not on others. Nevertheless, this shows that voice\nconversion models - and kNN-VC in particular - are increasingly applicable in a\nrange of non-standard downstream tasks. But there are still limitations when\nsamples are very far from the training distribution. Code, samples, trained\nmodels: https://rf5.github.io/sacair2023-knnvc-demo/.",
          "arxiv_id": "2310.08104v1"
        }
      ],
      "17": [
        {
          "title": "Exploring Sequence-to-Sequence Transformer-Transducer Models for Keyword Spotting",
          "year": "2022-11",
          "abstract": "In this paper, we present a novel approach to adapt a sequence-to-sequence\nTransformer-Transducer ASR system to the keyword spotting (KWS) task. We\nachieve this by replacing the keyword in the text transcription with a special\ntoken <kw> and training the system to detect the <kw> token in an audio stream.\nAt inference time, we create a decision function inspired by conventional KWS\napproaches, to make our approach more suitable for the KWS task. Furthermore,\nwe introduce a specific keyword spotting loss by adapting the\nsequence-discriminative Minimum Bayes-Risk training technique. We find that our\napproach significantly outperforms ASR based KWS systems. When compared with a\nconventional keyword spotting system, our proposal has similar performance\nwhile bringing the advantages and flexibility of sequence-to-sequence training.\nAdditionally, when combined with the conventional KWS system, our approach can\nimprove the performance at any operation point.",
          "arxiv_id": "2211.06478v1"
        },
        {
          "title": "Progressive Continual Learning for Spoken Keyword Spotting",
          "year": "2022-01",
          "abstract": "Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. To tackle such challenges, we propose a\nprogressive continual learning strategy for small-footprint spoken keyword\nspotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a\nnetwork instantiator to generate the task-specific sub-networks for remembering\npreviously learned keywords. As a result, the PCL-KWS approach incrementally\nlearns new keywords without forgetting prior knowledge. Besides, the\nkeyword-aware network scaling mechanism of PCL-KWS constrains the growth of\nmodel parameters while achieving high performance. Experimental results show\nthat after learning five new tasks sequentially, our proposed PCL-KWS approach\narchives the new state-of-the-art performance of 92.8% average accuracy for all\nthe tasks on Google Speech Command dataset compared with other baselines.",
          "arxiv_id": "2201.12546v2"
        },
        {
          "title": "Personalized Keyword Spotting through Multi-task Learning",
          "year": "2022-06",
          "abstract": "Keyword spotting (KWS) plays an essential role in enabling speech-based user\ninteraction on smart devices, and conventional KWS (C-KWS) approaches have\nconcentrated on detecting user-agnostic pre-defined keywords. However, in\npractice, most user interactions come from target users enrolled in the device\nwhich motivates to construct personalized keyword spotting. We design two\npersonalized KWS tasks; (1) Target user Biased KWS (TB-KWS) and (2) Target user\nOnly KWS (TO-KWS). To solve the tasks, we propose personalized keyword spotting\nthrough multi-task learning (PK-MTL) that consists of multi-task learning and\ntask-adaptation. First, we introduce applying multi-task learning on keyword\nspotting and speaker verification to leverage user information to the keyword\nspotting system. Next, we design task-specific scoring functions to adapt to\nthe personalized KWS tasks thoroughly. We evaluate our framework on\nconventional and personalized scenarios, and the results show that PK-MTL can\ndramatically reduce the false alarm rate, especially in various practical\nscenarios.",
          "arxiv_id": "2206.13708v1"
        }
      ],
      "18": [
        {
          "title": "An Automatic Speech Recognition System for Bengali Language based on Wav2Vec2 and Transfer Learning",
          "year": "2022-09",
          "abstract": "An independent, automated method of decoding and transcribing oral speech is\nknown as automatic speech recognition (ASR). A typical ASR system extracts\nfeature from audio recordings or streams and run one or more algorithms to map\nthe features to corresponding texts. Numerous of research has been done in the\nfield of speech signal processing in recent years. When given adequate\nresources, both conventional ASR and emerging end-to-end (E2E) speech\nrecognition have produced promising results. However, for low-resource\nlanguages like Bengali, the current state of ASR lags behind, although the low\nresource state does not reflect upon the fact that this language is spoken by\nover 500 million people all over the world. Despite its popularity, there\naren't many diverse open-source datasets available, which makes it difficult to\nconduct research on Bengali speech recognition systems. This paper is a part of\nthe competition named `BUET CSE Fest DL Sprint'. The purpose of this paper is\nto improve the speech recognition performance of the Bengali language by\nadopting speech recognition technology on the E2E structure based on the\ntransfer learning framework. The proposed method effectively models the Bengali\nlanguage and achieves 3.819 score in `Levenshtein Mean Distance' on the test\ndataset of 7747 samples, when only 1000 samples of train dataset were used to\ntrain.",
          "arxiv_id": "2209.08119v2"
        },
        {
          "title": "Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages",
          "year": "2024-11",
          "abstract": "This paper presents a novel multistage fine-tuning strategy designed to\nenhance automatic speech recognition (ASR) performance in low-resource\nlanguages using OpenAI's Whisper model. In this approach we aim to build ASR\nmodel for languages with limited digital resources by sequentially adapting the\nmodel across linguistically similar languages. We experimented this on the\nMalasar language, a Dravidian language spoken by approximately ten thousand\npeople in the Western Ghats of South India. Malasar language faces critical\nchallenges for technological intervention due to its lack of a native script\nand absence of digital or spoken data resources. Working in collaboration with\nWycliffe India and Malasar community members, we created a spoken Malasar\ncorpus paired with transcription in Tamil script, a closely related major\nlanguage. In our approach to build ASR model for Malasar, we first build an\nintermediate Tamil ASR, leveraging higher data availability for Tamil annotated\nspeech. This intermediate model is subsequently fine-tuned on Malasar data,\nallowing for more effective ASR adaptation despite limited resources. The\nmultistage fine-tuning strategy demonstrated significant improvements over\ndirect fine-tuning on Malasar data alone, achieving a word error rate (WER) of\n51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning\nmethod. Further a WER reduction to 47.3% was achieved through punctuation\nremoval in post-processing, which addresses formatting inconsistencies that\nimpact evaluation. Our results underscore the effectiveness of sequential\nmultistage fine-tuning combined with targeted post-processing as a scalable\nstrategy for ASR system development in low-resource languages, especially where\nlinguistic similarities can be leveraged to bridge gaps in training data.",
          "arxiv_id": "2411.04573v1"
        },
        {
          "title": "Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech Recognition",
          "year": "2024-09",
          "abstract": "This paper addresses the challenge of integrating low-resource languages into\nmultilingual automatic speech recognition (ASR) systems. We introduce a novel\napplication of weighted cross-entropy, typically used for unbalanced datasets,\nto facilitate the integration of low-resource languages into pre-trained\nmultilingual ASR models within the context of continual multilingual learning.\nWe fine-tune the Whisper multilingual ASR model on five high-resource languages\nand one low-resource language, employing language-weighted dynamic\ncross-entropy and data augmentation. The results show a remarkable 6.69% word\nerror rate (WER) reduction for the low-resource language compared to the\nfine-tuned model without applying our approach, and a 48.86% WER reduction\ncompared to the original Whisper model. In addition, our approach yields an\naverage WER reduction of 3.29% across the six languages, showing no degradation\nfor the high-resource languages.",
          "arxiv_id": "2409.16954v1"
        }
      ],
      "19": [
        {
          "title": "JVS-MuSiC: Japanese multispeaker singing-voice corpus",
          "year": "2020-01",
          "abstract": "Thanks to developments in machine learning techniques, it has become possible\nto synthesize high-quality singing voices of a single singer. An open\nmultispeaker singing-voice corpus would further accelerate the research in\nsinging-voice synthesis. However, conventional singing-voice corpora only\nconsist of the singing voices of a single singer. We designed a Japanese\nmultispeaker singing-voice corpus called \"JVS-MuSiC\" with the aim to analyze\nand synthesize a variety of voices. The corpus consists of 100 singers'\nrecordings of the same song, Katatsumuri, which is a Japanese children's song.\nIt also includes another song that is different for each singer. In this paper,\nwe describe the design of the corpus and experimental analyses using JVS-MuSiC.\nWe investigated the relationship between 1) the similarity of singing voices\nand perceptual oneness of unison singing voices and between 2) the similarity\nof singing voices and that of speech. The results suggest that 1) there is a\npositive and moderate correlation between singing-voice similarity and the\noneness of unison and that 2) the correlation between singing-voice similarity\nand speech similarity is weak. This corpus is freely available online.",
          "arxiv_id": "2001.07044v1"
        },
        {
          "title": "Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System",
          "year": "2021-08",
          "abstract": "This paper presents Sinsy, a deep neural network (DNN)-based singing voice\nsynthesis (SVS) system. In recent years, DNNs have been utilized in statistical\nparametric SVS systems, and DNN-based SVS systems have demonstrated better\nperformance than conventional hidden Markov model-based ones. SVS systems are\nrequired to synthesize a singing voice with pitch and timing that strictly\nfollow a given musical score. Additionally, singing expressions that are not\ndescribed on the musical score, such as vibrato and timing fluctuations, should\nbe reproduced. The proposed system is composed of four modules: a time-lag\nmodel, a duration model, an acoustic model, and a vocoder, and singing voices\ncan be synthesized taking these characteristics of singing voices into account.\nTo better model a singing voice, the proposed system incorporates improved\napproaches to modeling pitch and vibrato and better training criteria into the\nacoustic model. In addition, we incorporated PeriodNet, a non-autoregressive\nneural vocoder with robustness for the pitch, into our systems to generate a\nhigh-fidelity singing voice waveform. Moreover, we propose automatic pitch\ncorrection techniques for DNN-based SVS to synthesize singing voices with\ncorrect pitch even if the training data has out-of-tune phrases. Experimental\nresults show our system can synthesize a singing voice with better timing, more\nnatural vibrato, and correct pitch, and it can achieve better mean opinion\nscores in subjective evaluation tests.",
          "arxiv_id": "2108.02776v1"
        },
        {
          "title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis",
          "year": "2023-12",
          "abstract": "Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://aaronz345.github.io/StyleSingerDemo/.",
          "arxiv_id": "2312.10741v5"
        }
      ],
      "20": [
        {
          "title": "Tackling data scarcity in speech translation using zero-shot multilingual machine translation techniques",
          "year": "2022-01",
          "abstract": "Recently, end-to-end speech translation (ST) has gained significant attention\nas it avoids error propagation. However, the approach suffers from data\nscarcity. It heavily depends on direct ST data and is less efficient in making\nuse of speech transcription and text translation data, which is often more\neasily available. In the related field of multilingual text translation,\nseveral techniques have been proposed for zero-shot translation. A main idea is\nto increase the similarity of semantically similar sentences in different\nlanguages. We investigate whether these ideas can be applied to speech\ntranslation, by building ST models trained on speech transcription and text\ntranslation data. We investigate the effects of data augmentation and auxiliary\nloss function. The techniques were successfully applied to few-shot ST using\nlimited ST data, with improvements of up to +12.9 BLEU points compared to\ndirect end-to-end ST and +3.1 BLEU points compared to ST models fine-tuned from\nASR model.",
          "arxiv_id": "2201.11172v1"
        },
        {
          "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
          "year": "2025-02",
          "abstract": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code.",
          "arxiv_id": "2502.03382v2"
        },
        {
          "title": "CVSS Corpus and Massively Multilingual Speech-to-Speech Translation",
          "year": "2022-01",
          "abstract": "We introduce CVSS, a massively multilingual-to-English speech-to-speech\ntranslation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21\nlanguages into English. CVSS is derived from the Common Voice speech corpus and\nthe CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the\ntranslation text from CoVoST 2 into speech using state-of-the-art TTS systems.\nTwo versions of translation speeches are provided: 1) CVSS-C: All the\ntranslation speeches are in a single high-quality canonical voice; 2) CVSS-T:\nThe translation speeches are in voices transferred from the corresponding\nsource speeches. In addition, CVSS provides normalized translation text which\nmatches the pronunciation in the translation speech. On each version of CVSS,\nwe built baseline multilingual direct S2ST models and cascade S2ST models,\nverifying the effectiveness of the corpus. To build strong cascade S2ST\nbaselines, we trained an ST model on CoVoST 2, which outperforms the previous\nstate-of-the-art trained on the corpus without extra data by 5.8 BLEU.\nNevertheless, the performance of the direct S2ST models approaches the strong\ncascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU\ndifference on ASR transcribed translation when initialized from matching ST\nmodels.",
          "arxiv_id": "2201.03713v3"
        }
      ],
      "21": [
        {
          "title": "Unsupervised classification to improve the quality of a bird song recording dataset",
          "year": "2023-02",
          "abstract": "Open audio databases such as Xeno-Canto are widely used to build datasets to\nexplore bird song repertoire or to train models for automatic bird sound\nclassification by deep learning algorithms. However, such databases suffer from\nthe fact that bird sounds are weakly labelled: a species name is attributed to\neach audio recording without timestamps that provide the temporal localization\nof the bird song of interest. Manual annotations can solve this issue, but they\nare time consuming, expert-dependent, and cannot run on large datasets. Another\nsolution consists in using a labelling function that automatically segments\naudio recordings before assigning a label to each segmented audio sample.\nAlthough labelling functions were introduced to expedite strong label\nassignment, their classification performance remains mostly unknown. To address\nthis issue and reduce label noise (wrong label assignment) in large bird song\ndatasets, we introduce a data-centric novel labelling function composed of\nthree successive steps: 1) time-frequency sound unit segmentation, 2) feature\ncomputation for each sound unit, and 3) classification of each sound unit as\nbird song or noise with either an unsupervised DBSCAN algorithm or the\nsupervised BirdNET neural network. The labelling function was optimized,\nvalidated, and tested on the songs of 44 West-Palearctic common bird species.\nWe first showed that the segmentation of bird songs alone aggregated from 10%\nto 83% of label noise depending on the species. We also demonstrated that our\nlabelling function was able to significantly reduce the initial label noise\npresent in the dataset by up to a factor of three. Finally, we discuss\ndifferent opportunities to design suitable labelling functions to build\nhigh-quality animal vocalizations with minimum expert annotation effort.",
          "arxiv_id": "2302.07560v1"
        },
        {
          "title": "Global birdsong embeddings enable superior transfer learning for bioacoustic classification",
          "year": "2023-07",
          "abstract": "Automated bioacoustic analysis aids understanding and protection of both\nmarine and terrestrial animals and their habitats across extensive\nspatiotemporal scales, and typically involves analyzing vast collections of\nacoustic data. With the advent of deep learning models, classification of\nimportant signals from these datasets has markedly improved. These models power\ncritical data analyses for research and decision-making in biodiversity\nmonitoring, animal behaviour studies, and natural resource management. However,\ndeep learning models are often data-hungry and require a significant amount of\nlabeled training data to perform well. While sufficient training data is\navailable for certain taxonomic groups (e.g., common bird species), many\nclasses (such as rare and endangered species, many non-bird taxa, and\ncall-type) lack enough data to train a robust model from scratch. This study\ninvestigates the utility of feature embeddings extracted from audio\nclassification models to identify bioacoustic classes other than the ones these\nmodels were originally trained on. We evaluate models on diverse datasets,\nincluding different bird calls and dialect types, bat calls, marine mammals\ncalls, and amphibians calls. The embeddings extracted from the models trained\non bird vocalization data consistently allowed higher quality classification\nthan the embeddings trained on general audio datasets. The results of this\nstudy indicate that high-quality feature embeddings from large-scale acoustic\nbird classifiers can be harnessed for few-shot transfer learning, enabling the\nlearning of new classes from a limited quantity of training data. Our findings\nreveal the potential for efficient analyses of novel bioacoustic tasks, even in\nscenarios where available training data is limited to a few samples.",
          "arxiv_id": "2307.06292v2"
        },
        {
          "title": "Unsupervised outlier detection to improve bird audio dataset labels",
          "year": "2025-04",
          "abstract": "The Xeno-Canto bird audio repository is an invaluable resource for those\ninterested in vocalizations and other sounds made by birds around the world.\nThis is particularly the case for machine learning researchers attempting to\nimprove on the bird species recognition accuracy of classification models.\nHowever, the task of extracting labeled datasets from the recordings found in\nthis crowd-sourced repository faces several challenges. One challenge of\nparticular significance to machine learning practitioners is that one bird\nspecies label is applied to each audio recording, but frequently other sounds\nare also captured including other bird species, other animal sounds,\nanthropogenic and other ambient sounds. These non-target bird species sounds\ncan result in dataset labeling discrepancies referred to as label noise. In\nthis work we present a cleaning process consisting of audio preprocessing\nfollowed by dimensionality reduction and unsupervised outlier detection (UOD)\nto reduce the label noise in a dataset derived from Xeno-Canto recordings. We\ninvestigate three neural network dimensionality reduction techniques: two\nflavors of convolutional autoencoders and variational deep embedding (VaDE\n(Jiang, 2017)). While both methods show some degree of effectiveness at\ndetecting outliers for most bird species datasets, we found significant\nvariation in the performance of the methods from one species to the next. We\nbelieve that the results of this investigation demonstrate that the application\nof our cleaning process can meaningfully reduce the label noise of bird species\ndatasets derived from Xeno-Canto audio repository but results vary across\nspecies.",
          "arxiv_id": "2504.18650v1"
        }
      ],
      "22": [
        {
          "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction",
          "year": "2022-01",
          "abstract": "Video recordings of speech contain correlated audio and visual information,\nproviding a strong signal for speech representation learning from the speaker's\nlip movements and the produced sound. We introduce Audio-Visual Hidden Unit\nBERT (AV-HuBERT), a self-supervised representation learning framework for\naudio-visual speech, which masks multi-stream video input and predicts\nautomatically discovered and iteratively refined multimodal hidden units.\nAV-HuBERT learns powerful audio-visual speech representation benefiting both\nlip-reading and automatic speech recognition. On the largest public lip-reading\nbenchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of\nlabeled data, outperforming the former state-of-the-art approach (33.6%)\ntrained with a thousand times more transcribed video data (31K hours). The\nlip-reading WER is further reduced to 26.9% when using all 433 hours of labeled\ndata from LRS3 and combined with self-training. Using our audio-visual\nrepresentation on the same benchmark for audio-only speech recognition leads to\na 40% relative WER reduction over the state-of-the-art performance (1.3% vs\n2.3%). Our code and models are available at\nhttps://github.com/facebookresearch/av_hubert",
          "arxiv_id": "2201.02184v2"
        },
        {
          "title": "Learning Contextually Fused Audio-visual Representations for Audio-visual Speech Recognition",
          "year": "2022-02",
          "abstract": "With the advance in self-supervised learning for audio and visual modalities,\nit has become possible to learn a robust audio-visual speech representation.\nThis would be beneficial for improving the audio-visual speech recognition\n(AVSR) performance, as the multi-modal inputs contain more fruitful information\nin principle. In this paper, based on existing self-supervised representation\nlearning methods for audio modality, we therefore propose an audio-visual\nrepresentation learning approach. The proposed approach explores both the\ncomplementarity of audio-visual modalities and long-term context dependency\nusing a transformer-based fusion module and a flexible masking strategy. After\npre-training, the model is able to extract fused representations required by\nAVSR. Without loss of generality, it can be applied to single-modal tasks, e.g.\naudio/visual speech recognition by simply masking out one modality in the\nfusion module. The proposed pre-trained model is evaluated on speech\nrecognition and lipreading tasks using one or two modalities, where the\nsuperiority is revealed.",
          "arxiv_id": "2202.07428v2"
        },
        {
          "title": "AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition",
          "year": "2023-09",
          "abstract": "Audio-visual speech contains synchronized audio and visual information that\nprovides cross-modal supervision to learn representations for both automatic\nspeech recognition (ASR) and visual speech recognition (VSR). We introduce\ncontinuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a\nsemi-supervised method to train an audio-visual speech recognition (AVSR) model\non a combination of labeled and unlabeled videos with continuously regenerated\npseudo-labels. Our models are trained for speech recognition from audio-visual\ninputs and can perform speech recognition using both audio and visual\nmodalities, or only one modality. Our method uses the same audio-visual model\nfor both supervised training and pseudo-label generation, mitigating the need\nfor external speech recognition models to generate pseudo-labels. AV-CPL\nobtains significant improvements in VSR performance on the LRS3 dataset while\nmaintaining practical ASR and AVSR performance. Finally, using visual-only\nspeech data, our method is able to leverage unlabeled visual speech to improve\nVSR.",
          "arxiv_id": "2309.17395v1"
        }
      ],
      "23": [
        {
          "title": "Separate This, and All of these Things Around It: Music Source Separation via Hyperellipsoidal Queries",
          "year": "2025-01",
          "abstract": "Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.",
          "arxiv_id": "2501.16171v1"
        },
        {
          "title": "Task-Aware Unified Source Separation",
          "year": "2024-10",
          "abstract": "Several attempts have been made to handle multiple source separation tasks\nsuch as speech enhancement, speech separation, sound event separation, music\nsource separation (MSS), or cinematic audio source separation (CASS) with a\nsingle model. These models are trained on large-scale data including speech,\ninstruments, or sound events and can often successfully separate a wide range\nof sources. However, it is still challenging for such models to cover all\nseparation tasks because some of them are contradictory (e.g., musical\ninstruments are separated in MSS while they have to be grouped in CASS). To\novercome this issue and support all the major separation tasks, we propose a\ntask-aware unified source separation (TUSS) model. The model uses a variable\nnumber of learnable prompts to specify which source to separate, and changes\nits behavior depending on the given prompts, enabling it to handle all the\nmajor separation tasks including contradictory ones. Experimental results\ndemonstrate that the proposed TUSS model successfully handles the five major\nseparation tasks mentioned earlier. We also provide some audio examples,\nincluding both synthetic mixtures and real recordings, to demonstrate how\nflexibly the TUSS model changes its behavior at inference depending on the\nprompts.",
          "arxiv_id": "2410.23987v1"
        },
        {
          "title": "Universal Source Separation with Weakly Labelled Data",
          "year": "2023-05",
          "abstract": "Universal source separation (USS) is a fundamental research task for\ncomputational auditory scene analysis, which aims to separate mono recordings\ninto individual source tracks. There are three potential challenges awaiting\nthe solution to the audio source separation task. First, previous audio source\nseparation systems mainly focus on separating one or a limited number of\nspecific sources. There is a lack of research on building a unified system that\ncan separate arbitrary sources via a single model. Second, most previous\nsystems require clean source data to train a separator, while clean source data\nare scarce. Third, there is a lack of USS system that can automatically detect\nand separate active sound classes in a hierarchical level. To use large-scale\nweakly labeled/unlabeled audio data for audio source separation, we propose a\nuniversal audio source separation framework containing: 1) an audio tagging\nmodel trained on weakly labeled data as a query net; and 2) a conditional\nsource separation model that takes query net outputs as conditions to separate\narbitrary sound sources. We investigate various query nets, source separation\nmodels, and training strategies and propose a hierarchical USS strategy to\nautomatically detect and separate sound classes from the AudioSet ontology. By\nsolely leveraging the weakly labelled AudioSet, our USS system is successful in\nseparating a wide variety of sound classes, including sound event separation,\nmusic source separation, and speech enhancement. The USS system achieves an\naverage signal-to-distortion ratio improvement (SDRi) of 5.57 dB over 527 sound\nclasses of AudioSet; 10.57 dB on the DCASE 2018 Task 2 dataset; 8.12 dB on the\nMUSDB18 dataset; an SDRi of 7.28 dB on the Slakh2100 dataset; and an SSNR of\n9.00 dB on the voicebank-demand dataset. We release the source code at\nhttps://github.com/bytedance/uss",
          "arxiv_id": "2305.07447v1"
        }
      ],
      "24": [
        {
          "title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation",
          "year": "2023-03",
          "abstract": "Speech-driven 3D face animation aims to generate realistic facial expressions\nthat match the speech content and emotion. However, existing methods often\nneglect emotional facial expressions or fail to disentangle them from speech\ncontent. To address this issue, this paper proposes an end-to-end neural\nnetwork to disentangle different emotions in speech so as to generate rich 3D\nfacial expressions. Specifically, we introduce the emotion disentangling\nencoder (EDE) to disentangle the emotion and content in the speech by\ncross-reconstructed speech signals with different emotion labels. Then an\nemotion-guided feature fusion decoder is employed to generate a 3D talking face\nwith enhanced emotion. The decoder is driven by the disentangled identity,\nemotional, and content embeddings so as to generate controllable personal and\nemotional styles. Finally, considering the scarcity of the 3D emotional talking\nface data, we resort to the supervision of facial blendshapes, which enables\nthe reconstruction of plausible 3D faces from 2D emotional data, and contribute\na large-scale 3D emotional talking face dataset (3D-ETF) to train the network.\nOur experiments and user studies demonstrate that our approach outperforms\nstate-of-the-art methods and exhibits more diverse facial movements. We\nrecommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/emotalk",
          "arxiv_id": "2303.11089v2"
        },
        {
          "title": "StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation",
          "year": "2022-08",
          "abstract": "We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.",
          "arxiv_id": "2208.10922v2"
        },
        {
          "title": "AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person",
          "year": "2021-08",
          "abstract": "Automatically generating videos in which synthesized speech is synchronized\nwith lip movements in a talking head has great potential in many human-computer\ninteraction scenarios. In this paper, we present an automatic method to\ngenerate synchronized speech and talking-head videos on the basis of text and a\nsingle face image of an arbitrary person as input. In contrast to previous\ntext-driven talking head generation methods, which can only synthesize the\nvoice of a specific person, the proposed method is capable of synthesizing\nspeech for any person that is inaccessible in the training stage. Specifically,\nthe proposed method decomposes the generation of synchronized speech and\ntalking head videos into two stages, i.e., a text-to-speech (TTS) stage and a\nspeech-driven talking head generation stage. The proposed TTS module is a\nface-conditioned multi-speaker TTS model that gets the speaker identity\ninformation from face images instead of speech, which allows us to synthesize a\npersonalized voice on the basis of the input face image. To generate the\ntalking head videos from the face images, a facial landmark-based method that\ncan predict both lip movements and head rotations is proposed. Extensive\nexperiments demonstrate that the proposed method is able to generate\nsynchronized speech and talking head videos for arbitrary persons and\nnon-persons. Synthesized speech shows consistency with the given face regarding\nto the synthesized voice's timbre and one's appearance in the image, and the\nproposed landmark-based talking head method outperforms the state-of-the-art\nlandmark-based method on generating natural talking head videos.",
          "arxiv_id": "2108.04325v2"
        }
      ],
      "25": [
        {
          "title": "SAMOS: A Neural MOS Prediction Model Leveraging Semantic Representations and Acoustic Features",
          "year": "2024-11",
          "abstract": "Assessing the naturalness of speech using mean opinion score (MOS) prediction\nmodels has positive implications for the automatic evaluation of speech\nsynthesis systems. Early MOS prediction models took the raw waveform or\namplitude spectrum of speech as input, whereas more advanced methods employed\nself-supervised-learning (SSL) based models to extract semantic representations\nfrom speech for MOS prediction. These methods utilized limited aspects of\nspeech information for MOS prediction, resulting in restricted prediction\naccuracy. Therefore, in this paper, we propose SAMOS, a MOS prediction model\nthat leverages both Semantic and Acoustic information of speech to be assessed.\nSpecifically, the proposed SAMOS leverages a pretrained wav2vec2 to extract\nsemantic representations and uses the feature extractor of a pretrained\nBiVocoder to extract acoustic features. These two types of features are then\nfed into the prediction network, which includes multi-task heads and an\naggregation layer, to obtain the final MOS score. Experimental results\ndemonstrate that the proposed SAMOS outperforms current state-of-the-art MOS\nprediction models on the BVCC dataset and performs comparable performance on\nthe BC2019 dataset, according to the results of system-level evaluation\nmetrics.",
          "arxiv_id": "2411.11232v1"
        },
        {
          "title": "MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment",
          "year": "2021-04",
          "abstract": "The objective speech quality assessment is usually conducted by comparing\nreceived speech signal with its clean reference, while human beings are capable\nof evaluating the speech quality without any reference, such as in the mean\nopinion score (MOS) tests. Non-intrusive speech quality assessment has\nattracted much attention recently due to the lack of access to clean reference\nsignals for objective evaluations in real scenarios. In this paper, we propose\na novel non-intrusive speech quality measurement model, MetricNet, which\nleverages label distribution learning and joint speech reconstruction learning\nto achieve significantly improved performance compared to the existing\nnon-intrusive speech quality measurement models. We demonstrate that the\nproposed approach yields promisingly high correlation to the intrusive\nobjective evaluation of speech quality on clean, noisy and processed speech\ndata.",
          "arxiv_id": "2104.01227v1"
        },
        {
          "title": "Selecting N-lowest scores for training MOS prediction models",
          "year": "2025-06",
          "abstract": "The automatic speech quality assessment (SQA) has been extensively studied to\npredict the speech quality without time-consuming questionnaires. Recently,\nneural-based SQA models have been actively developed for speech samples\nproduced by text-to-speech or voice conversion, with a primary focus on\ntraining mean opinion score (MOS) prediction models. The quality of each speech\nsample may not be consistent across the entire duration, and it remains unclear\nwhich segments of the speech receive the primary focus from humans when\nassigning subjective evaluation for MOS calculation. We hypothesize that when\nhumans rate speech, they tend to assign more weight to low-quality speech\nsegments, and the variance in ratings for each sample is mainly due to\naccidental assignment of higher scores when overlooking the poor quality speech\nsegments. Motivated by the hypothesis, we analyze the VCC2018 and BVCC\ndatasets. Based on the hypothesis, we propose the more reliable representative\nvalue N_low-MOS, the mean of the $N$-lowest opinion scores. Our experiments\nshow that LCC and SRCC improve compared to regular MOS when employing N_low-MOS\nto MOSNet training. This result suggests that N_low-MOS is a more intrinsic\nrepresentative value of subjective speech quality and makes MOSNet a better\ncomparator of VC models.",
          "arxiv_id": "2506.18326v1"
        }
      ],
      "26": [
        {
          "title": "Comparison of linear and nonlinear methods for decoding selective attention to speech from ear-EEG recordings",
          "year": "2024-01",
          "abstract": "Many people with hearing loss struggle to comprehend speech in crowded\nauditory scenes, even when they are using hearing aids. It has recently been\ndemonstrated that the focus of a listener's selective attention to speech can\nbe decoded from their electroencephalography (EEG) recordings, raising the\nprospect of smart EEG-steered hearing aids which restore speech comprehension\nin adverse acoustic environments (such as the cocktail party). To this end, we\nhere assess the feasibility of using a novel, ultra-wearable ear-EEG device to\nclassify the selective attention of normal-hearing listeners who participated\nin a two-talker competing-speakers experiment.\n  Eighteen participants took part in a diotic listening task, whereby they were\nasked to attend to one narrator whilst ignoring the other. Encoding models were\nestimated from the recorded signals, and these confirmed that the device has\nthe ability to capture auditory responses that are consistent with those\nreported in high-density EEG studies. Several state-of-the-art auditory\nattention decoding algorithms were next compared, including\nstimulus-reconstruction algorithms based on linear regression as well as\nnon-linear deep neural networks, and canonical correlation analysis (CCA).\n  Meaningful markers of selective auditory attention could be extracted from\nthe ear-EEG signals of all 18 participants, even when those markers were\nderived from relatively short EEG segments of just five seconds in duration.\nAlgorithms which related the EEG signals to the rising edges of the speech\ntemporal envelope (onset envelope) were more successful than those which made\nuse of the temporal envelope itself. The CCA algorithm achieved the highest\nmean attention decoding accuracy, although differences between the performances\nof the three algorithms were both small and not statistically significant when\nEEG segments of short durations were employed.",
          "arxiv_id": "2401.05187v2"
        },
        {
          "title": "Frequency-Based Alignment of EEG and Audio Signals Using Contrastive Learning and SincNet for Auditory Attention Detection",
          "year": "2025-03",
          "abstract": "Humans exhibit a remarkable ability to focus auditory attention in complex\nacoustic environments, such as cocktail parties. Auditory attention detection\n(AAD) aims to identify the attended speaker by analyzing brain signals, such as\nelectroencephalography (EEG) data. Existing AAD algorithms often leverage deep\nlearning's powerful nonlinear modeling capabilities, few consider the neural\nmechanisms underlying auditory processing in the brain. In this paper, we\npropose SincAlignNet, a novel network based on an improved SincNet and\ncontrastive learning, designed to align audio and EEG features for auditory\nattention detection. The SincNet component simulates the brain's processing of\naudio during auditory attention, while contrastive learning guides the model to\nlearn the relationship between EEG signals and attended speech. During\ninference, we calculate the cosine similarity between EEG and audio features\nand also explore direct inference of the attended speaker using EEG data.\nCross-trial evaluations results demonstrate that SincAlignNet outperforms\nstate-of-the-art AAD methods on two publicly available datasets, KUL and DTU,\nachieving average accuracies of 78.3% and 92.2%, respectively, with a 1-second\ndecision window. The model exhibits strong interpretability, revealing that the\nleft and right temporal lobes are more active during both male and female\nspeaker scenarios. Furthermore, we found that using data from only six\nelectrodes near the temporal lobes maintains similar or even better performance\ncompared to using 64 electrodes. These findings indicate that efficient\nlow-density EEG online decoding is achievable, marking an important step toward\nthe practical implementation of neuro-guided hearing aids in real-world\napplications. Code is available at: https://github.com/LiaoEuan/SincAlignNet.",
          "arxiv_id": "2503.04156v1"
        },
        {
          "title": "Using Ear-EEG to Decode Auditory Attention in Multiple-speaker Environment",
          "year": "2024-09",
          "abstract": "Auditory Attention Decoding (AAD) can help to determine the identity of the\nattended speaker during an auditory selective attention task, by analyzing and\nprocessing measurements of electroencephalography (EEG) data. Most studies on\nAAD are based on scalp-EEG signals in two-speaker scenarios, which are far from\nreal application. Ear-EEG has recently gained significant attention due to its\nmotion tolerance and invisibility during data acquisition, making it easy to\nincorporate with other devices for applications. In this work, participants\nselectively attended to one of the four spatially separated speakers' speech in\nan anechoic room. The EEG data were concurrently collected from a scalp-EEG\nsystem and an ear-EEG system (cEEGrids). Temporal response functions (TRFs) and\nstimulus reconstruction (SR) were utilized using ear-EEG data. Results showed\nthat the attended speech TRFs were stronger than each unattended speech and\ndecoding accuracy was 41.3\\% in the 60s (chance level of 25\\%). To further\ninvestigate the impact of electrode placement and quantity, SR was utilized in\nboth scalp-EEG and ear-EEG, revealing that while the number of electrodes had a\nminor effect, their positioning had a significant influence on the decoding\naccuracy. One kind of auditory spatial attention detection (ASAD) method,\nSTAnet, was testified with this ear-EEG database, resulting in 93.1% in\n1-second decoding window. The implementation code and database for our work are\navailable on GitHub: https://github.com/zhl486/Ear_EEG_code.git and Zenodo:\nhttps://zenodo.org/records/10803261.",
          "arxiv_id": "2409.08710v1"
        }
      ],
      "27": [
        {
          "title": "Distributed collaborative anomalous sound detection by embedding sharing",
          "year": "2024-03",
          "abstract": "To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.",
          "arxiv_id": "2403.16610v1"
        },
        {
          "title": "SSDPT: Self-Supervised Dual-Path Transformer for Anomalous Sound Detection in Machine Condition Monitoring",
          "year": "2022-08",
          "abstract": "Anomalous sound detection for machine condition monitoring has great\npotential in the development of Industry 4.0. However, these anomalous sounds\nof machines are usually unavailable in normal conditions. Therefore, the models\nemployed have to learn acoustic representations with normal sounds for\ntraining, and detect anomalous sounds while testing. In this article, we\npropose a self-supervised dual-path Transformer (SSDPT) network to detect\nanomalous sounds in machine monitoring. The SSDPT network splits the acoustic\nfeatures into segments and employs several DPT blocks for time and frequency\nmodeling. DPT blocks use attention modules to alternately model the interactive\ninformation about the frequency and temporal components of the segmented\nacoustic features. To address the problem of lack of anomalous sound, we adopt\na self-supervised learning approach to train the network with normal sound.\nSpecifically, this approach randomly masks and reconstructs the acoustic\nfeatures, and jointly classifies machine identity information to improve the\nperformance of anomalous sound detection. We evaluated our method on the\nDCASE2021 task2 dataset. The experimental results show that the SSDPT network\nachieves a significant increase in the harmonic mean AUC score, in comparison\nto present state-of-the-art methods of anomalous sound detection.",
          "arxiv_id": "2208.03421v1"
        },
        {
          "title": "Transformer-based Autoencoder with ID Constraint for Unsupervised Anomalous Sound Detection",
          "year": "2023-10",
          "abstract": "Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous\nsounds of devices when only normal sound data is available. The autoencoder\n(AE) and self-supervised learning based methods are two mainstream methods.\nHowever, the AE-based methods could be limited as the feature learned from\nnormal sounds can also fit with anomalous sounds, reducing the ability of the\nmodel in detecting anomalies from sound. The self-supervised methods are not\nalways stable and perform differently, even for machines of the same type. In\naddition, the anomalous sound may be short-lived, making it even harder to\ndistinguish from normal sound. This paper proposes an ID constrained\nTransformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly\nscore computation for unsupervised ASD. Machine ID is employed to constrain the\nlatent space of the Transformer-based autoencoder (TransAE) by introducing a\nsimple ID classifier to learn the difference in the distribution for the same\nmachine type and enhance the ability of the model in distinguishing anomalous\nsound. Moreover, weighted anomaly score computation is introduced to highlight\nthe anomaly scores of anomalous events that only appear for a short time.\nExperiments performed on DCASE 2020 Challenge Task2 development dataset\ndemonstrate the effectiveness and superiority of our proposed method.",
          "arxiv_id": "2310.08950v1"
        }
      ],
      "28": [
        {
          "title": "Style Transfer of Audio Effects with Differentiable Signal Processing",
          "year": "2022-07",
          "abstract": "We present a framework that can impose the audio effects and production style\nfrom one recording to another by example with the goal of simplifying the audio\nproduction process. We train a deep neural network to analyze an input\nrecording and a style reference recording, and predict the control parameters\nof audio effects used to render the output. In contrast to past work, we\nintegrate audio effects as differentiable operators in our framework, perform\nbackpropagation through audio effects, and optimize end-to-end using an\naudio-domain loss. We use a self-supervised training strategy enabling\nautomatic control of audio effects without the use of any labeled or paired\ntraining data. We survey a range of existing and new approaches for\ndifferentiable signal processing, showing how each can be integrated into our\nframework while discussing their trade-offs. We evaluate our approach on both\nspeech and music tasks, demonstrating that our approach generalizes both to\nunseen recordings and even to sample rates different than those seen during\ntraining. Our approach produces convincing production style transfer results\nwith the ability to transform input recordings to produced recordings, yielding\naudio effect control parameters that enable interpretability and user\ninteraction.",
          "arxiv_id": "2207.08759v1"
        },
        {
          "title": "Modulation Extraction for LFO-driven Audio Effects",
          "year": "2023-05",
          "abstract": "Low frequency oscillator (LFO) driven audio effects such as phaser, flanger,\nand chorus, modify an input signal using time-varying filters and delays,\nresulting in characteristic sweeping or widening effects. It has been shown\nthat these effects can be modeled using neural networks when conditioned with\nthe ground truth LFO signal. However, in most cases, the LFO signal is not\naccessible and measurement from the audio signal is nontrivial, hindering the\nmodeling process. To address this, we propose a framework capable of extracting\narbitrary LFO signals from processed audio across multiple digital audio\neffects, parameter settings, and instrument configurations. Since our system\nimposes no restrictions on the LFO signal shape, we demonstrate its ability to\nextract quasiperiodic, combined, and distorted modulation signals that are\nrelevant to effect modeling. Furthermore, we show how coupling the extraction\nmodel with a simple processing network enables training of end-to-end black-box\nmodels of unseen analog or digital LFO-driven audio effects using only dry and\nwet audio pairs, overcoming the need to access the audio effect or internal LFO\nsignal. We make our code available and provide the trained audio effect models\nin a real-time VST plugin.",
          "arxiv_id": "2305.13262v1"
        },
        {
          "title": "Differentiable Grey-box Modelling of Phaser Effects using Frame-based Spectral Processing",
          "year": "2023-06",
          "abstract": "Machine learning approaches to modelling analog audio effects have seen\nintensive investigation in recent years, particularly in the context of\nnon-linear time-invariant effects such as guitar amplifiers. For modulation\neffects such as phasers, however, new challenges emerge due to the presence of\nthe low-frequency oscillator which controls the slowly time-varying nature of\nthe effect. Existing approaches have either required foreknowledge of this\ncontrol signal, or have been non-causal in implementation. This work presents a\ndifferentiable digital signal processing approach to modelling phaser effects\nin which the underlying control signal and time-varying spectral response of\nthe effect are jointly learned. The proposed model processes audio in short\nframes to implement a time-varying filter in the frequency domain, with a\ntransfer function based on typical analog phaser circuit topology. We show that\nthe model can be trained to emulate an analog reference device, while retaining\ninterpretable and adjustable parameters. The frame duration is an important\nhyper-parameter of the proposed model, so an investigation was carried out into\nits effect on model accuracy. The optimal frame length depends on both the rate\nand transient decay-time of the target effect, but the frame length can be\naltered at inference time without a significant change in accuracy.",
          "arxiv_id": "2306.01332v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:23:55Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}