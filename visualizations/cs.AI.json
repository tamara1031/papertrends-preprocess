{
  "topics": {
    "data": {
      "0": {
        "name": "0_medical_clinical_data_patient",
        "keywords": [
          [
            "medical",
            0.013898638154052472
          ],
          [
            "clinical",
            0.012631291847424113
          ],
          [
            "data",
            0.008342387185685352
          ],
          [
            "patient",
            0.00733674699627463
          ],
          [
            "models",
            0.007258231792583261
          ],
          [
            "model",
            0.007055927370613538
          ],
          [
            "segmentation",
            0.006664559298513278
          ],
          [
            "learning",
            0.006588014809685675
          ],
          [
            "images",
            0.006448568367717522
          ],
          [
            "health",
            0.006356621194758684
          ]
        ],
        "count": 10711
      },
      "1": {
        "name": "1_policy_RL_learning_reinforcement",
        "keywords": [
          [
            "policy",
            0.017140767072604316
          ],
          [
            "RL",
            0.016551943414546213
          ],
          [
            "learning",
            0.013931840345223656
          ],
          [
            "reinforcement",
            0.012920721670530837
          ],
          [
            "reinforcement learning",
            0.012845988441545206
          ],
          [
            "agent",
            0.01243377286401304
          ],
          [
            "Reinforcement",
            0.010752256341816625
          ],
          [
            "Learning",
            0.010740262789579933
          ],
          [
            "robot",
            0.010651718387757239
          ],
          [
            "agents",
            0.0098122079001407
          ]
        ],
        "count": 8616
      },
      "2": {
        "name": "2_visual_video_multimodal_image",
        "keywords": [
          [
            "visual",
            0.022342728457767892
          ],
          [
            "video",
            0.015065549445867129
          ],
          [
            "multimodal",
            0.014653865489887372
          ],
          [
            "image",
            0.014538890537949965
          ],
          [
            "text",
            0.01256632995148911
          ],
          [
            "language",
            0.010798959483056854
          ],
          [
            "vision",
            0.010792627813413513
          ],
          [
            "modal",
            0.010078716385262195
          ],
          [
            "reasoning",
            0.009566433080948462
          ],
          [
            "MLLMs",
            0.009306558418042399
          ]
        ],
        "count": 3260
      },
      "3": {
        "name": "3_speech_audio_speaker_Speech",
        "keywords": [
          [
            "speech",
            0.042787272866756175
          ],
          [
            "audio",
            0.03106378081336068
          ],
          [
            "speaker",
            0.01660283270303751
          ],
          [
            "Speech",
            0.016475216336997334
          ],
          [
            "ASR",
            0.015392923372306466
          ],
          [
            "Audio",
            0.011385644113592785
          ],
          [
            "model",
            0.00929203200593539
          ],
          [
            "acoustic",
            0.008951429705220055
          ],
          [
            "voice",
            0.008841904196190072
          ],
          [
            "recognition",
            0.00859206221696521
          ]
        ],
        "count": 2814
      },
      "4": {
        "name": "4_traffic_driving_autonomous_prediction",
        "keywords": [
          [
            "traffic",
            0.028552972981732573
          ],
          [
            "driving",
            0.02687282205971366
          ],
          [
            "autonomous",
            0.014770245149771584
          ],
          [
            "prediction",
            0.012510893847146291
          ],
          [
            "vehicle",
            0.012247110492814173
          ],
          [
            "vehicles",
            0.012225470714170848
          ],
          [
            "trajectory",
            0.011699514212089255
          ],
          [
            "road",
            0.01053787698455941
          ],
          [
            "autonomous driving",
            0.010529521743192686
          ],
          [
            "safety",
            0.009347548108983444
          ]
        ],
        "count": 2781
      },
      "5": {
        "name": "5_image_diffusion_generation_Diffusion",
        "keywords": [
          [
            "image",
            0.029588740986289604
          ],
          [
            "diffusion",
            0.022162280464846182
          ],
          [
            "generation",
            0.017629537558269293
          ],
          [
            "Diffusion",
            0.01586079287252878
          ],
          [
            "images",
            0.015119574927336876
          ],
          [
            "text",
            0.01385835773102364
          ],
          [
            "diffusion models",
            0.013771070667799154
          ],
          [
            "video",
            0.013628554869884202
          ],
          [
            "models",
            0.011780191340366347
          ],
          [
            "Image",
            0.011250938430846576
          ]
        ],
        "count": 2577
      },
      "6": {
        "name": "6_code_software_LLMs_Code",
        "keywords": [
          [
            "code",
            0.047479557367285694
          ],
          [
            "software",
            0.02040767319736616
          ],
          [
            "LLMs",
            0.015269134366309191
          ],
          [
            "Code",
            0.014770907950830903
          ],
          [
            "code generation",
            0.013434741856476726
          ],
          [
            "generation",
            0.012117156090914832
          ],
          [
            "LLM",
            0.010130587667839194
          ],
          [
            "programming",
            0.008785060830767745
          ],
          [
            "language",
            0.008522322969401802
          ],
          [
            "test",
            0.008401581541204262
          ]
        ],
        "count": 2282
      },
      "7": {
        "name": "7_graph_Graph_node_GNNs",
        "keywords": [
          [
            "graph",
            0.05239348836122004
          ],
          [
            "Graph",
            0.029065902213947066
          ],
          [
            "node",
            0.026035370612290578
          ],
          [
            "GNNs",
            0.023840186913881827
          ],
          [
            "graphs",
            0.022342315119649226
          ],
          [
            "GNN",
            0.016681582188062993
          ],
          [
            "nodes",
            0.016562020995244155
          ],
          [
            "Networks",
            0.011264128672007137
          ],
          [
            "networks",
            0.011242193857072483
          ],
          [
            "Neural",
            0.010684492095300183
          ]
        ],
        "count": 2231
      },
      "8": {
        "name": "8_reasoning_LLMs_Reasoning_mathematical",
        "keywords": [
          [
            "reasoning",
            0.0487925911896971
          ],
          [
            "LLMs",
            0.01738963063700144
          ],
          [
            "Reasoning",
            0.015141121899448143
          ],
          [
            "mathematical",
            0.012229612113950147
          ],
          [
            "language",
            0.011049645309240906
          ],
          [
            "models",
            0.010905022070583081
          ],
          [
            "language models",
            0.009242797995807838
          ],
          [
            "tasks",
            0.008960273528733086
          ],
          [
            "Large",
            0.008892009828987838
          ],
          [
            "problems",
            0.00862503081206164
          ]
        ],
        "count": 2193
      },
      "9": {
        "name": "9_AI_systems_human_intelligence",
        "keywords": [
          [
            "AI",
            0.0687446876266566
          ],
          [
            "systems",
            0.01924953064607576
          ],
          [
            "human",
            0.015321007174423782
          ],
          [
            "intelligence",
            0.01356116627675513
          ],
          [
            "ethical",
            0.013171726598842222
          ],
          [
            "Artificial",
            0.012090356320230618
          ],
          [
            "Intelligence",
            0.011610767818257236
          ],
          [
            "artificial",
            0.010584293134477387
          ],
          [
            "risks",
            0.010331827497411497
          ],
          [
            "research",
            0.009726934498039148
          ]
        ],
        "count": 2178
      },
      "10": {
        "name": "10_recommendation_user_item_recommender",
        "keywords": [
          [
            "recommendation",
            0.03617577559480946
          ],
          [
            "user",
            0.02818114216223104
          ],
          [
            "item",
            0.02209166423504631
          ],
          [
            "recommender",
            0.019448348354792068
          ],
          [
            "items",
            0.01690727094230124
          ],
          [
            "users",
            0.016151461513844608
          ],
          [
            "recommender systems",
            0.014841248656566585
          ],
          [
            "Recommendation",
            0.014512219303218072
          ],
          [
            "systems",
            0.012149302213283381
          ],
          [
            "recommendations",
            0.011192322545549097
          ]
        ],
        "count": 2106
      },
      "11": {
        "name": "11_3D_point_object_view",
        "keywords": [
          [
            "3D",
            0.04468023878326696
          ],
          [
            "point",
            0.01971090143610506
          ],
          [
            "object",
            0.01467835205811571
          ],
          [
            "view",
            0.01423455292475541
          ],
          [
            "scene",
            0.014113827215295187
          ],
          [
            "depth",
            0.01387282208525463
          ],
          [
            "point cloud",
            0.012545993828136632
          ],
          [
            "camera",
            0.011505386818974245
          ],
          [
            "LiDAR",
            0.01124361327551577
          ],
          [
            "cloud",
            0.010778229869805462
          ]
        ],
        "count": 1889
      },
      "12": {
        "name": "12_LLMs_attacks_safety_attack",
        "keywords": [
          [
            "LLMs",
            0.0208858057746358
          ],
          [
            "attacks",
            0.02024735494672207
          ],
          [
            "safety",
            0.018153068048709992
          ],
          [
            "attack",
            0.01766179728014275
          ],
          [
            "LLM",
            0.015307470709787142
          ],
          [
            "jailbreak",
            0.013939755883195817
          ],
          [
            "adversarial",
            0.012613117659418948
          ],
          [
            "security",
            0.012084431884100067
          ],
          [
            "privacy",
            0.011650010973042715
          ],
          [
            "harmful",
            0.01145847785552041
          ]
        ],
        "count": 1832
      },
      "13": {
        "name": "13_optimization_problem_problems_algorithm",
        "keywords": [
          [
            "optimization",
            0.02086871278614961
          ],
          [
            "problem",
            0.01928281526114744
          ],
          [
            "problems",
            0.018648212104323176
          ],
          [
            "algorithm",
            0.015511976364915504
          ],
          [
            "algorithms",
            0.013035922322093652
          ],
          [
            "solutions",
            0.011678035387684275
          ],
          [
            "search",
            0.011421519438935317
          ],
          [
            "instances",
            0.011174596515079937
          ],
          [
            "solution",
            0.010443994192897153
          ],
          [
            "Optimization",
            0.009298065910111062
          ]
        ],
        "count": 1756
      },
      "14": {
        "name": "14_RAG_retrieval_LLMs_QA",
        "keywords": [
          [
            "RAG",
            0.023566809509918955
          ],
          [
            "retrieval",
            0.017705986483015735
          ],
          [
            "LLMs",
            0.016084565417561526
          ],
          [
            "QA",
            0.013989564148071619
          ],
          [
            "question",
            0.013463700345533066
          ],
          [
            "Retrieval",
            0.012994345397705092
          ],
          [
            "knowledge",
            0.011054047215778759
          ],
          [
            "LLM",
            0.011029277502238028
          ],
          [
            "language",
            0.01046280556820494
          ],
          [
            "query",
            0.010433076829833113
          ]
        ],
        "count": 1747
      },
      "15": {
        "name": "15_explanations_XAI_explanation_AI",
        "keywords": [
          [
            "explanations",
            0.03705779530843321
          ],
          [
            "XAI",
            0.028010375870405977
          ],
          [
            "explanation",
            0.0215543194915963
          ],
          [
            "AI",
            0.01408507016401733
          ],
          [
            "methods",
            0.011510894534546954
          ],
          [
            "decision",
            0.01136663079266479
          ],
          [
            "explainability",
            0.011341493733771694
          ],
          [
            "Explainable",
            0.010514408592271898
          ],
          [
            "interpretability",
            0.010202016236608975
          ],
          [
            "Explanations",
            0.010007129372007192
          ]
        ],
        "count": 1711
      },
      "16": {
        "name": "16_series_time series_time_forecasting",
        "keywords": [
          [
            "series",
            0.046538526044346884
          ],
          [
            "time series",
            0.0453481345256579
          ],
          [
            "time",
            0.03143143898760679
          ],
          [
            "forecasting",
            0.023766733320341907
          ],
          [
            "Time",
            0.021118376602765204
          ],
          [
            "Series",
            0.018862536604030606
          ],
          [
            "data",
            0.01234737372749745
          ],
          [
            "series forecasting",
            0.012294534721539732
          ],
          [
            "time series forecasting",
            0.010874458348061982
          ],
          [
            "series data",
            0.010266399806548906
          ]
        ],
        "count": 1590
      },
      "17": {
        "name": "17_quantization_memory_attention_LLMs",
        "keywords": [
          [
            "quantization",
            0.016086920128321337
          ],
          [
            "memory",
            0.015393265599402246
          ],
          [
            "attention",
            0.013563765161335692
          ],
          [
            "LLMs",
            0.012701524274472662
          ],
          [
            "MoE",
            0.012640529006599087
          ],
          [
            "LoRA",
            0.012475276084275577
          ],
          [
            "inference",
            0.01244360661011762
          ],
          [
            "tuning",
            0.01122430610806367
          ],
          [
            "models",
            0.010463403022966593
          ],
          [
            "training",
            0.010411467753664914
          ]
        ],
        "count": 1586
      },
      "18": {
        "name": "18_FL_Federated_clients_federated",
        "keywords": [
          [
            "FL",
            0.04947126913413841
          ],
          [
            "Federated",
            0.04246445703503642
          ],
          [
            "clients",
            0.03356995066055433
          ],
          [
            "federated",
            0.02931801151844781
          ],
          [
            "client",
            0.02249513144546699
          ],
          [
            "privacy",
            0.02239928675191767
          ],
          [
            "federated learning",
            0.022127658138002708
          ],
          [
            "Learning",
            0.01848982545345514
          ],
          [
            "data",
            0.0181225043671682
          ],
          [
            "local",
            0.017664007824946735
          ]
        ],
        "count": 1433
      },
      "19": {
        "name": "19_agents_agent_LLM_GUI",
        "keywords": [
          [
            "agents",
            0.03112026324618563
          ],
          [
            "agent",
            0.0265969212395721
          ],
          [
            "LLM",
            0.016892372826040808
          ],
          [
            "GUI",
            0.013730089903791463
          ],
          [
            "Agent",
            0.013183133453369312
          ],
          [
            "LLMs",
            0.01151145536898424
          ],
          [
            "tasks",
            0.010510895340429188
          ],
          [
            "language",
            0.010217514384216897
          ],
          [
            "reasoning",
            0.009218731312885265
          ],
          [
            "multi",
            0.008766256015130066
          ]
        ],
        "count": 1355
      },
      "20": {
        "name": "20_network_wireless_channel_communication",
        "keywords": [
          [
            "network",
            0.019235183885617013
          ],
          [
            "wireless",
            0.01768707934774217
          ],
          [
            "channel",
            0.01666480446353616
          ],
          [
            "communication",
            0.015478727388589127
          ],
          [
            "networks",
            0.011958308758283591
          ],
          [
            "CSI",
            0.010108905344640073
          ],
          [
            "edge",
            0.008023094232260665
          ],
          [
            "resource",
            0.007893572189088729
          ],
          [
            "communications",
            0.007840413833061555
          ],
          [
            "semantic",
            0.007752310052530118
          ]
        ],
        "count": 1325
      },
      "21": {
        "name": "21_knowledge_entities_Knowledge_entity",
        "keywords": [
          [
            "knowledge",
            0.024196654358415108
          ],
          [
            "entities",
            0.023396507190819846
          ],
          [
            "Knowledge",
            0.02281167402703628
          ],
          [
            "entity",
            0.021564589217570113
          ],
          [
            "KG",
            0.01981625535486839
          ],
          [
            "graph",
            0.01980474854223215
          ],
          [
            "KGs",
            0.018303583211890396
          ],
          [
            "knowledge graph",
            0.017591805195006357
          ],
          [
            "relations",
            0.015325462909384054
          ],
          [
            "graphs",
            0.015097752303896907
          ]
        ],
        "count": 1156
      },
      "22": {
        "name": "22_logic_SAT_ASP_semantics",
        "keywords": [
          [
            "logic",
            0.02915481412090344
          ],
          [
            "SAT",
            0.024776030578509987
          ],
          [
            "ASP",
            0.02292034043250556
          ],
          [
            "semantics",
            0.01550088515613583
          ],
          [
            "solvers",
            0.014324162247611953
          ],
          [
            "Programming",
            0.014229767607350377
          ],
          [
            "Logic",
            0.014076472847503366
          ],
          [
            "problem",
            0.012916312217141849
          ],
          [
            "programs",
            0.012691900871717308
          ],
          [
            "reasoning",
            0.012328221292188503
          ]
        ],
        "count": 1095
      },
      "23": {
        "name": "23_students_student_AI_education",
        "keywords": [
          [
            "students",
            0.0442340305886956
          ],
          [
            "student",
            0.025895651232514645
          ],
          [
            "AI",
            0.025618440172059345
          ],
          [
            "education",
            0.023592251745568296
          ],
          [
            "educational",
            0.0190276042105083
          ],
          [
            "learning",
            0.014864195048133666
          ],
          [
            "ChatGPT",
            0.013173673812165726
          ],
          [
            "feedback",
            0.010798206860038455
          ],
          [
            "teaching",
            0.010748567689053082
          ],
          [
            "course",
            0.010323008851087667
          ]
        ],
        "count": 1077
      },
      "24": {
        "name": "24_news_detection_hate_media",
        "keywords": [
          [
            "news",
            0.03123206601342656
          ],
          [
            "detection",
            0.02360971340795026
          ],
          [
            "hate",
            0.02046232055756599
          ],
          [
            "media",
            0.01947479330292096
          ],
          [
            "fake",
            0.0189471385863061
          ],
          [
            "social",
            0.018603068663892296
          ],
          [
            "hate speech",
            0.017258820439172103
          ],
          [
            "content",
            0.016989003589382743
          ],
          [
            "social media",
            0.01583860360938473
          ],
          [
            "text",
            0.01341772795176233
          ]
        ],
        "count": 1033
      },
      "25": {
        "name": "25_pruning_hardware_DNN_memory",
        "keywords": [
          [
            "pruning",
            0.020614464322346385
          ],
          [
            "hardware",
            0.016688089199581143
          ],
          [
            "DNN",
            0.0160569250853642
          ],
          [
            "memory",
            0.013310769506799824
          ],
          [
            "quantization",
            0.013269277023300774
          ],
          [
            "energy",
            0.012705568649370148
          ],
          [
            "training",
            0.012588491960209747
          ],
          [
            "neural",
            0.012583347161802849
          ],
          [
            "devices",
            0.012334653322348281
          ],
          [
            "accuracy",
            0.012093674326467206
          ]
        ],
        "count": 1033
      },
      "26": {
        "name": "26_translation_language_languages_Translation",
        "keywords": [
          [
            "translation",
            0.03328823149824461
          ],
          [
            "language",
            0.02029465214993737
          ],
          [
            "languages",
            0.020120605515736727
          ],
          [
            "Translation",
            0.01631088080842382
          ],
          [
            "English",
            0.01506907502376177
          ],
          [
            "machine translation",
            0.014603601030278105
          ],
          [
            "multilingual",
            0.01421618945765964
          ],
          [
            "models",
            0.013489852165190271
          ],
          [
            "NMT",
            0.01237905210192263
          ],
          [
            "training",
            0.011885352651116858
          ]
        ],
        "count": 920
      },
      "27": {
        "name": "27_domain_label_labels_target",
        "keywords": [
          [
            "domain",
            0.02100069936734239
          ],
          [
            "label",
            0.01654821673910395
          ],
          [
            "labels",
            0.014994558186209322
          ],
          [
            "target",
            0.013784775732706847
          ],
          [
            "supervised",
            0.012936407995943979
          ],
          [
            "learning",
            0.012922420325107218
          ],
          [
            "distribution",
            0.012881141455295996
          ],
          [
            "data",
            0.011918356938156101
          ],
          [
            "Domain",
            0.011545768533182026
          ],
          [
            "samples",
            0.011353083617466918
          ]
        ],
        "count": 920
      },
      "28": {
        "name": "28_face_facial_images_recognition",
        "keywords": [
          [
            "face",
            0.031212619522560096
          ],
          [
            "facial",
            0.022769766962893127
          ],
          [
            "images",
            0.017957788584312684
          ],
          [
            "recognition",
            0.016895047128306303
          ],
          [
            "Face",
            0.014499918247138907
          ],
          [
            "image",
            0.013328050177264811
          ],
          [
            "detection",
            0.013140938124923713
          ],
          [
            "face recognition",
            0.012171365361746313
          ],
          [
            "dataset",
            0.009053085704936181
          ],
          [
            "biometric",
            0.00873401453603122
          ]
        ],
        "count": 902
      },
      "29": {
        "name": "29_images_remote sensing_remote_detection",
        "keywords": [
          [
            "images",
            0.016060083892406214
          ],
          [
            "remote sensing",
            0.014561191908260456
          ],
          [
            "remote",
            0.014084624919508425
          ],
          [
            "detection",
            0.014006326887567243
          ],
          [
            "sensing",
            0.013178109114693031
          ],
          [
            "segmentation",
            0.011561413710817776
          ],
          [
            "imagery",
            0.010685510214819349
          ],
          [
            "classification",
            0.010304327457543545
          ],
          [
            "image",
            0.010125622182104768
          ],
          [
            "dataset",
            0.010116315632304647
          ]
        ],
        "count": 895
      },
      "30": {
        "name": "30_dialogue_Dialogue_conversational_dialog",
        "keywords": [
          [
            "dialogue",
            0.054680032949136204
          ],
          [
            "Dialogue",
            0.02198705354639043
          ],
          [
            "conversational",
            0.01612162717918996
          ],
          [
            "dialog",
            0.01605879341869731
          ],
          [
            "conversation",
            0.013733138509608946
          ],
          [
            "user",
            0.013373235240466545
          ],
          [
            "responses",
            0.01283230545020421
          ],
          [
            "response",
            0.012817383824426987
          ],
          [
            "dialogues",
            0.012776425775726503
          ],
          [
            "dialogue systems",
            0.012731220463918898
          ]
        ],
        "count": 895
      },
      "31": {
        "name": "31_equations_neural_PDE_PDEs",
        "keywords": [
          [
            "equations",
            0.01986783333096735
          ],
          [
            "neural",
            0.018647205629521012
          ],
          [
            "PDE",
            0.016562141033835822
          ],
          [
            "PDEs",
            0.016115275153230576
          ],
          [
            "physics",
            0.015391943125059985
          ],
          [
            "differential",
            0.014587511338263616
          ],
          [
            "Neural",
            0.013823265893561417
          ],
          [
            "differential equations",
            0.013506606928509616
          ],
          [
            "PINNs",
            0.013383537440591751
          ],
          [
            "physical",
            0.013231217581229136
          ]
        ],
        "count": 892
      },
      "32": {
        "name": "32_causal_Causal_variables_observational",
        "keywords": [
          [
            "causal",
            0.08798880839767878
          ],
          [
            "Causal",
            0.03050478024484898
          ],
          [
            "variables",
            0.02378264455736578
          ],
          [
            "observational",
            0.016529831165390946
          ],
          [
            "effect",
            0.016194185000907264
          ],
          [
            "treatment",
            0.014473026553871579
          ],
          [
            "causal inference",
            0.013886046626526684
          ],
          [
            "effects",
            0.012884894445551485
          ],
          [
            "data",
            0.01282218187631021
          ],
          [
            "causal discovery",
            0.012809698583871047
          ]
        ],
        "count": 862
      },
      "33": {
        "name": "33_action_recognition_HAR_video",
        "keywords": [
          [
            "action",
            0.019468900165052698
          ],
          [
            "recognition",
            0.01861259353244552
          ],
          [
            "HAR",
            0.018229199792362302
          ],
          [
            "video",
            0.015805861689251376
          ],
          [
            "temporal",
            0.01290450611120623
          ],
          [
            "activity",
            0.01289586663806677
          ],
          [
            "human",
            0.012600424860010725
          ],
          [
            "action recognition",
            0.012235219950451647
          ],
          [
            "Recognition",
            0.01175672404339583
          ],
          [
            "Human",
            0.011438579496900345
          ]
        ],
        "count": 835
      },
      "34": {
        "name": "34_neural_networks_neural networks_gradient",
        "keywords": [
          [
            "neural",
            0.024842522210726754
          ],
          [
            "networks",
            0.02470053013685978
          ],
          [
            "neural networks",
            0.018983945135644227
          ],
          [
            "gradient",
            0.018025865991175345
          ],
          [
            "activation",
            0.01644905965495389
          ],
          [
            "network",
            0.016258656418807367
          ],
          [
            "deep",
            0.015378268824480217
          ],
          [
            "training",
            0.014065523744724938
          ],
          [
            "functions",
            0.013580964223535932
          ],
          [
            "ReLU",
            0.012603942229322838
          ]
        ],
        "count": 803
      },
      "35": {
        "name": "35_adversarial_attacks_robustness_Adversarial",
        "keywords": [
          [
            "adversarial",
            0.06772699298074693
          ],
          [
            "attacks",
            0.034298955827313786
          ],
          [
            "robustness",
            0.02905386230656337
          ],
          [
            "Adversarial",
            0.025965416134249065
          ],
          [
            "attack",
            0.024177646720276263
          ],
          [
            "adversarial examples",
            0.0195445562079785
          ],
          [
            "adversarial attacks",
            0.019475737514877772
          ],
          [
            "examples",
            0.016989370990277523
          ],
          [
            "perturbations",
            0.01611216267315067
          ],
          [
            "adversarial training",
            0.013280111322502314
          ]
        ],
        "count": 801
      },
      "36": {
        "name": "36_segmentation_object_object detection_detection",
        "keywords": [
          [
            "segmentation",
            0.031537077167362
          ],
          [
            "object",
            0.026679553501655958
          ],
          [
            "object detection",
            0.017405345533134473
          ],
          [
            "detection",
            0.01656029277469527
          ],
          [
            "semantic segmentation",
            0.016459099910296805
          ],
          [
            "semantic",
            0.014550977546988026
          ],
          [
            "Segmentation",
            0.01439223449578094
          ],
          [
            "Object",
            0.012786214596903168
          ],
          [
            "objects",
            0.012197470404034483
          ],
          [
            "image",
            0.011114557716875554
          ]
        ],
        "count": 766
      },
      "37": {
        "name": "37_quantum_Quantum_classical_quantum computing",
        "keywords": [
          [
            "quantum",
            0.13809427884095035
          ],
          [
            "Quantum",
            0.055364165622076356
          ],
          [
            "classical",
            0.0313070044163184
          ],
          [
            "quantum computing",
            0.018002835895177916
          ],
          [
            "circuits",
            0.016659550248316786
          ],
          [
            "computing",
            0.016577931616335
          ],
          [
            "circuit",
            0.015002116316532118
          ],
          [
            "machine",
            0.012817104636668051
          ],
          [
            "machine learning",
            0.012422990598343408
          ],
          [
            "learning",
            0.011406018773114784
          ]
        ],
        "count": 693
      },
      "38": {
        "name": "38_fairness_Fairness_fair_bias",
        "keywords": [
          [
            "fairness",
            0.09328039830295969
          ],
          [
            "Fairness",
            0.02751633815243732
          ],
          [
            "fair",
            0.02636070953419738
          ],
          [
            "bias",
            0.019364140770257762
          ],
          [
            "sensitive",
            0.01281574944969341
          ],
          [
            "algorithmic",
            0.012789917703799104
          ],
          [
            "discrimination",
            0.012521812662816338
          ],
          [
            "decision",
            0.012335638377505611
          ],
          [
            "ML",
            0.012165569473831093
          ],
          [
            "Fair",
            0.012066367184225445
          ]
        ],
        "count": 646
      },
      "39": {
        "name": "39_path_planning_control_Path",
        "keywords": [
          [
            "path",
            0.020776525903425252
          ],
          [
            "planning",
            0.019146881438249472
          ],
          [
            "control",
            0.014992232493950364
          ],
          [
            "Path",
            0.014377427614289517
          ],
          [
            "collision",
            0.013200069032858124
          ],
          [
            "algorithm",
            0.01317472825863756
          ],
          [
            "agents",
            0.013164106871381356
          ],
          [
            "agent",
            0.012777375909572591
          ],
          [
            "robot",
            0.01248085397029404
          ],
          [
            "obstacles",
            0.012276921187244019
          ]
        ],
        "count": 643
      },
      "40": {
        "name": "40_bias_biases_LLMs_gender",
        "keywords": [
          [
            "bias",
            0.033516247402951664
          ],
          [
            "biases",
            0.030178419304697342
          ],
          [
            "LLMs",
            0.027474434446173455
          ],
          [
            "gender",
            0.022012447774689865
          ],
          [
            "language",
            0.015072698264092925
          ],
          [
            "social",
            0.013582639087166861
          ],
          [
            "Language",
            0.013330857462430143
          ],
          [
            "political",
            0.013302620169948648
          ],
          [
            "Bias",
            0.013157829131808473
          ],
          [
            "language models",
            0.012614805680241443
          ]
        ],
        "count": 603
      },
      "41": {
        "name": "41_SNNs_SNN_Spiking_spiking",
        "keywords": [
          [
            "SNNs",
            0.04893942731586012
          ],
          [
            "SNN",
            0.042585773396228335
          ],
          [
            "Spiking",
            0.037393547007863355
          ],
          [
            "spiking",
            0.030136233059272818
          ],
          [
            "spike",
            0.02300497474751199
          ],
          [
            "energy",
            0.01975297160634165
          ],
          [
            "Neural",
            0.018586350347188713
          ],
          [
            "neural",
            0.017274349514191278
          ],
          [
            "Networks",
            0.01555037622451848
          ],
          [
            "neurons",
            0.015267236506108459
          ]
        ],
        "count": 545
      },
      "42": {
        "name": "42_scientific_papers_research_citation",
        "keywords": [
          [
            "scientific",
            0.03128856401393215
          ],
          [
            "papers",
            0.026546712910720496
          ],
          [
            "research",
            0.022449448627918964
          ],
          [
            "citation",
            0.017088010426850903
          ],
          [
            "review",
            0.014832335559563255
          ],
          [
            "academic",
            0.014072339793432694
          ],
          [
            "literature",
            0.013174357130625088
          ],
          [
            "LLMs",
            0.012254571918216321
          ],
          [
            "AI",
            0.011754221315768825
          ],
          [
            "Scientific",
            0.010592377460458836
          ]
        ],
        "count": 538
      },
      "43": {
        "name": "43_continual_continual learning_forgetting_Continual",
        "keywords": [
          [
            "continual",
            0.03871695603144279
          ],
          [
            "continual learning",
            0.03723213572222476
          ],
          [
            "forgetting",
            0.036288058706717606
          ],
          [
            "Continual",
            0.02955648060084056
          ],
          [
            "learning",
            0.023923305299008393
          ],
          [
            "catastrophic forgetting",
            0.02200331126472058
          ],
          [
            "catastrophic",
            0.021910975557439576
          ],
          [
            "new",
            0.01696291514074838
          ],
          [
            "incremental",
            0.016944315956485323
          ],
          [
            "Learning",
            0.015741874262729987
          ]
        ],
        "count": 535
      },
      "44": {
        "name": "44_preference_reward_RLHF_DPO",
        "keywords": [
          [
            "preference",
            0.043625092786228926
          ],
          [
            "reward",
            0.03398702763082659
          ],
          [
            "RLHF",
            0.03350230555284216
          ],
          [
            "DPO",
            0.02859892618741392
          ],
          [
            "alignment",
            0.02692469120421337
          ],
          [
            "preferences",
            0.026316298883767335
          ],
          [
            "Preference",
            0.025011789224718587
          ],
          [
            "human",
            0.02111277180321116
          ],
          [
            "human preferences",
            0.020066842703793466
          ],
          [
            "Optimization",
            0.015095861041539777
          ]
        ],
        "count": 526
      },
      "45": {
        "name": "45_IoT_detection_attacks_traffic",
        "keywords": [
          [
            "IoT",
            0.029971562624019184
          ],
          [
            "detection",
            0.027880182284900766
          ],
          [
            "attacks",
            0.02235108568093187
          ],
          [
            "traffic",
            0.02224987108580252
          ],
          [
            "intrusion",
            0.02103653129746572
          ],
          [
            "network",
            0.0210096837227636
          ],
          [
            "security",
            0.018300721982767998
          ],
          [
            "IDS",
            0.018111709011491478
          ],
          [
            "Intrusion",
            0.018054688098192485
          ],
          [
            "intrusion detection",
            0.017827384799950333
          ]
        ],
        "count": 488
      },
      "46": {
        "name": "46_privacy_unlearning_data_attacks",
        "keywords": [
          [
            "privacy",
            0.05052797412410254
          ],
          [
            "unlearning",
            0.022207193036877446
          ],
          [
            "data",
            0.018400788908587935
          ],
          [
            "attacks",
            0.01672914731262522
          ],
          [
            "private",
            0.01643956319021526
          ],
          [
            "Privacy",
            0.013968403015872662
          ],
          [
            "model",
            0.012271068505590104
          ],
          [
            "inference",
            0.012032364023260675
          ],
          [
            "machine",
            0.011914498712812914
          ],
          [
            "training",
            0.01190639128777252
          ]
        ],
        "count": 457
      },
      "47": {
        "name": "47_energy_power_grid_renewable",
        "keywords": [
          [
            "energy",
            0.04649928152901978
          ],
          [
            "power",
            0.03396698053077114
          ],
          [
            "grid",
            0.024933446514352284
          ],
          [
            "renewable",
            0.01543291256639661
          ],
          [
            "electricity",
            0.015117015322330167
          ],
          [
            "control",
            0.013020664882712373
          ],
          [
            "Energy",
            0.01257525068692744
          ],
          [
            "Power",
            0.012212381455356735
          ],
          [
            "smart",
            0.011570363339450357
          ],
          [
            "renewable energy",
            0.011521253586877302
          ]
        ],
        "count": 411
      },
      "48": {
        "name": "48_summarization_summaries_Summarization_summary",
        "keywords": [
          [
            "summarization",
            0.06812958299322214
          ],
          [
            "summaries",
            0.040923611844917385
          ],
          [
            "Summarization",
            0.032757860183358495
          ],
          [
            "summary",
            0.02464927045530049
          ],
          [
            "document",
            0.02041859537445823
          ],
          [
            "abstractive",
            0.017271128659750775
          ],
          [
            "text",
            0.013966940528693537
          ],
          [
            "text summarization",
            0.013131285969837305
          ],
          [
            "documents",
            0.010791113514846993
          ],
          [
            "evaluation",
            0.010072124643330833
          ]
        ],
        "count": 410
      },
      "49": {
        "name": "49_market_stock_financial_trading",
        "keywords": [
          [
            "market",
            0.048077269830471475
          ],
          [
            "stock",
            0.03964231829750402
          ],
          [
            "financial",
            0.036657343653824774
          ],
          [
            "trading",
            0.03557557708235247
          ],
          [
            "price",
            0.02346295982963658
          ],
          [
            "markets",
            0.017503409731334442
          ],
          [
            "portfolio",
            0.017133615306455678
          ],
          [
            "investment",
            0.016730894175138192
          ],
          [
            "volatility",
            0.011928501442806024
          ],
          [
            "prices",
            0.011514801643603008
          ]
        ],
        "count": 410
      },
      "50": {
        "name": "50_weather_climate_forecasting_resolution",
        "keywords": [
          [
            "weather",
            0.03774298187316222
          ],
          [
            "climate",
            0.027960290778348808
          ],
          [
            "forecasting",
            0.025882098907809115
          ],
          [
            "resolution",
            0.01613617928186059
          ],
          [
            "forecast",
            0.014576351369871893
          ],
          [
            "forecasts",
            0.014141893324830703
          ],
          [
            "weather forecasting",
            0.014108946822403245
          ],
          [
            "Weather",
            0.013019590503924615
          ],
          [
            "prediction",
            0.012637605089864012
          ],
          [
            "data",
            0.012376040016946092
          ]
        ],
        "count": 375
      },
      "51": {
        "name": "51_bandit_regret_bandits_Bandits",
        "keywords": [
          [
            "bandit",
            0.045387150144514214
          ],
          [
            "regret",
            0.04077741490021741
          ],
          [
            "bandits",
            0.029871980977392826
          ],
          [
            "Bandits",
            0.02638410346815957
          ],
          [
            "arm",
            0.024867030863843183
          ],
          [
            "algorithm",
            0.022454498018096226
          ],
          [
            "arms",
            0.021675223845397648
          ],
          [
            "armed",
            0.019363768040794743
          ],
          [
            "algorithms",
            0.01732688335816618
          ],
          [
            "problem",
            0.01601369411149791
          ]
        ],
        "count": 375
      },
      "52": {
        "name": "52_emotion_Emotion_emotions_emotional",
        "keywords": [
          [
            "emotion",
            0.06277416853659722
          ],
          [
            "Emotion",
            0.033184625847401894
          ],
          [
            "emotions",
            0.02883315411094366
          ],
          [
            "emotional",
            0.023838389773918278
          ],
          [
            "emotion recognition",
            0.019151159077737515
          ],
          [
            "multimodal",
            0.018550098239007953
          ],
          [
            "recognition",
            0.016522431866641304
          ],
          [
            "modality",
            0.015558344479126558
          ],
          [
            "Multimodal",
            0.015556193095157527
          ],
          [
            "affective",
            0.01550490020745143
          ]
        ],
        "count": 365
      },
      "53": {
        "name": "53_ViT_Vision_attention_vision",
        "keywords": [
          [
            "ViT",
            0.03162957057813456
          ],
          [
            "Vision",
            0.025950426173012367
          ],
          [
            "attention",
            0.02461101456080134
          ],
          [
            "vision",
            0.020889954544449654
          ],
          [
            "Transformers",
            0.020006951529526078
          ],
          [
            "Transformer",
            0.014797579946545417
          ],
          [
            "image",
            0.013054303560780169
          ],
          [
            "ImageNet",
            0.01302933888749624
          ],
          [
            "token",
            0.012656848898478625
          ],
          [
            "transformers",
            0.011274001317461488
          ]
        ],
        "count": 349
      }
    },
    "correlations": [
      [
        1.0,
        -0.7555717551890896,
        -0.7245513885260944,
        -0.7579911803352035,
        -0.7599895258664191,
        -0.7172673446141851,
        -0.7263389631584827,
        -0.7527310047905933,
        -0.7310676133134255,
        -0.7088171471917339,
        -0.7525792221837337,
        -0.7307857914426203,
        -0.7303078155181146,
        -0.7513344519268487,
        -0.7279540543739761,
        -0.7419738315935747,
        -0.7369007615613858,
        -0.7272299917044276,
        -0.7494446938682968,
        -0.7473276290131126,
        -0.7535879514993358,
        -0.7385104526141866,
        -0.7609904377514659,
        -0.7478729717387812,
        -0.749712050069953,
        -0.7496127200423314,
        -0.7334538686924721,
        -0.7192066351194157,
        -0.72890272471887,
        -0.7048315001373449,
        -0.755758117610922,
        -0.7510294274804743,
        -0.7556578553914115,
        -0.7541203439041465,
        -0.7297474528755068,
        -0.750314368420902,
        -0.6706704664524443,
        -0.7621173423363106,
        -0.7544082577223374,
        -0.7584849203433848,
        -0.7239591760425433,
        -0.7557953994337527,
        -0.7348973126961917,
        -0.7557059377116218,
        -0.7466712886588808,
        -0.7425088761229681,
        -0.7119268901612252,
        -0.7564234362766107,
        -0.7473313069472505,
        -0.7605742566353425,
        -0.7415846225793767,
        -0.7600685067770332,
        -0.759664325672581,
        -0.734215200745824
      ],
      [
        -0.7555717551890896,
        1.0,
        -0.7294960178304597,
        -0.760922866522449,
        -0.7037774878353911,
        -0.7393431184647088,
        -0.7110023928838642,
        -0.7351630648226963,
        -0.7115356452487083,
        -0.7238729791597167,
        -0.7375955040362383,
        -0.7409937783839466,
        -0.7177203132893675,
        -0.6363943376031964,
        -0.7400755825229548,
        -0.7523419194464502,
        -0.7183097280619858,
        -0.7239572995887751,
        -0.7546913916124254,
        -0.5208965879718289,
        -0.718441299000496,
        -0.7322821539034832,
        -0.7471719617538797,
        -0.7461445180245718,
        -0.7551111411597704,
        -0.7293324233455237,
        -0.7384058835455267,
        -0.7197911339186839,
        -0.7432814578742343,
        -0.7465180414105999,
        -0.754664163579115,
        -0.7261115714222294,
        -0.7455001583906451,
        -0.6953783436383144,
        -0.6961091963351675,
        -0.7206532459281646,
        -0.7418151554891106,
        -0.7508663421181007,
        -0.756849847365203,
        -0.6013921496225473,
        -0.7208627436657327,
        -0.7459732395359625,
        -0.7321507264498984,
        -0.6006893808766366,
        -0.6196780074563604,
        -0.7381509557049859,
        -0.7263380636272541,
        -0.7152778548044212,
        -0.7563820504283634,
        -0.7410546421243656,
        -0.7482764598917626,
        -0.6669951942706343,
        -0.7621078647641689,
        -0.7420621776738991
      ],
      [
        -0.7245513885260944,
        -0.7294960178304597,
        1.0,
        -0.7255665169801426,
        -0.7418335196222654,
        -0.4940846190954195,
        -0.686532014027115,
        -0.7465242493767906,
        -0.6488591543348365,
        -0.72940062017697,
        -0.7457311345043676,
        -0.7120504903351799,
        -0.6848924011409742,
        -0.7433328412719444,
        -0.684547500576635,
        -0.7455928011973791,
        -0.7331296214248499,
        -0.6902555437181752,
        -0.7602684418491448,
        -0.7188997493010058,
        -0.7448451838258818,
        -0.7286863203033683,
        -0.7593726178571895,
        -0.7490211003886944,
        -0.7301357664687131,
        -0.7430074508288895,
        -0.7045591969492733,
        -0.7260773127153748,
        -0.6772892872346983,
        -0.6887247616625546,
        -0.7508863043861049,
        -0.7473597095862949,
        -0.7517339497789545,
        -0.6192827031735227,
        -0.7267878242152244,
        -0.7296605728045544,
        -0.6732763606386354,
        -0.762187104382616,
        -0.7601765179827624,
        -0.74348942877528,
        -0.6840556256361016,
        -0.7550838486070606,
        -0.731857362607335,
        -0.7430793730518458,
        -0.7170180934204267,
        -0.7363679436745135,
        -0.7360703520765907,
        -0.7501849861982921,
        -0.7482417226002592,
        -0.7609013783684113,
        -0.748409568326087,
        -0.7549923637199616,
        -0.7234904562269835,
        -0.6646986671685561
      ],
      [
        -0.7579911803352035,
        -0.760922866522449,
        -0.7255665169801426,
        1.0,
        -0.7612207735769738,
        -0.7258159789810266,
        -0.733413687552078,
        -0.7583347091343899,
        -0.7475592519028895,
        -0.7369333401945988,
        -0.7480075762842221,
        -0.7531603011472257,
        -0.7370087942393506,
        -0.7557103451967553,
        -0.7438960554983032,
        -0.7590855104496301,
        -0.7438037969130076,
        -0.7329924328617665,
        -0.7619585483499124,
        -0.750703305165804,
        -0.7452264168836633,
        -0.7528632157028431,
        -0.7642446719793357,
        -0.75081056219686,
        -0.6863021279889018,
        -0.7435814098966514,
        -0.7015383948950925,
        -0.7301322534855068,
        -0.7159956951769264,
        -0.7457365961237905,
        -0.73961942338042,
        -0.7486603024480238,
        -0.7598197266564715,
        -0.7334479022886962,
        -0.7340030910159114,
        -0.7403896007101751,
        -0.7502003747066224,
        -0.7630636307741467,
        -0.7603659716913604,
        -0.7622880755497425,
        -0.732970491652591,
        -0.754174817757461,
        -0.74607406240833,
        -0.7560490125705521,
        -0.7437561507929762,
        -0.7431061358183116,
        -0.7415776664780639,
        -0.7526742440302122,
        -0.7557778232065314,
        -0.7621863924252315,
        -0.7575223905019364,
        -0.7612297025338346,
        -0.6702320371354982,
        -0.7382830021550442
      ],
      [
        -0.7599895258664191,
        -0.7037774878353911,
        -0.7418335196222654,
        -0.7612207735769738,
        1.0,
        -0.7451023716938585,
        -0.7341675370265239,
        -0.7294035129054686,
        -0.7419336104865892,
        -0.730210215042713,
        -0.7490065821649818,
        -0.7100950533600338,
        -0.7245424458736676,
        -0.7370156374032819,
        -0.7509095653399649,
        -0.749653406515453,
        -0.7213500231270785,
        -0.7497494024043052,
        -0.7540046332220101,
        -0.7051274894595845,
        -0.7225706558293992,
        -0.7469777265270046,
        -0.7567494043819525,
        -0.7559778558765274,
        -0.7524768751473054,
        -0.739550402197914,
        -0.7541129998277418,
        -0.7462210779855569,
        -0.7431555492394397,
        -0.7391944485798894,
        -0.7624435539421222,
        -0.740849193655267,
        -0.7527235440119642,
        -0.7434934554854782,
        -0.7270239324980101,
        -0.7297087574505507,
        -0.7118670791603057,
        -0.7604842604094304,
        -0.760402115631815,
        -0.7028448289526622,
        -0.74904226440737,
        -0.7509205321443517,
        -0.7442010701546404,
        -0.7526848077116448,
        -0.739998940205187,
        -0.6133737479805719,
        -0.7375969961181434,
        -0.7348576082349276,
        -0.761908554945486,
        -0.7565939025614603,
        -0.6949511956393655,
        -0.7512864475835567,
        -0.7601020766084032,
        -0.7388615992607552
      ],
      [
        -0.7172673446141851,
        -0.7393431184647088,
        -0.4940846190954195,
        -0.7258159789810266,
        -0.7451023716938585,
        1.0,
        -0.6919256308638997,
        -0.7425333611732041,
        -0.7276283576662168,
        -0.7191242543942871,
        -0.7409180917252327,
        -0.6802233925247176,
        -0.7220372823394771,
        -0.7295986607648492,
        -0.730462978003539,
        -0.7472790705443022,
        -0.7253862202445651,
        -0.7101892221587056,
        -0.7593943784800952,
        -0.7407229003793434,
        -0.7315376594191312,
        -0.7340732523267415,
        -0.760578484769104,
        -0.7485721416972013,
        -0.7222885795913254,
        -0.7343121421053128,
        -0.7182818388256278,
        -0.7172061710134212,
        -0.5679359615510173,
        -0.567624535835409,
        -0.7563325164486783,
        -0.7274664677841072,
        -0.7534326451429236,
        -0.7095725358358855,
        -0.7092873151235137,
        -0.7036864305357509,
        -0.6679931956834853,
        -0.7593875963045518,
        -0.7574877657417108,
        -0.7458355957170391,
        -0.7170936569504662,
        -0.7505003363987255,
        -0.7336955698848492,
        -0.7463601281426333,
        -0.7108654541658919,
        -0.7281950830705411,
        -0.7179191780610625,
        -0.7354252238947927,
        -0.7543135779712484,
        -0.7591960017544512,
        -0.7336652619940984,
        -0.7542216903580197,
        -0.7529147785451806,
        -0.6900598962698948
      ],
      [
        -0.7263389631584827,
        -0.7110023928838642,
        -0.686532014027115,
        -0.733413687552078,
        -0.7341675370265239,
        -0.6919256308638997,
        1.0,
        -0.714604853878454,
        -0.4811621249721618,
        -0.6902573306373005,
        -0.722319846988088,
        -0.7212693751561492,
        -0.37934978987512014,
        -0.7159021500295147,
        -0.44762071954432414,
        -0.7420844712211278,
        -0.7160098147543617,
        -0.4521895367541408,
        -0.752974272610315,
        -0.5521255272010248,
        -0.740071596530026,
        -0.7103896584846598,
        -0.744167985116311,
        -0.7197180300177306,
        -0.7254036721838688,
        -0.7185823628567184,
        -0.5200585901360731,
        -0.7174219581428474,
        -0.7190146944199685,
        -0.725620491888632,
        -0.7414559096450932,
        -0.7354149099635483,
        -0.7498545595761148,
        -0.7306398710931183,
        -0.7132584898862331,
        -0.7174039308888842,
        -0.7114610086590227,
        -0.7577640781175303,
        -0.7539927062541014,
        -0.7354581287711284,
        -0.42611890832352917,
        -0.7487004223162116,
        -0.5951632969917919,
        -0.7335594872728362,
        -0.6417047151345399,
        -0.7182616370206724,
        -0.7101415453588009,
        -0.7361705934381428,
        -0.7197767453918087,
        -0.7470394961245087,
        -0.7408543080361483,
        -0.7491321072478347,
        -0.7484881723784635,
        -0.7067771406342505
      ],
      [
        -0.7527310047905933,
        -0.7351630648226963,
        -0.7465242493767906,
        -0.7583347091343899,
        -0.7294035129054686,
        -0.7425333611732041,
        -0.714604853878454,
        1.0,
        -0.7249211182019046,
        -0.7491229943530282,
        -0.7112396457838821,
        -0.7391438796266774,
        -0.733329496463716,
        -0.7031578900309033,
        -0.7331466093886783,
        -0.7389442142918943,
        -0.7227406174602925,
        -0.7384858617054361,
        -0.7506300328188342,
        -0.7322403007041123,
        -0.7226087635614497,
        -0.5403090720802736,
        -0.7405680383386388,
        -0.7552863783724492,
        -0.7351905186904075,
        -0.7235006378978035,
        -0.7419179946710109,
        -0.7265055909276663,
        -0.7445440922303557,
        -0.7441757633123647,
        -0.7578439631722759,
        -0.7106725694572,
        -0.7290859960292602,
        -0.7476544890866306,
        -0.6581987983503012,
        -0.7324689429046454,
        -0.7432173217336671,
        -0.7522022761288584,
        -0.7546183170346724,
        -0.7315659995930264,
        -0.733912798342965,
        -0.6852299025275048,
        -0.7354622193702536,
        -0.7433528441721471,
        -0.7459297380635499,
        -0.7153211386919851,
        -0.7255735122970461,
        -0.7262675006599846,
        -0.7519523452330106,
        -0.7467881641634158,
        -0.7389853253260248,
        -0.7450964135757566,
        -0.7587252818663599,
        -0.7341206530854248
      ],
      [
        -0.7310676133134255,
        -0.7115356452487083,
        -0.6488591543348365,
        -0.7475592519028895,
        -0.7419336104865892,
        -0.7276283576662168,
        -0.4811621249721618,
        -0.7249211182019046,
        1.0,
        -0.6749109194098525,
        -0.728183398779505,
        -0.7449146793946887,
        -0.15828994085105422,
        -0.7229146310006989,
        -0.26368047740243095,
        -0.7177872163582412,
        -0.728177774999305,
        -0.26668680233007025,
        -0.7588203737478718,
        -0.4639356936234338,
        -0.7511554627503486,
        -0.6776602338064882,
        -0.6892421136729203,
        -0.721536309892826,
        -0.7307094077707121,
        -0.7422745484490952,
        -0.38173896519846545,
        -0.7368954925016185,
        -0.734882812204029,
        -0.7465367875790045,
        -0.7318402144800963,
        -0.7467460467749502,
        -0.7284445240733499,
        -0.7368984629162623,
        -0.7365865954313748,
        -0.7307347280587764,
        -0.740716015078358,
        -0.7582898641403881,
        -0.7531885698361924,
        -0.7268463619788229,
        -0.2190393495016768,
        -0.7563982341167215,
        -0.5652844913578827,
        -0.7408683980304268,
        -0.5322252768049949,
        -0.7427799143278012,
        -0.7256522846429023,
        -0.7430034772802911,
        -0.7314853343798842,
        -0.743162292446065,
        -0.7527290784685248,
        -0.7533816136362462,
        -0.7485248226711714,
        -0.73611195510001
      ],
      [
        -0.7088171471917339,
        -0.7238729791597167,
        -0.72940062017697,
        -0.7369333401945988,
        -0.730210215042713,
        -0.7191242543942871,
        -0.6902573306373005,
        -0.7491229943530282,
        -0.6749109194098525,
        1.0,
        -0.7125023097970784,
        -0.7423611283525222,
        -0.6712407091422099,
        -0.7269915255305154,
        -0.6949310260593631,
        -0.6772820723844623,
        -0.7249152917531944,
        -0.6985593140969926,
        -0.7498655944335048,
        -0.6443645542600607,
        -0.7235040127377634,
        -0.7299879802939502,
        -0.742883331632676,
        -0.4901015914680643,
        -0.7106100125310609,
        -0.7341894752793275,
        -0.7016454704488446,
        -0.7386961902046058,
        -0.7195109012998206,
        -0.7249245010768397,
        -0.7423391966045791,
        -0.7372296228066658,
        -0.7471149991048773,
        -0.7396311973486138,
        -0.7217233216508931,
        -0.7292669187030714,
        -0.7417802899816835,
        -0.7547716574760526,
        -0.7131046243909946,
        -0.7320365357960428,
        -0.6659987440372153,
        -0.7469738924845258,
        -0.6473247182304857,
        -0.7423862313131073,
        -0.6901184603177013,
        -0.7162437364437355,
        -0.6903731631166841,
        -0.708284093372272,
        -0.7489811144136168,
        -0.7262375432154405,
        -0.7318980555851629,
        -0.7523126029081114,
        -0.7458488470005344,
        -0.7337250706600545
      ],
      [
        -0.7525792221837337,
        -0.7375955040362383,
        -0.7457311345043676,
        -0.7480075762842221,
        -0.7490065821649818,
        -0.7409180917252327,
        -0.722319846988088,
        -0.7112396457838821,
        -0.728183398779505,
        -0.7125023097970784,
        1.0,
        -0.7560425896173086,
        -0.7175071403635656,
        -0.7269482192151251,
        -0.7182900775779044,
        -0.7208672473805381,
        -0.7342647340625965,
        -0.7264269500853223,
        -0.7512981580184978,
        -0.7186525843483483,
        -0.7351777568300928,
        -0.716788783293099,
        -0.7557684065209802,
        -0.7343510536082665,
        -0.7122126909485411,
        -0.747273558644592,
        -0.7329322081270021,
        -0.7431553258230399,
        -0.7410037914612606,
        -0.7497299055804998,
        -0.7266669034017617,
        -0.7532145993877635,
        -0.7507207847158077,
        -0.7457918174501967,
        -0.7341286394307076,
        -0.7475513740978406,
        -0.7530996913804179,
        -0.7607978813096969,
        -0.7346177082295078,
        -0.7486887510886217,
        -0.7117187723256331,
        -0.757161331956947,
        -0.7258185084377085,
        -0.7505967612540096,
        -0.6883789632850968,
        -0.740561550544833,
        -0.7214377060042119,
        -0.7486374858592226,
        -0.7505418559387373,
        -0.7445985037343996,
        -0.7544910971152452,
        -0.7321207485833613,
        -0.7518297232841772,
        -0.7465581855124166
      ],
      [
        -0.7307857914426203,
        -0.7409937783839466,
        -0.7120504903351799,
        -0.7531603011472257,
        -0.7100950533600338,
        -0.6802233925247176,
        -0.7212693751561492,
        -0.7391438796266774,
        -0.7449146793946887,
        -0.7423611283525222,
        -0.7560425896173086,
        1.0,
        -0.7503055366663014,
        -0.737476157917938,
        -0.7543936425107067,
        -0.759223876605625,
        -0.7409015141912141,
        -0.7475740467647604,
        -0.7630965884533463,
        -0.7411601868074517,
        -0.7386188113961438,
        -0.7481586503141554,
        -0.7619157384089481,
        -0.7553935261395366,
        -0.7482134027679491,
        -0.7383450378902416,
        -0.7488091559862788,
        -0.7393202518816784,
        -0.7072998997715338,
        -0.7150453029377719,
        -0.7634354565274287,
        -0.7346612117132547,
        -0.7598739081646727,
        -0.7185393511194323,
        -0.7229978174444657,
        -0.7420016620901895,
        -0.5853084650923348,
        -0.7602085848559681,
        -0.7624670103500815,
        -0.7325702057636265,
        -0.7490775466760993,
        -0.7512142399633102,
        -0.7494292143700758,
        -0.7535738974653148,
        -0.7487191848734271,
        -0.7370471890568071,
        -0.7434518261127135,
        -0.7433049941557364,
        -0.7630976803952704,
        -0.761226665761984,
        -0.7382620379410338,
        -0.7526313207102651,
        -0.7598708510493585,
        -0.7178606895603986
      ],
      [
        -0.7303078155181146,
        -0.7177203132893675,
        -0.6848924011409742,
        -0.7370087942393506,
        -0.7245424458736676,
        -0.7220372823394771,
        -0.37934978987512014,
        -0.733329496463716,
        -0.15828994085105422,
        -0.6712407091422099,
        -0.7175071403635656,
        -0.7503055366663014,
        1.0,
        -0.7293040044934826,
        -0.07367921508399611,
        -0.7363347465041874,
        -0.7235835281072583,
        -0.042653402739600485,
        -0.7406905363251396,
        -0.3772009226958314,
        -0.7454310294780058,
        -0.7062207977568751,
        -0.7474902231399597,
        -0.7176493740110754,
        -0.7113149367790064,
        -0.7295015433785876,
        -0.2568858936930017,
        -0.7358455079400722,
        -0.7264174966169599,
        -0.7439313551638888,
        -0.7266363736331027,
        -0.7492068801759588,
        -0.7442978385328072,
        -0.7445996965620891,
        -0.7330379948488643,
        -0.5073934206656474,
        -0.7458958355359304,
        -0.7596366445571805,
        -0.7471074362589871,
        -0.734919790416481,
        0.1183723250008438,
        -0.7548386426615473,
        -0.5152661944583031,
        -0.739542506159247,
        -0.4864631481641314,
        -0.6582695389290039,
        -0.679676718389971,
        -0.7355755351540624,
        -0.7195996017603279,
        -0.7353678320716006,
        -0.7504920880478705,
        -0.7519891944943553,
        -0.7458279826839981,
        -0.73496990707753
      ],
      [
        -0.7513344519268487,
        -0.6363943376031964,
        -0.7433328412719444,
        -0.7557103451967553,
        -0.7370156374032819,
        -0.7295986607648492,
        -0.7159021500295147,
        -0.7031578900309033,
        -0.7229146310006989,
        -0.7269915255305154,
        -0.7269482192151251,
        -0.737476157917938,
        -0.7293040044934826,
        1.0,
        -0.741455258181569,
        -0.743518785266717,
        -0.7044438930802912,
        -0.7299604483867036,
        -0.7505094785467397,
        -0.6935383902730778,
        -0.7202166777211267,
        -0.721709606839261,
        -0.7142338687808605,
        -0.7470436002956344,
        -0.7461879051631903,
        -0.7184313855800281,
        -0.7340658630324336,
        -0.7170591668114149,
        -0.7399346573425114,
        -0.7363505204392156,
        -0.7599027047925071,
        -0.7128406476744407,
        -0.7376329682066889,
        -0.7394291639179912,
        -0.6798857313712523,
        -0.7291088015905727,
        -0.742465717508947,
        -0.7361515106661836,
        -0.7463801581919707,
        -0.683361930613779,
        -0.72900658643564,
        -0.738565293415471,
        -0.7251661640965641,
        -0.7323763748568926,
        -0.7148092734922115,
        -0.7330621687852046,
        -0.7138979073417766,
        -0.7165860111592397,
        -0.7573137087406564,
        -0.7436051637709057,
        -0.7441562238699175,
        -0.49961099428648753,
        -0.7617181259204565,
        -0.7344549636850173
      ],
      [
        -0.7279540543739761,
        -0.7400755825229548,
        -0.684547500576635,
        -0.7438960554983032,
        -0.7509095653399649,
        -0.730462978003539,
        -0.44762071954432414,
        -0.7331466093886783,
        -0.26368047740243095,
        -0.6949310260593631,
        -0.7182900775779044,
        -0.7543936425107067,
        -0.07367921508399611,
        -0.741455258181569,
        1.0,
        -0.7419954489422402,
        -0.7333711212913316,
        -0.20363775515001953,
        -0.7578720159106631,
        -0.45999206570643614,
        -0.7552398370245698,
        -0.6814336092503807,
        -0.7495246294847145,
        -0.720554370690189,
        -0.725971712806234,
        -0.7477730506788134,
        -0.35129063500694035,
        -0.73462092343217,
        -0.7369157055588027,
        -0.7499803107306474,
        -0.7230855847890543,
        -0.7575141423136592,
        -0.7497192671832636,
        -0.7475293284721789,
        -0.7513087131837306,
        -0.7287619362621123,
        -0.7516222370341523,
        -0.760797677465971,
        -0.7525884126675584,
        -0.7449694998366021,
        -0.15809801077671118,
        -0.7623268844656144,
        -0.5364790858947346,
        -0.7458949859077563,
        -0.5252722833167724,
        -0.742526801363349,
        -0.7202135513364898,
        -0.7473437714452559,
        -0.7010459658317127,
        -0.7393074348171038,
        -0.7543390567282061,
        -0.7585826135940059,
        -0.7507020213520148,
        -0.7400530995771235
      ],
      [
        -0.7419738315935747,
        -0.7523419194464502,
        -0.7455928011973791,
        -0.7590855104496301,
        -0.749653406515453,
        -0.7472790705443022,
        -0.7420844712211278,
        -0.7389442142918943,
        -0.7177872163582412,
        -0.6772820723844623,
        -0.7208672473805381,
        -0.759223876605625,
        -0.7363347465041874,
        -0.743518785266717,
        -0.7419954489422402,
        1.0,
        -0.7426089207337483,
        -0.7440666900352748,
        -0.7624857257938307,
        -0.741099522308713,
        -0.7534147335829573,
        -0.7439192392046097,
        -0.7376326674952278,
        -0.7319326302068432,
        -0.7454068318960745,
        -0.739052002545993,
        -0.7462186926085055,
        -0.7515083575027062,
        -0.7487651329103331,
        -0.746345544237437,
        -0.7581623410676526,
        -0.7408198339506208,
        -0.7348546570411605,
        -0.755203130321364,
        -0.7230415821241503,
        -0.7422488909352222,
        -0.7528378898750655,
        -0.7614398716426867,
        -0.7424229623239753,
        -0.7572700352119356,
        -0.7327595762637373,
        -0.7491334965006499,
        -0.7344035942474686,
        -0.7583709539663623,
        -0.7423362336590602,
        -0.7406211598512762,
        -0.7427736251856938,
        -0.7549951265956767,
        -0.7570740260876001,
        -0.7522254358097104,
        -0.7525338207154333,
        -0.7567475284676404,
        -0.7588234235919513,
        -0.7470626514608032
      ],
      [
        -0.7369007615613858,
        -0.7183097280619858,
        -0.7331296214248499,
        -0.7438037969130076,
        -0.7213500231270785,
        -0.7253862202445651,
        -0.7160098147543617,
        -0.7227406174602925,
        -0.728177774999305,
        -0.7249152917531944,
        -0.7342647340625965,
        -0.7409015141912141,
        -0.7235835281072583,
        -0.7044438930802912,
        -0.7333711212913316,
        -0.7426089207337483,
        1.0,
        -0.7244253672321475,
        -0.755699547529123,
        -0.7204149081730413,
        -0.7262240595686638,
        -0.730741866608343,
        -0.7510899812129009,
        -0.7487309970516312,
        -0.7366838503934119,
        -0.7225406038591355,
        -0.7321262362966297,
        -0.726509993787533,
        -0.7368849024016526,
        -0.7245016342179109,
        -0.7569559097877414,
        -0.7157890885725385,
        -0.7288825448338117,
        -0.7284242761386656,
        -0.6932020988356415,
        -0.7360577774576158,
        -0.7331578138286444,
        -0.7547601241426867,
        -0.7584000701179597,
        -0.7299897588788613,
        -0.7225612592011033,
        -0.7383054547070781,
        -0.7292281649218635,
        -0.7397760071972399,
        -0.7364020010403722,
        -0.6952111375418348,
        -0.710925943046058,
        -0.7073615391795613,
        -0.751773935699199,
        -0.7191247159131482,
        -0.5956255968290802,
        -0.7371029682887058,
        -0.7565327905760952,
        -0.7236631320597364
      ],
      [
        -0.7272299917044276,
        -0.7239572995887751,
        -0.6902555437181752,
        -0.7329924328617665,
        -0.7497494024043052,
        -0.7101892221587056,
        -0.4521895367541408,
        -0.7384858617054361,
        -0.26668680233007025,
        -0.6985593140969926,
        -0.7264269500853223,
        -0.7475740467647604,
        -0.042653402739600485,
        -0.7299604483867036,
        -0.20363775515001953,
        -0.7440666900352748,
        -0.7244253672321475,
        1.0,
        -0.7508732066355215,
        -0.45819560830319794,
        -0.7429046828825012,
        -0.7098540861388032,
        -0.752901522733403,
        -0.7269633738381964,
        -0.7240033398920158,
        -0.6916213448643527,
        -0.2798810295076709,
        -0.7243742214278277,
        -0.7311346960477072,
        -0.7450911034510062,
        -0.7289856058807727,
        -0.746841775778824,
        -0.7481396771442472,
        -0.7471927033370247,
        -0.722710819287437,
        -0.7214154373797669,
        -0.7404516235472001,
        -0.7595834400301764,
        -0.7527388892559215,
        -0.7439014078049575,
        -0.12358502890173247,
        -0.7533596756408625,
        -0.547057429632271,
        -0.7259350798739427,
        -0.5042290768195716,
        -0.7348916838096389,
        -0.7100949576968919,
        -0.7302418484349035,
        -0.722284167756073,
        -0.7445180888960645,
        -0.7509188069431867,
        -0.752026309483657,
        -0.7484269444911722,
        -0.7066207779080644
      ],
      [
        -0.7494446938682968,
        -0.7546913916124254,
        -0.7602684418491448,
        -0.7619585483499124,
        -0.7540046332220101,
        -0.7593943784800952,
        -0.752974272610315,
        -0.7506300328188342,
        -0.7588203737478718,
        -0.7498655944335048,
        -0.7512981580184978,
        -0.7630965884533463,
        -0.7406905363251396,
        -0.7505094785467397,
        -0.7578720159106631,
        -0.7624857257938307,
        -0.755699547529123,
        -0.7508732066355215,
        1.0,
        -0.7530164712114262,
        -0.7173888344297057,
        -0.7549123490860525,
        -0.7637655713405032,
        -0.759437580399478,
        -0.7610123026990039,
        -0.739566839365152,
        -0.7576689073856029,
        -0.7504404218079208,
        -0.7561388776900794,
        -0.755382662618139,
        -0.7635941249752523,
        -0.7586878050749941,
        -0.7618849898804202,
        -0.7581345773418158,
        -0.7466107848602754,
        -0.732351169538169,
        -0.7581175518720094,
        -0.7593397452251014,
        -0.7399475375895594,
        -0.7611416512102217,
        -0.7520618936074484,
        -0.7588705640626825,
        -0.7567722806325784,
        -0.7349577378101322,
        -0.7584181385533278,
        -0.723447748120579,
        -0.5508860109828145,
        -0.7449992540259538,
        -0.7634467365887981,
        -0.7561381393255071,
        -0.7581991226613843,
        -0.7554122040081324,
        -0.7621696350442897,
        -0.7556679750529054
      ],
      [
        -0.7473276290131126,
        -0.5208965879718289,
        -0.7188997493010058,
        -0.750703305165804,
        -0.7051274894595845,
        -0.7407229003793434,
        -0.5521255272010248,
        -0.7322403007041123,
        -0.4639356936234338,
        -0.6443645542600607,
        -0.7186525843483483,
        -0.7411601868074517,
        -0.3772009226958314,
        -0.6935383902730778,
        -0.45999206570643614,
        -0.741099522308713,
        -0.7204149081730413,
        -0.45819560830319794,
        -0.7530164712114262,
        1.0,
        -0.7230802808606709,
        -0.7188979840574086,
        -0.7392326609047687,
        -0.7268789110535173,
        -0.7334660176112944,
        -0.7445677784843143,
        -0.5286611507913125,
        -0.7396280627997955,
        -0.737420445311076,
        -0.7535835097359327,
        -0.7276904292303,
        -0.746770571853477,
        -0.7408300677490796,
        -0.710288109211153,
        -0.7349426141561255,
        -0.721478720633782,
        -0.7476526263319021,
        -0.7552402491422909,
        -0.7451957368378487,
        -0.6065357678666894,
        -0.44543202741532706,
        -0.756436284667899,
        -0.6065160040381807,
        -0.7236801655034757,
        -0.5428337839083017,
        -0.735837943426811,
        -0.7270953563079369,
        -0.7272512025624069,
        -0.7404648357615263,
        -0.7329616108710988,
        -0.7498610546212044,
        -0.715888333539642,
        -0.7504636497790942,
        -0.7400908240030619
      ],
      [
        -0.7535879514993358,
        -0.718441299000496,
        -0.7448451838258818,
        -0.7452264168836633,
        -0.7225706558293992,
        -0.7315376594191312,
        -0.740071596530026,
        -0.7226087635614497,
        -0.7511554627503486,
        -0.7235040127377634,
        -0.7351777568300928,
        -0.7386188113961438,
        -0.7454310294780058,
        -0.7202166777211267,
        -0.7552398370245698,
        -0.7534147335829573,
        -0.7262240595686638,
        -0.7429046828825012,
        -0.7173888344297057,
        -0.7230802808606709,
        1.0,
        -0.7383373323597937,
        -0.7591027450654122,
        -0.7549481001142371,
        -0.745400544036123,
        -0.6148036530904581,
        -0.7498486705052558,
        -0.737865614099624,
        -0.7375445109320405,
        -0.7241834604225171,
        -0.7606835468248314,
        -0.6278569331266493,
        -0.7574938948752067,
        -0.7410645229299626,
        -0.5379875402916026,
        -0.7327383565078791,
        -0.7355474607012188,
        -0.7540015210606288,
        -0.7586745769414976,
        -0.7405901012548508,
        -0.7494887342936305,
        -0.7165745869291873,
        -0.7442480547697509,
        -0.7419149415821005,
        -0.7485605858753257,
        -0.5567218958097955,
        -0.7251850524447999,
        -0.7045919799913865,
        -0.7619533793548662,
        -0.7559965606900436,
        -0.7410289558329323,
        -0.7422738319379898,
        -0.7594948200204521,
        -0.7293282415622219
      ],
      [
        -0.7385104526141866,
        -0.7322821539034832,
        -0.7286863203033683,
        -0.7528632157028431,
        -0.7469777265270046,
        -0.7340732523267415,
        -0.7103896584846598,
        -0.5403090720802736,
        -0.6776602338064882,
        -0.7299879802939502,
        -0.716788783293099,
        -0.7481586503141554,
        -0.7062207977568751,
        -0.721709606839261,
        -0.6814336092503807,
        -0.7439192392046097,
        -0.730741866608343,
        -0.7098540861388032,
        -0.7549123490860525,
        -0.7188979840574086,
        -0.7383373323597937,
        1.0,
        -0.7365319868941954,
        -0.7415970976912536,
        -0.7348817453898258,
        -0.742576839304911,
        -0.7082463242206481,
        -0.7242766077892825,
        -0.742012375107779,
        -0.7427996759005766,
        -0.7455663181697616,
        -0.7404777196839252,
        -0.7457859893619834,
        -0.743811336453366,
        -0.7208001709388832,
        -0.7398554843364877,
        -0.7397162602726219,
        -0.7600237694106815,
        -0.758135351925348,
        -0.7427073694720077,
        -0.7055103216831382,
        -0.7497284480143074,
        -0.7148179349148069,
        -0.7361455132039124,
        -0.7314549536729386,
        -0.7365794089791886,
        -0.7232807959363089,
        -0.74573801869136,
        -0.7422243209629431,
        -0.7519475205583304,
        -0.749214379691664,
        -0.7471347061078011,
        -0.7588289535781827,
        -0.7376109369188333
      ],
      [
        -0.7609904377514659,
        -0.7471719617538797,
        -0.7593726178571895,
        -0.7642446719793357,
        -0.7567494043819525,
        -0.760578484769104,
        -0.744167985116311,
        -0.7405680383386388,
        -0.6892421136729203,
        -0.742883331632676,
        -0.7557684065209802,
        -0.7619157384089481,
        -0.7474902231399597,
        -0.7142338687808605,
        -0.7495246294847145,
        -0.7376326674952278,
        -0.7510899812129009,
        -0.752901522733403,
        -0.7637655713405032,
        -0.7392326609047687,
        -0.7591027450654122,
        -0.7365319868941954,
        1.0,
        -0.7548610679072001,
        -0.7619739771101366,
        -0.7504584193967456,
        -0.7350748578957582,
        -0.7562074709025273,
        -0.7617869659506912,
        -0.7615121492156978,
        -0.7621745393407316,
        -0.7505363234004212,
        -0.7487264796725479,
        -0.7555446165482156,
        -0.742406880264955,
        -0.7594001878916656,
        -0.7592889577862223,
        -0.7519820406044911,
        -0.7619328995370942,
        -0.7478585162471325,
        -0.7506459797479039,
        -0.7536099729549754,
        -0.7529551039815279,
        -0.7553777000827009,
        -0.749620516373392,
        -0.7578073712989786,
        -0.754923043375828,
        -0.7514753269673351,
        -0.7633910068725931,
        -0.7614342843896926,
        -0.7621535225881431,
        -0.7519379510987135,
        -0.7638311298874108,
        -0.7596189639543642
      ],
      [
        -0.7478729717387812,
        -0.7461445180245718,
        -0.7490211003886944,
        -0.75081056219686,
        -0.7559778558765274,
        -0.7485721416972013,
        -0.7197180300177306,
        -0.7552863783724492,
        -0.721536309892826,
        -0.4901015914680643,
        -0.7343510536082665,
        -0.7553935261395366,
        -0.7176493740110754,
        -0.7470436002956344,
        -0.720554370690189,
        -0.7319326302068432,
        -0.7487309970516312,
        -0.7269633738381964,
        -0.759437580399478,
        -0.7268789110535173,
        -0.7549481001142371,
        -0.7415970976912536,
        -0.7548610679072001,
        1.0,
        -0.7460559573109258,
        -0.7521526632957745,
        -0.7282728755293697,
        -0.7458651480615868,
        -0.7443229879457105,
        -0.7519137039445543,
        -0.7464392654848833,
        -0.7526135773413454,
        -0.7577680228424457,
        -0.753407397294898,
        -0.746218088897107,
        -0.7535760968217706,
        -0.7518653101848365,
        -0.762171401932748,
        -0.7439331844215571,
        -0.7555406642900535,
        -0.7121127633680939,
        -0.7583398856599411,
        -0.7129843713903117,
        -0.7468257573616761,
        -0.7322643415182271,
        -0.7515258981912281,
        -0.737354303247592,
        -0.7526992047065393,
        -0.7540324609612561,
        -0.7539886915110418,
        -0.755951570403725,
        -0.755333623984489,
        -0.7524559534002935,
        -0.7505013071240297
      ],
      [
        -0.749712050069953,
        -0.7551111411597704,
        -0.7301357664687131,
        -0.6863021279889018,
        -0.7524768751473054,
        -0.7222885795913254,
        -0.7254036721838688,
        -0.7351905186904075,
        -0.7307094077707121,
        -0.7106100125310609,
        -0.7122126909485411,
        -0.7482134027679491,
        -0.7113149367790064,
        -0.7461879051631903,
        -0.725971712806234,
        -0.7454068318960745,
        -0.7366838503934119,
        -0.7240033398920158,
        -0.7610123026990039,
        -0.7334660176112944,
        -0.745400544036123,
        -0.7348817453898258,
        -0.7619739771101366,
        -0.7460559573109258,
        1.0,
        -0.7507779096379301,
        -0.7104276174147164,
        -0.7341045843708922,
        -0.7038598935752309,
        -0.617807137172216,
        -0.754646205108888,
        -0.7529312731903886,
        -0.7550708903684482,
        -0.7450590108294848,
        -0.7371449109071551,
        -0.7336320847364264,
        -0.6743973403080241,
        -0.7619313808978295,
        -0.7539195913669701,
        -0.7574773890277252,
        -0.7022335154140166,
        -0.7567904899534472,
        -0.7190028401572254,
        -0.7550728084462341,
        -0.7405784590880526,
        -0.5454631354815497,
        -0.7286443075574114,
        -0.7523584563593331,
        -0.7333536513746617,
        -0.7303049919045428,
        -0.7458972981655977,
        -0.7576119737864866,
        -0.7293335027807111,
        -0.740764012379391
      ],
      [
        -0.7496127200423314,
        -0.7293324233455237,
        -0.7430074508288895,
        -0.7435814098966514,
        -0.739550402197914,
        -0.7343121421053128,
        -0.7185823628567184,
        -0.7235006378978035,
        -0.7422745484490952,
        -0.7341894752793275,
        -0.747273558644592,
        -0.7383450378902416,
        -0.7295015433785876,
        -0.7184313855800281,
        -0.7477730506788134,
        -0.739052002545993,
        -0.7225406038591355,
        -0.6916213448643527,
        -0.739566839365152,
        -0.7445677784843143,
        -0.6148036530904581,
        -0.742576839304911,
        -0.7504584193967456,
        -0.7521526632957745,
        -0.7507779096379301,
        1.0,
        -0.7422062026766387,
        -0.73239526039292,
        -0.7399144483142461,
        -0.7242692233947259,
        -0.7630735326777642,
        -0.3747330551226784,
        -0.7547608295407509,
        -0.7469757944112203,
        -0.2224621340441212,
        -0.7104668287652213,
        -0.7315176808247348,
        -0.7488688786367055,
        -0.756417349894749,
        -0.7457737064423815,
        -0.7344487211993226,
        -0.5322005398032772,
        -0.73940482970522,
        -0.7289662244381339,
        -0.7497063853121086,
        -0.6990633449588128,
        -0.7187496053822471,
        -0.6704545470464465,
        -0.760886482374118,
        -0.7535477594269062,
        -0.7346436192393877,
        -0.7436580218338884,
        -0.7608055400132032,
        -0.7125740098126763
      ],
      [
        -0.7334538686924721,
        -0.7384058835455267,
        -0.7045591969492733,
        -0.7015383948950925,
        -0.7541129998277418,
        -0.7182818388256278,
        -0.5200585901360731,
        -0.7419179946710109,
        -0.38173896519846545,
        -0.7016454704488446,
        -0.7329322081270021,
        -0.7488091559862788,
        -0.2568858936930017,
        -0.7340658630324336,
        -0.35129063500694035,
        -0.7462186926085055,
        -0.7321262362966297,
        -0.2798810295076709,
        -0.7576689073856029,
        -0.5286611507913125,
        -0.7498486705052558,
        -0.7082463242206481,
        -0.7350748578957582,
        -0.7282728755293697,
        -0.7104276174147164,
        -0.7422062026766387,
        1.0,
        -0.7229969536802319,
        -0.7350191099067265,
        -0.7433159955478614,
        -0.7310987100722501,
        -0.7493703267594041,
        -0.7492840090777856,
        -0.7452653952565751,
        -0.7387402553226465,
        -0.7311998240194213,
        -0.743734614401422,
        -0.7597629959378667,
        -0.7543222311331812,
        -0.7467189381247366,
        -0.26978484562883465,
        -0.7582216223709413,
        -0.5879814271667412,
        -0.7399929811971877,
        -0.5433326630304375,
        -0.7410009705383211,
        -0.7169670298006976,
        -0.7440669681034326,
        -0.715952982718222,
        -0.7476736596579026,
        -0.7524466773079185,
        -0.7559364141621432,
        -0.7471381500015195,
        -0.7317936304445201
      ],
      [
        -0.7192066351194157,
        -0.7197911339186839,
        -0.7260773127153748,
        -0.7301322534855068,
        -0.7462210779855569,
        -0.7172061710134212,
        -0.7174219581428474,
        -0.7265055909276663,
        -0.7368954925016185,
        -0.7386961902046058,
        -0.7431553258230399,
        -0.7393202518816784,
        -0.7358455079400722,
        -0.7170591668114149,
        -0.73462092343217,
        -0.7515083575027062,
        -0.726509993787533,
        -0.7243742214278277,
        -0.7504404218079208,
        -0.7396280627997955,
        -0.737865614099624,
        -0.7242766077892825,
        -0.7562074709025273,
        -0.7458651480615868,
        -0.7341045843708922,
        -0.73239526039292,
        -0.7229969536802319,
        1.0,
        -0.7231629400078815,
        -0.7120816945810392,
        -0.7563045208033335,
        -0.7370580265298396,
        -0.7526430046990521,
        -0.7298070881085956,
        -0.7085089883032205,
        -0.7137237177898401,
        -0.7042442335655645,
        -0.7599086152471922,
        -0.7562382709633029,
        -0.7484669087593532,
        -0.7270037111956245,
        -0.7466840497608773,
        -0.7303931428143196,
        -0.7172892627019111,
        -0.735160868553927,
        -0.7235985818700112,
        -0.7049365722311162,
        -0.7479633373550743,
        -0.7527496775127331,
        -0.7532684391055049,
        -0.7415062372611684,
        -0.7478903484962972,
        -0.7531708471936353,
        -0.7168070725288653
      ],
      [
        -0.72890272471887,
        -0.7432814578742343,
        -0.6772892872346983,
        -0.7159956951769264,
        -0.7431555492394397,
        -0.5679359615510173,
        -0.7190146944199685,
        -0.7445440922303557,
        -0.734882812204029,
        -0.7195109012998206,
        -0.7410037914612606,
        -0.7072998997715338,
        -0.7264174966169599,
        -0.7399346573425114,
        -0.7369157055588027,
        -0.7487651329103331,
        -0.7368849024016526,
        -0.7311346960477072,
        -0.7561388776900794,
        -0.737420445311076,
        -0.7375445109320405,
        -0.742012375107779,
        -0.7617869659506912,
        -0.7443229879457105,
        -0.7038598935752309,
        -0.7399144483142461,
        -0.7350191099067265,
        -0.7231629400078815,
        1.0,
        -0.44897515010773603,
        -0.7557568352740376,
        -0.7398801868081829,
        -0.7590978471954389,
        -0.6530621553302315,
        -0.7148268285629429,
        -0.7171004990588965,
        -0.6953986998023938,
        -0.7601453083084326,
        -0.7459272045642023,
        -0.7495428395838868,
        -0.7184497038554674,
        -0.7477453010528816,
        -0.7357585814246179,
        -0.7455427073798404,
        -0.7400743383641536,
        -0.7015138809368292,
        -0.7198753266182414,
        -0.7458822613778657,
        -0.7581105655252696,
        -0.7532752472094888,
        -0.7349174211038628,
        -0.754059879601235,
        -0.7146638497723987,
        -0.7104112739932913
      ],
      [
        -0.7048315001373449,
        -0.7465180414105999,
        -0.6887247616625546,
        -0.7457365961237905,
        -0.7391944485798894,
        -0.567624535835409,
        -0.725620491888632,
        -0.7441757633123647,
        -0.7465367875790045,
        -0.7249245010768397,
        -0.7497299055804998,
        -0.7150453029377719,
        -0.7439313551638888,
        -0.7363505204392156,
        -0.7499803107306474,
        -0.746345544237437,
        -0.7245016342179109,
        -0.7450911034510062,
        -0.755382662618139,
        -0.7535835097359327,
        -0.7241834604225171,
        -0.7427996759005766,
        -0.7615121492156978,
        -0.7519137039445543,
        -0.617807137172216,
        -0.7242692233947259,
        -0.7433159955478614,
        -0.7120816945810392,
        -0.44897515010773603,
        1.0,
        -0.7616350642306173,
        -0.7279842005407645,
        -0.7588437705734306,
        -0.7364067578863756,
        -0.6216315686890743,
        -0.7292954400884178,
        -0.5605223223674614,
        -0.7591955252364395,
        -0.7592653209629381,
        -0.753152902525875,
        -0.7400469717883627,
        -0.7411477841396557,
        -0.7321862001322852,
        -0.743469839727946,
        -0.7512641101558746,
        -0.5955958883249302,
        -0.7138080074664881,
        -0.7396753057132808,
        -0.7580875317571253,
        -0.7543812420722962,
        -0.543844679940846,
        -0.7560487357013858,
        -0.7547374568022021,
        -0.6973325429872564
      ],
      [
        -0.755758117610922,
        -0.754664163579115,
        -0.7508863043861049,
        -0.73961942338042,
        -0.7624435539421222,
        -0.7563325164486783,
        -0.7414559096450932,
        -0.7578439631722759,
        -0.7318402144800963,
        -0.7423391966045791,
        -0.7266669034017617,
        -0.7634354565274287,
        -0.7266363736331027,
        -0.7599027047925071,
        -0.7230855847890543,
        -0.7581623410676526,
        -0.7569559097877414,
        -0.7289856058807727,
        -0.7635941249752523,
        -0.7276904292303,
        -0.7606835468248314,
        -0.7455663181697616,
        -0.7621745393407316,
        -0.7464392654848833,
        -0.754646205108888,
        -0.7630735326777642,
        -0.7310987100722501,
        -0.7563045208033335,
        -0.7557568352740376,
        -0.7616350642306173,
        1.0,
        -0.7639191561268067,
        -0.7601741280744996,
        -0.7578883681144037,
        -0.760608257127352,
        -0.7578164225864275,
        -0.7610095908322245,
        -0.764857334518035,
        -0.7633275341092345,
        -0.759825480120883,
        -0.7292788062597577,
        -0.7649843106546711,
        -0.7498940860988255,
        -0.7554432732664883,
        -0.7373930911718851,
        -0.7616963780142383,
        -0.7557557676489802,
        -0.7633262516053736,
        -0.7296227286826737,
        -0.7605324502928263,
        -0.7629251071614902,
        -0.7636777140617683,
        -0.723194892846206,
        -0.7582264214237053
      ],
      [
        -0.7510294274804743,
        -0.7261115714222294,
        -0.7473597095862949,
        -0.7486603024480238,
        -0.740849193655267,
        -0.7274664677841072,
        -0.7354149099635483,
        -0.7106725694572,
        -0.7467460467749502,
        -0.7372296228066658,
        -0.7532145993877635,
        -0.7346612117132547,
        -0.7492068801759588,
        -0.7128406476744407,
        -0.7575141423136592,
        -0.7408198339506208,
        -0.7157890885725385,
        -0.746841775778824,
        -0.7586878050749941,
        -0.746770571853477,
        -0.6278569331266493,
        -0.7404777196839252,
        -0.7505363234004212,
        -0.7526135773413454,
        -0.7529312731903886,
        -0.3747330551226784,
        -0.7493703267594041,
        -0.7370580265298396,
        -0.7398801868081829,
        -0.7279842005407645,
        -0.7639191561268067,
        1.0,
        -0.7490998666819488,
        -0.748442458690919,
        -0.20062332492899926,
        -0.7254976274176194,
        -0.741113089585149,
        -0.7400118039409891,
        -0.7594565231187347,
        -0.7429230618145388,
        -0.7433316578994404,
        -0.5320718205329105,
        -0.7313094869705286,
        -0.7377572021180886,
        -0.7522448778656086,
        -0.7267493796071458,
        -0.7297489753154373,
        -0.7119897845473343,
        -0.7615530548686095,
        -0.7564680399167724,
        -0.7171285197036504,
        -0.7456547877497268,
        -0.7605363464651396,
        -0.73265089721414
      ],
      [
        -0.7556578553914115,
        -0.7455001583906451,
        -0.7517339497789545,
        -0.7598197266564715,
        -0.7527235440119642,
        -0.7534326451429236,
        -0.7498545595761148,
        -0.7290859960292602,
        -0.7284445240733499,
        -0.7471149991048773,
        -0.7507207847158077,
        -0.7598739081646727,
        -0.7442978385328072,
        -0.7376329682066889,
        -0.7497192671832636,
        -0.7348546570411605,
        -0.7288825448338117,
        -0.7481396771442472,
        -0.7618849898804202,
        -0.7408300677490796,
        -0.7574938948752067,
        -0.7457859893619834,
        -0.7487264796725479,
        -0.7577680228424457,
        -0.7550708903684482,
        -0.7547608295407509,
        -0.7492840090777856,
        -0.7526430046990521,
        -0.7590978471954389,
        -0.7588437705734306,
        -0.7601741280744996,
        -0.7490998666819488,
        1.0,
        -0.7538082544089713,
        -0.7458332703986632,
        -0.7543956537503569,
        -0.7579254390309249,
        -0.7581652559009582,
        -0.7400158655934298,
        -0.7557903192033695,
        -0.7293717808307478,
        -0.7573466542663503,
        -0.7462639696218217,
        -0.7550036512060486,
        -0.749708634954438,
        -0.756584525845456,
        -0.7454623893414574,
        -0.752999858941091,
        -0.7610360853614353,
        -0.7547157136534228,
        -0.7460917437574958,
        -0.7469088168279931,
        -0.7597601430244264,
        -0.753313904146524
      ],
      [
        -0.7541203439041465,
        -0.6953783436383144,
        -0.6192827031735227,
        -0.7334479022886962,
        -0.7434934554854782,
        -0.7095725358358855,
        -0.7306398710931183,
        -0.7476544890866306,
        -0.7368984629162623,
        -0.7396311973486138,
        -0.7457918174501967,
        -0.7185393511194323,
        -0.7445996965620891,
        -0.7394291639179912,
        -0.7475293284721789,
        -0.755203130321364,
        -0.7284242761386656,
        -0.7471927033370247,
        -0.7581345773418158,
        -0.710288109211153,
        -0.7410645229299626,
        -0.743811336453366,
        -0.7555446165482156,
        -0.753407397294898,
        -0.7450590108294848,
        -0.7469757944112203,
        -0.7452653952565751,
        -0.7298070881085956,
        -0.6530621553302315,
        -0.7364067578863756,
        -0.7578883681144037,
        -0.748442458690919,
        -0.7538082544089713,
        1.0,
        -0.732506413244441,
        -0.7460175312806623,
        -0.7173744789572238,
        -0.7594183969831345,
        -0.7627844341201581,
        -0.7283081209689144,
        -0.7450365557864795,
        -0.7509926894202716,
        -0.7461217825891366,
        -0.7427901423410195,
        -0.7336537366635243,
        -0.7372319831827052,
        -0.7354147839337742,
        -0.7461999754092274,
        -0.7591064990902834,
        -0.759807821829825,
        -0.750644959697563,
        -0.7395180024453553,
        -0.7468687943792163,
        -0.7288188172828673
      ],
      [
        -0.7297474528755068,
        -0.6961091963351675,
        -0.7267878242152244,
        -0.7340030910159114,
        -0.7270239324980101,
        -0.7092873151235137,
        -0.7132584898862331,
        -0.6581987983503012,
        -0.7365865954313748,
        -0.7217233216508931,
        -0.7341286394307076,
        -0.7229978174444657,
        -0.7330379948488643,
        -0.6798857313712523,
        -0.7513087131837306,
        -0.7230415821241503,
        -0.6932020988356415,
        -0.722710819287437,
        -0.7466107848602754,
        -0.7349426141561255,
        -0.5379875402916026,
        -0.7208001709388832,
        -0.742406880264955,
        -0.746218088897107,
        -0.7371449109071551,
        -0.2224621340441212,
        -0.7387402553226465,
        -0.7085089883032205,
        -0.7148268285629429,
        -0.6216315686890743,
        -0.760608257127352,
        -0.20062332492899926,
        -0.7458332703986632,
        -0.732506413244441,
        1.0,
        -0.6868927800006005,
        -0.7090496807415445,
        -0.7391849253310512,
        -0.7541410448469994,
        -0.7378179035666517,
        -0.7252445450797784,
        -0.47579807977694644,
        -0.7278133903396304,
        -0.7104261393081712,
        -0.7417261278289011,
        -0.6507563211200396,
        -0.6974187062681703,
        -0.6915317096861626,
        -0.7580749461734185,
        -0.7490507322055056,
        -0.6494028391373299,
        -0.733647578394627,
        -0.7554491095769758,
        -0.689205210840925
      ],
      [
        -0.750314368420902,
        -0.7206532459281646,
        -0.7296605728045544,
        -0.7403896007101751,
        -0.7297087574505507,
        -0.7036864305357509,
        -0.7174039308888842,
        -0.7324689429046454,
        -0.7307347280587764,
        -0.7292669187030714,
        -0.7475513740978406,
        -0.7420016620901895,
        -0.5073934206656474,
        -0.7291088015905727,
        -0.7287619362621123,
        -0.7422488909352222,
        -0.7360577774576158,
        -0.7214154373797669,
        -0.732351169538169,
        -0.721478720633782,
        -0.7327383565078791,
        -0.7398554843364877,
        -0.7594001878916656,
        -0.7535760968217706,
        -0.7336320847364264,
        -0.7104668287652213,
        -0.7311998240194213,
        -0.7137237177898401,
        -0.7171004990588965,
        -0.7292954400884178,
        -0.7578164225864275,
        -0.7254976274176194,
        -0.7543956537503569,
        -0.7460175312806623,
        -0.6868927800006005,
        1.0,
        -0.7317881513282053,
        -0.758326306506305,
        -0.7466150703748065,
        -0.748095749571738,
        -0.7144577072358678,
        -0.7344472154666868,
        -0.7433988196787021,
        -0.7439457294782043,
        -0.7324665407618072,
        -0.6238051462488607,
        -0.6864632673292603,
        -0.7411115821175496,
        -0.7601923658029193,
        -0.7540489136102377,
        -0.7461192623330093,
        -0.7365710240404533,
        -0.7591181944404872,
        -0.7325191545458432
      ],
      [
        -0.6706704664524443,
        -0.7418151554891106,
        -0.6732763606386354,
        -0.7502003747066224,
        -0.7118670791603057,
        -0.6679931956834853,
        -0.7114610086590227,
        -0.7432173217336671,
        -0.740716015078358,
        -0.7417802899816835,
        -0.7530996913804179,
        -0.5853084650923348,
        -0.7458958355359304,
        -0.742465717508947,
        -0.7516222370341523,
        -0.7528378898750655,
        -0.7331578138286444,
        -0.7404516235472001,
        -0.7581175518720094,
        -0.7476526263319021,
        -0.7355474607012188,
        -0.7397162602726219,
        -0.7592889577862223,
        -0.7518653101848365,
        -0.6743973403080241,
        -0.7315176808247348,
        -0.743734614401422,
        -0.7042442335655645,
        -0.6953986998023938,
        -0.5605223223674614,
        -0.7610095908322245,
        -0.741113089585149,
        -0.7579254390309249,
        -0.7173744789572238,
        -0.7090496807415445,
        -0.7317881513282053,
        1.0,
        -0.7620880131944007,
        -0.7621762822386359,
        -0.7414106756064638,
        -0.7448330857643326,
        -0.7499918919992876,
        -0.7469544063372613,
        -0.7438636786346222,
        -0.7493680185741824,
        -0.6467861920901308,
        -0.739435441645939,
        -0.7433221104165648,
        -0.7583098760068736,
        -0.7610655705124283,
        -0.7263042344682284,
        -0.7533997081434418,
        -0.7609592707227614,
        -0.6778264123313409
      ],
      [
        -0.7621173423363106,
        -0.7508663421181007,
        -0.762187104382616,
        -0.7630636307741467,
        -0.7604842604094304,
        -0.7593875963045518,
        -0.7577640781175303,
        -0.7522022761288584,
        -0.7582898641403881,
        -0.7547716574760526,
        -0.7607978813096969,
        -0.7602085848559681,
        -0.7596366445571805,
        -0.7361515106661836,
        -0.760797677465971,
        -0.7614398716426867,
        -0.7547601241426867,
        -0.7595834400301764,
        -0.7593397452251014,
        -0.7552402491422909,
        -0.7540015210606288,
        -0.7600237694106815,
        -0.7519820406044911,
        -0.762171401932748,
        -0.7619313808978295,
        -0.7488688786367055,
        -0.7597629959378667,
        -0.7599086152471922,
        -0.7601453083084326,
        -0.7591955252364395,
        -0.764857334518035,
        -0.7400118039409891,
        -0.7581652559009582,
        -0.7594183969831345,
        -0.7391849253310512,
        -0.758326306506305,
        -0.7620880131944007,
        1.0,
        -0.7639280892268845,
        -0.7547440666754441,
        -0.758977719943376,
        -0.7477519496379041,
        -0.7560250311675156,
        -0.7586724947299954,
        -0.7597597825141897,
        -0.7559409735000875,
        -0.7540276110522095,
        -0.748019468051843,
        -0.76440659862891,
        -0.7590776529044272,
        -0.7566885308245684,
        -0.7502816278054153,
        -0.7625427462609676,
        -0.7586947820348328
      ],
      [
        -0.7544082577223374,
        -0.756849847365203,
        -0.7601765179827624,
        -0.7603659716913604,
        -0.760402115631815,
        -0.7574877657417108,
        -0.7539927062541014,
        -0.7546183170346724,
        -0.7531885698361924,
        -0.7131046243909946,
        -0.7346177082295078,
        -0.7624670103500815,
        -0.7471074362589871,
        -0.7463801581919707,
        -0.7525884126675584,
        -0.7424229623239753,
        -0.7584000701179597,
        -0.7527388892559215,
        -0.7399475375895594,
        -0.7451957368378487,
        -0.7586745769414976,
        -0.758135351925348,
        -0.7619328995370942,
        -0.7439331844215571,
        -0.7539195913669701,
        -0.756417349894749,
        -0.7543222311331812,
        -0.7562382709633029,
        -0.7459272045642023,
        -0.7592653209629381,
        -0.7633275341092345,
        -0.7594565231187347,
        -0.7400158655934298,
        -0.7627844341201581,
        -0.7541410448469994,
        -0.7466150703748065,
        -0.7621762822386359,
        -0.7639280892268845,
        1.0,
        -0.7608906364691632,
        -0.6741281895846112,
        -0.760184744205087,
        -0.7489989920134983,
        -0.7603002648558416,
        -0.7475143207660968,
        -0.7581419697286935,
        -0.7275094974131087,
        -0.7565966878632442,
        -0.7596934949059202,
        -0.7536596596792191,
        -0.7613298569412204,
        -0.7468146381271612,
        -0.7612720352393296,
        -0.7586255328016178
      ],
      [
        -0.7584849203433848,
        -0.6013921496225473,
        -0.74348942877528,
        -0.7622880755497425,
        -0.7028448289526622,
        -0.7458355957170391,
        -0.7354581287711284,
        -0.7315659995930264,
        -0.7268463619788229,
        -0.7320365357960428,
        -0.7486887510886217,
        -0.7325702057636265,
        -0.734919790416481,
        -0.683361930613779,
        -0.7449694998366021,
        -0.7572700352119356,
        -0.7299897588788613,
        -0.7439014078049575,
        -0.7611416512102217,
        -0.6065357678666894,
        -0.7405901012548508,
        -0.7427073694720077,
        -0.7478585162471325,
        -0.7555406642900535,
        -0.7574773890277252,
        -0.7457737064423815,
        -0.7467189381247366,
        -0.7484669087593532,
        -0.7495428395838868,
        -0.753152902525875,
        -0.759825480120883,
        -0.7429230618145388,
        -0.7557903192033695,
        -0.7283081209689144,
        -0.7378179035666517,
        -0.748095749571738,
        -0.7414106756064638,
        -0.7547440666754441,
        -0.7608906364691632,
        1.0,
        -0.7422281512096283,
        -0.7578841758474503,
        -0.7461807772559519,
        -0.744303371068262,
        -0.7282778897768067,
        -0.750636469469859,
        -0.7510318853586863,
        -0.7293276087873879,
        -0.7628755128542006,
        -0.756381660314347,
        -0.7540003208697745,
        -0.7121373549923478,
        -0.7637709578622947,
        -0.7520727238731993
      ],
      [
        -0.7239591760425433,
        -0.7208627436657327,
        -0.6840556256361016,
        -0.732970491652591,
        -0.74904226440737,
        -0.7170936569504662,
        -0.42611890832352917,
        -0.733912798342965,
        -0.2190393495016768,
        -0.6659987440372153,
        -0.7117187723256331,
        -0.7490775466760993,
        0.1183723250008438,
        -0.72900658643564,
        -0.15809801077671118,
        -0.7327595762637373,
        -0.7225612592011033,
        -0.12358502890173247,
        -0.7520618936074484,
        -0.44543202741532706,
        -0.7494887342936305,
        -0.7055103216831382,
        -0.7506459797479039,
        -0.7121127633680939,
        -0.7022335154140166,
        -0.7344487211993226,
        -0.26978484562883465,
        -0.7270037111956245,
        -0.7184497038554674,
        -0.7400469717883627,
        -0.7292788062597577,
        -0.7433316578994404,
        -0.7293717808307478,
        -0.7450365557864795,
        -0.7252445450797784,
        -0.7144577072358678,
        -0.7448330857643326,
        -0.758977719943376,
        -0.6741281895846112,
        -0.7422281512096283,
        1.0,
        -0.7535679895795481,
        -0.5334013918844138,
        -0.735108077795724,
        -0.5046836113182993,
        -0.7350906550431582,
        -0.7016850779265063,
        -0.7385676568238387,
        -0.7233015936961849,
        -0.7397013409151452,
        -0.7451620676068336,
        -0.7504299141349985,
        -0.7417984812718891,
        -0.726711781533484
      ],
      [
        -0.7557953994337527,
        -0.7459732395359625,
        -0.7550838486070606,
        -0.754174817757461,
        -0.7509205321443517,
        -0.7505003363987255,
        -0.7487004223162116,
        -0.6852299025275048,
        -0.7563982341167215,
        -0.7469738924845258,
        -0.757161331956947,
        -0.7512142399633102,
        -0.7548386426615473,
        -0.738565293415471,
        -0.7623268844656144,
        -0.7491334965006499,
        -0.7383054547070781,
        -0.7533596756408625,
        -0.7588705640626825,
        -0.756436284667899,
        -0.7165745869291873,
        -0.7497284480143074,
        -0.7536099729549754,
        -0.7583398856599411,
        -0.7567904899534472,
        -0.5322005398032772,
        -0.7582216223709413,
        -0.7466840497608773,
        -0.7477453010528816,
        -0.7411477841396557,
        -0.7649843106546711,
        -0.5320718205329105,
        -0.7573466542663503,
        -0.7509926894202716,
        -0.47579807977694644,
        -0.7344472154666868,
        -0.7499918919992876,
        -0.7477519496379041,
        -0.760184744205087,
        -0.7578841758474503,
        -0.7535679895795481,
        1.0,
        -0.751413429024696,
        -0.7386496771166801,
        -0.759416439221691,
        -0.7335833009037971,
        -0.7419260580485753,
        -0.6669845253144082,
        -0.7640736681640733,
        -0.756363350640817,
        -0.743438705416227,
        -0.754456569245691,
        -0.7623353703339184,
        -0.7377183704968704
      ],
      [
        -0.7348973126961917,
        -0.7321507264498984,
        -0.731857362607335,
        -0.74607406240833,
        -0.7442010701546404,
        -0.7336955698848492,
        -0.5951632969917919,
        -0.7354622193702536,
        -0.5652844913578827,
        -0.6473247182304857,
        -0.7258185084377085,
        -0.7494292143700758,
        -0.5152661944583031,
        -0.7251661640965641,
        -0.5364790858947346,
        -0.7344035942474686,
        -0.7292281649218635,
        -0.547057429632271,
        -0.7567722806325784,
        -0.6065160040381807,
        -0.7442480547697509,
        -0.7148179349148069,
        -0.7529551039815279,
        -0.7129843713903117,
        -0.7190028401572254,
        -0.73940482970522,
        -0.5879814271667412,
        -0.7303931428143196,
        -0.7357585814246179,
        -0.7321862001322852,
        -0.7498940860988255,
        -0.7313094869705286,
        -0.7462639696218217,
        -0.7461217825891366,
        -0.7278133903396304,
        -0.7433988196787021,
        -0.7469544063372613,
        -0.7560250311675156,
        -0.7489989920134983,
        -0.7461807772559519,
        -0.5334013918844138,
        -0.751413429024696,
        1.0,
        -0.7468295173563502,
        -0.6531442992096079,
        -0.733827560218264,
        -0.716522038494802,
        -0.7390986933248301,
        -0.7199230169660646,
        -0.7442815052280367,
        -0.740282223080801,
        -0.7535450772276668,
        -0.7513032620337394,
        -0.7348083965671278
      ],
      [
        -0.7557059377116218,
        -0.6006893808766366,
        -0.7430793730518458,
        -0.7560490125705521,
        -0.7526848077116448,
        -0.7463601281426333,
        -0.7335594872728362,
        -0.7433528441721471,
        -0.7408683980304268,
        -0.7423862313131073,
        -0.7505967612540096,
        -0.7535738974653148,
        -0.739542506159247,
        -0.7323763748568926,
        -0.7458949859077563,
        -0.7583709539663623,
        -0.7397760071972399,
        -0.7259350798739427,
        -0.7349577378101322,
        -0.7236801655034757,
        -0.7419149415821005,
        -0.7361455132039124,
        -0.7553777000827009,
        -0.7468257573616761,
        -0.7550728084462341,
        -0.7289662244381339,
        -0.7399929811971877,
        -0.7172892627019111,
        -0.7455427073798404,
        -0.743469839727946,
        -0.7554432732664883,
        -0.7377572021180886,
        -0.7550036512060486,
        -0.7427901423410195,
        -0.7104261393081712,
        -0.7439457294782043,
        -0.7438636786346222,
        -0.7586724947299954,
        -0.7603002648558416,
        -0.744303371068262,
        -0.735108077795724,
        -0.7386496771166801,
        -0.7468295173563502,
        1.0,
        -0.7395651378219537,
        -0.743383131114726,
        -0.7142751276714885,
        -0.747916979888281,
        -0.7589955385990055,
        -0.7593664411254775,
        -0.7486556262122643,
        -0.7443032416321366,
        -0.7605416049934324,
        -0.7419649060360893
      ],
      [
        -0.7466712886588808,
        -0.6196780074563604,
        -0.7170180934204267,
        -0.7437561507929762,
        -0.739998940205187,
        -0.7108654541658919,
        -0.6417047151345399,
        -0.7459297380635499,
        -0.5322252768049949,
        -0.6901184603177013,
        -0.6883789632850968,
        -0.7487191848734271,
        -0.4864631481641314,
        -0.7148092734922115,
        -0.5252722833167724,
        -0.7423362336590602,
        -0.7364020010403722,
        -0.5042290768195716,
        -0.7584181385533278,
        -0.5428337839083017,
        -0.7485605858753257,
        -0.7314549536729386,
        -0.749620516373392,
        -0.7322643415182271,
        -0.7405784590880526,
        -0.7497063853121086,
        -0.5433326630304375,
        -0.735160868553927,
        -0.7400743383641536,
        -0.7512641101558746,
        -0.7373930911718851,
        -0.7522448778656086,
        -0.749708634954438,
        -0.7336537366635243,
        -0.7417261278289011,
        -0.7324665407618072,
        -0.7493680185741824,
        -0.7597597825141897,
        -0.7475143207660968,
        -0.7282778897768067,
        -0.5046836113182993,
        -0.759416439221691,
        -0.6531442992096079,
        -0.7395651378219537,
        1.0,
        -0.7477810670557917,
        -0.7313140043559261,
        -0.7437606042444026,
        -0.737129658036961,
        -0.7501558177315638,
        -0.7574140427678986,
        -0.7138043881546576,
        -0.753058047068531,
        -0.7457167002834404
      ],
      [
        -0.7425088761229681,
        -0.7381509557049859,
        -0.7363679436745135,
        -0.7431061358183116,
        -0.6133737479805719,
        -0.7281950830705411,
        -0.7182616370206724,
        -0.7153211386919851,
        -0.7427799143278012,
        -0.7162437364437355,
        -0.740561550544833,
        -0.7370471890568071,
        -0.6582695389290039,
        -0.7330621687852046,
        -0.742526801363349,
        -0.7406211598512762,
        -0.6952111375418348,
        -0.7348916838096389,
        -0.723447748120579,
        -0.735837943426811,
        -0.5567218958097955,
        -0.7365794089791886,
        -0.7578073712989786,
        -0.7515258981912281,
        -0.5454631354815497,
        -0.6990633449588128,
        -0.7410009705383211,
        -0.7235985818700112,
        -0.7015138809368292,
        -0.5955958883249302,
        -0.7616963780142383,
        -0.7267493796071458,
        -0.756584525845456,
        -0.7372319831827052,
        -0.6507563211200396,
        -0.6238051462488607,
        -0.6467861920901308,
        -0.7559409735000875,
        -0.7581419697286935,
        -0.750636469469859,
        -0.7350906550431582,
        -0.7335833009037971,
        -0.733827560218264,
        -0.743383131114726,
        -0.7477810670557917,
        1.0,
        -0.6769108445451224,
        -0.7114437959602735,
        -0.7584332794810797,
        -0.7458952472000497,
        -0.7274254092223914,
        -0.7515914271033004,
        -0.7580105761531073,
        -0.7281688000864046
      ],
      [
        -0.7119268901612252,
        -0.7263380636272541,
        -0.7360703520765907,
        -0.7415776664780639,
        -0.7375969961181434,
        -0.7179191780610625,
        -0.7101415453588009,
        -0.7255735122970461,
        -0.7256522846429023,
        -0.6903731631166841,
        -0.7214377060042119,
        -0.7434518261127135,
        -0.679676718389971,
        -0.7138979073417766,
        -0.7202135513364898,
        -0.7427736251856938,
        -0.710925943046058,
        -0.7100949576968919,
        -0.5508860109828145,
        -0.7270953563079369,
        -0.7251850524447999,
        -0.7232807959363089,
        -0.754923043375828,
        -0.737354303247592,
        -0.7286443075574114,
        -0.7187496053822471,
        -0.7169670298006976,
        -0.7049365722311162,
        -0.7198753266182414,
        -0.7138080074664881,
        -0.7557557676489802,
        -0.7297489753154373,
        -0.7454623893414574,
        -0.7354147839337742,
        -0.6974187062681703,
        -0.6864632673292603,
        -0.739435441645939,
        -0.7540276110522095,
        -0.7275094974131087,
        -0.7510318853586863,
        -0.7016850779265063,
        -0.7419260580485753,
        -0.716522038494802,
        -0.7142751276714885,
        -0.7313140043559261,
        -0.6769108445451224,
        1.0,
        -0.7259148993043912,
        -0.7508767199503061,
        -0.7382802643838615,
        -0.7269179402005356,
        -0.7435634227456118,
        -0.7567350746937989,
        -0.7320938134411672
      ],
      [
        -0.7564234362766107,
        -0.7152778548044212,
        -0.7501849861982921,
        -0.7526742440302122,
        -0.7348576082349276,
        -0.7354252238947927,
        -0.7361705934381428,
        -0.7262675006599846,
        -0.7430034772802911,
        -0.708284093372272,
        -0.7486374858592226,
        -0.7433049941557364,
        -0.7355755351540624,
        -0.7165860111592397,
        -0.7473437714452559,
        -0.7549951265956767,
        -0.7073615391795613,
        -0.7302418484349035,
        -0.7449992540259538,
        -0.7272512025624069,
        -0.7045919799913865,
        -0.74573801869136,
        -0.7514753269673351,
        -0.7526992047065393,
        -0.7523584563593331,
        -0.6704545470464465,
        -0.7440669681034326,
        -0.7479633373550743,
        -0.7458822613778657,
        -0.7396753057132808,
        -0.7633262516053736,
        -0.7119897845473343,
        -0.752999858941091,
        -0.7461999754092274,
        -0.6915317096861626,
        -0.7411115821175496,
        -0.7433221104165648,
        -0.748019468051843,
        -0.7565966878632442,
        -0.7293276087873879,
        -0.7385676568238387,
        -0.6669845253144082,
        -0.7390986933248301,
        -0.747916979888281,
        -0.7437606042444026,
        -0.7114437959602735,
        -0.7259148993043912,
        1.0,
        -0.7604498161027223,
        -0.7319121001941442,
        -0.6974636404117948,
        -0.7477558341350107,
        -0.7608221172036405,
        -0.7373973822306457
      ],
      [
        -0.7473313069472505,
        -0.7563820504283634,
        -0.7482417226002592,
        -0.7557778232065314,
        -0.761908554945486,
        -0.7543135779712484,
        -0.7197767453918087,
        -0.7519523452330106,
        -0.7314853343798842,
        -0.7489811144136168,
        -0.7505418559387373,
        -0.7630976803952704,
        -0.7195996017603279,
        -0.7573137087406564,
        -0.7010459658317127,
        -0.7570740260876001,
        -0.751773935699199,
        -0.722284167756073,
        -0.7634467365887981,
        -0.7404648357615263,
        -0.7619533793548662,
        -0.7422243209629431,
        -0.7633910068725931,
        -0.7540324609612561,
        -0.7333536513746617,
        -0.760886482374118,
        -0.715952982718222,
        -0.7527496775127331,
        -0.7581105655252696,
        -0.7580875317571253,
        -0.7296227286826737,
        -0.7615530548686095,
        -0.7610360853614353,
        -0.7591064990902834,
        -0.7580749461734185,
        -0.7601923658029193,
        -0.7583098760068736,
        -0.76440659862891,
        -0.7596934949059202,
        -0.7628755128542006,
        -0.7233015936961849,
        -0.7640736681640733,
        -0.7199230169660646,
        -0.7589955385990055,
        -0.737129658036961,
        -0.7584332794810797,
        -0.7508767199503061,
        -0.7604498161027223,
        1.0,
        -0.7571615884829315,
        -0.7601890478943196,
        -0.7612711394807681,
        -0.7612565385857837,
        -0.7532631769643889
      ],
      [
        -0.7605742566353425,
        -0.7410546421243656,
        -0.7609013783684113,
        -0.7621863924252315,
        -0.7565939025614603,
        -0.7591960017544512,
        -0.7470394961245087,
        -0.7467881641634158,
        -0.743162292446065,
        -0.7262375432154405,
        -0.7445985037343996,
        -0.761226665761984,
        -0.7353678320716006,
        -0.7436051637709057,
        -0.7393074348171038,
        -0.7522254358097104,
        -0.7191247159131482,
        -0.7445180888960645,
        -0.7561381393255071,
        -0.7329616108710988,
        -0.7559965606900436,
        -0.7519475205583304,
        -0.7614342843896926,
        -0.7539886915110418,
        -0.7303049919045428,
        -0.7535477594269062,
        -0.7476736596579026,
        -0.7532684391055049,
        -0.7532752472094888,
        -0.7543812420722962,
        -0.7605324502928263,
        -0.7564680399167724,
        -0.7547157136534228,
        -0.759807821829825,
        -0.7490507322055056,
        -0.7540489136102377,
        -0.7610655705124283,
        -0.7590776529044272,
        -0.7536596596792191,
        -0.756381660314347,
        -0.7397013409151452,
        -0.756363350640817,
        -0.7442815052280367,
        -0.7593664411254775,
        -0.7501558177315638,
        -0.7458952472000497,
        -0.7382802643838615,
        -0.7319121001941442,
        -0.7571615884829315,
        1.0,
        -0.732233289373822,
        -0.7535196232543815,
        -0.7625471600337664,
        -0.7571763563258223
      ],
      [
        -0.7415846225793767,
        -0.7482764598917626,
        -0.748409568326087,
        -0.7575223905019364,
        -0.6949511956393655,
        -0.7336652619940984,
        -0.7408543080361483,
        -0.7389853253260248,
        -0.7527290784685248,
        -0.7318980555851629,
        -0.7544910971152452,
        -0.7382620379410338,
        -0.7504920880478705,
        -0.7441562238699175,
        -0.7543390567282061,
        -0.7525338207154333,
        -0.5956255968290802,
        -0.7509188069431867,
        -0.7581991226613843,
        -0.7498610546212044,
        -0.7410289558329323,
        -0.749214379691664,
        -0.7621535225881431,
        -0.755951570403725,
        -0.7458972981655977,
        -0.7346436192393877,
        -0.7524466773079185,
        -0.7415062372611684,
        -0.7349174211038628,
        -0.543844679940846,
        -0.7629251071614902,
        -0.7171285197036504,
        -0.7460917437574958,
        -0.750644959697563,
        -0.6494028391373299,
        -0.7461192623330093,
        -0.7263042344682284,
        -0.7566885308245684,
        -0.7613298569412204,
        -0.7540003208697745,
        -0.7451620676068336,
        -0.743438705416227,
        -0.740282223080801,
        -0.7486556262122643,
        -0.7574140427678986,
        -0.7274254092223914,
        -0.7269179402005356,
        -0.6974636404117948,
        -0.7601890478943196,
        -0.732233289373822,
        1.0,
        -0.7602195819020875,
        -0.7620763001510169,
        -0.7329729807576827
      ],
      [
        -0.7600685067770332,
        -0.6669951942706343,
        -0.7549923637199616,
        -0.7612297025338346,
        -0.7512864475835567,
        -0.7542216903580197,
        -0.7491321072478347,
        -0.7450964135757566,
        -0.7533816136362462,
        -0.7523126029081114,
        -0.7321207485833613,
        -0.7526313207102651,
        -0.7519891944943553,
        -0.49961099428648753,
        -0.7585826135940059,
        -0.7567475284676404,
        -0.7371029682887058,
        -0.752026309483657,
        -0.7554122040081324,
        -0.715888333539642,
        -0.7422738319379898,
        -0.7471347061078011,
        -0.7519379510987135,
        -0.755333623984489,
        -0.7576119737864866,
        -0.7436580218338884,
        -0.7559364141621432,
        -0.7478903484962972,
        -0.754059879601235,
        -0.7560487357013858,
        -0.7636777140617683,
        -0.7456547877497268,
        -0.7469088168279931,
        -0.7395180024453553,
        -0.733647578394627,
        -0.7365710240404533,
        -0.7533997081434418,
        -0.7502816278054153,
        -0.7468146381271612,
        -0.7121373549923478,
        -0.7504299141349985,
        -0.754456569245691,
        -0.7535450772276668,
        -0.7443032416321366,
        -0.7138043881546576,
        -0.7515914271033004,
        -0.7435634227456118,
        -0.7477558341350107,
        -0.7612711394807681,
        -0.7535196232543815,
        -0.7602195819020875,
        1.0,
        -0.763585594648017,
        -0.7548807980683412
      ],
      [
        -0.759664325672581,
        -0.7621078647641689,
        -0.7234904562269835,
        -0.6702320371354982,
        -0.7601020766084032,
        -0.7529147785451806,
        -0.7484881723784635,
        -0.7587252818663599,
        -0.7485248226711714,
        -0.7458488470005344,
        -0.7518297232841772,
        -0.7598708510493585,
        -0.7458279826839981,
        -0.7617181259204565,
        -0.7507020213520148,
        -0.7588234235919513,
        -0.7565327905760952,
        -0.7484269444911722,
        -0.7621696350442897,
        -0.7504636497790942,
        -0.7594948200204521,
        -0.7588289535781827,
        -0.7638311298874108,
        -0.7524559534002935,
        -0.7293335027807111,
        -0.7608055400132032,
        -0.7471381500015195,
        -0.7531708471936353,
        -0.7146638497723987,
        -0.7547374568022021,
        -0.723194892846206,
        -0.7605363464651396,
        -0.7597601430244264,
        -0.7468687943792163,
        -0.7554491095769758,
        -0.7591181944404872,
        -0.7609592707227614,
        -0.7625427462609676,
        -0.7612720352393296,
        -0.7637709578622947,
        -0.7417984812718891,
        -0.7623353703339184,
        -0.7513032620337394,
        -0.7605416049934324,
        -0.753058047068531,
        -0.7580105761531073,
        -0.7567350746937989,
        -0.7608221172036405,
        -0.7612565385857837,
        -0.7625471600337664,
        -0.7620763001510169,
        -0.763585594648017,
        1.0,
        -0.7562064567920942
      ],
      [
        -0.734215200745824,
        -0.7420621776738991,
        -0.6646986671685561,
        -0.7382830021550442,
        -0.7388615992607552,
        -0.6900598962698948,
        -0.7067771406342505,
        -0.7341206530854248,
        -0.73611195510001,
        -0.7337250706600545,
        -0.7465581855124166,
        -0.7178606895603986,
        -0.73496990707753,
        -0.7344549636850173,
        -0.7400530995771235,
        -0.7470626514608032,
        -0.7236631320597364,
        -0.7066207779080644,
        -0.7556679750529054,
        -0.7400908240030619,
        -0.7293282415622219,
        -0.7376109369188333,
        -0.7596189639543642,
        -0.7505013071240297,
        -0.740764012379391,
        -0.7125740098126763,
        -0.7317936304445201,
        -0.7168070725288653,
        -0.7104112739932913,
        -0.6973325429872564,
        -0.7582264214237053,
        -0.73265089721414,
        -0.753313904146524,
        -0.7288188172828673,
        -0.689205210840925,
        -0.7325191545458432,
        -0.6778264123313409,
        -0.7586947820348328,
        -0.7586255328016178,
        -0.7520727238731993,
        -0.726711781533484,
        -0.7377183704968704,
        -0.7348083965671278,
        -0.7419649060360893,
        -0.7457167002834404,
        -0.7281688000864046,
        -0.7320938134411672,
        -0.7373973822306457,
        -0.7532631769643889,
        -0.7571763563258223,
        -0.7329729807576827,
        -0.7548807980683412,
        -0.7562064567920942,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        34,
        20,
        4,
        4,
        8,
        1,
        4,
        14,
        12,
        28,
        7,
        2,
        0,
        24,
        1,
        12,
        6,
        0,
        1,
        30,
        0,
        12,
        14,
        2,
        4,
        2,
        8,
        1,
        3,
        0,
        4,
        0,
        6,
        4,
        7,
        5,
        2,
        2,
        8,
        5,
        5,
        2,
        8,
        5,
        4,
        1,
        5,
        7,
        3,
        5,
        0,
        4,
        2,
        0
      ],
      "2020-02": [
        23,
        41,
        1,
        7,
        10,
        1,
        8,
        23,
        12,
        24,
        4,
        2,
        1,
        29,
        4,
        12,
        7,
        1,
        2,
        25,
        4,
        17,
        12,
        9,
        5,
        2,
        10,
        5,
        6,
        0,
        5,
        0,
        12,
        2,
        20,
        9,
        4,
        8,
        16,
        10,
        5,
        1,
        4,
        3,
        3,
        2,
        11,
        2,
        2,
        3,
        3,
        7,
        2,
        5
      ],
      "2020-03": [
        27,
        38,
        3,
        5,
        14,
        5,
        6,
        14,
        12,
        24,
        10,
        3,
        0,
        17,
        1,
        12,
        14,
        0,
        0,
        25,
        1,
        22,
        6,
        1,
        1,
        0,
        6,
        3,
        3,
        2,
        3,
        0,
        8,
        3,
        22,
        10,
        5,
        1,
        3,
        9,
        4,
        0,
        6,
        5,
        5,
        2,
        13,
        9,
        1,
        6,
        2,
        4,
        3,
        4
      ],
      "2020-04": [
        23,
        28,
        2,
        1,
        15,
        4,
        9,
        8,
        11,
        22,
        10,
        5,
        0,
        25,
        10,
        9,
        15,
        0,
        2,
        32,
        1,
        27,
        6,
        3,
        9,
        1,
        13,
        2,
        2,
        0,
        18,
        1,
        5,
        7,
        22,
        11,
        3,
        3,
        2,
        5,
        4,
        2,
        9,
        5,
        5,
        4,
        9,
        12,
        2,
        4,
        1,
        5,
        2,
        3
      ],
      "2020-05": [
        42,
        31,
        3,
        1,
        16,
        4,
        12,
        10,
        19,
        20,
        14,
        3,
        1,
        27,
        3,
        13,
        15,
        0,
        1,
        32,
        1,
        25,
        14,
        9,
        4,
        0,
        3,
        5,
        2,
        0,
        15,
        2,
        10,
        4,
        25,
        12,
        5,
        4,
        10,
        7,
        7,
        0,
        6,
        5,
        8,
        6,
        8,
        5,
        3,
        9,
        3,
        1,
        7,
        3
      ],
      "2020-06": [
        31,
        62,
        4,
        3,
        20,
        3,
        15,
        18,
        18,
        27,
        14,
        3,
        1,
        24,
        4,
        16,
        13,
        0,
        1,
        36,
        0,
        20,
        12,
        6,
        1,
        2,
        2,
        8,
        5,
        0,
        9,
        3,
        17,
        4,
        29,
        11,
        2,
        6,
        10,
        11,
        10,
        2,
        11,
        12,
        9,
        5,
        13,
        9,
        1,
        5,
        1,
        14,
        0,
        2
      ],
      "2020-07": [
        37,
        56,
        2,
        4,
        23,
        2,
        14,
        19,
        18,
        29,
        16,
        3,
        0,
        16,
        2,
        14,
        15,
        2,
        1,
        35,
        3,
        16,
        15,
        3,
        1,
        0,
        6,
        3,
        3,
        1,
        3,
        1,
        16,
        1,
        23,
        10,
        8,
        1,
        8,
        12,
        4,
        1,
        10,
        11,
        10,
        3,
        11,
        7,
        3,
        2,
        1,
        9,
        5,
        3
      ],
      "2020-08": [
        20,
        38,
        1,
        6,
        15,
        5,
        8,
        12,
        11,
        35,
        21,
        6,
        1,
        20,
        4,
        9,
        12,
        1,
        6,
        29,
        6,
        13,
        27,
        4,
        9,
        4,
        6,
        2,
        6,
        1,
        9,
        0,
        8,
        9,
        16,
        12,
        12,
        5,
        6,
        9,
        4,
        1,
        8,
        4,
        8,
        3,
        11,
        15,
        3,
        2,
        2,
        5,
        3,
        2
      ],
      "2020-09": [
        41,
        32,
        5,
        12,
        15,
        12,
        15,
        31,
        18,
        39,
        19,
        10,
        2,
        30,
        6,
        31,
        22,
        2,
        3,
        22,
        5,
        40,
        20,
        8,
        6,
        1,
        13,
        6,
        15,
        3,
        15,
        1,
        14,
        4,
        53,
        25,
        14,
        2,
        14,
        11,
        8,
        6,
        17,
        12,
        8,
        10,
        22,
        16,
        9,
        5,
        4,
        9,
        6,
        6
      ],
      "2020-10": [
        55,
        47,
        6,
        27,
        13,
        21,
        32,
        37,
        25,
        44,
        21,
        21,
        2,
        31,
        16,
        27,
        33,
        3,
        9,
        45,
        9,
        51,
        11,
        10,
        17,
        7,
        41,
        12,
        18,
        6,
        28,
        4,
        26,
        7,
        67,
        50,
        18,
        3,
        29,
        15,
        20,
        10,
        13,
        17,
        5,
        10,
        24,
        17,
        10,
        5,
        4,
        7,
        4,
        12
      ],
      "2020-11": [
        39,
        53,
        8,
        18,
        25,
        12,
        19,
        44,
        16,
        37,
        22,
        20,
        0,
        30,
        3,
        27,
        28,
        0,
        11,
        45,
        8,
        22,
        6,
        6,
        8,
        5,
        13,
        15,
        13,
        1,
        11,
        4,
        10,
        13,
        58,
        25,
        25,
        3,
        10,
        9,
        10,
        6,
        7,
        16,
        10,
        7,
        18,
        15,
        6,
        6,
        8,
        11,
        10,
        9
      ],
      "2020-12": [
        51,
        44,
        3,
        16,
        18,
        14,
        19,
        38,
        16,
        39,
        26,
        18,
        1,
        35,
        8,
        25,
        16,
        3,
        15,
        42,
        4,
        36,
        8,
        10,
        15,
        6,
        24,
        16,
        7,
        2,
        15,
        3,
        17,
        14,
        81,
        30,
        22,
        7,
        18,
        15,
        6,
        3,
        15,
        18,
        12,
        13,
        19,
        14,
        10,
        11,
        11,
        12,
        7,
        11
      ],
      "2021-01": [
        54,
        24,
        3,
        9,
        19,
        9,
        15,
        27,
        6,
        37,
        21,
        9,
        1,
        27,
        4,
        21,
        21,
        3,
        12,
        20,
        7,
        27,
        8,
        9,
        23,
        3,
        10,
        10,
        11,
        3,
        7,
        4,
        11,
        11,
        47,
        24,
        24,
        11,
        9,
        6,
        11,
        2,
        11,
        10,
        8,
        10,
        22,
        19,
        2,
        9,
        3,
        7,
        5,
        6
      ],
      "2021-02": [
        48,
        65,
        9,
        28,
        26,
        19,
        24,
        48,
        9,
        44,
        17,
        13,
        1,
        37,
        2,
        20,
        27,
        5,
        6,
        38,
        6,
        23,
        16,
        4,
        16,
        7,
        12,
        24,
        15,
        1,
        10,
        3,
        21,
        4,
        73,
        51,
        27,
        4,
        15,
        11,
        12,
        4,
        15,
        15,
        11,
        10,
        24,
        14,
        9,
        10,
        3,
        18,
        4,
        9
      ],
      "2021-03": [
        39,
        48,
        4,
        24,
        24,
        17,
        39,
        29,
        11,
        50,
        32,
        33,
        0,
        37,
        2,
        16,
        28,
        3,
        11,
        36,
        9,
        29,
        10,
        6,
        16,
        5,
        29,
        19,
        13,
        3,
        9,
        3,
        10,
        7,
        83,
        50,
        34,
        7,
        11,
        18,
        17,
        8,
        12,
        11,
        13,
        20,
        10,
        22,
        2,
        9,
        4,
        9,
        8,
        13
      ],
      "2021-04": [
        56,
        35,
        8,
        33,
        29,
        19,
        21,
        39,
        13,
        51,
        22,
        18,
        2,
        39,
        11,
        26,
        36,
        4,
        14,
        37,
        7,
        40,
        7,
        8,
        17,
        4,
        28,
        17,
        16,
        1,
        11,
        4,
        7,
        9,
        67,
        44,
        25,
        15,
        11,
        10,
        13,
        9,
        18,
        10,
        8,
        17,
        12,
        21,
        12,
        5,
        7,
        2,
        6,
        26
      ],
      "2021-05": [
        44,
        45,
        3,
        14,
        17,
        24,
        24,
        51,
        18,
        54,
        26,
        14,
        0,
        31,
        4,
        30,
        17,
        1,
        12,
        38,
        6,
        20,
        11,
        8,
        13,
        7,
        26,
        18,
        19,
        2,
        19,
        5,
        21,
        9,
        71,
        29,
        25,
        6,
        10,
        7,
        9,
        9,
        21,
        10,
        11,
        16,
        27,
        17,
        14,
        9,
        3,
        10,
        7,
        13
      ],
      "2021-06": [
        57,
        62,
        4,
        29,
        20,
        19,
        21,
        80,
        30,
        41,
        21,
        21,
        0,
        44,
        9,
        38,
        44,
        6,
        15,
        40,
        9,
        50,
        13,
        13,
        21,
        10,
        31,
        37,
        13,
        5,
        15,
        3,
        29,
        10,
        74,
        57,
        38,
        6,
        24,
        10,
        13,
        9,
        13,
        16,
        12,
        12,
        34,
        22,
        10,
        14,
        6,
        20,
        10,
        20
      ],
      "2021-07": [
        54,
        54,
        11,
        11,
        22,
        17,
        31,
        51,
        14,
        47,
        32,
        29,
        0,
        40,
        0,
        22,
        30,
        2,
        13,
        28,
        9,
        27,
        13,
        10,
        7,
        7,
        17,
        22,
        19,
        5,
        14,
        1,
        18,
        10,
        73,
        45,
        32,
        10,
        15,
        5,
        6,
        3,
        18,
        14,
        7,
        14,
        15,
        23,
        6,
        9,
        9,
        8,
        10,
        10
      ],
      "2021-08": [
        32,
        21,
        7,
        24,
        19,
        15,
        24,
        32,
        22,
        39,
        36,
        28,
        1,
        30,
        8,
        24,
        26,
        3,
        17,
        20,
        9,
        19,
        27,
        7,
        17,
        4,
        16,
        16,
        11,
        1,
        9,
        5,
        14,
        11,
        50,
        38,
        37,
        2,
        11,
        9,
        13,
        8,
        14,
        5,
        6,
        15,
        16,
        28,
        6,
        12,
        5,
        7,
        5,
        13
      ],
      "2021-09": [
        55,
        47,
        13,
        26,
        31,
        21,
        23,
        48,
        26,
        38,
        35,
        24,
        0,
        36,
        9,
        20,
        31,
        7,
        9,
        56,
        4,
        38,
        31,
        4,
        19,
        5,
        47,
        22,
        17,
        1,
        20,
        9,
        14,
        9,
        48,
        37,
        37,
        11,
        14,
        8,
        13,
        4,
        14,
        10,
        8,
        17,
        22,
        22,
        9,
        6,
        6,
        7,
        11,
        10
      ],
      "2021-10": [
        62,
        66,
        7,
        36,
        37,
        27,
        26,
        66,
        32,
        51,
        39,
        33,
        2,
        44,
        3,
        25,
        34,
        3,
        24,
        54,
        7,
        30,
        14,
        5,
        22,
        5,
        13,
        16,
        17,
        2,
        19,
        6,
        22,
        13,
        89,
        43,
        44,
        18,
        15,
        7,
        23,
        7,
        12,
        14,
        19,
        12,
        31,
        30,
        10,
        16,
        5,
        12,
        8,
        19
      ],
      "2021-11": [
        55,
        35,
        9,
        18,
        22,
        20,
        30,
        48,
        10,
        52,
        25,
        37,
        3,
        29,
        2,
        17,
        48,
        5,
        17,
        34,
        7,
        25,
        9,
        5,
        18,
        8,
        18,
        19,
        13,
        4,
        8,
        7,
        23,
        4,
        70,
        40,
        32,
        4,
        13,
        1,
        19,
        4,
        12,
        19,
        10,
        15,
        32,
        30,
        5,
        7,
        6,
        10,
        5,
        17
      ],
      "2021-12": [
        51,
        49,
        13,
        20,
        31,
        19,
        23,
        60,
        10,
        49,
        27,
        39,
        2,
        25,
        8,
        16,
        28,
        4,
        15,
        45,
        5,
        44,
        13,
        10,
        20,
        6,
        23,
        17,
        13,
        5,
        10,
        7,
        12,
        8,
        68,
        40,
        44,
        6,
        11,
        10,
        7,
        3,
        13,
        25,
        12,
        10,
        18,
        23,
        12,
        9,
        6,
        11,
        6,
        21
      ],
      "2022-01": [
        42,
        46,
        6,
        19,
        27,
        12,
        25,
        50,
        24,
        54,
        21,
        18,
        2,
        38,
        4,
        17,
        36,
        3,
        19,
        37,
        8,
        31,
        3,
        9,
        14,
        5,
        12,
        8,
        13,
        4,
        5,
        8,
        11,
        6,
        53,
        35,
        20,
        6,
        12,
        9,
        8,
        8,
        6,
        14,
        10,
        13,
        17,
        13,
        8,
        12,
        3,
        8,
        4,
        11
      ],
      "2022-02": [
        55,
        54,
        10,
        31,
        22,
        13,
        40,
        64,
        24,
        53,
        29,
        31,
        0,
        34,
        6,
        20,
        40,
        5,
        21,
        35,
        13,
        36,
        13,
        10,
        12,
        8,
        13,
        17,
        13,
        2,
        8,
        8,
        22,
        12,
        67,
        40,
        26,
        4,
        20,
        14,
        16,
        6,
        9,
        13,
        11,
        15,
        30,
        20,
        5,
        8,
        4,
        11,
        5,
        7
      ],
      "2022-03": [
        60,
        41,
        7,
        44,
        21,
        23,
        44,
        47,
        25,
        57,
        37,
        42,
        2,
        37,
        8,
        15,
        41,
        9,
        13,
        43,
        9,
        36,
        12,
        8,
        16,
        4,
        30,
        25,
        14,
        9,
        16,
        3,
        16,
        14,
        83,
        46,
        62,
        6,
        14,
        18,
        13,
        15,
        16,
        22,
        14,
        16,
        23,
        17,
        10,
        11,
        8,
        6,
        12,
        28
      ],
      "2022-04": [
        50,
        40,
        9,
        34,
        19,
        24,
        34,
        51,
        16,
        59,
        36,
        28,
        0,
        28,
        8,
        23,
        26,
        5,
        20,
        32,
        12,
        35,
        14,
        8,
        23,
        2,
        29,
        14,
        16,
        7,
        24,
        4,
        17,
        6,
        58,
        38,
        30,
        5,
        15,
        11,
        12,
        7,
        15,
        10,
        15,
        16,
        25,
        23,
        12,
        7,
        6,
        5,
        6,
        24
      ],
      "2022-05": [
        66,
        45,
        10,
        25,
        23,
        29,
        40,
        83,
        31,
        58,
        34,
        42,
        6,
        40,
        9,
        32,
        30,
        12,
        20,
        55,
        8,
        57,
        17,
        11,
        22,
        8,
        37,
        28,
        14,
        3,
        14,
        11,
        28,
        7,
        91,
        58,
        32,
        13,
        27,
        11,
        30,
        9,
        13,
        23,
        11,
        13,
        27,
        37,
        12,
        7,
        6,
        17,
        17,
        22
      ],
      "2022-06": [
        65,
        70,
        10,
        31,
        30,
        22,
        36,
        59,
        22,
        59,
        22,
        42,
        1,
        33,
        5,
        36,
        43,
        9,
        34,
        55,
        9,
        34,
        10,
        16,
        11,
        7,
        20,
        24,
        13,
        8,
        12,
        5,
        35,
        20,
        78,
        52,
        51,
        17,
        20,
        11,
        19,
        12,
        13,
        26,
        17,
        20,
        30,
        24,
        6,
        12,
        4,
        14,
        5,
        23
      ],
      "2022-07": [
        62,
        42,
        9,
        24,
        26,
        22,
        38,
        44,
        13,
        39,
        38,
        43,
        1,
        33,
        6,
        20,
        50,
        6,
        21,
        27,
        7,
        35,
        6,
        10,
        17,
        4,
        21,
        25,
        17,
        10,
        14,
        6,
        20,
        16,
        76,
        49,
        44,
        12,
        20,
        8,
        13,
        12,
        12,
        21,
        2,
        21,
        22,
        24,
        8,
        10,
        4,
        11,
        18,
        23
      ],
      "2022-08": [
        35,
        30,
        8,
        21,
        20,
        18,
        22,
        45,
        14,
        42,
        39,
        28,
        1,
        20,
        3,
        20,
        39,
        3,
        18,
        34,
        6,
        35,
        12,
        6,
        15,
        8,
        8,
        20,
        14,
        1,
        10,
        9,
        21,
        11,
        54,
        29,
        25,
        13,
        11,
        11,
        19,
        7,
        20,
        9,
        11,
        7,
        21,
        20,
        6,
        8,
        7,
        5,
        11,
        13
      ],
      "2022-09": [
        61,
        58,
        5,
        34,
        23,
        30,
        33,
        51,
        21,
        47,
        39,
        33,
        5,
        23,
        7,
        22,
        42,
        5,
        27,
        36,
        8,
        45,
        13,
        6,
        20,
        7,
        22,
        22,
        13,
        3,
        9,
        2,
        23,
        13,
        69,
        38,
        38,
        6,
        20,
        9,
        22,
        5,
        12,
        20,
        12,
        14,
        25,
        34,
        11,
        11,
        10,
        13,
        10,
        18
      ],
      "2022-10": [
        87,
        102,
        17,
        59,
        42,
        47,
        49,
        90,
        45,
        56,
        31,
        45,
        3,
        45,
        15,
        37,
        46,
        16,
        35,
        46,
        20,
        50,
        9,
        20,
        23,
        6,
        40,
        34,
        20,
        6,
        23,
        13,
        44,
        14,
        93,
        49,
        62,
        8,
        17,
        13,
        35,
        10,
        20,
        13,
        18,
        20,
        19,
        30,
        12,
        16,
        3,
        24,
        14,
        19
      ],
      "2022-11": [
        63,
        77,
        21,
        38,
        26,
        38,
        41,
        76,
        24,
        51,
        22,
        29,
        3,
        41,
        9,
        37,
        40,
        10,
        24,
        45,
        10,
        49,
        12,
        11,
        18,
        6,
        32,
        35,
        22,
        3,
        18,
        4,
        33,
        11,
        75,
        44,
        47,
        15,
        18,
        9,
        19,
        9,
        16,
        23,
        11,
        17,
        33,
        31,
        10,
        17,
        14,
        7,
        9,
        19
      ],
      "2022-12": [
        51,
        54,
        4,
        25,
        24,
        27,
        33,
        41,
        32,
        53,
        20,
        40,
        1,
        45,
        8,
        21,
        34,
        4,
        18,
        31,
        10,
        24,
        8,
        13,
        17,
        6,
        35,
        19,
        15,
        6,
        15,
        4,
        28,
        12,
        61,
        33,
        28,
        14,
        17,
        8,
        22,
        4,
        18,
        13,
        8,
        16,
        20,
        33,
        16,
        13,
        8,
        12,
        10,
        21
      ],
      "2023-01": [
        40,
        50,
        7,
        19,
        19,
        30,
        40,
        41,
        15,
        62,
        24,
        36,
        1,
        29,
        7,
        20,
        35,
        4,
        19,
        25,
        10,
        26,
        11,
        14,
        21,
        5,
        13,
        11,
        9,
        5,
        17,
        9,
        29,
        16,
        52,
        42,
        31,
        12,
        12,
        10,
        19,
        2,
        13,
        12,
        10,
        13,
        20,
        24,
        6,
        10,
        7,
        18,
        5,
        17
      ],
      "2023-02": [
        56,
        67,
        14,
        28,
        23,
        44,
        48,
        66,
        32,
        63,
        35,
        47,
        9,
        41,
        9,
        33,
        45,
        5,
        23,
        52,
        8,
        46,
        12,
        15,
        26,
        7,
        22,
        29,
        13,
        2,
        12,
        11,
        16,
        12,
        87,
        41,
        31,
        10,
        25,
        9,
        28,
        10,
        19,
        22,
        18,
        15,
        37,
        33,
        15,
        12,
        6,
        18,
        10,
        18
      ],
      "2023-03": [
        62,
        59,
        17,
        43,
        37,
        50,
        57,
        54,
        31,
        103,
        16,
        66,
        3,
        48,
        3,
        40,
        65,
        8,
        30,
        48,
        9,
        37,
        18,
        20,
        15,
        8,
        37,
        23,
        16,
        3,
        18,
        8,
        28,
        11,
        92,
        53,
        76,
        13,
        30,
        16,
        30,
        9,
        15,
        33,
        16,
        16,
        30,
        30,
        21,
        10,
        8,
        15,
        5,
        25
      ],
      "2023-04": [
        42,
        45,
        10,
        20,
        29,
        38,
        50,
        40,
        37,
        79,
        24,
        49,
        11,
        31,
        9,
        24,
        39,
        9,
        21,
        40,
        11,
        45,
        11,
        22,
        28,
        9,
        33,
        19,
        31,
        6,
        10,
        8,
        12,
        18,
        56,
        36,
        55,
        12,
        16,
        7,
        46,
        10,
        19,
        16,
        10,
        22,
        28,
        32,
        16,
        12,
        11,
        9,
        5,
        25
      ],
      "2023-05": [
        91,
        64,
        21,
        68,
        47,
        91,
        107,
        90,
        107,
        125,
        49,
        39,
        18,
        53,
        27,
        44,
        59,
        22,
        31,
        65,
        16,
        68,
        26,
        22,
        29,
        9,
        86,
        35,
        26,
        11,
        30,
        10,
        45,
        16,
        93,
        59,
        57,
        13,
        24,
        8,
        133,
        12,
        33,
        27,
        31,
        12,
        50,
        32,
        45,
        19,
        15,
        18,
        15,
        33
      ],
      "2023-06": [
        85,
        93,
        13,
        51,
        39,
        77,
        64,
        96,
        60,
        104,
        38,
        49,
        6,
        25,
        13,
        47,
        56,
        13,
        27,
        55,
        13,
        49,
        13,
        30,
        22,
        7,
        50,
        23,
        10,
        4,
        12,
        12,
        48,
        8,
        87,
        64,
        48,
        14,
        21,
        16,
        75,
        9,
        21,
        24,
        19,
        19,
        41,
        32,
        17,
        18,
        11,
        14,
        12,
        31
      ],
      "2023-07": [
        61,
        57,
        16,
        33,
        31,
        46,
        58,
        67,
        50,
        99,
        42,
        43,
        8,
        35,
        12,
        41,
        50,
        8,
        32,
        54,
        13,
        43,
        19,
        21,
        19,
        5,
        17,
        23,
        17,
        10,
        11,
        8,
        32,
        20,
        55,
        45,
        58,
        13,
        18,
        19,
        72,
        8,
        28,
        18,
        22,
        19,
        45,
        30,
        19,
        10,
        18,
        14,
        12,
        19
      ],
      "2023-08": [
        61,
        37,
        11,
        48,
        50,
        61,
        77,
        84,
        58,
        97,
        71,
        57,
        17,
        31,
        12,
        23,
        44,
        11,
        39,
        60,
        16,
        54,
        27,
        17,
        16,
        4,
        31,
        14,
        29,
        4,
        16,
        17,
        23,
        14,
        70,
        46,
        56,
        14,
        10,
        14,
        84,
        7,
        25,
        22,
        16,
        15,
        35,
        39,
        7,
        23,
        9,
        6,
        19,
        38
      ],
      "2023-09": [
        75,
        49,
        15,
        64,
        49,
        71,
        61,
        66,
        56,
        93,
        44,
        62,
        15,
        26,
        15,
        40,
        67,
        18,
        29,
        48,
        10,
        43,
        6,
        22,
        29,
        6,
        38,
        24,
        22,
        11,
        12,
        14,
        24,
        14,
        67,
        44,
        54,
        21,
        13,
        9,
        122,
        9,
        20,
        34,
        22,
        24,
        48,
        28,
        16,
        21,
        15,
        10,
        18,
        31
      ],
      "2023-10": [
        88,
        81,
        21,
        66,
        39,
        73,
        114,
        99,
        101,
        120,
        43,
        51,
        35,
        49,
        29,
        46,
        88,
        28,
        41,
        67,
        12,
        74,
        10,
        29,
        29,
        10,
        62,
        27,
        19,
        8,
        37,
        16,
        46,
        18,
        106,
        60,
        52,
        14,
        16,
        15,
        173,
        13,
        34,
        34,
        52,
        22,
        48,
        47,
        34,
        30,
        21,
        28,
        12,
        33
      ],
      "2023-11": [
        87,
        50,
        26,
        44,
        50,
        89,
        69,
        67,
        86,
        149,
        32,
        48,
        20,
        37,
        26,
        30,
        66,
        18,
        37,
        44,
        18,
        40,
        6,
        30,
        36,
        8,
        38,
        23,
        28,
        10,
        23,
        13,
        39,
        10,
        65,
        42,
        60,
        25,
        20,
        6,
        128,
        7,
        28,
        29,
        32,
        31,
        42,
        33,
        21,
        20,
        19,
        8,
        14,
        32
      ],
      "2023-12": [
        70,
        59,
        26,
        48,
        43,
        122,
        78,
        89,
        67,
        116,
        46,
        69,
        28,
        38,
        20,
        39,
        63,
        10,
        36,
        79,
        13,
        51,
        12,
        33,
        20,
        8,
        20,
        24,
        31,
        7,
        15,
        12,
        42,
        16,
        71,
        44,
        53,
        10,
        24,
        14,
        107,
        10,
        26,
        17,
        30,
        19,
        51,
        39,
        12,
        16,
        25,
        12,
        17,
        37
      ],
      "2024-01": [
        73,
        40,
        20,
        55,
        34,
        65,
        76,
        71,
        58,
        121,
        33,
        51,
        29,
        36,
        25,
        40,
        67,
        12,
        40,
        71,
        8,
        41,
        11,
        27,
        29,
        9,
        49,
        25,
        17,
        7,
        18,
        18,
        22,
        19,
        80,
        35,
        41,
        13,
        24,
        10,
        120,
        6,
        33,
        22,
        31,
        34,
        36,
        36,
        23,
        22,
        19,
        13,
        13,
        26
      ],
      "2024-02": [
        77,
        69,
        28,
        55,
        38,
        75,
        121,
        110,
        137,
        165,
        46,
        47,
        57,
        37,
        41,
        38,
        79,
        42,
        40,
        100,
        11,
        52,
        16,
        32,
        31,
        9,
        65,
        29,
        24,
        6,
        24,
        23,
        47,
        9,
        94,
        66,
        55,
        25,
        33,
        15,
        241,
        14,
        41,
        29,
        83,
        23,
        66,
        48,
        26,
        25,
        18,
        26,
        19,
        27
      ],
      "2024-03": [
        105,
        76,
        27,
        46,
        45,
        113,
        125,
        95,
        86,
        140,
        59,
        83,
        38,
        37,
        37,
        48,
        83,
        26,
        34,
        81,
        13,
        44,
        12,
        27,
        40,
        13,
        45,
        32,
        30,
        5,
        16,
        12,
        47,
        24,
        73,
        60,
        74,
        21,
        23,
        16,
        149,
        14,
        43,
        43,
        61,
        28,
        64,
        37,
        32,
        16,
        11,
        12,
        16,
        36
      ],
      "2024-04": [
        89,
        42,
        24,
        41,
        42,
        114,
        100,
        54,
        72,
        132,
        42,
        70,
        32,
        31,
        40,
        41,
        59,
        27,
        50,
        83,
        3,
        52,
        8,
        34,
        21,
        8,
        62,
        21,
        25,
        14,
        10,
        13,
        26,
        14,
        79,
        55,
        69,
        20,
        22,
        12,
        141,
        12,
        25,
        29,
        39,
        18,
        50,
        50,
        33,
        12,
        12,
        5,
        17,
        41
      ],
      "2024-05": [
        99,
        90,
        30,
        68,
        48,
        106,
        112,
        95,
        110,
        182,
        49,
        64,
        50,
        49,
        49,
        52,
        115,
        41,
        52,
        87,
        16,
        45,
        12,
        41,
        28,
        14,
        45,
        27,
        29,
        10,
        20,
        14,
        44,
        12,
        115,
        66,
        61,
        25,
        31,
        9,
        191,
        10,
        37,
        32,
        60,
        22,
        44,
        49,
        30,
        20,
        26,
        17,
        16,
        42
      ],
      "2024-06": [
        99,
        66,
        37,
        123,
        40,
        133,
        147,
        116,
        125,
        185,
        51,
        87,
        59,
        42,
        81,
        45,
        99,
        41,
        34,
        107,
        14,
        43,
        12,
        37,
        29,
        7,
        80,
        32,
        27,
        14,
        26,
        19,
        50,
        16,
        105,
        57,
        59,
        20,
        29,
        17,
        261,
        17,
        40,
        36,
        82,
        31,
        83,
        41,
        40,
        29,
        17,
        17,
        24,
        45
      ],
      "2024-07": [
        86,
        73,
        24,
        76,
        62,
        105,
        118,
        93,
        82,
        162,
        52,
        68,
        47,
        35,
        62,
        44,
        82,
        34,
        48,
        68,
        15,
        46,
        14,
        31,
        39,
        10,
        54,
        20,
        33,
        12,
        14,
        7,
        42,
        15,
        80,
        47,
        70,
        22,
        32,
        9,
        190,
        14,
        33,
        47,
        37,
        33,
        67,
        56,
        28,
        21,
        15,
        7,
        24,
        52
      ],
      "2024-08": [
        86,
        45,
        25,
        79,
        46,
        83,
        117,
        83,
        70,
        117,
        42,
        67,
        42,
        34,
        54,
        41,
        90,
        24,
        26,
        79,
        16,
        44,
        28,
        30,
        21,
        10,
        43,
        18,
        24,
        7,
        16,
        15,
        39,
        13,
        77,
        55,
        69,
        16,
        23,
        13,
        167,
        17,
        27,
        28,
        49,
        31,
        65,
        55,
        14,
        16,
        19,
        14,
        24,
        32
      ],
      "2024-09": [
        98,
        66,
        33,
        127,
        57,
        89,
        109,
        63,
        96,
        148,
        42,
        79,
        37,
        29,
        72,
        34,
        67,
        22,
        43,
        89,
        9,
        44,
        9,
        25,
        24,
        9,
        54,
        29,
        39,
        4,
        23,
        16,
        29,
        17,
        92,
        65,
        81,
        28,
        20,
        19,
        188,
        13,
        41,
        29,
        48,
        38,
        60,
        53,
        28,
        27,
        15,
        5,
        27,
        36
      ],
      "2024-10": [
        123,
        85,
        58,
        119,
        59,
        176,
        173,
        112,
        219,
        187,
        64,
        83,
        73,
        59,
        96,
        47,
        117,
        46,
        50,
        155,
        12,
        74,
        8,
        40,
        42,
        9,
        74,
        31,
        22,
        8,
        23,
        25,
        57,
        22,
        134,
        84,
        91,
        26,
        52,
        25,
        279,
        14,
        43,
        38,
        115,
        40,
        90,
        57,
        39,
        29,
        21,
        18,
        19,
        50
      ],
      "2024-11": [
        89,
        53,
        41,
        71,
        53,
        117,
        130,
        95,
        106,
        173,
        46,
        81,
        46,
        44,
        54,
        28,
        78,
        35,
        40,
        88,
        18,
        44,
        9,
        23,
        30,
        12,
        50,
        32,
        30,
        17,
        22,
        10,
        37,
        18,
        74,
        54,
        65,
        18,
        26,
        10,
        138,
        8,
        34,
        25,
        44,
        30,
        58,
        55,
        20,
        30,
        18,
        9,
        14,
        43
      ],
      "2024-12": [
        89,
        64,
        40,
        75,
        67,
        151,
        136,
        102,
        150,
        219,
        54,
        95,
        43,
        55,
        76,
        28,
        102,
        24,
        43,
        111,
        7,
        46,
        10,
        38,
        32,
        11,
        69,
        24,
        34,
        12,
        17,
        25,
        46,
        16,
        76,
        75,
        93,
        19,
        28,
        20,
        209,
        14,
        38,
        34,
        68,
        28,
        51,
        54,
        30,
        21,
        25,
        13,
        15,
        52
      ],
      "2025-01": [
        89,
        63,
        39,
        75,
        51,
        77,
        108,
        78,
        135,
        185,
        40,
        69,
        43,
        31,
        63,
        38,
        75,
        26,
        43,
        80,
        11,
        33,
        14,
        37,
        32,
        15,
        29,
        28,
        31,
        9,
        18,
        17,
        33,
        14,
        66,
        61,
        64,
        27,
        16,
        9,
        155,
        6,
        28,
        19,
        48,
        23,
        56,
        56,
        20,
        28,
        12,
        11,
        14,
        37
      ],
      "2025-02": [
        112,
        78,
        42,
        89,
        55,
        141,
        181,
        114,
        297,
        257,
        58,
        88,
        81,
        47,
        112,
        55,
        107,
        66,
        45,
        137,
        21,
        45,
        23,
        39,
        30,
        18,
        84,
        21,
        31,
        16,
        20,
        16,
        52,
        23,
        99,
        90,
        63,
        30,
        24,
        11,
        274,
        16,
        40,
        39,
        126,
        32,
        76,
        62,
        30,
        34,
        13,
        21,
        25,
        49
      ],
      "2025-03": [
        94,
        73,
        54,
        74,
        68,
        164,
        163,
        81,
        224,
        247,
        47,
        125,
        36,
        35,
        70,
        39,
        70,
        53,
        55,
        141,
        3,
        31,
        9,
        23,
        25,
        14,
        44,
        22,
        47,
        16,
        21,
        19,
        57,
        19,
        85,
        81,
        96,
        27,
        26,
        18,
        225,
        15,
        38,
        45,
        73,
        20,
        55,
        64,
        30,
        27,
        19,
        13,
        22,
        54
      ],
      "2025-04": [
        96,
        51,
        44,
        64,
        50,
        87,
        146,
        70,
        224,
        250,
        62,
        92,
        41,
        35,
        82,
        44,
        79,
        36,
        43,
        102,
        12,
        34,
        9,
        35,
        33,
        9,
        58,
        26,
        32,
        10,
        14,
        21,
        34,
        19,
        87,
        76,
        77,
        29,
        33,
        17,
        180,
        14,
        36,
        36,
        44,
        39,
        55,
        59,
        26,
        24,
        21,
        9,
        23,
        39
      ],
      "2025-05": [
        133,
        124,
        69,
        149,
        57,
        155,
        219,
        134,
        497,
        329,
        57,
        91,
        77,
        43,
        123,
        51,
        125,
        75,
        56,
        233,
        28,
        69,
        17,
        45,
        29,
        17,
        60,
        47,
        40,
        20,
        33,
        41,
        66,
        21,
        126,
        120,
        71,
        42,
        53,
        25,
        337,
        18,
        61,
        51,
        121,
        40,
        98,
        65,
        41,
        41,
        34,
        18,
        29,
        73
      ],
      "2025-06": [
        113,
        87,
        48,
        133,
        72,
        135,
        183,
        95,
        394,
        221,
        50,
        105,
        55,
        42,
        85,
        39,
        110,
        37,
        49,
        196,
        17,
        52,
        21,
        47,
        21,
        18,
        52,
        28,
        27,
        8,
        26,
        14,
        83,
        27,
        90,
        81,
        89,
        40,
        36,
        14,
        240,
        20,
        56,
        36,
        93,
        34,
        75,
        62,
        30,
        35,
        28,
        23,
        21,
        49
      ],
      "2025-07": [
        108,
        65,
        35,
        103,
        59,
        115,
        173,
        85,
        263,
        275,
        53,
        100,
        45,
        41,
        86,
        44,
        86,
        31,
        39,
        172,
        25,
        51,
        32,
        34,
        27,
        11,
        58,
        33,
        35,
        8,
        24,
        23,
        47,
        15,
        94,
        62,
        79,
        36,
        25,
        18,
        199,
        12,
        42,
        37,
        77,
        39,
        76,
        60,
        22,
        40,
        35,
        10,
        31,
        39
      ],
      "2025-08": [
        127,
        77,
        44,
        122,
        62,
        94,
        163,
        103,
        359,
        251,
        74,
        79,
        64,
        42,
        117,
        57,
        102,
        55,
        43,
        194,
        20,
        45,
        24,
        46,
        36,
        12,
        52,
        24,
        41,
        12,
        34,
        16,
        42,
        21,
        81,
        78,
        101,
        39,
        34,
        23,
        230,
        17,
        41,
        40,
        87,
        39,
        79,
        64,
        22,
        42,
        23,
        11,
        42,
        46
      ],
      "2025-09": [
        42,
        30,
        20,
        47,
        24,
        43,
        61,
        35,
        133,
        128,
        9,
        45,
        27,
        17,
        53,
        22,
        38,
        19,
        12,
        94,
        8,
        19,
        7,
        10,
        8,
        8,
        23,
        14,
        11,
        9,
        11,
        7,
        22,
        8,
        36,
        31,
        39,
        18,
        21,
        11,
        97,
        6,
        22,
        13,
        34,
        22,
        49,
        35,
        13,
        17,
        18,
        3,
        7,
        24
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Label-dependent and event-guided interpretable disease risk prediction using EHRs",
          "year": "2022-01",
          "abstract": "Electronic health records (EHRs) contain patients' heterogeneous data that\nare collected from medical providers involved in the patient's care, including\nmedical notes, clinical events, laboratory test results, symptoms, and\ndiagnoses. In the field of modern healthcare, predicting whether patients would\nexperience any risks based on their EHRs has emerged as a promising research\narea, in which artificial intelligence (AI) plays a key role. To make AI models\npractically applicable, it is required that the prediction results should be\nboth accurate and interpretable. To achieve this goal, this paper proposed a\nlabel-dependent and event-guided risk prediction model (LERP) to predict the\npresence of multiple disease risks by mainly extracting information from\nunstructured medical notes. Our model is featured in the following aspects.\nFirst, we adopt a label-dependent mechanism that gives greater attention to\nwords from medical notes that are semantically similar to the names of risk\nlabels. Secondly, as the clinical events (e.g., treatments and drugs) can also\nindicate the health status of patients, our model utilizes the information from\nevents and uses them to generate an event-guided representation of medical\nnotes. Thirdly, both label-dependent and event-guided representations are\nintegrated to make a robust prediction, in which the interpretability is\nenabled by the attention weights over words from medical notes. To demonstrate\nthe applicability of the proposed method, we apply it to the MIMIC-III dataset,\nwhich contains real-world EHRs collected from hospitals. Our method is\nevaluated in both quantitative and qualitative ways.",
          "arxiv_id": "2201.06783v1"
        },
        {
          "title": "PAM: A Propagation-Based Model for Segmenting Any 3D Objects across Multi-Modal Medical Images",
          "year": "2024-08",
          "abstract": "Volumetric segmentation is important in medical imaging, but current methods\nface challenges like requiring lots of manual annotations and being tailored to\nspecific tasks, which limits their versatility. General segmentation models\nused for natural images don't perform well with the unique features of medical\nimages. There's a strong need for an adaptable approach that can effectively\nhandle different 3D medical structures and imaging modalities. In this study,\nwe present PAM (Propagating Anything Model), a segmentation approach that uses\na 2D prompt, like a bounding box or sketch, to create a complete 3D\nsegmentation of medical image volumes. PAM works by modeling relationships\nbetween slices, maintaining information flow across the 3D structure. It\ncombines a CNN-based UNet for processing within slices and a Transformer-based\nattention module for propagating information between slices, leading to better\ngeneralizability across various imaging modalities. PAM significantly\noutperformed existing models like MedSAM and SegVol, with an average\nimprovement of over 18.1% in dice similarity coefficient (DSC) across 44\nmedical datasets and various object types. It also showed stable performance\ndespite prompt deviations and different propagation setups, and faster\ninference speeds compared to other models. PAM's one-view prompt design made it\nmore efficient, reducing interaction time by about 63.6% compared to two-view\nprompts. Thanks to its focus on structural relationships, PAM handled unseen\nand complex objects well, showing a unique ability to generalize to new\nsituations. PAM represents an advancement in medical image segmentation,\neffectively reducing the need for extensive manual work and specialized\ntraining. Its adaptability makes it a promising tool for more automated and\nreliable analysis in clinical settings.",
          "arxiv_id": "2408.13836v2"
        },
        {
          "title": "Medical Transformer: Universal Brain Encoder for 3D MRI Analysis",
          "year": "2021-04",
          "abstract": "Transfer learning has gained attention in medical image analysis due to\nlimited annotated 3D medical datasets for training data-driven deep learning\nmodels in the real world. Existing 3D-based methods have transferred the\npre-trained models to downstream tasks, which achieved promising results with\nonly a small number of training samples. However, they demand a massive amount\nof parameters to train the model for 3D medical imaging. In this work, we\npropose a novel transfer learning framework, called Medical Transformer, that\neffectively models 3D volumetric images in the form of a sequence of 2D image\nslices. To make a high-level representation in 3D-form empowering spatial\nrelations better, we take a multi-view approach that leverages plenty of\ninformation from the three planes of 3D volume, while providing\nparameter-efficient training. For building a source model generally applicable\nto various tasks, we pre-train the model in a self-supervised learning manner\nfor masked encoding vector prediction as a proxy task, using a large-scale\nnormal, healthy brain magnetic resonance imaging (MRI) dataset. Our pre-trained\nmodel is evaluated on three downstream tasks: (i) brain disease diagnosis, (ii)\nbrain age prediction, and (iii) brain tumor segmentation, which are actively\nstudied in brain MRI research. The experimental results show that our Medical\nTransformer outperforms the state-of-the-art transfer learning methods,\nefficiently reducing the number of parameters up to about 92% for\nclassification and",
          "arxiv_id": "2104.13633v1"
        }
      ],
      "1": [
        {
          "title": "An advantage based policy transfer algorithm for reinforcement learning with measures of transferability",
          "year": "2023-11",
          "abstract": "Reinforcement learning (RL) enables sequential decision-making in complex and\nhigh-dimensional environments through interaction with the environment. In most\nreal-world applications, however, a high number of interactions are infeasible.\nIn these environments, transfer RL algorithms, which can be used for the\ntransfer of knowledge from one or multiple source environments to a target\nenvironment, have been shown to increase learning speed and improve initial and\nasymptotic performance. However, most existing transfer RL algorithms are\non-policy and sample inefficient, fail in adversarial target tasks, and often\nrequire heuristic choices in algorithm design. This paper proposes an\noff-policy Advantage-based Policy Transfer algorithm, APT-RL, for fixed domain\nenvironments. Its novelty is in using the popular notion of ``advantage'' as a\nregularizer, to weigh the knowledge that should be transferred from the source,\nrelative to new knowledge learned in the target, removing the need for\nheuristic choices. Further, we propose a new transfer performance measure to\nevaluate the performance of our algorithm and unify existing transfer RL\nframeworks. Finally, we present a scalable, theoretically-backed task\nsimilarity measurement algorithm to illustrate the alignments between our\nproposed transferability measure and similarities between source and target\nenvironments. We compare APT-RL with several baselines, including existing\ntransfer-RL algorithms, in three high-dimensional continuous control tasks. Our\nexperiments demonstrate that APT-RL outperforms existing transfer RL algorithms\nand is at least as good as learning from scratch in adversarial tasks.",
          "arxiv_id": "2311.06731v2"
        },
        {
          "title": "Policy Constraint by Only Support Constraint for Offline Reinforcement Learning",
          "year": "2025-03",
          "abstract": "Offline reinforcement learning (RL) aims to optimize a policy by using\npre-collected datasets, to maximize cumulative rewards. However, offline\nreinforcement learning suffers challenges due to the distributional shift\nbetween the learned and behavior policies, leading to errors when computing\nQ-values for out-of-distribution (OOD) actions. To mitigate this issue, policy\nconstraint methods aim to constrain the learned policy's distribution with the\ndistribution of the behavior policy or confine action selection within the\nsupport of the behavior policy. However, current policy constraint methods tend\nto exhibit excessive conservatism, hindering the policy from further surpassing\nthe behavior policy's performance. In this work, we present Only Support\nConstraint (OSC) which is derived from maximizing the total probability of\nlearned policy in the support of behavior policy, to address the conservatism\nof policy constraint. OSC presents a regularization term that only restricts\npolicies to the support without imposing extra constraints on actions within\nthe support. Additionally, to fully harness the performance of the new policy\nconstraints, OSC utilizes a diffusion model to effectively characterize the\nsupport of behavior policies. Experimental evaluations across a variety of\noffline RL benchmarks demonstrate that OSC significantly enhances performance,\nalleviating the challenges associated with distributional shifts and mitigating\nconservatism of policy constraints. Code is available at\nhttps://github.com/MoreanP/OSC.",
          "arxiv_id": "2503.05207v1"
        },
        {
          "title": "Offline Reinforcement Learning from Images with Latent Space Models",
          "year": "2020-12",
          "abstract": "Offline reinforcement learning (RL) refers to the problem of learning\npolicies from a static dataset of environment interactions. Offline RL enables\nextensive use and re-use of historical datasets, while also alleviating safety\nconcerns associated with online exploration, thereby expanding the real-world\napplicability of RL. Most prior work in offline RL has focused on tasks with\ncompact state representations. However, the ability to learn directly from rich\nobservation spaces like images is critical for real-world applications such as\nrobotics. In this work, we build on recent advances in model-based algorithms\nfor offline RL, and extend them to high-dimensional visual observation spaces.\nModel-based offline RL algorithms have achieved state of the art results in\nstate based tasks and have strong theoretical guarantees. However, they rely\ncrucially on the ability to quantify uncertainty in the model predictions,\nwhich is particularly challenging with image observations. To overcome this\nchallenge, we propose to learn a latent-state dynamics model, and represent the\nuncertainty in the latent space. Our approach is both tractable in practice and\ncorresponds to maximizing a lower bound of the ELBO in the unknown POMDP. In\nexperiments on a range of challenging image-based locomotion and manipulation\ntasks, we find that our algorithm significantly outperforms previous offline\nmodel-free RL methods as well as state-of-the-art online visual model-based RL\nmethods. Moreover, we also find that our approach excels on an image-based\ndrawer closing task on a real robot using a pre-existing dataset. All results\nincluding videos can be found online at https://sites.google.com/view/lompo/ .",
          "arxiv_id": "2012.11547v1"
        }
      ],
      "2": [
        {
          "title": "JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images",
          "year": "2024-09",
          "abstract": "Existing vision-language understanding benchmarks largely consist of images\nof objects in their usual contexts. As a consequence, recent multimodal large\nlanguage models can perform well with only a shallow visual understanding by\nrelying on background language biases. Thus, strong performance on these\nbenchmarks does not necessarily correlate with strong visual understanding. In\nthis paper, we release JourneyBench, a comprehensive human-annotated benchmark\nof generated images designed to assess the model's fine-grained multimodal\nreasoning abilities across five tasks: complementary multimodal chain of\nthought, multi-image VQA, imaginary image captioning, VQA with hallucination\ntriggers, and fine-grained retrieval with sample-specific distractors. Unlike\nexisting benchmarks, JourneyBench explicitly requires fine-grained multimodal\nreasoning in unusual imaginary scenarios where language bias and holistic image\ngist are insufficient. We benchmark state-of-the-art models on JourneyBench and\nanalyze performance along a number of fine-grained dimensions. Results across\nall five tasks show that JourneyBench is exceptionally challenging for even the\nbest models, indicating that models' visual reasoning abilities are not as\nstrong as they first appear. We discuss the implications of our findings and\npropose avenues for further research.",
          "arxiv_id": "2409.12953v4"
        },
        {
          "title": "Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners",
          "year": "2022-05",
          "abstract": "The goal of this work is to build flexible video-language models that can\ngeneralize to various video-to-text tasks from few examples, such as\ndomain-specific captioning, question answering, and future event prediction.\nExisting few-shot video-language learners focus exclusively on the encoder,\nresulting in the absence of a video-to-text decoder to handle generative tasks.\nVideo captioners have been pretrained on large-scale video-language datasets,\nbut they rely heavily on finetuning and lack the ability to generate text for\nunseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language\nLearner via Image and Language models, which demonstrates strong performance on\nfew-shot video-to-text tasks without the necessity of pretraining or finetuning\non any video datasets. We use the image-language models to translate the video\ncontent into frame captions, object, attribute, and event phrases, and compose\nthem into a temporal structure template. We then instruct a language model,\nwith a prompt containing a few in-context examples, to generate a target output\nfrom the composed content. The flexibility of prompting allows the model to\ncapture any form of text input, such as automatic speech recognition (ASR)\ntranscripts. Our experiments demonstrate the power of language models in\nunderstanding videos on a wide variety of video-language tasks, including video\ncaptioning, video question answering, video caption retrieval, and video future\nevent prediction. Especially, on video future event prediction, our few-shot\nmodel significantly outperforms state-of-the-art supervised models trained on\nlarge-scale video datasets. Code and resources are publicly available for\nresearch purposes at https://github.com/MikeWangWZHL/VidIL .",
          "arxiv_id": "2205.10747v4"
        },
        {
          "title": "FiVL: A Framework for Improved Vision-Language Alignment through the Lens of Training, Evaluation and Explainability",
          "year": "2024-12",
          "abstract": "Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. Furthermore, current\nvision-language benchmarks are not specifically measuring the degree to which\nthe answer require the visual input. This limitation makes it challenging to\nconfirm that the image is truly necessary, particularly in tasks like visual\nquestion answering. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nalso evaluate their effectiveness in achieving it. We demonstrate the value of\nour datasets through three approaches. First, we introduce a novel training\ntask based on our augmented training dataset, resulting in better performance\nthan the baseline. Second, we present benchmarks to assess the model's ability\nto use image as substantive evidence, rather than relying solely on linguistic\npriors. Finally, we identify attention heads with the strongest vision-language\nalignment, enabling explainability on visual-driven hallucinations. The code is\navailable at https://github.com/IntelLabs/fivl.",
          "arxiv_id": "2412.14672v2"
        }
      ],
      "3": [
        {
          "title": "Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS",
          "year": "2025-08",
          "abstract": "Dysarthric speech poses significant challenges in developing assistive\ntechnologies, primarily due to the limited availability of data. Recent\nadvances in neural speech synthesis, especially zero-shot voice cloning,\nfacilitate synthetic speech generation for data augmentation; however, they may\nintroduce biases towards dysarthric speech. In this paper, we investigate the\neffectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using\nTORGO dataset, focusing on intelligibility, speaker similarity, and prosody\npreservation. We also analyze potential biases using fairness metrics like\nDisparate Impact and Parity Difference to assess disparities across dysarthric\nseverity levels. Results show that F5-TTS exhibits a strong bias toward speech\nintelligibility over speaker and prosody preservation in dysarthric speech\nsynthesis. Insights from this study can help integrate fairness-aware\ndysarthric speech synthesis, fostering the advancement of more inclusive speech\ntechnologies.",
          "arxiv_id": "2508.05102v3"
        },
        {
          "title": "Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding",
          "year": "2025-01",
          "abstract": "Spoken language understanding (SLU) is indispensable for half of all living\nlanguages that lack a formal writing system. Unlike for high-resource\nlanguages, for these languages, we cannot offload semantic understanding of\nspeech to the cascade of automatic speech recognition (ASR) and text-based\nlarge language models (LLMs). Even if low-resource languages possess a writing\nsystem, ASR for these languages remains unreliable due to limited bimodal\nspeech and text training data. Nonetheless, the evaluation of multilingual SLU\nis limited to shallow tasks such as intent classification or language\nidentification. This is why we present Fleurs-SLU, a multilingual SLU benchmark\nthat encompasses (i) 692 hours of speech for topical utterance classification\nin 102 languages and (ii) multiple-choice question answering via listening\ncomprehension spanning 944 hours of speech across 92 languages. We extensively\nevaluate end-to-end speech classification models, cascaded systems that combine\nspeech-to-text transcription with subsequent LLM-based classification, and\nmultimodal speech-LLMs on Fleurs-SLU. Our results show that cascaded systems\nare more robust in multilingual SLU, though well-pretrained speech encoders can\nperform competitively in topical speech classification. Closed-source\nspeech-LLMs match or surpass the performance of cascaded systems. We observe a\nstrong correlation between robust multilingual ASR, effective speech-to-text\ntranslation, and strong multilingual SLU, indicating mutual benefits between\nacoustic and semantic speech representations.",
          "arxiv_id": "2501.06117v3"
        },
        {
          "title": "AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation",
          "year": "2023-12",
          "abstract": "This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.",
          "arxiv_id": "2312.02512v2"
        }
      ],
      "4": [
        {
          "title": "Transferable and Adaptable Driving Behavior Prediction",
          "year": "2022-02",
          "abstract": "While autonomous vehicles still struggle to solve challenging situations\nduring on-road driving, humans have long mastered the essence of driving with\nefficient, transferable, and adaptable driving capability. By mimicking humans'\ncognition model and semantic understanding during driving, we propose HATN, a\nhierarchical framework to generate high-quality, transferable, and adaptable\npredictions for driving behaviors in multi-agent dense-traffic environments.\nOur hierarchical method consists of a high-level intention identification\npolicy and a low-level trajectory generation policy. We introduce a novel\nsemantic sub-task definition and generic state representation for each\nsub-task. With these techniques, the hierarchical framework is transferable\nacross different driving scenarios. Besides, our model is able to capture\nvariations of driving behaviors among individuals and scenarios by an online\nadaptation module. We demonstrate our algorithms in the task of trajectory\nprediction for real traffic data at intersections and roundabouts from the\nINTERACTION dataset. Through extensive numerical studies, it is evident that\nour method significantly outperformed other methods in terms of prediction\naccuracy, transferability, and adaptability. Pushing the state-of-the-art\nperformance by a considerable margin, we also provide a cognitive view of\nunderstanding the driving behavior behind such improvement. We highlight that\nin the future, more research attention and effort are deserved for\ntransferability and adaptability. It is not only due to the promising\nperformance elevation of prediction and planning algorithms, but more\nfundamentally, they are crucial for the scalable and general deployment of\nautonomous vehicles.",
          "arxiv_id": "2202.05140v2"
        },
        {
          "title": "Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios",
          "year": "2025-03",
          "abstract": "Autonomous driving has made significant progress in both academia and\nindustry, including performance improvements in perception task and the\ndevelopment of end-to-end autonomous driving systems. However, the safety and\nrobustness assessment of autonomous driving has not received sufficient\nattention. Current evaluations of autonomous driving are typically conducted in\nnatural driving scenarios. However, many accidents often occur in edge cases,\nalso known as safety-critical scenarios. These safety-critical scenarios are\ndifficult to collect, and there is currently no clear definition of what\nconstitutes a safety-critical scenario. In this work, we explore the safety and\nrobustness of autonomous driving in safety-critical scenarios. First, we\nprovide a definition of safety-critical scenarios, including static traffic\nscenarios such as adversarial attack scenarios and natural distribution shifts,\nas well as dynamic traffic scenarios such as accident scenarios. Then, we\ndevelop an autonomous driving safety testing platform to comprehensively\nevaluate autonomous driving systems, encompassing not only the assessment of\nperception modules but also system-level evaluations. Our work systematically\nconstructs a safety verification process for autonomous driving, providing\ntechnical support for the industry to establish standardized test framework and\nreduce risks in real-world road deployment.",
          "arxiv_id": "2503.23708v2"
        },
        {
          "title": "Prediction Based Decision Making for Autonomous Highway Driving",
          "year": "2022-09",
          "abstract": "Autonomous driving decision-making is a challenging task due to the inherent\ncomplexity and uncertainty in traffic. For example, adjacent vehicles may\nchange their lane or overtake at any time to pass a slow vehicle or to help\ntraffic flow. Anticipating the intention of surrounding vehicles, estimating\ntheir future states and integrating them into the decision-making process of an\nautomated vehicle can enhance the reliability of autonomous driving in complex\ndriving scenarios. This paper proposes a Prediction-based Deep Reinforcement\nLearning (PDRL) decision-making model that considers the manoeuvre intentions\nof surrounding vehicles in the decision-making process for highway driving. The\nmodel is trained using real traffic data and tested in various traffic\nconditions through a simulation platform. The results show that the proposed\nPDRL model improves the decision-making performance compared to a Deep\nReinforcement Learning (DRL) model by decreasing collision numbers, resulting\nin safer driving.",
          "arxiv_id": "2209.02106v1"
        }
      ],
      "5": [
        {
          "title": "RL for Consistency Models: Faster Reward Guided Text-to-Image Generation",
          "year": "2024-03",
          "abstract": "Reinforcement learning (RL) has improved guided image generation with\ndiffusion models by directly optimizing rewards that capture image quality,\naesthetics, and instruction following capabilities. However, the resulting\ngenerative policies inherit the same iterative sampling process of diffusion\nmodels that causes slow generation. To overcome this limitation, consistency\nmodels proposed learning a new class of generative models that directly map\nnoise to data, resulting in a model that can generate an image in as few as one\nsampling iteration. In this work, to optimize text-to-image generative models\nfor task specific rewards and enable fast training and inference, we propose a\nframework for fine-tuning consistency models via RL. Our framework, called\nReinforcement Learning for Consistency Model (RLCM), frames the iterative\ninference process of a consistency model as an RL procedure. Comparing to RL\nfinetuned diffusion models, RLCM trains significantly faster, improves the\nquality of the generation measured under the reward objectives, and speeds up\nthe inference procedure by generating high quality images with as few as two\ninference steps. Experimentally, we show that RLCM can adapt text-to-image\nconsistency models to objectives that are challenging to express with\nprompting, such as image compressibility, and those derived from human\nfeedback, such as aesthetic quality. Our code is available at\nhttps://rlcm.owenoertell.com.",
          "arxiv_id": "2404.03673v2"
        },
        {
          "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
          "year": "2023-03",
          "abstract": "This survey reviews the progress of diffusion models in generating images\nfrom text, ~\\textit{i.e.} text-to-image diffusion models. As a self-contained\nwork, this survey starts with a brief introduction of how diffusion models work\nfor image synthesis, followed by the background for text-conditioned image\nsynthesis. Based on that, we present an organized review of pioneering methods\nand their improvements on text-to-image generation. We further summarize\napplications beyond image generation, such as text-guided generation for\nvarious modalities like videos, and text-guided image editing. Beyond the\nprogress made so far, we discuss existing challenges and promising future\ndirections.",
          "arxiv_id": "2303.07909v3"
        },
        {
          "title": "BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models",
          "year": "2023-12",
          "abstract": "Diffusion models have made tremendous progress in text-driven image and video\ngeneration. Now text-to-image foundation models are widely applied to various\ndownstream image synthesis tasks, such as controllable image generation and\nimage editing, while downstream video synthesis tasks are less explored for\nseveral reasons. First, it requires huge memory and computation overhead to\ntrain a video generation foundation model. Even with video foundation models,\nadditional costly training is still required for downstream video synthesis\ntasks. Second, although some works extend image diffusion models into videos in\na training-free manner, temporal consistency cannot be well preserved. Finally,\nthese adaption methods are specifically designed for one task and fail to\ngeneralize to different tasks. To mitigate these issues, we propose a\ntraining-free general-purpose video synthesis framework, coined as {\\bf\nBIVDiff}, via bridging specific image diffusion models and general\ntext-to-video foundation diffusion models. Specifically, we first use a\nspecific image diffusion model (e.g., ControlNet and Instruct Pix2Pix) for\nframe-wise video generation, then perform Mixed Inversion on the generated\nvideo, and finally input the inverted latents into the video diffusion models\n(e.g., VidRD and ZeroScope) for temporal smoothing. This decoupled framework\nenables flexible image model selection for different purposes with strong task\ngeneralization and high efficiency. To validate the effectiveness and general\nuse of BIVDiff, we perform a wide range of video synthesis tasks, including\ncontrollable video generation, video editing, video inpainting, and\noutpainting.",
          "arxiv_id": "2312.02813v2"
        }
      ],
      "6": [
        {
          "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding",
          "year": "2025-05",
          "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence.",
          "arxiv_id": "2505.07768v1"
        },
        {
          "title": "Test-Driven Development for Code Generation",
          "year": "2024-02",
          "abstract": "Recent Large Language Models (LLMs) have demonstrated significant\ncapabilities in generating code snippets directly from problem statements. This\nincreasingly automated process mirrors traditional human-led software\ndevelopment, where code is often written in response to a requirement.\nHistorically, Test-Driven Development (TDD) has proven its merit, requiring\ndevelopers to write tests before the functional code, ensuring alignment with\nthe initial problem statements. Applying TDD principles to LLM-based code\ngeneration offers one distinct benefit: it enables developers to verify the\ncorrectness of generated code against predefined tests. This paper investigates\nif and how TDD can be incorporated into AI-assisted code-generation processes.\nWe experimentally evaluate our hypothesis that providing LLMs like GPT-4 and\nLlama 3 with tests in addition to the problem statements enhances code\ngeneration outcomes. We experimented with established function-level code\ngeneration benchmarks such as MBPP and HumanEval. Our results consistently\ndemonstrate that including test cases leads to higher success in solving\nprogramming challenges. We assert that TDD is a promising paradigm for helping\nensure that the code generated by LLMs effectively captures the requirements.",
          "arxiv_id": "2402.13521v2"
        },
        {
          "title": "An Empirical Study on Capability of Large Language Models in Understanding Code Semantics",
          "year": "2024-07",
          "abstract": "Large Language Models for Code (code LLMs) have demonstrated remarkable\nperformance across various software engineering (SE) tasks, increasing the\napplication of code LLMs in software development. Despite the success of code\nLLMs, there remain significant concerns about the actual capabilities and\nreliability of these models, \"whether these models really learn the semantics\nof code from the training data and leverage the learned knowledge to perform\nthe SE tasks\". In this paper, we introduce EMPICA, a comprehensive framework\ndesigned to systematically and empirically evaluate the capabilities of code\nLLMs in understanding code semantics. Specifically, EMPICA systematically\nintroduces controlled modifications/transformations into the input code and\nexamines the models' responses. Generally, code LLMs must be robust to\nsemantically equivalent code inputs and be sensitive to non-equivalent ones for\nall SE tasks. Specifically, for every SE task, given an input code snippet c\nand its semantic equivalent variants, code LLMs must robustly produce\nconsistent/equivalent outputs while they are expected to generate different\noutputs for c and its semantic non-equivalent variants. Our experimental\nresults on three representative code understanding tasks, including code\nsummarization, method name prediction, and output prediction, reveal that the\nrobustness and sensitivity of the state-of-the-art code LLMs to code\ntransformations vary significantly across tasks and transformation operators.\nIn addition, the code LLMs exhibit better robustness to the semantic preserving\ntransformations than their sensitivity to the semantic non-preserving\ntransformations. These results highlight a need to enhance the model's\ncapabilities of understanding code semantics, especially the sensitivity\nproperty.",
          "arxiv_id": "2407.03611v1"
        }
      ],
      "7": [
        {
          "title": "High-Order Pooling for Graph Neural Networks with Tensor Decomposition",
          "year": "2022-05",
          "abstract": "Graph Neural Networks (GNNs) are attracting growing attention due to their\neffectiveness and flexibility in modeling a variety of graph-structured data.\nExiting GNN architectures usually adopt simple pooling operations (eg. sum,\naverage, max) when aggregating messages from a local neighborhood for updating\nnode representation or pooling node representations from the entire graph to\ncompute the graph representation. Though simple and effective, these linear\noperations do not model high-order non-linear interactions among nodes. We\npropose the Tensorized Graph Neural Network (tGNN), a highly expressive GNN\narchitecture relying on tensor decomposition to model high-order non-linear\nnode interactions. tGNN leverages the symmetric CP decomposition to efficiently\nparameterize permutation-invariant multilinear maps for modeling node\ninteractions. Theoretical and empirical analysis on both node and graph\nclassification tasks show the superiority of tGNN over competitive baselines.\nIn particular, tGNN achieves the most solid results on two OGB node\nclassification datasets and one OGB graph classification dataset.",
          "arxiv_id": "2205.11691v2"
        },
        {
          "title": "Graph Transformer Networks: Learning Meta-path Graphs to Improve GNNs",
          "year": "2021-06",
          "abstract": "Graph Neural Networks (GNNs) have been widely applied to various fields due\nto their powerful representations of graph-structured data. Despite the success\nof GNNs, most existing GNNs are designed to learn node representations on the\nfixed and homogeneous graphs. The limitations especially become problematic\nwhen learning representations on a misspecified graph or a heterogeneous graph\nthat consists of various types of nodes and edges. To address this limitations,\nwe propose Graph Transformer Networks (GTNs) that are capable of generating new\ngraph structures, which preclude noisy connections and include useful\nconnections (e.g., meta-paths) for tasks, while learning effective node\nrepresentations on the new graphs in an end-to-end fashion. We further propose\nenhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that\nimprove scalability of graph transformations. Compared to GTNs, FastGTNs are\n230x faster and use 100x less memory while allowing the identical graph\ntransformations as GTNs. In addition, we extend graph transformations to the\nsemantic proximity of nodes allowing non-local operations beyond meta-paths.\nExtensive experiments on both homogeneous graphs and heterogeneous graphs show\nthat GTNs and FastGTNs with non-local operations achieve the state-of-the-art\nperformance for node classification tasks. The code is available:\nhttps://github.com/seongjunyun/Graph_Transformer_Networks",
          "arxiv_id": "2106.06218v1"
        },
        {
          "title": "KerGNNs: Interpretable Graph Neural Networks with Graph Kernels",
          "year": "2022-01",
          "abstract": "Graph kernels are historically the most widely-used technique for graph\nclassification tasks. However, these methods suffer from limited performance\nbecause of the hand-crafted combinatorial features of graphs. In recent years,\ngraph neural networks (GNNs) have become the state-of-the-art method in\ndownstream graph-related tasks due to their superior performance. Most GNNs are\nbased on Message Passing Neural Network (MPNN) frameworks. However, recent\nstudies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL)\nalgorithm in graph isomorphism test. To address the limitations of existing\ngraph kernel and GNN methods, in this paper, we propose a novel GNN framework,\ntermed \\textit{Kernel Graph Neural Networks} (KerGNNs), which integrates graph\nkernels into the message passing process of GNNs. Inspired by convolution\nfilters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden\ngraphs as graph filters which are combined with subgraphs to update node\nembeddings using graph kernels. In addition, we show that MPNNs can be viewed\nas special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks\nand use cross-validation to make fair comparisons with benchmarks. We show that\nour method achieves competitive performance compared with existing\nstate-of-the-art methods, demonstrating the potential to increase the\nrepresentation ability of GNNs. We also show that the trained graph filters in\nKerGNNs can reveal the local graph structures of the dataset, which\nsignificantly improves the model interpretability compared with conventional\nGNN models.",
          "arxiv_id": "2201.00491v2"
        }
      ],
      "8": [
        {
          "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
          "year": "2024-10",
          "abstract": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
          "arxiv_id": "2410.17635v2"
        },
        {
          "title": "Improving LLMs' Generalized Reasoning Abilities by Graph Problems",
          "year": "2025-07",
          "abstract": "Large Language Models (LLMs) have made remarkable strides in reasoning tasks,\nyet their performance often falters on novel and complex problems.\nDomain-specific continued pretraining (CPT) methods, such as those tailored for\nmathematical reasoning, have shown promise but lack transferability to broader\nreasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning\n(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,\nspanning pathfinding, network analysis, numerical computation, and topological\nreasoning, require sophisticated logical and relational reasoning, making them\nideal for teaching diverse reasoning patterns. To achieve this, we introduce\nGraphPile, the first large-scale corpus specifically designed for CPT using GPR\ndata. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes\nchain-of-thought, program-of-thought, trace of execution, and real-world graph\ndata. Using GraphPile, we train GraphMind on popular base models Llama 3 and\n3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in\nmathematical reasoning and up to 21.2 percent improvement in non-mathematical\nreasoning tasks such as logical and commonsense reasoning. By being the first\nto harness GPR for enhancing reasoning patterns and introducing the first\ndataset of its kind, our work bridges the gap between domain-specific\npretraining and universal reasoning capabilities, advancing the adaptability\nand robustness of LLMs.",
          "arxiv_id": "2507.17168v1"
        },
        {
          "title": "Deductive Verification of Chain-of-Thought Reasoning",
          "year": "2023-06",
          "abstract": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought\n(CoT) prompting in performing various reasoning tasks. While CoT allows models\nto produce more comprehensive reasoning processes, its emphasis on intermediate\nreasoning steps can inadvertently introduce hallucinations and accumulated\nerrors, thereby limiting models' ability to solve complex reasoning tasks.\nInspired by how humans engage in careful and meticulous deductive logical\nreasoning processes to solve tasks, we seek to enable language models to\nperform explicit and rigorous deductive reasoning, and also ensure the\ntrustworthiness of their reasoning process through self-verification. However,\ndirectly verifying the validity of an entire deductive reasoning process is\nchallenging, even with advanced models like ChatGPT. In light of this, we\npropose to decompose a reasoning verification process into a series of\nstep-by-step subprocesses, each only receiving their necessary context and\npremises. To facilitate this procedure, we propose Natural Program, a natural\nlanguage-based deductive reasoning format. Our approach enables models to\ngenerate precise reasoning steps where subsequent steps are more rigorously\ngrounded on prior steps. It also empowers language models to carry out\nreasoning self-verification in a step-by-step manner. By integrating this\nverification process into each deductive reasoning stage, we significantly\nenhance the rigor and trustfulness of generated reasoning steps. Along this\nprocess, we also improve the answer correctness on complex reasoning tasks.\nCode will be released at https://github.com/lz1oceani/verify_cot.",
          "arxiv_id": "2306.03872v3"
        }
      ],
      "9": [
        {
          "title": "Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications",
          "year": "2025-05",
          "abstract": "Military weapon systems and command-and-control infrastructure augmented by\nartificial intelligence (AI) have seen rapid development and deployment in\nrecent years. However, the sociotechnical impacts of AI on combat systems,\nmilitary decision-making, and the norms of warfare have been understudied. We\nfocus on a specific subset of lethal autonomous weapon systems (LAWS) that use\nAI for targeting or battlefield decisions. We refer to this subset as\nAI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they\nintroduce novel risks -- including unanticipated escalation, poor reliability\nin unfamiliar environments, and erosion of human oversight -- all of which\nthreaten both military effectiveness and the openness of AI research. These\nrisks cannot be addressed by high-level policy alone; effective regulation must\nbe grounded in the technical behavior of AI models. We argue that AI\nresearchers must be involved throughout the regulatory lifecycle. Thus, we\npropose a clear, behavior-based definition of AI-LAWS -- systems that introduce\nunique risks through their use of modern AI -- as a foundation for technically\ngrounded regulation, given that existing frameworks do not distinguish them\nfrom conventional LAWS. Using this definition, we propose several\ntechnically-informed policy directions and invite greater participation from\nthe AI research community in military AI policy discussions.",
          "arxiv_id": "2505.18371v1"
        },
        {
          "title": "Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation",
          "year": "2023-05",
          "abstract": "Trustworthy Artificial Intelligence (AI) is based on seven technical\nrequirements sustained over three main pillars that should be met throughout\nthe system's entire life cycle: it should be (1) lawful, (2) ethical, and (3)\nrobust, both from a technical and a social perspective. However, attaining\ntruly trustworthy AI concerns a wider vision that comprises the trustworthiness\nof all processes and actors that are part of the system's life cycle, and\nconsiders previous aspects from different lenses. A more holistic vision\ncontemplates four essential axes: the global principles for ethical use and\ndevelopment of AI-based systems, a philosophical take on AI ethics, a\nrisk-based approach to AI regulation, and the mentioned pillars and\nrequirements. The seven requirements (human agency and oversight; robustness\nand safety; privacy and data governance; transparency; diversity,\nnon-discrimination and fairness; societal and environmental wellbeing; and\naccountability) are analyzed from a triple perspective: What each requirement\nfor trustworthy AI is, Why it is needed, and How each requirement can be\nimplemented in practice. On the other hand, a practical approach to implement\ntrustworthy AI systems allows defining the concept of responsibility of\nAI-based systems facing the law, through a given auditing process. Therefore, a\nresponsible AI system is the resulting notion we introduce in this work, and a\nconcept of utmost necessity that can be realized through auditing processes,\nsubject to the challenges posed by the use of regulatory sandboxes. Our\nmultidisciplinary vision of trustworthy AI culminates in a debate on the\ndiverging views published lately about the future of AI. Our reflections in\nthis matter conclude that regulation is a key for reaching a consensus among\nthese views, and that trustworthy and responsible AI systems will be crucial\nfor the present and future of our society.",
          "arxiv_id": "2305.02231v2"
        },
        {
          "title": "Rebuilding Trust: Queer in AI Approach to Artificial Intelligence Risk Management",
          "year": "2021-09",
          "abstract": "Trustworthy artificial intelligence (AI) has become an important topic\nbecause trust in AI systems and their creators has been lost. Researchers,\ncorporations, and governments have long and painful histories of excluding\nmarginalized groups from technology development, deployment, and oversight. As\na result, these technologies are less useful and even harmful to minoritized\ngroups. We argue that any AI development, deployment, and monitoring framework\nthat aspires to trust must incorporate both feminist, non-exploitative\nparticipatory design principles and strong, outside, and continual monitoring\nand testing. We additionally explain the importance of considering aspects of\ntrustworthiness beyond just transparency, fairness, and accountability,\nspecifically, to consider justice and shifting power to the disempowered as\ncore values to any trustworthy AI system. Creating trustworthy AI starts by\nfunding, supporting, and empowering grassroots organizations like Queer in AI\nso the field of AI has the diversity and inclusion to credibly and effectively\ndevelop trustworthy AI. We leverage the expert knowledge Queer in AI has\ndeveloped through its years of work and advocacy to discuss if and how gender,\nsexuality, and other aspects of queer identity should be used in datasets and\nAI systems and how harms along these lines should be mitigated. Based on this,\nwe share a gendered approach to AI and further propose a queer epistemology and\nanalyze the benefits it can bring to AI. We additionally discuss how to\nregulate AI with this queer epistemology in vision, proposing frameworks for\nmaking policies related to AI & gender diversity and privacy & queer data\nprotection.",
          "arxiv_id": "2110.09271v3"
        }
      ],
      "10": [
        {
          "title": "Learning Heterogeneous Temporal Patterns of User Preference for Timely Recommendation",
          "year": "2021-04",
          "abstract": "Recommender systems have achieved great success in modeling user's\npreferences on items and predicting the next item the user would consume.\nRecently, there have been many efforts to utilize time information of users'\ninteractions with items to capture inherent temporal patterns of user behaviors\nand offer timely recommendations at a given time. Existing studies regard the\ntime information as a single type of feature and focus on how to associate it\nwith user preferences on items. However, we argue they are insufficient for\nfully learning the time information because the temporal patterns of user\npreference are usually heterogeneous. A user's preference for a particular item\nmay 1) increase periodically or 2) evolve over time under the influence of\nsignificant recent events, and each of these two kinds of temporal pattern\nappears with some unique characteristics. In this paper, we first define the\nunique characteristics of the two kinds of temporal pattern of user preference\nthat should be considered in time-aware recommender systems. Then we propose a\nnovel recommender system for timely recommendations, called TimelyRec, which\njointly learns the heterogeneous temporal patterns of user preference\nconsidering all of the defined characteristics. In TimelyRec, a cascade of two\nencoders captures the temporal patterns of user preference using a proposed\nattention module for each encoder. Moreover, we introduce an evaluation\nscenario that evaluates the performance on predicting an interesting item and\nwhen to recommend the item simultaneously in top-K recommendation (i.e.,\nitem-timing recommendation). Our extensive experiments on a scenario for item\nrecommendation and the proposed scenario for item-timing recommendation on\nreal-world datasets demonstrate the superiority of TimelyRec and the proposed\nattention modules.",
          "arxiv_id": "2104.14200v1"
        },
        {
          "title": "Cross-domain recommendation via user interest alignment",
          "year": "2023-01",
          "abstract": "Cross-domain recommendation aims to leverage knowledge from multiple domains\nto alleviate the data sparsity and cold-start problems in traditional\nrecommender systems. One popular paradigm is to employ overlapping user\nrepresentations to establish domain connections, thereby improving\nrecommendation performance in all scenarios. Nevertheless, the general practice\nof this approach is to train user embeddings in each domain separately and then\naggregate them in a plain manner, often ignoring potential cross-domain\nsimilarities between users and items. Furthermore, considering that their\ntraining objective is recommendation task-oriented without specific\nregularizations, the optimized embeddings disregard the interest alignment\namong user's views, and even violate the user's original interest distribution.\nTo address these challenges, we propose a novel cross-domain recommendation\nframework, namely COAST, to improve recommendation performance on dual domains\nby perceiving the cross-domain similarity between entities and aligning user\ninterests. Specifically, we first construct a unified cross-domain\nheterogeneous graph and redefine the message passing mechanism of graph\nconvolutional networks to capture high-order similarity of users and items\nacross domains. Targeted at user interest alignment, we develop deep insights\nfrom two more fine-grained perspectives of user-user and user-item interest\ninvariance across domains by virtue of affluent unsupervised and semantic\nsignals. We conduct intensive experiments on multiple tasks, constructed from\ntwo large recommendation data sets. Extensive results show COAST consistently\nand significantly outperforms state-of-the-art cross-domain recommendation\nalgorithms as well as classic single-domain recommendation methods.",
          "arxiv_id": "2301.11467v1"
        },
        {
          "title": "Multi-Behavior Sequential Recommendation with Temporal Graph Transformer",
          "year": "2022-06",
          "abstract": "Modeling time-evolving preferences of users with their sequential item\ninteractions, has attracted increasing attention in many online applications.\nHence, sequential recommender systems have been developed to learn the dynamic\nuser interests from the historical interactions for suggesting items. However,\nthe interaction pattern encoding functions in most existing sequential\nrecommender systems have focused on single type of user-item interactions. In\nmany real-life online platforms, user-item interactive behaviors are often\nmulti-typed (e.g., click, add-to-favorite, purchase) with complex cross-type\nbehavior inter-dependencies. Learning from informative representations of users\nand items based on their multi-typed interaction data, is of great importance\nto accurately characterize the time-evolving user preference. In this work, we\ntackle the dynamic user-item relation learning with the awareness of\nmulti-behavior interactive patterns. Towards this end, we propose a new\nTemporal Graph Transformer (TGT) recommendation framework to jointly capture\ndynamic short-term and long-range user-item interactive patterns, by exploring\nthe evolving correlations across different types of behaviors. The new TGT\nmethod endows the sequential recommendation architecture to distill dedicated\nknowledge for type-specific behavior relational context and the implicit\nbehavior dependencies. Experiments on the real-world datasets indicate that our\nmethod TGT consistently outperforms various state-of-the-art recommendation\nmethods. Our model implementation codes are available at\nhttps://github.com/akaxlh/TGT.",
          "arxiv_id": "2206.02687v1"
        }
      ],
      "11": [
        {
          "title": "Neural Correspondence Field for Object Pose Estimation",
          "year": "2022-07",
          "abstract": "We propose a method for estimating the 6DoF pose of a rigid object with an\navailable 3D model from a single RGB image. Unlike classical\ncorrespondence-based methods which predict 3D object coordinates at pixels of\nthe input image, the proposed method predicts 3D object coordinates at 3D query\npoints sampled in the camera frustum. The move from pixels to 3D points, which\nis inspired by recent PIFu-style methods for 3D reconstruction, enables\nreasoning about the whole object, including its (self-)occluded parts. For a 3D\nquery point associated with a pixel-aligned image feature, we train a\nfully-connected neural network to predict: (i) the corresponding 3D object\ncoordinates, and (ii) the signed distance to the object surface, with the first\ndefined only for query points in the surface vicinity. We call the mapping\nrealized by this network as Neural Correspondence Field. The object pose is\nthen robustly estimated from the predicted 3D-3D correspondences by the\nKabsch-RANSAC algorithm. The proposed method achieves state-of-the-art results\non three BOP datasets and is shown superior especially in challenging cases\nwith occlusion. The project website is at: linhuang17.github.io/NCF.",
          "arxiv_id": "2208.00113v1"
        },
        {
          "title": "ODIN: A Single Model for 2D and 3D Segmentation",
          "year": "2024-01",
          "abstract": "State-of-the-art models on contemporary 3D segmentation benchmarks like\nScanNet consume and label dataset-provided 3D point clouds, obtained through\npost processing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website (https://odin-seg.github.io).",
          "arxiv_id": "2401.02416v3"
        },
        {
          "title": "Point Cloud Self-supervised Learning via 3D to Multi-view Masked Learner",
          "year": "2023-11",
          "abstract": "Recently, multi-modal masked autoencoders (MAE) has been introduced in 3D\nself-supervised learning, offering enhanced feature learning by leveraging both\n2D and 3D data to capture richer cross-modal representations. However, these\napproaches have two limitations: (1) they inefficiently require both 2D and 3D\nmodalities as inputs, even though the inherent multi-view properties of 3D\npoint clouds already contain 2D modality. (2) input 2D modality causes the\nreconstruction learning to unnecessarily rely on visible 2D information,\nhindering 3D geometric representation learning. To address these challenges, we\npropose a 3D to Multi-View Learner (Multi-View ML) that only utilizes 3D\nmodalities as inputs and effectively capture rich spatial information in 3D\npoint clouds. Specifically, we first project 3D point clouds to multi-view 2D\nimages at the feature level based on 3D-based pose. Then, we introduce two\ncomponents: (1) a 3D to multi-view autoencoder that reconstructs point clouds\nand multi-view images from 3D and projected 2D features; (2) a multi-scale\nmulti-head (MSMH) attention mechanism that facilitates local-global information\ninteractions in each decoder transformer block through attention heads at\nvarious scales. Additionally, a novel two-stage self-training strategy is\nproposed to align 2D and 3D representations. Our method outperforms\nstate-of-the-art counterparts across various downstream tasks, including 3D\nclassification, part segmentation, and object detection.",
          "arxiv_id": "2311.10887v2"
        }
      ],
      "12": [
        {
          "title": "PAL: Proxy-Guided Black-Box Attack on Large Language Models",
          "year": "2024-02",
          "abstract": "Large Language Models (LLMs) have surged in popularity in recent months, but\nthey have demonstrated concerning capabilities to generate harmful content when\nmanipulated. While techniques like safety fine-tuning aim to minimize harmful\nuse, recent works have shown that LLMs remain vulnerable to attacks that elicit\ntoxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs\n(PAL), the first optimization-based attack on LLMs in a black-box query-only\nsetting. In particular, it relies on a surrogate model to guide the\noptimization and a sophisticated loss designed for real-world LLM APIs. Our\nattack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on\nLlama-2-7B, compared to 4% for the current state of the art. We also propose\nGCG++, an improvement to the GCG attack that reaches 94% ASR on white-box\nLlama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple\nbaseline for query-based attacks. We believe the techniques proposed in this\nwork will enable more comprehensive safety testing of LLMs and, in the long\nterm, the development of better security guardrails. The code can be found at\nhttps://github.com/chawins/pal.",
          "arxiv_id": "2402.09674v1"
        },
        {
          "title": "QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language",
          "year": "2025-02",
          "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to $64\\%$ on GPT-4-1106. Our code is\navailable at https://github.com/horizonsinzqs/QueryAttack.",
          "arxiv_id": "2502.09723v3"
        },
        {
          "title": "Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?",
          "year": "2024-12",
          "abstract": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard.",
          "arxiv_id": "2412.03235v2"
        }
      ],
      "13": [
        {
          "title": "SeaPearl: A Constraint Programming Solver guided by Reinforcement Learning",
          "year": "2021-02",
          "abstract": "The design of efficient and generic algorithms for solving combinatorial\noptimization problems has been an active field of research for many years.\nStandard exact solving approaches are based on a clever and complete\nenumeration of the solution set. A critical and non-trivial design choice with\nsuch methods is the branching strategy, directing how the search is performed.\nThe last decade has shown an increasing interest in the design of machine\nlearning-based heuristics to solve combinatorial optimization problems. The\ngoal is to leverage knowledge from historical data to solve similar new\ninstances of a problem. Used alone, such heuristics are only able to provide\napproximate solutions efficiently, but cannot prove optimality nor bounds on\ntheir solution. Recent works have shown that reinforcement learning can be\nsuccessfully used for driving the search phase of constraint programming (CP)\nsolvers. However, it has also been shown that this hybridization is challenging\nto build, as standard CP frameworks do not natively include machine learning\nmechanisms, leading to some sources of inefficiencies. This paper presents the\nproof of concept for SeaPearl, a new CP solver implemented in Julia, that\nsupports machine learning routines in order to learn branching decisions using\nreinforcement learning. Support for modeling the learning component is also\nprovided. We illustrate the modeling and solution performance of this new\nsolver on two problems. Although not yet competitive with industrial solvers,\nSeaPearl aims to provide a flexible and open-source framework in order to\nfacilitate future research in the hybridization of constraint programming and\nmachine learning.",
          "arxiv_id": "2102.09193v2"
        },
        {
          "title": "Rolling Horizon based Temporal Decomposition for the Offline Pickup and Delivery Problem with Time Windows",
          "year": "2023-03",
          "abstract": "The offline pickup and delivery problem with time windows (PDPTW) is a\nclassical combinatorial optimization problem in the transportation community,\nwhich has proven to be very challenging computationally. Due to the complexity\nof the problem, practical problem instances can be solved only via heuristics,\nwhich trade-off solution quality for computational tractability. Among the\nvarious heuristics, a common strategy is problem decomposition, that is, the\nreduction of a large-scale problem into a collection of smaller sub-problems,\nwith spatial and temporal decompositions being two natural approaches. While\nspatial decomposition has been successful in certain settings, effective\ntemporal decomposition has been challenging due to the difficulty of stitching\ntogether the sub-problem solutions across the decomposition boundaries. In this\nwork, we introduce a novel temporal decomposition scheme for solving a class of\nPDPTWs that have narrow time windows, for which it is able to provide both fast\nand high-quality solutions. We utilize techniques that have been popularized\nrecently in the context of online dial-a-ride problems along with the general\nidea of rolling horizon optimization. To the best of our knowledge, this is the\nfirst attempt to solve offline PDPTWs using such an approach. To show the\nperformance and scalability of our framework, we use the optimization of\nparatransit services as a motivating example. We compare our results with an\noffline heuristic algorithm using Google OR-Tools. In smaller problem\ninstances, the baseline approach is as competitive as our framework. However,\nin larger problem instances, our framework is more scalable and can provide\ngood solutions to problem instances of varying degrees of difficulty, while the\nbaseline algorithm often fails to find a feasible solution within comparable\ncompute times.",
          "arxiv_id": "2303.03475v1"
        },
        {
          "title": "A Novel Meta-Heuristic Optimization Algorithm Inspired by the Spread of Viruses",
          "year": "2020-06",
          "abstract": "According to the no-free-lunch theorem, there is no single meta-heuristic\nalgorithm that can optimally solve all optimization problems. This motivates\nmany researchers to continuously develop new optimization algorithms. In this\npaper, a novel nature-inspired meta-heuristic optimization algorithm called\nvirus spread optimization (VSO) is proposed. VSO loosely mimics the spread of\nviruses among hosts, and can be effectively applied to solving many challenging\nand continuous optimization problems. We devise a new representation scheme and\nviral operations that are radically different from previously proposed\nvirus-based optimization algorithms. First, the viral RNA of each host in VSO\ndenotes a potential solution for which different viral operations will help to\ndiversify the searching strategies in order to largely enhance the solution\nquality. In addition, an imported infection mechanism, inheriting the searched\noptima from another colony, is introduced to possibly avoid the prematuration\nof any potential solution in solving complex problems. VSO has an excellent\ncapability to conduct adaptive neighborhood searches around the discovered\noptima for achieving better solutions. Furthermore, with a flexible infection\nmechanism, VSO can quickly escape from local optima. To clearly demonstrate\nboth its effectiveness and efficiency, VSO is critically evaluated on a series\nof well-known benchmark functions. Moreover, VSO is validated on its\napplicability through two real-world examples including the financial portfolio\noptimization and optimization of hyper-parameters of support vector machines\nfor classification problems. The results show that VSO has attained superior\nperformance in terms of solution fitness, convergence rate, scalability,\nreliability, and flexibility when compared to those results of the conventional\nas well as state-of-the-art meta-heuristic optimization algorithms.",
          "arxiv_id": "2006.06282v1"
        }
      ],
      "14": [
        {
          "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning",
          "year": "2025-05",
          "abstract": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG.",
          "arxiv_id": "2505.23794v1"
        },
        {
          "title": "On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems",
          "year": "2025-02",
          "abstract": "Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging.",
          "arxiv_id": "2502.14759v1"
        },
        {
          "title": "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs",
          "year": "2025-02",
          "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nfor domain-specific question-answering (QA) tasks by leveraging external\nknowledge sources. However, traditional RAG systems primarily focus on\nrelevance-based retrieval and often struggle with redundancy, especially when\nreasoning requires connecting information from multiple sources. This paper\nintroduces Vendi-RAG, a framework based on an iterative process that jointly\noptimizes retrieval diversity and answer quality. This joint optimization leads\nto significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\nthe Vendi Score (VS), a flexible similarity-based diversity metric, to promote\nsemantic diversity in document retrieval. It then uses an LLM judge that\nevaluates candidate answers, generated after a reasoning step, and outputs a\nscore that the retriever uses to balance relevance and diversity among the\nretrieved documents during each iteration. Experiments on three challenging\ndatasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\neffectiveness in multi-hop reasoning tasks. The framework achieves significant\naccuracy improvements over traditional single-step and multi-step RAG\napproaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\nbest baseline. The benefits of Vendi-RAG are even more pronounced as the number\nof retrieved documents increases. Finally, we evaluated Vendi-RAG across\ndifferent LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\nobserved consistent improvements, demonstrating that the framework's advantages\nare model-agnostic.",
          "arxiv_id": "2502.11228v2"
        }
      ],
      "15": [
        {
          "title": "CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models",
          "year": "2021-09",
          "abstract": "We propose CX-ToM, short for counterfactual explanations with theory-of mind,\na new explainable AI (XAI) framework for explaining decisions made by a deep\nconvolutional neural network (CNN). In contrast to the current methods in XAI\nthat generate explanations as a single shot response, we pose explanation as an\niterative communication process, i.e. dialog, between the machine and human\nuser. More concretely, our CX-ToM framework generates sequence of explanations\nin a dialog by mediating the differences between the minds of machine and human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. Moreover, most state-of-the-art XAI\nframeworks provide attention (or heat map) based explanations. In our work, we\nshow that these attention based explanations are not sufficient for increasing\nhuman trust in the underlying CNN model. In CX-ToM, we instead use\ncounterfactual explanations called fault-lines which we define as follows:\ngiven an input image I for which a CNN classification model M predicts class\nc_pred, a fault-line identifies the minimal semantic-level features (e.g.,\nstripes on zebra, pointed ears of dog), referred to as explainable concepts,\nthat need to be added to or deleted from I in order to alter the classification\ncategory of I by M to another specified class c_alt. We argue that, due to the\niterative, conceptual and counterfactual nature of CX-ToM explanations, our\nframework is practical and more natural for both expert and non-expert users to\nunderstand the internal workings of complex deep learning models. Extensive\nquantitative and qualitative experiments verify our hypotheses, demonstrating\nthat our CX-ToM significantly outperforms the state-of-the-art explainable AI\nmodels.",
          "arxiv_id": "2109.01401v3"
        },
        {
          "title": "Explainable AI without Interpretable Model",
          "year": "2020-09",
          "abstract": "Explainability has been a challenge in AI for as long as AI has existed. With\nthe recently increased use of AI in society, it has become more important than\never that AI systems would be able to explain the reasoning behind their\nresults also to end-users in situations such as being eliminated from a\nrecruitment process or having a bank loan application refused by an AI system.\nEspecially if the AI system has been trained using Machine Learning, it tends\nto contain too many parameters for them to be analysed and understood, which\nhas caused them to be called `black-box' systems. Most Explainable AI (XAI)\nmethods are based on extracting an interpretable model that can be used for\nproducing explanations. However, the interpretable model does not necessarily\nmap accurately to the original black-box model. Furthermore, the\nunderstandability of interpretable models for an end-user remains questionable.\nThe notions of Contextual Importance and Utility (CIU) presented in this paper\nmake it possible to produce human-like explanations of black-box outcomes\ndirectly, without creating an interpretable model. Therefore, CIU explanations\nmap accurately to the black-box model itself. CIU is completely model-agnostic\nand can be used with any black-box system. In addition to feature importance,\nthe utility concept that is well-known in Decision Theory provides a new\ndimension to explanations compared to most existing XAI methods. Finally, CIU\ncan produce explanations at any level of abstraction and using different\nvocabularies and other means of interaction, which makes it possible to adjust\nexplanations and interaction according to the context and to the target users.",
          "arxiv_id": "2009.13996v1"
        },
        {
          "title": "Disagreement amongst counterfactual explanations: How transparency can be deceptive",
          "year": "2023-04",
          "abstract": "Counterfactual explanations are increasingly used as an Explainable\nArtificial Intelligence (XAI) technique to provide stakeholders of complex\nmachine learning algorithms with explanations for data-driven decisions. The\npopularity of counterfactual explanations resulted in a boom in the algorithms\ngenerating them. However, not every algorithm creates uniform explanations for\nthe same instance. Even though in some contexts multiple possible explanations\nare beneficial, there are circumstances where diversity amongst counterfactual\nexplanations results in a potential disagreement problem among stakeholders.\nEthical issues arise when for example, malicious agents use this diversity to\nfairwash an unfair machine learning model by hiding sensitive features. As\nlegislators worldwide tend to start including the right to explanations for\ndata-driven, high-stakes decisions in their policies, these ethical issues\nshould be understood and addressed. Our literature review on the disagreement\nproblem in XAI reveals that this problem has never been empirically assessed\nfor counterfactual explanations. Therefore, in this work, we conduct a\nlarge-scale empirical analysis, on 40 datasets, using 12 explanation-generating\nmethods, for two black-box models, yielding over 192.0000 explanations. Our\nstudy finds alarmingly high disagreement levels between the methods tested. A\nmalicious user is able to both exclude and include desired features when\nmultiple counterfactual explanations are available. This disagreement seems to\nbe driven mainly by the dataset characteristics and the type of counterfactual\nalgorithm. XAI centers on the transparency of algorithmic decision-making, but\nour analysis advocates for transparency about this self-proclaimed transparency",
          "arxiv_id": "2304.12667v1"
        }
      ],
      "16": [
        {
          "title": "LLM-ABBA: Understanding time series via symbolic approximation",
          "year": "2024-11",
          "abstract": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
          "arxiv_id": "2411.18506v3"
        },
        {
          "title": "BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting",
          "year": "2025-08",
          "abstract": "Time series forecasting is a long-standing and highly challenging research\ntopic. Recently, driven by the rise of large language models (LLMs), research\nhas increasingly shifted from purely time series methods toward harnessing\ntextual modalities to enhance forecasting performance. However, the vast\ndiscrepancy between text and temporal data often leads current multimodal\narchitectures to over-emphasise one modality while neglecting the other,\nresulting in information loss that harms forecasting performance. To address\nthis modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment\nfor LLM-Based Time Series Forecasting), a lightweight time series forecasting\nframework that maintains balance between the two modalities. Specifically, raw\ntime series are processed by the time series encoder, while descriptive\nstatistics of raw time series are fed to an LLM with learnable prompt,\nproducing compact textual embeddings. To ensure balanced cross-modal context\nalignment of time series and textual embeddings, a simple yet effective scaling\nstrategy combined with a contrastive objective then maps these textual\nembeddings into the latent space of the time series embeddings. Finally, the\naligned textual semantic embeddings and time series embeddings are together\nintegrated for forecasting. Extensive experiments on standard benchmarks show\nthat, with minimal trainable parameters, BALM-TSF achieves state-of-the-art\nperformance in both long-term and few-shot forecasting, confirming its ability\nto harness complementary information from text and time series. Code is\navailable at https://github.com/ShiqiaoZhou/BALM-TSF.",
          "arxiv_id": "2509.00622v1"
        },
        {
          "title": "A Novel Hyperdimensional Computing Framework for Online Time Series Forecasting on the Edge",
          "year": "2024-02",
          "abstract": "In recent years, both online and offline deep learning models have been\ndeveloped for time series forecasting. However, offline deep forecasting models\nfail to adapt effectively to changes in time-series data, while online deep\nforecasting models are often expensive and have complex training procedures. In\nthis paper, we reframe the online nonlinear time-series forecasting problem as\none of linear hyperdimensional time-series forecasting. Nonlinear\nlow-dimensional time-series data is mapped to high-dimensional\n(hyperdimensional) spaces for linear hyperdimensional prediction, allowing\nfast, efficient and lightweight online time-series forecasting. Our framework,\nTSF-HD, adapts to time-series distribution shifts using a novel co-training\nframework for its hyperdimensional mapping and its linear hyperdimensional\npredictor. TSF-HD is shown to outperform the state of the art, while having\nreduced inference latency, for both short-term and long-term time series\nforecasting. Our code is publicly available at\nhttp://github.com/tsfhd2024/tsf-hd.git",
          "arxiv_id": "2402.01999v1"
        }
      ],
      "17": [
        {
          "title": "Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving",
          "year": "2025-03",
          "abstract": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
          "arxiv_id": "2503.00392v1"
        },
        {
          "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy",
          "year": "2024-10",
          "abstract": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
          "arxiv_id": "2410.03111v1"
        },
        {
          "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization",
          "year": "2023-05",
          "abstract": "Large language models (LLMs) face the challenges in fine-tuning and\ndeployment due to their high memory demands and computational costs. While\nparameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage\nof the optimizer state during fine-tuning, the inherent size of pre-trained LLM\nweights continues to be a pressing concern. Even though quantization techniques\nare widely proposed to ease memory demands and accelerate LLM inference, most\nof these techniques are geared towards the deployment phase. To bridge this\ngap, this paper presents Parameter-Efficient and Quantization-aware Adaptation\n(PEQA) - a simple yet effective method that combines the advantages of PEFT\nwith quantized LLMs. By updating solely the quantization scales, PEQA can be\ndirectly applied to quantized LLMs, ensuring seamless task transitions.\nParallel to existing PEFT methods, PEQA significantly reduces the memory\noverhead associated with the optimizer state. Furthermore, it leverages the\nadvantages of quantization to substantially reduce model sizes. Even after\nfine-tuning, the quantization structure of a PEQA-tuned LLM remains intact,\nallowing for accelerated inference on the deployment stage. We employ\nPEQA-tuning for task-specific adaptation on LLMs with up to 65 billion\nparameters. To assess the logical reasoning and language comprehension of\nPEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction\ndataset. Our results show that even when LLMs are quantized to below 4-bit\nprecision, their capabilities in language modeling, few-shot in-context\nlearning, and comprehension can be resiliently restored to (or even improved\nover) their full-precision original performances with PEQA.",
          "arxiv_id": "2305.14152v2"
        }
      ],
      "18": [
        {
          "title": "Zero-Knowledge Federated Learning: A New Trustworthy and Privacy-Preserving Distributed Learning Paradigm",
          "year": "2025-03",
          "abstract": "Federated Learning (FL) has emerged as a promising paradigm in distributed\nmachine learning, enabling collaborative model training while preserving data\nprivacy. However, despite its many advantages, FL still contends with\nsignificant challenges -- most notably regarding security and trust.\nZero-Knowledge Proofs (ZKPs) offer a potential solution by establishing trust\nand enhancing system integrity throughout the FL process. Although several\nstudies have explored ZKP-based FL (ZK-FL), a systematic framework and\ncomprehensive analysis are still lacking. This article makes two key\ncontributions. First, we propose a structured ZK-FL framework that categorizes\nand analyzes the technical roles of ZKPs across various FL stages and tasks.\nSecond, we introduce a novel algorithm, Verifiable Client Selection FL\n(Veri-CS-FL), which employs ZKPs to refine the client selection process. In\nVeri-CS-FL, participating clients generate verifiable proofs for the\nperformance metrics of their local models and submit these concise proofs to\nthe server for efficient verification. The server then selects clients with\nhigh-quality local models for uploading, subsequently aggregating the\ncontributions from these selected clients. By integrating ZKPs, Veri-CS-FL not\nonly ensures the accuracy of performance metrics but also fortifies trust among\nparticipants while enhancing the overall efficiency and security of FL systems.",
          "arxiv_id": "2503.15550v2"
        },
        {
          "title": "FLAGS Framework for Comparative Analysis of Federated Learning Algorithms",
          "year": "2022-12",
          "abstract": "Federated Learning (FL) has become a key choice for distributed machine\nlearning. Initially focused on centralized aggregation, recent works in FL have\nemphasized greater decentralization to adapt to the highly heterogeneous\nnetwork edge. Among these, Hierarchical, Device-to-Device and Gossip Federated\nLearning (HFL, D2DFL \\& GFL respectively) can be considered as foundational FL\nalgorithms employing fundamental aggregation strategies. A number of FL\nalgorithms were subsequently proposed employing multiple fundamental\naggregation schemes jointly. Existing research, however, subjects the FL\nalgorithms to varied conditions and gauges the performance of these algorithms\nmainly against Federated Averaging (FedAvg) only. This work consolidates the FL\nlandscape and offers an objective analysis of the major FL algorithms through a\ncomprehensive cross-evaluation for a wide range of operating conditions. In\naddition to the three foundational FL algorithms, this work also analyzes six\nderived algorithms. To enable a uniform assessment, a multi-FL framework named\nFLAGS: Federated Learning AlGorithms Simulation has been developed for rapid\nconfiguration of multiple FL algorithms. Our experiments indicate that fully\ndecentralized FL algorithms achieve comparable accuracy under multiple\noperating conditions, including asynchronous aggregation and the presence of\nstragglers. Furthermore, decentralized FL can also operate in noisy\nenvironments and with a comparably higher local update rate. However, the\nimpact of extremely skewed data distributions on decentralized FL is much more\nadverse than on centralized variants. The results indicate that it may not be\nnecessary to restrict the devices to a single FL algorithm; rather, multi-FL\nnodes may operate with greater efficiency.",
          "arxiv_id": "2212.07179v1"
        },
        {
          "title": "FedDiverse: Tackling Data Heterogeneity in Federated Learning with Diversity-Driven Client Selection",
          "year": "2025-04",
          "abstract": "Federated Learning (FL) enables decentralized training of machine learning\nmodels on distributed data while preserving privacy. However, in real-world FL\nsettings, client data is often non-identically distributed and imbalanced,\nresulting in statistical data heterogeneity which impacts the generalization\ncapabilities of the server's model across clients, slows convergence and\nreduces performance. In this paper, we address this challenge by proposing\nfirst a characterization of statistical data heterogeneity by means of 6\nmetrics of global and client attribute imbalance, class imbalance, and spurious\ncorrelations. Next, we create and share 7 computer vision datasets for binary\nand multiclass image classification tasks in Federated Learning that cover a\nbroad range of statistical data heterogeneity and hence simulate real-world\nsituations. Finally, we propose FEDDIVERSE, a novel client selection algorithm\nin FL which is designed to manage and leverage data heterogeneity across\nclients by promoting collaboration between clients with complementary data\ndistributions. Experiments on the seven proposed FL datasets demonstrate\nFEDDIVERSE's effectiveness in enhancing the performance and robustness of a\nvariety of FL methods while having low communication and computational\noverhead.",
          "arxiv_id": "2504.11216v2"
        }
      ],
      "19": [
        {
          "title": "EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms",
          "year": "2024-06",
          "abstract": "The rise of powerful large language models (LLMs) has spurred a new trend in\nbuilding LLM-based autonomous agents for solving complex tasks, especially\nmulti-agent systems. Despite the remarkable progress, we notice that existing\nworks are heavily dependent on human-designed frameworks, which greatly limits\nthe functional scope and scalability of agent systems. How to automatically\nextend the specialized agent to multi-agent systems to improve task-solving\ncapability still remains a significant challenge. In this paper, we introduce\nEvoAgent, a generic method to automatically extend specialized agents to\nmulti-agent systems via the evolutionary algorithm, thereby improving the\neffectiveness of LLM-based agents in solving tasks. Specifically, we consider\nthe existing agent frameworks as the initial individual and then apply a series\nof evolutionary operators (e.g., mutation, crossover, selection, etc.) to\ngenerate multiple agents with diverse settings. Experimental results across\nvarious tasks show that EvoAgent can significantly enhance the task-solving\ncapability of LLM-based agents, and can be generalized to any LLM-based agent\nframework to extend them into multi-agent systems. Resources are available at\nhttps://evo-agent.github.io/.",
          "arxiv_id": "2406.14228v3"
        },
        {
          "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
          "year": "2023-09",
          "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.",
          "arxiv_id": "2309.07864v3"
        },
        {
          "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL",
          "year": "2025-08",
          "abstract": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
          "arxiv_id": "2508.13167v1"
        }
      ],
      "20": [
        {
          "title": "A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems",
          "year": "2024-04",
          "abstract": "Recently, deep learning (DL) has been emerging as a promising approach for\nchannel estimation and signal detection in wireless communications. The\nmajority of the existing studies investigating the use of DL techniques in this\ndomain focus on analysing channel impulse responses that are generated from\nonly one channel distribution such as additive white Gaussian channel noise and\nRayleigh channels. In practice, to cope with the dynamic nature of the wireless\nchannel, DL methods must be re-trained on newly non-aged collected data which\nis costly, inefficient, and impractical. To tackle this challenge, this paper\nproposes a novel universal deep neural network (Uni-DNN) that can achieve high\ndetection performance in various wireless environments without retraining the\nmodel. In particular, our proposed Uni-DNN model consists of a wireless channel\nclassifier and a signal detector which are constructed by using DNNs. The\nwireless channel classifier enables the signal detector to generalise and\nperform optimally for multiple wireless channel distributions. In addition, to\nfurther improve the signal detection performance of the proposed model,\nconvolutional neural network is employed. Extensive simulations using the\northogonal frequency division multiplexing scheme demonstrate that the bit\nerror rate performance of our proposed solution can outperform conventional\nDL-based approaches as well as least square and minimum mean square error\nchannel estimators in practical low pilot density scenarios.",
          "arxiv_id": "2404.02648v1"
        },
        {
          "title": "Adaptive Resource Allocation for Semantic Communication Networks",
          "year": "2023-12",
          "abstract": "Semantic communication, recognized as a promising technology for future\nintelligent applications, has received widespread research attention. Despite\nthe potential of semantic communication to enhance transmission reliability,\nespecially in low signal-to-noise (SNR) environments, the critical issue of\nresource allocation and compatibility in the dynamic wireless environment\nremains largely unexplored. In this paper, we propose an adaptive semantic\nresource allocation paradigm with semantic-bit quantization (SBQ) compatibly\nfor existing wireless communications, where the inaccurate environment\nperception introduced by the additional mapping relationship between semantic\nmetrics and transmission metrics is solved. In order to investigate the\nperformance of semantic communication networks, the quality of service for\nsemantic communication (SC-QoS), including the semantic quantization efficiency\n(SQE) and transmission latency, is proposed for the first time. A problem of\nmaximizing the overall effective SC-QoS is formulated by jointly optimizing the\ntransmit beamforming of the base station, the bits for semantic representation,\nthe subchannel assignment, and the bandwidth resource allocation. To address\nthe non-convex formulated problem, an intelligent resource allocation scheme is\nproposed based on a hybrid deep reinforcement learning (DRL) algorithm, where\nthe intelligent agent can perceive both semantic tasks and dynamic wireless\nenvironments. Simulation results demonstrate that our design can effectively\ncombat semantic noise and achieve superior performance in wireless\ncommunications compared to several benchmark schemes. Furthermore, compared to\nmapping-guided paradigm based resource allocation schemes, our proposed\nadaptive scheme can achieve up to 13% performance improvement in terms of\nSC-QoS.",
          "arxiv_id": "2312.01081v1"
        },
        {
          "title": "Leveraging AI and Intelligent Reflecting Surface for Energy-Efficient Communication in 6G IoT",
          "year": "2020-12",
          "abstract": "The ever-increasing data traffic, various delay-sensitive services, and the\nmassive deployment of energy-limited Internet of Things (IoT) devices have\nbrought huge challenges to the current communication networks, motivating\nacademia and industry to move to the sixth-generation (6G) network. With the\npowerful capability of data transmission and processing, 6G is considered as an\nenabler for IoT communication with low latency and energy cost. In this paper,\nwe propose an artificial intelligence (AI) and intelligent reflecting surface\n(IRS) empowered energy-efficiency communication system for 6G IoT. First, we\ndesign a smart and efficient communication architecture including the IRS-aided\ndata transmission and the AI-driven network resource management mechanisms.\nSecond, an energy efficiency-maximizing model under given transmission latency\nfor 6G IoT system is formulated, which jointly optimizes the settings of all\ncommunication participants, i.e. IoT transmission power, IRS-reflection phase\nshift, and BS detection matrix. Third, a deep reinforcement learning (DRL)\nempowered network resource control and allocation scheme is proposed to solve\nthe formulated optimization model. Based on the network and channel status, the\nDRL-enabled scheme facilities the energy-efficiency and low-latency\ncommunication. Finally, experimental results verified the effectiveness of our\nproposed communication system for 6G IoT.",
          "arxiv_id": "2012.14716v1"
        }
      ],
      "21": [
        {
          "title": "Extending Transductive Knowledge Graph Embedding Models for Inductive Logical Relational Inference",
          "year": "2023-09",
          "abstract": "Many downstream inference tasks for knowledge graphs, such as relation\nprediction, have been handled successfully by knowledge graph embedding\ntechniques in the transductive setting. To address the inductive setting\nwherein new entities are introduced into the knowledge graph at inference time,\nmore recent work opts for models which learn implicit representations of the\nknowledge graph through a complex function of a network's subgraph structure,\noften parametrized by graph neural network architectures. These come at the\ncost of increased parametrization, reduced interpretability and limited\ngeneralization to other downstream inference tasks. In this work, we bridge the\ngap between traditional transductive knowledge graph embedding approaches and\nmore recent inductive relation prediction models by introducing a generalized\nform of harmonic extension which leverages representations learned through\ntransductive embedding methods to infer representations of new entities\nintroduced at inference time as in the inductive setting. This harmonic\nextension technique provides the best such approximation, can be implemented\nvia an efficient iterative scheme, and can be employed to answer a family of\nconjunctive logical queries over the knowledge graph, further expanding the\ncapabilities of transductive embedding methods. In experiments on a number of\nlarge-scale knowledge graph embedding benchmarks, we find that this approach\nfor extending the functionality of transductive knowledge graph embedding\nmodels to perform knowledge graph completion and answer logical queries in the\ninductive setting is competitive with--and in some scenarios\noutperforms--several state-of-the-art models derived explicitly for such\ninductive tasks.",
          "arxiv_id": "2309.03773v1"
        },
        {
          "title": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs",
          "year": "2023-05",
          "abstract": "Inductive knowledge graph completion has been considered as the task of\npredicting missing triplets between new entities that are not observed during\ntraining. While most inductive knowledge graph completion methods assume that\nall entities can be new, they do not allow new relations to appear at inference\ntime. This restriction prohibits the existing methods from appropriately\nhandling real-world knowledge graphs where new entities accompany new\nrelations. In this paper, we propose an INductive knowledge GRAph eMbedding\nmethod, InGram, that can generate embeddings of new relations as well as new\nentities at inference time. Given a knowledge graph, we define a relation graph\nas a weighted graph consisting of relations and the affinity weights between\nthem. Based on the relation graph and the original knowledge graph, InGram\nlearns how to aggregate neighboring embeddings to generate relation and entity\nembeddings using an attention mechanism. Experimental results show that InGram\noutperforms 14 different state-of-the-art methods on varied inductive learning\nscenarios.",
          "arxiv_id": "2305.19987v3"
        },
        {
          "title": "Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding",
          "year": "2021-10",
          "abstract": "Knowledge graphs (KGs) consisting of a large number of triples have become\nwidespread recently, and many knowledge graph embedding (KGE) methods are\nproposed to embed entities and relations of a KG into continuous vector spaces.\nSuch embedding methods simplify the operations of conducting various in-KG\ntasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering).\nThey can be viewed as general solutions for representing KGs. However, existing\nKGE methods are not applicable to inductive settings, where a model trained on\nsource KGs will be tested on target KGs with entities unseen during model\ntraining. Existing works focusing on KGs in inductive settings can only solve\nthe inductive relation prediction task. They can not handle other out-of-KG\ntasks as general as KGE methods since they don't produce embeddings for\nentities. In this paper, to achieve inductive knowledge graph embedding, we\npropose a model MorsE, which does not learn embeddings for entities but learns\ntransferable meta-knowledge that can be used to produce entity embeddings. Such\nmeta-knowledge is modeled by entity-independent modules and learned by\nmeta-learning. Experimental results show that our model significantly\noutperforms corresponding baselines for in-KG and out-of-KG tasks in inductive\nsettings.",
          "arxiv_id": "2110.14170v3"
        }
      ],
      "22": [
        {
          "title": "Temporal Answer Set Programming",
          "year": "2020-09",
          "abstract": "We present an overview on Temporal Logic Programming under the perspective of\nits application for Knowledge Representation and declarative problem solving.\nSuch programs are the result of combining usual rules with temporal modal\noperators, as in Linear-time Temporal Logic (LTL). We focus on recent results\nof the non-monotonic formalism called Temporal Equilibrium Logic (TEL) that is\ndefined for the full syntax of LTL, but performs a model selection criterion\nbased on Equilibrium Logic, a well known logical characterization of Answer Set\nProgramming (ASP). We obtain a proper extension of the stable models semantics\nfor the general case of arbitrary temporal formulas. We recall the basic\ndefinitions for TEL and its monotonic basis, the temporal logic of\nHere-and-There (THT), and study the differences between infinite and finite\ntraces. We also provide other useful results, such as the translation into\nother formalisms like Quantified Equilibrium Logic or Second-order LTL, and\nsome techniques for computing temporal stable models based on automata. In a\nsecond part, we focus on practical aspects, defining a syntactic fragment\ncalled temporal logic programs closer to ASP, and explain how this has been\nexploited in the construction of the solver TELINGO.",
          "arxiv_id": "2009.06544v3"
        },
        {
          "title": "Pearce's Characterisation in an Epistemic Domain",
          "year": "2025-02",
          "abstract": "Answer-set programming (ASP) is a successful problem-solving approach in\nlogic-based AI. In ASP, problems are represented as declarative logic programs,\nand solutions are identified through their answer sets. Equilibrium logic (EL)\nis a general-purpose nonmonotonic reasoning formalism, based on a monotonic\nlogic called here-and-there logic. EL was basically proposed by Pearce as a\nfoundational framework of ASP. Epistemic specifications (ES) are extensions of\nASP-programs with subjective literals. These new modal constructs in the\nASP-language make it possible to check whether a regular literal of ASP is true\nin every (or some) answer-set of a program. ES-programs are interpreted by\nworld-views, which are essentially collections of answer-sets. (Reflexive)\nautoepistemic logic is a nonmonotonic formalism, modeling self-belief\n(knowledge) of ideally rational agents. A relatively new semantics for ES is\nbased on a combination of EL and (reflexive) autoepistemic logic. In this\npaper, we first propose an overarching framework in the epistemic ASP domain.\nWe then establish a correspondence between existing (reflexive) (auto)epistemic\nequilibrium logics and our easily-adaptable comprehensive framework, building\non Pearce's characterisation of answer-sets as equilibrium models. We achieve\nthis by extending Ferraris' work on answer sets for propositional theories to\nthe epistemic case and reveal the relationship between some ES-semantic\nproposals.",
          "arxiv_id": "2502.09221v1"
        },
        {
          "title": "diff-SAT -- A Software for Sampling and Probabilistic Reasoning for SAT and Answer Set Programming",
          "year": "2021-01",
          "abstract": "This paper describes diff-SAT, an Answer Set and SAT solver which combines\nregular solving with the capability to use probabilistic clauses, facts and\nrules, and to sample an optimal world-view (multiset of satisfying Boolean\nvariable assignments or answer sets) subject to user-provided probabilistic\nconstraints. The sampling process minimizes a user-defined differentiable\nobjective function using a gradient descent based optimization method called\nDifferentiable Satisfiability Solving ($\\partial\\mathrm{SAT}$) respectively\nDifferentiable Answer Set Programming ($\\partial\\mathrm{ASP}$). Use cases are\ni.a. probabilistic logic programming (in form of Probabilistic Answer Set\nProgramming), Probabilistic Boolean Satisfiability solving (PSAT), and\ndistribution-aware sampling of model multisets (answer sets or Boolean\ninterpretations).",
          "arxiv_id": "2101.00589v1"
        }
      ],
      "23": [
        {
          "title": "Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education",
          "year": "2025-06",
          "abstract": "As generative AI (Gen-AI) tools become more prevalent in education, there is\na growing need to understand how educators, not just students, can actively\nshape their design and use. This study investigates how two instructors\nintegrated four custom GPT tools into a Masters-level Qualitative Research\nMethods course for Urban Planning Policy students. Addressing two key gaps: the\ndominant framing of students as passive AI users, and the limited use of AI in\nqualitative methods education. The study explores how Gen-AI can support\ndisciplinary learning when aligned with pedagogical intent. Drawing on the\nTechnological Pedagogical Content Knowledge (TPACK) framework and action\nresearch methodology, the instructors designed GPTs to scaffold tasks such as\nresearch question formulation, interview practice, fieldnote analysis, and\ndesign thinking. Thematic analysis of student reflections, AI chat logs, and\nfinal assignments revealed that the tools enhanced student reflexivity,\nimproved interview techniques, and supported structured analytic thinking.\nHowever, students also expressed concerns about cognitive overload, reduced\nimmersion in data, and the formulaic nature of AI responses. The study offers\nthree key insights: AI can be a powerful scaffold for active learning when\npaired with human facilitation; custom GPTs can serve as cognitive partners in\niterative research practice; and educator-led design is critical to\npedagogically meaningful AI integration. This research contributes to emerging\nscholarship on AI in higher education by demonstrating how empowering educators\nto design custom tools can promote more reflective, responsible, and\ncollaborative learning with AI.",
          "arxiv_id": "2507.21074v1"
        },
        {
          "title": "Integrating AI Tutors in a Programming Course",
          "year": "2024-07",
          "abstract": "RAGMan is an LLM-powered tutoring system that can support a variety of\ncourse-specific and homework-specific AI tutors. RAGMan leverages Retrieval\nAugmented Generation (RAG), as well as strict instructions, to ensure the\nalignment of the AI tutors' responses. By using RAGMan's AI tutors, students\nreceive assistance with their specific homework assignments without directly\nobtaining solutions, while also having the ability to ask general\nprogramming-related questions.\n  RAGMan was deployed as an optional resource in an introductory programming\ncourse with an enrollment of 455 students. It was configured as a set of five\nhomework-specific AI tutors. This paper describes the interactions the students\nhad with the AI tutors, the students' feedback, and a comparative grade\nanalysis. Overall, about half of the students engaged with the AI tutors, and\nthe vast majority of the interactions were legitimate homework questions. When\nstudents posed questions within the intended scope, the AI tutors delivered\naccurate responses 98% of the time. Within the students used AI tutors, 78%\nreported that the tutors helped their learning. Beyond AI tutors' ability to\nprovide valuable suggestions, students reported appreciating them for fostering\na safe learning environment free from judgment.",
          "arxiv_id": "2407.15718v1"
        },
        {
          "title": "Evaluating AI-Powered Learning Assistants in Engineering Higher Education: Student Engagement, Ethical Challenges, and Policy Implications",
          "year": "2025-06",
          "abstract": "As generative AI tools become increasingly integrated into higher education,\nunderstanding how students interact with and perceive these technologies is\nessential for responsible and effective adoption. This study evaluates the use\nof the Educational AI Hub, an AI-powered learning framework, in undergraduate\ncivil and environmental engineering courses at a large R1 public university.\nUsing a mixed-methods approach that combines pre- and post-surveys, system\nusage logs, and qualitative analysis of the open-ended prompts and questions\nstudents posed to the AI chatbot, the research explores students' perceptions\nof trust, ethical concerns, usability, and learning outcomes. Findings reveal\nthat students appreciated the AI assistant for its convenience and comfort,\nwith nearly half reporting greater ease in using the AI tool compared to\nseeking help from instructors or teaching assistants. The tool was seen as most\nhelpful for completing homework and understanding course concepts, though\nperceptions of its instructional quality were mixed. Ethical concerns emerged\nas a key barrier to full engagement: while most students viewed AI use as\nethically acceptable, many expressed uncertainties about institutional policies\nand apprehension about potential academic misconduct. This study contributes to\nthe growing body of research on AI in education by highlighting the importance\nof usability, policy clarity, and faculty guidance in fostering meaningful AI\nengagement. The findings suggest that while students are ready to embrace AI as\na supplement to human instruction, thoughtful integration and transparent\ninstitutional frameworks are critical for ensuring student confidence, trust,\nand learning effectiveness.",
          "arxiv_id": "2506.05699v1"
        }
      ],
      "24": [
        {
          "title": "It's All in the Embedding! Fake News Detection Using Document Embeddings",
          "year": "2023-04",
          "abstract": "With the current shift in the mass media landscape from journalistic rigor to\nsocial media, personalized social media is becoming the new norm. Although the\ndigitalization progress of the media brings many advantages, it also increases\nthe risk of spreading disinformation, misinformation, and malformation through\nthe use of fake news. The emergence of this harmful phenomenon has managed to\npolarize society and manipulate public opinion on particular topics, e.g.,\nelections, vaccinations, etc. Such information propagated on social media can\ndistort public perceptions and generate social unrest while lacking the rigor\nof traditional journalism. Natural Language Processing and Machine Learning\ntechniques are essential for developing efficient tools that can detect fake\nnews. Models that use the context of textual data are essential for resolving\nthe fake news detection problem, as they manage to encode linguistic features\nwithin the vector representation of words. In this paper, we propose a new\napproach that uses document embeddings to build multiple models that accurately\nlabel news articles as reliable or fake. We also present a benchmark on\ndifferent architectures that detect fake news using binary or multi-labeled\nclassification. We evaluated the models on five large news corpora using\naccuracy, precision, and recall. We obtained better results than more complex\nstate-of-the-art Deep Neural Network models. We observe that the most important\nfactor for obtaining high accuracy is the document encoding, not the\nclassification model's complexity.",
          "arxiv_id": "2304.07781v1"
        },
        {
          "title": "CrediRAG: Network-Augmented Credibility-Based Retrieval for Misinformation Detection in Reddit",
          "year": "2024-10",
          "abstract": "Fake news threatens democracy and exacerbates the polarization and divisions\nin society; therefore, accurately detecting online misinformation is the\nfoundation of addressing this issue. We present CrediRAG, the first fake news\ndetection model that combines language models with access to a rich external\npolitical knowledge base with a dense social network to detect fake news across\nsocial media at scale. CrediRAG uses a news retriever to initially assign a\nmisinformation score to each post based on the source credibility of similar\nnews articles to the post title content. CrediRAG then improves the initial\nretrieval estimations through a novel weighted post-to-post network connected\nbased on shared commenters and weighted by the average stance of all shared\ncommenters across every pair of posts. We achieve 11% increase in the F1-score\nin detecting misinformative posts over state-of-the-art methods. Extensive\nexperiments conducted on curated real-world Reddit data of over 200,000 posts\ndemonstrate the superior performance of CrediRAG on existing baselines. Thus,\nour approach offers a more accurate and scalable solution to combat the spread\nof fake news across social media platforms.",
          "arxiv_id": "2410.12061v2"
        },
        {
          "title": "Combining Machine Learning with Knowledge Engineering to detect Fake News in Social Networks-a survey",
          "year": "2022-01",
          "abstract": "Due to extensive spread of fake news on social and news media it became an\nemerging research topic now a days that gained attention. In the news media and\nsocial media the information is spread highspeed but without accuracy and hence\ndetection mechanism should be able to predict news fast enough to tackle the\ndissemination of fake news. It has the potential for negative impacts on\nindividuals and society. Therefore, detecting fake news on social media is\nimportant and also a technically challenging problem these days. We knew that\nMachine learning is helpful for building Artificial intelligence systems based\non tacit knowledge because it can help us to solve complex problems due to real\nword data. On the other side we knew that Knowledge engineering is helpful for\nrepresenting experts knowledge which people aware of that knowledge. Due to\nthis we proposed that integration of Machine learning and knowledge engineering\ncan be helpful in detection of fake news. In this paper we present what is fake\nnews, importance of fake news, overall impact of fake news on different areas,\ndifferent ways to detect fake news on social media, existing detections\nalgorithms that can help us to overcome the issue, similar application areas\nand at the end we proposed combination of data driven and engineered knowledge\nto combat fake news. We studied and compared three different modules text\nclassifiers, stance detection applications and fact checking existing\ntechniques that can help to detect fake news. Furthermore, we investigated the\nimpact of fake news on society. Experimental evaluation of publically available\ndatasets and our proposed fake news detection combination can serve better in\ndetection of fake news.",
          "arxiv_id": "2201.08032v1"
        }
      ],
      "25": [
        {
          "title": "On-Chip Hardware-Aware Quantization for Mixed Precision Neural Networks",
          "year": "2023-09",
          "abstract": "Low-bit quantization emerges as one of the most promising compression\napproaches for deploying deep neural networks on edge devices. Mixed-precision\nquantization leverages a mixture of bit-widths to unleash the accuracy and\nefficiency potential of quantized models. However, existing mixed-precision\nquantization methods rely on simulations in high-performance devices to achieve\naccuracy and efficiency trade-offs in immense search spaces. This leads to a\nnon-negligible gap between the estimated efficiency metrics and the actual\nhardware that makes quantized models far away from the optimal accuracy and\nefficiency, and also causes the quantization process to rely on additional\nhigh-performance devices. In this paper, we propose an On-Chip Hardware-Aware\nQuantization (OHQ) framework, performing hardware-aware mixed-precision\nquantization on deployed edge devices to achieve accurate and efficient\ncomputing. Specifically, for efficiency metrics, we built an On-Chip\nQuantization Aware pipeline, which allows the quantization process to perceive\nthe actual hardware efficiency of the quantization operator and avoid\noptimization errors caused by inaccurate simulation. For accuracy metrics, we\npropose Mask-Guided Quantization Estimation technology to effectively estimate\nthe accuracy impact of operators in the on-chip scenario, getting rid of the\ndependence of the quantization process on high computing power. By synthesizing\ninsights from quantized models and hardware through linear optimization, we can\nobtain optimized bit-width configurations to achieve outstanding performance on\naccuracy and efficiency. We evaluate inference accuracy and acceleration with\nquantization for various architectures and compression ratios on hardware. OHQ\nachieves 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively, and\ncan reduce latency by 15~30% compared to INT8 on real deployment.",
          "arxiv_id": "2309.01945v5"
        },
        {
          "title": "Hybrid In-memory Computing Architecture for the Training of Deep Neural Networks",
          "year": "2021-02",
          "abstract": "The cost involved in training deep neural networks (DNNs) on von-Neumann\narchitectures has motivated the development of novel solutions for efficient\nDNN training accelerators. We propose a hybrid in-memory computing (HIC)\narchitecture for the training of DNNs on hardware accelerators that results in\nmemory-efficient inference and outperforms baseline software accuracy in\nbenchmark tasks. We introduce a weight representation technique that exploits\nboth binary and multi-level phase-change memory (PCM) devices, and this leads\nto a memory-efficient inference accelerator. Unlike previous in-memory\ncomputing-based implementations, we use a low precision weight update\naccumulator that results in more memory savings. We trained the ResNet-32\nnetwork to classify CIFAR-10 images using HIC. For a comparable model size,\nHIC-based training outperforms baseline network, trained in floating-point\n32-bit (FP32) precision, by leveraging appropriate network width multiplier.\nFurthermore, we observe that HIC-based training results in about 50% less\ninference model size to achieve baseline comparable accuracy. We also show that\nthe temporal drift in PCM devices has a negligible effect on post-training\ninference accuracy for extended periods (year). Finally, our simulations\nindicate HIC-based training naturally ensures that the number of write-erase\ncycles seen by the devices is a small fraction of the endurance limit of PCM,\ndemonstrating the feasibility of this architecture for achieving hardware\nplatforms that can learn in the field.",
          "arxiv_id": "2102.05271v1"
        },
        {
          "title": "FxP-QNet: A Post-Training Quantizer for the Design of Mixed Low-Precision DNNs with Dynamic Fixed-Point Representation",
          "year": "2022-03",
          "abstract": "Deep neural networks (DNNs) have demonstrated their effectiveness in a wide\nrange of computer vision tasks, with the state-of-the-art results obtained\nthrough complex and deep structures that require intensive computation and\nmemory. Now-a-days, efficient model inference is crucial for consumer\napplications on resource-constrained platforms. As a result, there is much\ninterest in the research and development of dedicated deep learning (DL)\nhardware to improve the throughput and energy efficiency of DNNs. Low-precision\nrepresentation of DNN data-structures through quantization would bring great\nbenefits to specialized DL hardware. However, the rigorous quantization leads\nto a severe accuracy drop. As such, quantization opens a large hyper-parameter\nspace at bit-precision levels, the exploration of which is a major challenge.\nIn this paper, we propose a novel framework referred to as the Fixed-Point\nQuantizer of deep neural Networks (FxP-QNet) that flexibly designs a mixed\nlow-precision DNN for integer-arithmetic-only deployment. Specifically, the\nFxP-QNet gradually adapts the quantization level for each data-structure of\neach layer based on the trade-off between the network accuracy and the\nlow-precision requirements. Additionally, it employs post-training\nself-distillation and network prediction error statistics to optimize the\nquantization of floating-point values into fixed-point numbers. Examining\nFxP-QNet on state-of-the-art architectures and the benchmark ImageNet dataset,\nwe empirically demonstrate the effectiveness of FxP-QNet in achieving the\naccuracy-compression trade-off without the need for training. The results show\nthat FxP-QNet-quantized AlexNet, VGG-16, and ResNet-18 reduce the overall\nmemory requirements of their full-precision counterparts by 7.16x, 10.36x, and\n6.44x with less than 0.95%, 0.95%, and 1.99% accuracy drop, respectively.",
          "arxiv_id": "2203.12091v1"
        }
      ],
      "26": [
        {
          "title": "HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation",
          "year": "2022-07",
          "abstract": "Multilingual neural machine translation (MNMT) trained in multiple language\npairs has attracted considerable attention due to fewer model parameters and\nlower training costs by sharing knowledge among multiple languages.\nNonetheless, multilingual training is plagued by language interference\ndegeneration in shared parameters because of the negative interference among\ndifferent translation directions, especially on high-resource languages. In\nthis paper, we propose the multilingual translation model with the\nhigh-resource language-specific training (HLT-MT) to alleviate the negative\ninterference, which adopts the two-stage training with the language-specific\nselection mechanism. Specifically, we first train the multilingual model only\nwith the high-resource pairs and select the language-specific modules at the\ntop of the decoder to enhance the translation quality of high-resource\ndirections. Next, the model is further trained on all available corpora to\ntransfer knowledge from high-resource languages (HRLs) to low-resource\nlanguages (LRLs). Experimental results show that HLT-MT outperforms various\nstrong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic\nexperiments validate the effectiveness of our method in mitigating the negative\ninterference in multilingual training.",
          "arxiv_id": "2207.04906v2"
        },
        {
          "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models",
          "year": "2021-04",
          "abstract": "While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.",
          "arxiv_id": "2104.00290v2"
        },
        {
          "title": "Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation",
          "year": "2021-05",
          "abstract": "The data scarcity in low-resource languages has become a bottleneck to\nbuilding robust neural machine translation systems. Fine-tuning a multilingual\npre-trained model (e.g., mBART (Liu et al., 2020)) on the translation task is a\ngood approach for low-resource languages; however, its performance will be\ngreatly limited when there are unseen languages in the translation pairs. In\nthis paper, we present a continual pre-training (CPT) framework on mBART to\neffectively adapt it to unseen languages. We first construct noisy\nmixed-language text from the monolingual corpus of the target language in the\ntranslation pair to cover both the source and target languages, and then, we\ncontinue pre-training mBART to reconstruct the original monolingual text.\nResults show that our method can consistently improve the fine-tuning\nperformance upon the mBART baseline, as well as other strong baselines, across\nall tested low-resource translation pairs containing unseen languages.\nFurthermore, our approach also boosts the performance on translation pairs\nwhere both languages are seen in the original mBART's pre-training. The code is\navailable at https://github.com/zliucr/cpt-nmt.",
          "arxiv_id": "2105.03953v1"
        }
      ],
      "27": [
        {
          "title": "Cross-domain Contrastive Learning for Unsupervised Domain Adaptation",
          "year": "2021-06",
          "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na fully-labeled source domain to a different unlabeled target domain. Most\nexisting UDA methods learn domain-invariant feature representations by\nminimizing feature distances across domains. In this work, we build upon\ncontrastive self-supervised learning to align features so as to reduce the\ndomain discrepancy between training and testing sets. Exploring the same set of\ncategories shared by both domains, we introduce a simple yet effective\nframework CDCL, for domain alignment. In particular, given an anchor image from\none domain, we minimize its distances to cross-domain samples from the same\nclass relative to those from different categories. Since target labels are\nunavailable, we use a clustering-based approach with carefully initialized\ncenters to produce pseudo labels. In addition, we demonstrate that CDCL is a\ngeneral framework and can be adapted to the data-free setting, where the source\ndata are unavailable during training, with minimal modification. We conduct\nexperiments on two widely used domain adaptation benchmarks, i.e., Office-31\nand VisDA-2017, for image classification tasks, and demonstrate that CDCL\nachieves state-of-the-art performance on both datasets.",
          "arxiv_id": "2106.05528v2"
        },
        {
          "title": "Transferrable Contrastive Learning for Visual Domain Adaptation",
          "year": "2021-12",
          "abstract": "Self-supervised learning (SSL) has recently become the favorite among feature\nlearning methodologies. It is therefore appealing for domain adaptation\napproaches to consider incorporating SSL. The intuition is to enforce\ninstance-level feature consistency such that the predictor becomes somehow\ninvariant across domains. However, most existing SSL methods in the regime of\ndomain adaptation usually are treated as standalone auxiliary components,\nleaving the signatures of domain adaptation unattended. Actually, the optimal\nregion where the domain gap vanishes and the instance level constraint that SSL\nperuses may not coincide at all. From this point, we present a particular\nparadigm of self-supervised learning tailored for domain adaptation, i.e.,\nTransferrable Contrastive Learning (TCL), which links the SSL and the desired\ncross-domain transferability congruently. We find contrastive learning\nintrinsically a suitable candidate for domain adaptation, as its instance\ninvariance assumption can be conveniently promoted to cross-domain class-level\ninvariance favored by domain adaptation tasks. Based on particular memory bank\nconstructions and pseudo label strategies, TCL then penalizes cross-domain\nintra-class domain discrepancy between source and target through a clean and\nnovel contrastive loss. The free lunch is, thanks to the incorporation of\ncontrastive learning, TCL relies on a moving-averaged key encoder that\nnaturally achieves a temporally ensembled version of pseudo labels for target\ndata, which avoids pseudo label error propagation at no extra cost. TCL\ntherefore efficiently reduces cross-domain gaps. Through extensive experiments\non benchmarks (Office-Home, VisDA-2017, Digits-five, PACS and DomainNet) for\nboth single-source and multi-source domain adaptation tasks, TCL has\ndemonstrated state-of-the-art performances.",
          "arxiv_id": "2112.07516v1"
        },
        {
          "title": "Discovering Domain Disentanglement for Generalized Multi-source Domain Adaptation",
          "year": "2022-07",
          "abstract": "A typical multi-source domain adaptation (MSDA) approach aims to transfer\nknowledge learned from a set of labeled source domains, to an unlabeled target\ndomain. Nevertheless, prior works strictly assume that each source domain\nshares the identical group of classes with the target domain, which could\nhardly be guaranteed as the target label space is not observable. In this\npaper, we consider a more versatile setting of MSDA, namely Generalized\nMulti-source Domain Adaptation, wherein the source domains are partially\noverlapped, and the target domain is allowed to contain novel categories that\nare not presented in any source domains. This new setting is more elusive than\nany existing domain adaptation protocols due to the coexistence of the domain\nand category shifts across the source and target domains. To address this\nissue, we propose a variational domain disentanglement (VDD) framework, which\ndecomposes the domain representations and semantic features for each instance\nby encouraging dimension-wise independence. To identify the target samples of\nunknown classes, we leverage online pseudo labeling, which assigns the\npseudo-labels to unlabeled target data based on the confidence scores.\nQuantitative and qualitative experiments conducted on two benchmark datasets\ndemonstrate the validity of the proposed framework.",
          "arxiv_id": "2207.05070v1"
        }
      ],
      "28": [
        {
          "title": "Detect Faces Efficiently: A Survey and Evaluations",
          "year": "2021-12",
          "abstract": "Face detection is to search all the possible regions for faces in images and\nlocate the faces if there are any. Many applications including face\nrecognition, facial expression recognition, face tracking and head-pose\nestimation assume that both the location and the size of faces are known in the\nimage. In recent decades, researchers have created many typical and efficient\nface detectors from the Viola-Jones face detector to current CNN-based ones.\nHowever, with the tremendous increase in images and videos with variations in\nface scale, appearance, expression, occlusion and pose, traditional face\ndetectors are challenged to detect various \"in the wild\" faces. The emergence\nof deep learning techniques brought remarkable breakthroughs to face detection\nalong with the price of a considerable increase in computation. This paper\nintroduces representative deep learning-based methods and presents a deep and\nthorough analysis in terms of accuracy and efficiency. We further compare and\ndiscuss the popular and challenging datasets and their evaluation metrics. A\ncomprehensive comparison of several successful deep learning-based face\ndetectors is conducted to uncover their efficiency using two metrics: FLOPs and\nlatency. The paper can guide to choose appropriate face detectors for different\napplications and also to develop more efficient and accurate detectors.",
          "arxiv_id": "2112.01787v1"
        },
        {
          "title": "FlowFace: Semantic Flow-guided Shape-aware Face Swapping",
          "year": "2022-12",
          "abstract": "In this work, we propose a semantic flow-guided two-stage framework for\nshape-aware face swapping, namely FlowFace. Unlike most previous methods that\nfocus on transferring the source inner facial features but neglect facial\ncontours, our FlowFace can transfer both of them to a target face, thus leading\nto more realistic face swapping. Concretely, our FlowFace consists of a face\nreshaping network and a face swapping network. The face reshaping network\naddresses the shape outline differences between the source and target faces. It\nfirst estimates a semantic flow (i.e., face shape differences) between the\nsource and the target face, and then explicitly warps the target face shape\nwith the estimated semantic flow. After reshaping, the face swapping network\ngenerates inner facial features that exhibit the identity of the source face.\nWe employ a pre-trained face masked autoencoder (MAE) to extract facial\nfeatures from both the source face and the target face. In contrast to previous\nmethods that use identity embedding to preserve identity information, the\nfeatures extracted by our encoder can better capture facial appearances and\nidentity information. Then, we develop a cross-attention fusion module to\nadaptively fuse inner facial features from the source face with the target\nfacial attributes, thus leading to better identity preservation. Extensive\nquantitative and qualitative experiments on in-the-wild faces demonstrate that\nour FlowFace outperforms the state-of-the-art significantly.",
          "arxiv_id": "2212.02797v1"
        },
        {
          "title": "LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition",
          "year": "2024-03",
          "abstract": "In this work we focus on learning facial representations that can be adapted\nto train effective face recognition models, particularly in the absence of\nlabels. Firstly, compared with existing labelled face datasets, a vastly larger\nmagnitude of unlabeled faces exists in the real world. We explore the learning\nstrategy of these unlabeled facial images through self-supervised pretraining\nto transfer generalized face recognition performance. Moreover, motivated by\none recent finding, that is, the face saliency area is critical for face\nrecognition, in contrast to utilizing random cropped blocks of images for\nconstructing augmentations in pretraining, we utilize patches localized by\nextracted facial landmarks. This enables our method - namely LAndmark-based\nFacial Self-supervised learning LAFS), to learn key representation that is more\ncritical for face recognition. We also incorporate two landmark-specific\naugmentations which introduce more diversity of landmark information to further\nregularize the learning. With learned landmark-based facial representations, we\nfurther adapt the representation for face recognition with regularization\nmitigating variations in landmark positions. Our method achieves significant\nimprovement over the state-of-the-art on multiple face recognition benchmarks,\nespecially on more challenging few-shot scenarios.",
          "arxiv_id": "2403.08161v1"
        }
      ],
      "29": [
        {
          "title": "Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models",
          "year": "2023-04",
          "abstract": "Remote sensing imagery has attracted significant attention in recent years\ndue to its instrumental role in global environmental monitoring, land usage\nmonitoring, and more. As image databases grow each year, performing automatic\nsegmentation with deep learning models has gradually become the standard\napproach for processing the data. Despite the improved performance of current\nmodels, certain limitations remain unresolved. Firstly, training deep learning\nmodels for segmentation requires per-pixel annotations. Given the large size of\ndatasets, only a small portion is fully annotated and ready for training.\nAdditionally, the high intra-dataset variance in remote sensing data limits the\ntransfer learning ability of such models. Although recently proposed generic\nsegmentation models like SAM have shown promising results in zero-shot\ninstance-level segmentation, adapting them to semantic segmentation is a\nnon-trivial task. To tackle these challenges, we propose a novel method named\nText2Seg for remote sensing semantic segmentation. Text2Seg overcomes the\ndependency on extensive annotations by employing an automatic prompt generation\nprocess using different visual foundation models (VFMs), which are trained to\nunderstand semantic information in various ways. This approach not only reduces\nthe need for fully annotated datasets but also enhances the model's ability to\ngeneralize across diverse datasets. Evaluations on four widely adopted remote\nsensing datasets demonstrate that Text2Seg significantly improves zero-shot\nprediction performance compared to the vanilla SAM model, with relative\nimprovements ranging from 31% to 225%. Our code is available at\nhttps://github.com/Douglas2Code/Text2Seg.",
          "arxiv_id": "2304.10597v2"
        },
        {
          "title": "DepthSeg: Depth prompting in remote sensing semantic segmentation",
          "year": "2025-06",
          "abstract": "Remote sensing semantic segmentation is crucial for extracting detailed land\nsurface information, enabling applications such as environmental monitoring,\nland use planning, and resource assessment. In recent years, advancements in\nartificial intelligence have spurred the development of automatic remote\nsensing semantic segmentation methods. However, the existing semantic\nsegmentation methods focus on distinguishing spectral characteristics of\ndifferent objects while ignoring the differences in the elevation of the\ndifferent targets. This results in land cover misclassification in complex\nscenarios involving shadow occlusion and spectral confusion. In this paper, we\nintroduce a depth prompting two-dimensional (2D) remote sensing semantic\nsegmentation framework (DepthSeg). It automatically models depth/height\ninformation from 2D remote sensing images and integrates it into the semantic\nsegmentation framework to mitigate the effects of spectral confusion and shadow\nocclusion. During the feature extraction phase of DepthSeg, we introduce a\nlightweight adapter to enable cost-effective fine-tuning of the large-parameter\nvision transformer encoder pre-trained by natural images. In the depth\nprompting phase, we propose a depth prompter to model depth/height features\nexplicitly. In the semantic prediction phase, we introduce a semantic\nclassification decoder that couples the depth prompts with high-dimensional\nland-cover features, enabling accurate extraction of land-cover types.\nExperiments on the LiuZhou dataset validate the advantages of the DepthSeg\nframework in land cover mapping tasks. Detailed ablation studies further\nhighlight the significance of the depth prompts in remote sensing semantic\nsegmentation.",
          "arxiv_id": "2506.14382v1"
        },
        {
          "title": "Remote Sensing Image Super-resolution and Object Detection: Benchmark and State of the Art",
          "year": "2021-11",
          "abstract": "For the past two decades, there have been significant efforts to develop\nmethods for object detection in Remote Sensing (RS) images. In most cases, the\ndatasets for small object detection in remote sensing images are inadequate.\nMany researchers used scene classification datasets for object detection, which\nhas its limitations; for example, the large-sized objects outnumber the small\nobjects in object categories. Thus, they lack diversity; this further affects\nthe detection performance of small object detectors in RS images. This paper\nreviews current datasets and object detection methods (deep learning-based) for\nremote sensing images. We also propose a large-scale, publicly available\nbenchmark Remote Sensing Super-resolution Object Detection (RSSOD) dataset. The\nRSSOD dataset consists of 1,759 hand-annotated images with 22,091 instances of\nvery high resolution (VHR) images with a spatial resolution of ~0.05 m. There\nare five classes with varying frequencies of labels per class. The image\npatches are extracted from satellite images, including real image distortions\nsuch as tangential scale distortion and skew distortion. We also propose a\nnovel Multi-class Cyclic super-resolution Generative adversarial network with\nResidual feature aggregation (MCGR) and auxiliary YOLOv5 detector to benchmark\nimage super-resolution-based object detection and compare with the existing\nstate-of-the-art methods based on image super-resolution (SR). The proposed\nMCGR achieved state-of-the-art performance for image SR with an improvement of\n1.2dB PSNR compared to the current state-of-the-art NLSN method. MCGR achieved\nbest object detection mAPs of 0.758, 0.881, 0.841, and 0.983, respectively, for\nfive-class, four-class, two-class, and single classes, respectively surpassing\nthe performance of the state-of-the-art object detectors YOLOv5, EfficientDet,\nFaster RCNN, SSD, and RetinaNet.",
          "arxiv_id": "2111.03260v1"
        }
      ],
      "30": [
        {
          "title": "Task-oriented Dialogue Systems: performance vs. quality-optima, a review",
          "year": "2021-12",
          "abstract": "Task-oriented dialogue systems (TODS) are continuing to rise in popularity as\nvarious industries find ways to effectively harness their capabilities, saving\nboth time and money. However, even state-of-the-art TODS are not yet reaching\ntheir full potential. TODS typically have a primary design focus on completing\nthe task at hand, so the metric of task-resolution should take priority. Other\nconversational quality attributes that may point to the success, or otherwise,\nof the dialogue, may be ignored. This can cause interactions between human and\ndialogue system that leave the user dissatisfied or frustrated. This paper\nexplores the literature on evaluative frameworks of dialogue systems and the\nrole of conversational quality attributes in dialogue systems, looking at if,\nhow, and where they are utilised, and examining their correlation with the\nperformance of the dialogue system.",
          "arxiv_id": "2112.11176v1"
        },
        {
          "title": "Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems",
          "year": "2020-05",
          "abstract": "Open-domain dialogue systems aim to generate relevant, informative and\nengaging responses. Seq2seq neural response generation approaches do not have\nexplicit mechanisms to control the content or style of the generated response,\nand frequently result in uninformative utterances. In this paper, we propose\nusing a dialogue policy to plan the content and style of target responses in\nthe form of an action plan, which includes knowledge sentences related to the\ndialogue context, targeted dialogue acts, topic information, etc. The\nattributes within the action plan are obtained by automatically annotating the\npublicly released Topical-Chat dataset. We condition neural response generators\non the action plan which is then realized as target utterances at the turn and\nsentence levels. We also investigate different dialogue policy models to\npredict an action plan given the dialogue context. Through automated and human\nevaluation, we measure the appropriateness of the generated responses and check\nif the generation models indeed learn to realize the given action plans. We\ndemonstrate that a basic dialogue policy that operates at the sentence level\ngenerates better responses in comparison to turn level generation as well as\nbaseline models with no action plan. Additionally the basic dialogue policy has\nthe added effect of controllability.",
          "arxiv_id": "2005.12529v4"
        },
        {
          "title": "Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking",
          "year": "2022-08",
          "abstract": "As an essential component in task-oriented dialogue systems, dialogue state\ntracking (DST) aims to track human-machine interactions and generate state\nrepresentations for managing the dialogue. Representations of dialogue states\nare dependent on the domain ontology and the user's goals. In several\ntask-oriented dialogues with a limited scope of objectives, dialogue states can\nbe represented as a set of slot-value pairs. As the capabilities of dialogue\nsystems expand to support increasing naturalness in communication,\nincorporating dialogue act processing into dialogue model design becomes\nessential. The lack of such consideration limits the scalability of dialogue\nstate tracking models for dialogues having specific objectives and ontology. To\naddress this issue, we formulate and incorporate dialogue acts, and leverage\nrecent advances in machine reading comprehension to predict both categorical\nand non-categorical types of slots for multi-domain dialogue state tracking.\nExperimental results show that our models can improve the overall accuracy of\ndialogue state tracking on the MultiWOZ 2.1 dataset, and demonstrate that\nincorporating dialogue acts can guide dialogue state design for future\ntask-oriented dialogue systems.",
          "arxiv_id": "2208.02462v1"
        }
      ],
      "31": [
        {
          "title": "Physics Informed RNN-DCT Networks for Time-Dependent Partial Differential Equations",
          "year": "2022-02",
          "abstract": "Physics-informed neural networks allow models to be trained by physical laws\ndescribed by general nonlinear partial differential equations. However,\ntraditional architectures struggle to solve more challenging time-dependent\nproblems due to their architectural nature. In this work, we present a novel\nphysics-informed framework for solving time-dependent partial differential\nequations. Using only the governing differential equations and problem initial\nand boundary conditions, we generate a latent representation of the problem's\nspatio-temporal dynamics. Our model utilizes discrete cosine transforms to\nencode spatial frequencies and recurrent neural networks to process the time\nevolution. This efficiently and flexibly produces a compressed representation\nwhich is used for additional conditioning of physics-informed models. We show\nexperimental results on the Taylor-Green vortex solution to the Navier-Stokes\nequations. Our proposed model achieves state-of-the-art performance on the\nTaylor-Green vortex relative to other physics-informed baseline models.",
          "arxiv_id": "2202.12358v1"
        },
        {
          "title": "Separable Physics-Informed Neural Networks for the solution of elasticity problems",
          "year": "2024-01",
          "abstract": "A method for solving elasticity problems based on separable physics-informed\nneural networks (SPINN) in conjunction with the deep energy method (DEM) is\npresented. Numerical experiments have been carried out for a number of problems\nshowing that this method has a significantly higher convergence rate and\naccuracy than the vanilla physics-informed neural networks (PINN) and even\nSPINN based on a system of partial differential equations (PDEs). In addition,\nusing the SPINN in the framework of DEM approach it is possible to solve\nproblems of the linear theory of elasticity on complex geometries, which is\nunachievable with the help of PINNs in frames of partial differential\nequations. Considered problems are very close to the industrial problems in\nterms of geometry, loading, and material parameters.",
          "arxiv_id": "2401.13486v1"
        },
        {
          "title": "Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next",
          "year": "2022-01",
          "abstract": "Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode\nmodel equations, like Partial Differential Equations (PDE), as a component of\nthe neural network itself. PINNs are nowadays used to solve PDEs, fractional\nequations, integral-differential equations, and stochastic PDEs. This novel\nmethodology has arisen as a multi-task learning framework in which a NN must\nfit observed data while reducing a PDE residual. This article provides a\ncomprehensive review of the literature on PINNs: while the primary goal of the\nstudy was to characterize these networks and their related advantages and\ndisadvantages. The review also attempts to incorporate publications on a\nbroader range of collocation-based physics informed neural networks, which\nstars form the vanilla PINN, as well as many other variants, such as\nphysics-constrained neural networks (PCNN), variational hp-VPINN, and\nconservative PINN (CPINN). The study indicates that most research has focused\non customizing the PINN through different activation functions, gradient\noptimization techniques, neural network structures, and loss function\nstructures. Despite the wide range of applications for which PINNs have been\nused, by demonstrating their ability to be more feasible in some contexts than\nclassical numerical techniques like Finite Element Method (FEM), advancements\nare still possible, most notably theoretical issues that remain unresolved.",
          "arxiv_id": "2201.05624v4"
        }
      ],
      "32": [
        {
          "title": "Effective Bayesian Causal Inference via Structural Marginalisation and Autoregressive Orders",
          "year": "2024-02",
          "abstract": "The traditional two-stage approach to causal inference first identifies a\nsingle causal model (or equivalence class of models), which is then used to\nanswer causal queries. However, this neglects any epistemic model uncertainty.\nIn contrast, Bayesian causal inference does incorporate epistemic uncertainty\ninto query estimates via Bayesian marginalisation (posterior averaging) over\nall causal models. While principled, this marginalisation over entire causal\nmodels, i.e., both causal structures (graphs) and mechanisms, poses a\ntremendous computational challenge. In this work, we address this challenge by\ndecomposing structure marginalisation into the marginalisation over (i) causal\norders and (ii) directed acyclic graphs (DAGs) given an order. We can\nmarginalise the latter in closed form by limiting the number of parents per\nvariable and utilising Gaussian processes to model mechanisms. To marginalise\nover orders, we use a sampling-based approximation, for which we devise a novel\nauto-regressive distribution over causal orders (ARCO). Our method outperforms\nstate-of-the-art in structure learning on simulated non-linear additive noise\nbenchmarks, and yields competitive results on real-world data. Furthermore, we\ncan accurately infer interventional distributions and average causal effects.",
          "arxiv_id": "2402.14781v3"
        },
        {
          "title": "Topos Causal Models",
          "year": "2025-08",
          "abstract": "We propose topos causal models (TCMs), a novel class of causal models that\nexploit the key properties of a topos category: they are (co)complete, meaning\nall (co)limits exist, they admit a subobject classifier, and allow exponential\nobjects. The main goal of this paper is to show that these properties are\ncentral to many applications in causal inference. For example, subobject\nclassifiers allow a categorical formulation of causal intervention, which\ncreates sub-models. Limits and colimits allow causal diagrams of arbitrary\ncomplexity to be ``solved\", using a novel interpretation of causal\napproximation. Exponential objects enable reasoning about equivalence classes\nof operations on causal models, such as covered edge reversal and causal\nhomotopy. Analogous to structural causal models (SCMs), TCMs are defined by a\ncollection of functions, each defining a ``local autonomous\" causal mechanism\nthat assemble to induce a unique global function from exogenous to endogenous\nvariables. Since the category of TCMs is (co)complete, which we prove in this\npaper, every causal diagram has a ``solution\" in the form of a (co)limit: this\nimplies that any arbitrary causal model can be ``approximated\" by some global\nfunction with respect to the morphisms going into or out of the diagram.\nNatural transformations are crucial in measuring the quality of approximation.\nIn addition, we show that causal interventions are modeled by subobject\nclassifiers: any sub-model is defined by a monic arrow into its parent model.\nExponential objects permit reasoning about entire classes of causal\nequivalences and interventions. Finally, as TCMs form a topos, they admit an\ninternal logic defined as a Mitchell-Benabou language with an associated\nKripke-Joyal semantics. We show how to reason about causal models in TCMs using\nthis internal logic.",
          "arxiv_id": "2508.08295v1"
        },
        {
          "title": "Nonparametric Identifiability of Causal Representations from Unknown Interventions",
          "year": "2023-06",
          "abstract": "We study causal representation learning, the task of inferring latent causal\nvariables and their causal relations from high-dimensional mixtures of the\nvariables. Prior work relies on weak supervision, in the form of counterfactual\npre- and post-intervention views or temporal structure; places restrictive\nassumptions, such as linearity, on the mixing function or latent causal model;\nor requires partial knowledge of the generative process, such as the causal\ngraph or intervention targets. We instead consider the general setting in which\nboth the causal model and the mixing function are nonparametric. The learning\nsignal takes the form of multiple datasets, or environments, arising from\nunknown interventions in the underlying causal model. Our goal is to identify\nboth the ground truth latents and their causal graph up to a set of ambiguities\nwhich we show to be irresolvable from interventional data. We study the\nfundamental setting of two causal variables and prove that the observational\ndistribution and one perfect intervention per node suffice for identifiability,\nsubject to a genericity condition. This condition rules out spurious solutions\nthat involve fine-tuning of the intervened and observational distributions,\nmirroring similar conditions for nonlinear cause-effect inference. For an\narbitrary number of variables, we show that at least one pair of distinct\nperfect interventional domains per node guarantees identifiability. Further, we\ndemonstrate that the strengths of causal influences among the latent variables\nare preserved by all equivalent solutions, rendering the inferred\nrepresentation appropriate for drawing causal conclusions from new data. Our\nstudy provides the first identifiability results for the general nonparametric\nsetting with unknown interventions, and elucidates what is possible and\nimpossible for causal representation learning without more direct supervision.",
          "arxiv_id": "2306.00542v2"
        }
      ],
      "33": [
        {
          "title": "Non-stationary BERT: Exploring Augmented IMU Data For Robust Human Activity Recognition",
          "year": "2024-09",
          "abstract": "Human Activity Recognition (HAR) has gained great attention from researchers\ndue to the popularity of mobile devices and the need to observe users' daily\nactivity data for better human-computer interaction. In this work, we collect a\nhuman activity recognition dataset called OPPOHAR consisting of phone IMU data.\nTo facilitate the employment of HAR system in mobile phone and to achieve\nuser-specific activity recognition, we propose a novel light-weight network\ncalled Non-stationary BERT with a two-stage training method. We also propose a\nsimple yet effective data augmentation method to explore the deeper\nrelationship between the accelerator and gyroscope data from the IMU. The\nnetwork achieves the state-of-the-art performance testing on various activity\nrecognition datasets and the data augmentation method demonstrates its wide\napplicability.",
          "arxiv_id": "2409.16730v1"
        },
        {
          "title": "Few-shot Action Recognition via Intra- and Inter-Video Information Maximization",
          "year": "2023-05",
          "abstract": "Current few-shot action recognition involves two primary sources of\ninformation for classification:(1) intra-video information, determined by frame\ncontent within a single video clip, and (2) inter-video information, measured\nby relationships (e.g., feature similarity) among videos. However, existing\nmethods inadequately exploit these two information sources. In terms of\nintra-video information, current sampling operations for input videos may omit\ncritical action information, reducing the utilization efficiency of video data.\nFor the inter-video information, the action misalignment among videos makes it\nchallenging to calculate precise relationships. Moreover, how to jointly\nconsider both inter- and intra-video information remains under-explored for\nfew-shot action recognition. To this end, we propose a novel framework, Video\nInformation Maximization (VIM), for few-shot video action recognition. VIM is\nequipped with an adaptive spatial-temporal video sampler and a spatiotemporal\naction alignment model to maximize intra- and inter-video information,\nrespectively. The video sampler adaptively selects important frames and\namplifies critical spatial regions for each input video based on the task at\nhand. This preserves and emphasizes informative parts of video clips while\neliminating interference at the data level. The alignment model performs\ntemporal and spatial action alignment sequentially at the feature level,\nleading to more precise measurements of inter-video similarity. Finally, These\ngoals are facilitated by incorporating additional loss terms based on mutual\ninformation measurement. Consequently, VIM acts to maximize the distinctiveness\nof video information from limited video data. Extensive experimental results on\npublic datasets for few-shot action recognition demonstrate the effectiveness\nand benefits of our framework.",
          "arxiv_id": "2305.06114v1"
        },
        {
          "title": "Human Activity Recognition Using Cascaded Dual Attention CNN and Bi-Directional GRU Framework",
          "year": "2022-08",
          "abstract": "Vision-based human activity recognition has emerged as one of the essential\nresearch areas in video analytics domain. Over the last decade, numerous\nadvanced deep learning algorithms have been introduced to recognize complex\nhuman actions from video streams. These deep learning algorithms have shown\nimpressive performance for the human activity recognition task. However, these\nnewly introduced methods either exclusively focus on model performance or the\neffectiveness of these models in terms of computational efficiency and\nrobustness, resulting in a biased tradeoff in their proposals to deal with\nchallenging human activity recognition problem. To overcome the limitations of\ncontemporary deep learning models for human activity recognition, this paper\npresents a computationally efficient yet generic spatial-temporal cascaded\nframework that exploits the deep discriminative spatial and temporal features\nfor human activity recognition. For efficient representation of human actions,\nwe have proposed an efficient dual attentional convolutional neural network\n(CNN) architecture that leverages a unified channel-spatial attention mechanism\nto extract human-centric salient features in video frames. The dual\nchannel-spatial attention layers together with the convolutional layers learn\nto be more attentive in the spatial receptive fields having objects over the\nnumber of feature maps. The extracted discriminative salient features are then\nforwarded to stacked bi-directional gated recurrent unit (Bi-GRU) for long-term\ntemporal modeling and recognition of human actions using both forward and\nbackward pass gradient learning. Extensive experiments are conducted, where the\nobtained results show that the proposed framework attains an improvement in\nexecution time up to 167 times in terms of frames per second as compared to\nmost of the contemporary action recognition methods.",
          "arxiv_id": "2208.05034v1"
        }
      ],
      "34": [
        {
          "title": "A Use of Even Activation Functions in Neural Networks",
          "year": "2020-11",
          "abstract": "Despite broad interest in applying deep learning techniques to scientific\ndiscovery, learning interpretable formulas that accurately describe scientific\ndata is very challenging because of the vast landscape of possible functions\nand the \"black box\" nature of deep neural networks. The key to success is to\neffectively integrate existing knowledge or hypotheses about the underlying\nstructure of the data into the architecture of deep learning models to guide\nmachine learning. Currently, such integration is commonly done through\ncustomization of the loss functions. Here we propose an alternative approach to\nintegrate existing knowledge or hypotheses of data structure by constructing\ncustom activation functions that reflect this structure. Specifically, we study\na common case when the multivariate target function $f$ to be learned from the\ndata is partially exchangeable, \\emph{i.e.} $f(u,v,w)=f(v,u,w)$ for $u,v\\in\n\\mathbb{R}^d$. For instance, these conditions are satisfied for the\nclassification of images that is invariant under left-right flipping. Through\ntheoretical proof and experimental verification, we show that using an even\nactivation function in one of the fully connected layers improves neural\nnetwork performance. In our experimental 9-dimensional regression problems,\nreplacing one of the non-symmetric activation functions with the designated\n\"Seagull\" activation function $\\log(1+x^2)$ results in substantial improvement\nin network performance. Surprisingly, even activation functions are seldom used\nin neural networks. Our results suggest that customized activation functions\nhave great potential in neural networks.",
          "arxiv_id": "2011.11713v1"
        },
        {
          "title": "Novel Kernel Models and Exact Representor Theory for Neural Networks Beyond the Over-Parameterized Regime",
          "year": "2024-05",
          "abstract": "This paper presents two models of neural-networks and their training\napplicable to neural networks of arbitrary width, depth and topology, assuming\nonly finite-energy neural activations; and a novel representor theory for\nneural networks in terms of a matrix-valued kernel. The first model is exact\n(un-approximated) and global, casting the neural network as an elements in a\nreproducing kernel Banach space (RKBS); we use this model to provide tight\nbounds on Rademacher complexity. The second model is exact and local, casting\nthe change in neural network function resulting from a bounded change in\nweights and biases (ie. a training step) in reproducing kernel Hilbert space\n(RKHS) in terms of a local-intrinsic neural kernel (LiNK). This local model\nprovides insight into model adaptation through tight bounds on Rademacher\ncomplexity of network adaptation. We also prove that the neural tangent kernel\n(NTK) is a first-order approximation of the LiNK kernel. Finally, and noting\nthat the LiNK does not provide a representor theory for technical reasons, we\npresent an exact novel representor theory for layer-wise neural network\ntraining with unregularized gradient descent in terms of a local-extrinsic\nneural kernel (LeNK). This representor theory gives insight into the role of\nhigher-order statistics in neural network training and the effect of kernel\nevolution in neural-network kernel models. Throughout the paper (a) feedforward\nReLU networks and (b) residual networks (ResNet) are used as illustrative\nexamples.",
          "arxiv_id": "2405.15254v1"
        },
        {
          "title": "A survey on recently proposed activation functions for Deep Learning",
          "year": "2022-04",
          "abstract": "Artificial neural networks (ANN), typically referred to as neural networks,\nare a class of Machine Learning algorithms and have achieved widespread\nsuccess, having been inspired by the biological structure of the human brain.\nNeural networks are inherently powerful due to their ability to learn complex\nfunction approximations from data. This generalization ability has been able to\nimpact multidisciplinary areas involving image recognition, speech recognition,\nnatural language processing, and others. Activation functions are a crucial\nsub-component of neural networks. They define the output of a node in the\nnetwork given a set of inputs. This survey discusses the main concepts of\nactivation functions in neural networks, including; a brief introduction to\ndeep neural networks, a summary of what are activation functions and how they\nare used in neural networks, their most common properties, the different types\nof activation functions, some of the challenges, limitations, and alternative\nsolutions faced by activation functions, concluding with the final remarks.",
          "arxiv_id": "2204.02921v2"
        }
      ],
      "35": [
        {
          "title": "Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection",
          "year": "2022-01",
          "abstract": "Deep neural networks (DNNs) are threatened by adversarial examples.\nAdversarial detection, which distinguishes adversarial images from benign\nimages, is fundamental for robust DNN-based services. Image transformation is\none of the most effective approaches to detect adversarial examples. During the\nlast few years, a variety of image transformations have been studied and\ndiscussed to design reliable adversarial detectors. In this paper, we\nsystematically synthesize the recent progress on adversarial detection via\nimage transformations with a novel classification method. Then, we conduct\nextensive experiments to test the detection performance of image\ntransformations against state-of-the-art adversarial attacks. Furthermore, we\nreveal that each individual transformation is not capable of detecting\nadversarial examples in a robust way, and propose a DNN-based approach referred\nto as \\emph{AdvJudge}, which combines scores of 9 image transformations.\nWithout knowing which individual scores are misleading or not misleading,\nAdvJudge can make the right judgment, and achieve a significant improvement in\ndetection rate. Finally, we utilize an explainable AI tool to show the\ncontribution of each image transformation to adversarial detection.\nExperimental results show that the contribution of image transformations to\nadversarial detection is significantly different, the combination of them can\nsignificantly improve the generic detection ability against state-of-the-art\nadversarial attacks.",
          "arxiv_id": "2201.01080v3"
        },
        {
          "title": "Towards Speeding up Adversarial Training in Latent Spaces",
          "year": "2021-02",
          "abstract": "Adversarial training is wildly considered as one of the most effective way to\ndefend against adversarial examples. However, existing adversarial training\nmethods consume unbearable time, due to the fact that they need to generate\nadversarial examples in the large input space. To speed up adversarial\ntraining, we propose a novel adversarial training method that does not need to\ngenerate real adversarial examples. By adding perturbations to logits to\ngenerate Endogenous Adversarial Examples (EAEs) -- the adversarial examples in\nthe latent space, the time consuming gradient calculation can be avoided.\nExtensive experiments are conducted on CIFAR-10 and ImageNet, and the results\nshow that comparing to state-of-the-art methods, our EAE adversarial training\nnot only shortens the training time, but also enhances the robustness of the\nmodel and has less impact on the accuracy of clean examples than the existing\nmethods.",
          "arxiv_id": "2102.00662v2"
        },
        {
          "title": "Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing",
          "year": "2023-03",
          "abstract": "Deep neural networks can be easily fooled into making incorrect predictions\nthrough corruption of the input by adversarial perturbations:\nhuman-imperceptible artificial noise. So far adversarial training has been the\nmost successful defense against such adversarial attacks. This work focuses on\nimproving adversarial training to boost adversarial robustness. We first\nanalyze, from an instance-wise perspective, how adversarial vulnerability\nevolves during adversarial training. We find that during training an overall\nreduction of adversarial loss is achieved by sacrificing a considerable\nproportion of training samples to be more vulnerable to adversarial attack,\nwhich results in an uneven distribution of adversarial vulnerability among\ndata. Such \"uneven vulnerability\", is prevalent across several popular robust\ntraining methods and, more importantly, relates to overfitting in adversarial\ntraining. Motivated by this observation, we propose a new adversarial training\nmethod: Instance-adaptive Smoothness Enhanced Adversarial Training (ISEAT). It\njointly smooths both input and weight loss landscapes in an adaptive,\ninstance-specific, way to enhance robustness more for those samples with higher\nadversarial vulnerability. Extensive experiments demonstrate the superiority of\nour method over existing defense methods. Noticeably, our method, when combined\nwith the latest data augmentation and semi-supervised learning techniques,\nachieves state-of-the-art robustness against $\\ell_{\\infty}$-norm constrained\nattacks on CIFAR10 of 59.32% for Wide ResNet34-10 without extra data, and\n61.55% for Wide ResNet28-10 with extra data. Code is available at\nhttps://github.com/TreeLLi/Instance-adaptive-Smoothness-Enhanced-AT.",
          "arxiv_id": "2303.14077v2"
        }
      ],
      "36": [
        {
          "title": "Fine-grained Background Representation for Weakly Supervised Semantic Segmentation",
          "year": "2024-06",
          "abstract": "Generating reliable pseudo masks from image-level labels is challenging in\nthe weakly supervised semantic segmentation (WSSS) task due to the lack of\nspatial information. Prevalent class activation map (CAM)-based solutions are\nchallenged to discriminate the foreground (FG) objects from the suspicious\nbackground (BG) pixels (a.k.a. co-occurring) and learn the integral object\nregions. This paper proposes a simple fine-grained background representation\n(FBR) method to discover and represent diverse BG semantics and address the\nco-occurring problems. We abandon using the class prototype or pixel-level\nfeatures for BG representation. Instead, we develop a novel primitive, negative\nregion of interest (NROI), to capture the fine-grained BG semantic information\nand conduct the pixel-to-NROI contrast to distinguish the confusing BG pixels.\nWe also present an active sampling strategy to mine the FG negatives\non-the-fly, enabling efficient pixel-to-pixel intra-foreground contrastive\nlearning to activate the entire object region. Thanks to the simplicity of\ndesign and convenience in use, our proposed method can be seamlessly plugged\ninto various models, yielding new state-of-the-art results under various WSSS\nsettings across benchmarks. Leveraging solely image-level (I) labels as\nsupervision, our method achieves 73.2 mIoU and 45.6 mIoU segmentation results\non Pascal Voc and MS COCO test sets, respectively. Furthermore, by\nincorporating saliency maps as an additional supervision signal (I+S), we\nattain 74.9 mIoU on Pascal Voc test set. Concurrently, our FBR approach\ndemonstrates meaningful performance gains in weakly-supervised instance\nsegmentation (WSIS) tasks, showcasing its robustness and strong generalization\ncapabilities across diverse domains.",
          "arxiv_id": "2406.15755v1"
        },
        {
          "title": "SCNet: Enhancing Few-Shot Semantic Segmentation by Self-Contrastive Background Prototypes",
          "year": "2021-04",
          "abstract": "Few-shot semantic segmentation aims to segment novel-class objects in a query\nimage with only a few annotated examples in support images. Most of advanced\nsolutions exploit a metric learning framework that performs segmentation\nthrough matching each pixel to a learned foreground prototype. However, this\nframework suffers from biased classification due to incomplete construction of\nsample pairs with the foreground prototype only. To address this issue, in this\npaper, we introduce a complementary self-contrastive task into few-shot\nsemantic segmentation. Our new model is able to associate the pixels in a\nregion with the prototype of this region, no matter they are in the foreground\nor background. To this end, we generate self-contrastive background prototypes\ndirectly from the query image, with which we enable the construction of\ncomplete sample pairs and thus a complementary and auxiliary segmentation task\nto achieve the training of a better segmentation model. Extensive experiments\non PASCAL-5$^i$ and COCO-20$^i$ demonstrate clearly the superiority of our\nproposal. At no expense of inference efficiency, our model achieves\nstate-of-the results in both 1-shot and 5-shot settings for few-shot semantic\nsegmentation.",
          "arxiv_id": "2104.09216v3"
        },
        {
          "title": "Self-supervised Learning with Local Contrastive Loss for Detection and Semantic Segmentation",
          "year": "2022-07",
          "abstract": "We present a self-supervised learning (SSL) method suitable for semi-global\ntasks such as object detection and semantic segmentation. We enforce local\nconsistency between self-learned features, representing corresponding image\nlocations of transformed versions of the same image, by minimizing a\npixel-level local contrastive (LC) loss during training. LC-loss can be added\nto existing self-supervised learning methods with minimal overhead. We evaluate\nour SSL approach on two downstream tasks -- object detection and semantic\nsegmentation, using COCO, PASCAL VOC, and CityScapes datasets. Our method\noutperforms the existing state-of-the-art SSL approaches by 1.9% on COCO object\ndetection, 1.4% on PASCAL VOC detection, and 0.6% on CityScapes segmentation.",
          "arxiv_id": "2207.04398v2"
        }
      ],
      "37": [
        {
          "title": "Variational Quantum Cloning: Improving Practicality for Quantum Cryptanalysis",
          "year": "2020-12",
          "abstract": "Cryptanalysis on standard quantum cryptographic systems generally involves\nfinding optimal adversarial attack strategies on the underlying protocols. The\ncore principle of modelling quantum attacks in many cases reduces to the\nadversary's ability to clone unknown quantum states which facilitates the\nextraction of some meaningful secret information. Explicit optimal attack\nstrategies typically require high computational resources due to large circuit\ndepths or, in many cases, are unknown. In this work, we propose variational\nquantum cloning (VQC), a quantum machine learning based cryptanalysis algorithm\nwhich allows an adversary to obtain optimal (approximate) cloning strategies\nwith short depth quantum circuits, trained using hybrid classical-quantum\ntechniques. The algorithm contains operationally meaningful cost functions with\ntheoretical guarantees, quantum circuit structure learning and gradient descent\nbased optimisation. Our approach enables the end-to-end discovery of hardware\nefficient quantum circuits to clone specific families of quantum states, which\nin turn leads to an improvement in cloning fidelites when implemented on\nquantum hardware: the Rigetti Aspen chip. Finally, we connect these results to\nquantum cryptographic primitives, in particular quantum coin flipping. We\nderive attacks on two protocols as examples, based on quantum cloning and\nfacilitated by VQC. As a result, our algorithm can improve near term attacks on\nthese protocols, using approximate quantum cloning as a resource.",
          "arxiv_id": "2012.11424v1"
        },
        {
          "title": "Iterative Quantum Feature Maps",
          "year": "2025-06",
          "abstract": "Quantum machine learning models that leverage quantum circuits as quantum\nfeature maps (QFMs) are recognized for their enhanced expressive power in\nlearning tasks. Such models have demonstrated rigorous end-to-end quantum\nspeedups for specific families of classification problems. However, deploying\ndeep QFMs on real quantum hardware remains challenging due to circuit noise and\nhardware constraints. Additionally, variational quantum algorithms often suffer\nfrom computational bottlenecks, particularly in accurate gradient estimation,\nwhich significantly increases quantum resource demands during training. We\npropose Iterative Quantum Feature Maps (IQFMs), a hybrid quantum-classical\nframework that constructs a deep architecture by iteratively connecting shallow\nQFMs with classically computed augmentation weights. By incorporating\ncontrastive learning and a layer-wise training mechanism, IQFMs effectively\nreduces quantum runtime and mitigates noise-induced degradation. In tasks\ninvolving noisy quantum data, numerical experiments show that IQFMs outperforms\nquantum convolutional neural networks, without requiring the optimization of\nvariational quantum parameters. Even for a typical classical image\nclassification benchmark, a carefully designed IQFMs achieves performance\ncomparable to that of classical neural networks. This framework presents a\npromising path to address current limitations and harness the full potential of\nquantum-enhanced machine learning.",
          "arxiv_id": "2506.19461v1"
        },
        {
          "title": "Vectorized Attention with Learnable Encoding for Quantum Transformer",
          "year": "2025-08",
          "abstract": "Vectorized quantum block encoding provides a way to embed classical data into\nHilbert space, offering a pathway for quantum models, such as Quantum\nTransformers (QT), that replace classical self-attention with quantum circuit\nsimulations to operate more efficiently. Current QTs rely on deep parameterized\nquantum circuits (PQCs), rendering them vulnerable to QPU noise, and thus\nhindering their practical performance. In this paper, we propose the Vectorized\nQuantum Transformer (VQT), a model that supports ideal masked attention matrix\ncomputation through quantum approximation simulation and efficient training via\nvectorized nonlinear quantum encoder, yielding shot-efficient and gradient-free\nquantum circuit simulation (QCS) and reduced classical sampling overhead. In\naddition, we demonstrate an accuracy comparison for IBM and IonQ in quantum\ncircuit simulation and competitive results in benchmarking natural language\nprocessing tasks on IBM state-of-the-art and high-fidelity Kingston QPU. Our\nnoise intermediate-scale quantum friendly VQT approach unlocks a novel\narchitecture for end-to-end machine learning in quantum computing.",
          "arxiv_id": "2508.18464v2"
        }
      ],
      "38": [
        {
          "title": "Metrics and methods for a systematic comparison of fairness-aware machine learning algorithms",
          "year": "2020-10",
          "abstract": "Understanding and removing bias from the decisions made by machine learning\nmodels is essential to avoid discrimination against unprivileged groups.\nDespite recent progress in algorithmic fairness, there is still no clear answer\nas to which bias-mitigation approaches are most effective. Evaluation\nstrategies are typically use-case specific, rely on data with unclear bias, and\nemploy a fixed policy to convert model outputs to decision outcomes. To address\nthese problems, we performed a systematic comparison of a number of popular\nfairness algorithms applicable to supervised classification. Our study is the\nmost comprehensive of its kind. It utilizes three real and four synthetic\ndatasets, and two different ways of converting model outputs to decisions. It\nconsiders fairness, predictive-performance, calibration quality, and speed of\n28 different modelling pipelines, corresponding to both fairness-unaware and\nfairness-aware algorithms. We found that fairness-unaware algorithms typically\nfail to produce adequately fair models and that the simplest algorithms are not\nnecessarily the fairest ones. We also found that fairness-aware algorithms can\ninduce fairness without material drops in predictive power. Finally, we found\nthat dataset idiosyncracies (e.g., degree of intrinsic unfairness, nature of\ncorrelations) do affect the performance of fairness-aware approaches. Our\nresults allow the practitioner to narrow down the approach(es) they would like\nto adopt without having to know in advance their fairness requirements.",
          "arxiv_id": "2010.03986v1"
        },
        {
          "title": "Non-Comparative Fairness for Human-Auditing and Its Relation to Traditional Fairness Notions",
          "year": "2021-06",
          "abstract": "Bias evaluation in machine-learning based services (MLS) based on traditional\nalgorithmic fairness notions that rely on comparative principles is practically\ndifficult, making it necessary to rely on human auditor feedback. However, in\nspite of taking rigorous training on various comparative fairness notions,\nhuman auditors are known to disagree on various aspects of fairness notions in\npractice, making it difficult to collect reliable feedback. This paper offers a\nparadigm shift to the domain of algorithmic fairness via proposing a new\nfairness notion based on the principle of non-comparative justice. In contrary\nto traditional fairness notions where the outcomes of two individuals/groups\nare compared, our proposed notion compares the MLS' outcome with a desired\noutcome for each input. This desired outcome naturally describes a human\nauditor's expectation, and can be easily used to evaluate MLS on crowd-auditing\nplatforms. We show that any MLS can be deemed fair from the perspective of\ncomparative fairness (be it in terms of individual fairness, statistical\nparity, equal opportunity or calibration) if it is non-comparatively fair with\nrespect to a fair auditor. We also show that the converse holds true in the\ncontext of individual fairness. Given that such an evaluation relies on the\ntrustworthiness of the auditor, we also present an approach to identify fair\nand reliable auditors by estimating their biases with respect to a given set of\nsensitive attributes, as well as quantify the uncertainty in the estimation of\nbiases within a given MLS. Furthermore, all of the above results are also\nvalidated on COMPAS, German credit and Adult Census Income datasets.",
          "arxiv_id": "2107.01277v1"
        },
        {
          "title": "Fairness Perception from a Network-Centric Perspective",
          "year": "2020-10",
          "abstract": "Algorithmic fairness is a major concern in recent years as the influence of\nmachine learning algorithms becomes more widespread. In this paper, we\ninvestigate the issue of algorithmic fairness from a network-centric\nperspective. Specifically, we introduce a novel yet intuitive function known as\nnetwork-centric fairness perception and provide an axiomatic approach to\nanalyze its properties. Using a peer-review network as case study, we also\nexamine its utility in terms of assessing the perception of fairness in paper\nacceptance decisions. We show how the function can be extended to a group\nfairness metric known as fairness visibility and demonstrate its relationship\nto demographic parity. We also illustrate a potential pitfall of the fairness\nvisibility measure that can be exploited to mislead individuals into perceiving\nthat the algorithmic decisions are fair. We demonstrate how the problem can be\nalleviated by increasing the local neighborhood size of the fairness perception\nfunction.",
          "arxiv_id": "2010.05887v1"
        }
      ],
      "39": [
        {
          "title": "Systematic Comparison of Path Planning Algorithms using PathBench",
          "year": "2022-03",
          "abstract": "Path planning is an essential component of mobile robotics. Classical path\nplanning algorithms, such as wavefront and rapidly-exploring random tree (RRT)\nare used heavily in autonomous robots. With the recent advances in machine\nlearning, development of learning-based path planning algorithms has been\nexperiencing rapid growth. An unified path planning interface that facilitates\nthe development and benchmarking of existing and new algorithms is needed. This\npaper presents PathBench, a platform for developing, visualizing, training,\ntesting, and benchmarking of existing and future, classical and learning-based\npath planning algorithms in 2D and 3D grid world environments. Many existing\npath planning algorithms are supported; e.g. A*, Dijkstra, waypoint planning\nnetworks, value iteration networks, gated path planning networks; and\nintegrating new algorithms is easy and clearly specified. The benchmarking\nability of PathBench is explored in this paper by comparing algorithms across\nfive different hardware systems and three different map types, including\nbuilt-in PathBench maps, video game maps, and maps from real world databases.\nMetrics, such as path length, success rate, and computational time, were used\nto evaluate algorithms. Algorithmic analysis was also performed on a real world\nrobot to demonstrate PathBench's support for Robot Operating System (ROS).\nPathBench is open source.",
          "arxiv_id": "2203.03092v1"
        },
        {
          "title": "A Novel Knowledge-Based Genetic Algorithm for Robot Path Planning in Complex Environments",
          "year": "2022-09",
          "abstract": "In this paper, a novel knowledge-based genetic algorithm for path planning of\na mobile robot in unstructured complex environments is proposed, where five\nproblem-specific operators are developed for efficient robot path planning. The\nproposed genetic algorithm incorporates the domain knowledge of robot path\nplanning into its specialized operators, some of which also combine a local\nsearch technique. A unique and simple representation of the robot path is\nproposed and a simple but effective path evaluation method is developed, where\nthe collisions can be accurately detected and the quality of a robot path is\nwell reflected. The proposed algorithm is capable of finding a near-optimal\nrobot path in both static and dynamic complex environments. The effectiveness\nand efficiency of the proposed algorithm are demonstrated by simulation\nstudies. The irreplaceable role of the specialized genetic operators in the\nproposed genetic algorithm for solving the robot path planning problem is\ndemonstrated through a comparison study.",
          "arxiv_id": "2209.01482v1"
        },
        {
          "title": "MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement Learning in Mixed Dynamic Environments",
          "year": "2020-07",
          "abstract": "Multi-agent navigation in dynamic environments is of great industrial value\nwhen deploying a large scale fleet of robot to real-world applications. This\npaper proposes a decentralized partially observable multi-agent path planning\nwith evolutionary reinforcement learning (MAPPER) method to learn an effective\nlocal planning policy in mixed dynamic environments. Reinforcement\nlearning-based methods usually suffer performance degradation on long-horizon\ntasks with goal-conditioned sparse rewards, so we decompose the long-range\nnavigation task into many easier sub-tasks under the guidance of a global\nplanner, which increases agents' performance in large environments. Moreover,\nmost existing multi-agent planning approaches assume either perfect information\nof the surrounding environment or homogeneity of nearby dynamic agents, which\nmay not hold in practice. Our approach models dynamic obstacles' behavior with\nan image-based representation and trains a policy in mixed dynamic environments\nwithout homogeneity assumption. To ensure multi-agent training stability and\nperformance, we propose an evolutionary training approach that can be easily\nscaled to large and complex environments. Experiments show that MAPPER is able\nto achieve higher success rates and more stable performance when exposed to a\nlarge number of non-cooperative dynamic obstacles compared with traditional\nreaction-based planner LRA* and the state-of-the-art learning-based method.",
          "arxiv_id": "2007.15724v1"
        }
      ],
      "40": [
        {
          "title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models",
          "year": "2024-06",
          "abstract": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs.",
          "arxiv_id": "2406.13925v3"
        },
        {
          "title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models",
          "year": "2024-08",
          "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in\nnatural language generation, but they have also been observed to magnify\nsocietal biases, particularly those related to gender. In response to this\nissue, several benchmarks have been proposed to assess gender bias in LLMs.\nHowever, these benchmarks often lack practical flexibility or inadvertently\nintroduce biases. To address these shortcomings, we introduce GenderCARE, a\ncomprehensive framework that encompasses innovative Criteria, bias Assessment,\nReduction techniques, and Evaluation metrics for quantifying and mitigating\ngender bias in LLMs. To begin, we establish pioneering criteria for gender\nequality benchmarks, spanning dimensions such as inclusivity, diversity,\nexplainability, objectivity, robustness, and realisticity. Guided by these\ncriteria, we construct GenderPair, a novel pair-based benchmark designed to\nassess gender bias in LLMs comprehensively. Our benchmark provides standardized\nand realistic evaluations, including previously overlooked gender groups such\nas transgender and non-binary individuals. Furthermore, we develop effective\ndebiasing techniques that incorporate counterfactual data augmentation and\nspecialized fine-tuning strategies to reduce gender bias in LLMs without\ncompromising their overall performance. Extensive experiments demonstrate a\nsignificant reduction in various gender bias benchmarks, with reductions\npeaking at over 90% and averaging above 35% across 17 different LLMs.\nImportantly, these reductions come with minimal variability in mainstream\nlanguage tasks, remaining below 2%. By offering a realistic assessment and\ntailored reduction of gender biases, we hope that our GenderCARE can represent\na significant step towards achieving fairness and equity in LLMs. More details\nare available at https://github.com/kstanghere/GenderCARE-ccs24.",
          "arxiv_id": "2408.12494v2"
        },
        {
          "title": "Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs",
          "year": "2024-10",
          "abstract": "Large Language Models (LLMs) are being adopted across a wide range of tasks,\nincluding decision-making processes in industries where bias in AI systems is a\nsignificant concern. Recent research indicates that LLMs can harbor implicit\nbiases even when they pass explicit bias evaluations. Building upon the\nframeworks of the LLM Implicit Association Test (IAT) Bias and LLM Decision\nBias, this study highlights that newer or larger language models do not\nautomatically exhibit reduced bias; in some cases, they displayed higher bias\nscores than their predecessors, such as in Meta's Llama series and OpenAI's GPT\nmodels. This suggests that increasing model complexity without deliberate bias\nmitigation strategies can unintentionally amplify existing biases. The\nvariability in bias scores within and across providers underscores the need for\nstandardized evaluation metrics and benchmarks for bias assessment. The lack of\nconsistency indicates that bias mitigation is not yet a universally prioritized\ngoal in model development, which can lead to unfair or discriminatory outcomes.\nBy broadening the detection of implicit bias, this research provides a more\ncomprehensive understanding of the biases present in advanced models and\nunderscores the critical importance of addressing these issues to ensure the\ndevelopment of fair and responsible AI systems.",
          "arxiv_id": "2410.12864v1"
        }
      ],
      "41": [
        {
          "title": "Constructing Deep Spiking Neural Networks from Artificial Neural Networks with Knowledge Distillation",
          "year": "2023-04",
          "abstract": "Spiking neural networks (SNNs) are well known as the brain-inspired models\nwith high computing efficiency, due to a key component that they utilize spikes\nas information units, close to the biological neural systems. Although spiking\nbased models are energy efficient by taking advantage of discrete spike\nsignals, their performance is limited by current network structures and their\ntraining methods. As discrete signals, typical SNNs cannot apply the gradient\ndescent rules directly into parameters adjustment as artificial neural networks\n(ANNs). Aiming at this limitation, here we propose a novel method of\nconstructing deep SNN models with knowledge distillation (KD) that uses ANN as\nteacher model and SNN as student model. Through ANN-SNN joint training\nalgorithm, the student SNN model can learn rich feature information from the\nteacher ANN model through the KD method, yet it avoids training SNN from\nscratch when communicating with non-differentiable spikes. Our method can not\nonly build a more efficient deep spiking structure feasibly and reasonably, but\nuse few time steps to train whole model compared to direct training or ANN to\nSNN methods. More importantly, it has a superb ability of noise immunity for\nvarious types of artificial noises and natural signals. The proposed novel\nmethod provides efficient ways to improve the performance of SNN through\nconstructing deeper structures in a high-throughput fashion, with potential\nusage for light and efficient brain-inspired computing of practical scenarios.",
          "arxiv_id": "2304.05627v2"
        },
        {
          "title": "SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks",
          "year": "2023-12",
          "abstract": "Spiking Neural Networks (SNNs) are biologically-inspired models that are\ncapable of processing information in streams of action potentials. However,\nsimulating and training SNNs is computationally expensive due to the need to\nsolve large systems of coupled differential equations. In this paper, we\nintroduce SparseProp, a novel event-based algorithm for simulating and training\nsparse SNNs. Our algorithm reduces the computational cost of both the forward\nand backward pass operations from O(N) to O(log(N)) per network spike, thereby\nenabling numerically exact simulations of large spiking networks and their\nefficient training using backpropagation through time. By leveraging the\nsparsity of the network, SparseProp eliminates the need to iterate through all\nneurons at each spike, employing efficient state updates instead. We\ndemonstrate the efficacy of SparseProp across several classical\nintegrate-and-fire neuron models, including a simulation of a sparse SNN with\none million LIF neurons. This results in a speed-up exceeding four orders of\nmagnitude relative to previous event-based implementations. Our work provides\nan efficient and exact solution for training large-scale spiking neural\nnetworks and opens up new possibilities for building more sophisticated\nbrain-inspired models.",
          "arxiv_id": "2312.17216v1"
        },
        {
          "title": "STEMS: Spatial-Temporal Mapping Tool For Spiking Neural Networks",
          "year": "2025-02",
          "abstract": "Spiking Neural Networks (SNNs) are promising bio-inspired third-generation\nneural networks. Recent research has trained deep SNN models with accuracy on\npar with Artificial Neural Networks (ANNs). Although the event-driven and\nsparse nature of SNNs show potential for more energy efficient computation than\nANNs, SNN neurons have internal states which evolve over time. Keeping track of\nSNN states can significantly increase data movement and storage requirements,\npotentially losing its advantages with respect to ANNs. This paper investigates\nthe energy effects of having neuron states, and how it is influenced by the\nchosen mapping to realistic hardware architectures with advanced memory\nhierarchies. Therefore, we develop STEMS, a mapping design space exploration\ntool for SNNs. STEMS models SNN's stateful behavior and explores intra-layer\nand inter-layer mapping optimizations to minimize data movement, considering\nboth spatial and temporal SNN dimensions. Using STEMS, we show up to 12x\nreduction in off-chip data movement and 5x reduction in energy (on top of\nintra-layer optimizations), on two event-based vision SNN benchmarks. Finally,\nneuron states may not be needed for all SNN layers. By optimizing neuron states\nfor one of our benchmarks, we show 20x reduction in neuron states and 1.4x\nbetter performance without accuracy loss.",
          "arxiv_id": "2502.03287v2"
        }
      ],
      "42": [
        {
          "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
          "year": "2024-08",
          "abstract": "One of the grand challenges of artificial general intelligence is developing\nagents capable of conducting scientific research and discovering new knowledge.\nWhile frontier models have already been used as aides to human scientists, e.g.\nfor brainstorming ideas, writing code, or prediction tasks, they still conduct\nonly a small part of the scientific process. This paper presents the first\ncomprehensive framework for fully automatic scientific discovery, enabling\nfrontier large language models to perform research independently and\ncommunicate their findings. We introduce The AI Scientist, which generates\nnovel research ideas, writes code, executes experiments, visualizes results,\ndescribes its findings by writing a full scientific paper, and then runs a\nsimulated review process for evaluation. In principle, this process can be\nrepeated to iteratively develop ideas in an open-ended fashion, acting like the\nhuman scientific community. We demonstrate its versatility by applying it to\nthree distinct subfields of machine learning: diffusion modeling,\ntransformer-based language modeling, and learning dynamics. Each idea is\nimplemented and developed into a full paper at a cost of less than $15 per\npaper. To evaluate the generated papers, we design and validate an automated\nreviewer, which we show achieves near-human performance in evaluating paper\nscores. The AI Scientist can produce papers that exceed the acceptance\nthreshold at a top machine learning conference as judged by our automated\nreviewer. This approach signifies the beginning of a new era in scientific\ndiscovery in machine learning: bringing the transformative benefits of AI\nagents to the entire research process of AI itself, and taking us closer to a\nworld where endless affordable creativity and innovation can be unleashed on\nthe world's most challenging problems. Our code is open-sourced at\nhttps://github.com/SakanaAI/AI-Scientist",
          "arxiv_id": "2408.06292v3"
        },
        {
          "title": "Mapping the Increasing Use of LLMs in Scientific Papers",
          "year": "2024-04",
          "abstract": "Scientific publishing lays the foundation of science by disseminating\nresearch findings, fostering collaboration, encouraging reproducibility, and\nensuring that scientific knowledge is accessible, verifiable, and built upon\nover time. Recently, there has been immense speculation about how many people\nare using large language models (LLMs) like ChatGPT in their academic writing,\nand to what extent this tool might have an effect on global scientific\npractices. However, we lack a precise measure of the proportion of academic\nwriting substantially modified or produced by LLMs. To address this gap, we\nconduct the first systematic, large-scale analysis across 950,965 papers\npublished between January 2020 and February 2024 on the arXiv, bioRxiv, and\nNature portfolio journals, using a population-level statistical framework to\nmeasure the prevalence of LLM-modified content over time. Our statistical\nestimation operates on the corpus level and is more robust than inference on\nindividual instances. Our findings reveal a steady increase in LLM usage, with\nthe largest and fastest growth observed in Computer Science papers (up to\n17.5%). In comparison, Mathematics papers and the Nature portfolio showed the\nleast LLM modification (up to 6.3%). Moreover, at an aggregate level, our\nanalysis reveals that higher levels of LLM-modification are associated with\npapers whose first authors post preprints more frequently, papers in more\ncrowded research areas, and papers of shorter lengths. Our findings suggests\nthat LLMs are being broadly used in scientific writings.",
          "arxiv_id": "2404.01268v1"
        },
        {
          "title": "How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?",
          "year": "2025-04",
          "abstract": "The spread of scientific knowledge depends on how researchers discover and\ncite previous work. The adoption of large language models (LLMs) in the\nscientific research process introduces a new layer to these citation practices.\nHowever, it remains unclear to what extent LLMs align with human citation\npractices, how they perform across domains, and may influence citation\ndynamics. Here, we show that LLMs systematically reinforce the Matthew effect\nin citations by consistently favoring highly cited papers when generating\nreferences. This pattern persists across scientific domains despite significant\nfield-specific variations in existence rates, which refer to the proportion of\ngenerated references that match existing records in external bibliometric\ndatabases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers,\nwe find that LLM recommendations diverge from traditional citation patterns by\npreferring more recent references with shorter titles and fewer authors.\nEmphasizing their content-level relevance, the generated references are\nsemantically aligned with the content of each paper at levels comparable to the\nground truth references and display similar network effects while reducing\nauthor self-citations. These findings illustrate how LLMs may reshape citation\npractices and influence the trajectory of scientific discovery by reflecting\nand amplifying established trends. As LLMs become more integrated into the\nscientific research process, it is important to understand their role in\nshaping how scientific communities discover and build upon prior work.",
          "arxiv_id": "2504.02767v1"
        }
      ],
      "43": [
        {
          "title": "How Efficient Are Today's Continual Learning Algorithms?",
          "year": "2023-03",
          "abstract": "Supervised Continual learning involves updating a deep neural network (DNN)\nfrom an ever-growing stream of labeled data. While most work has focused on\novercoming catastrophic forgetting, one of the major motivations behind\ncontinual learning is being able to efficiently update a network with new\ninformation, rather than retraining from scratch on the training dataset as it\ngrows over time. Despite recent continual learning methods largely solving the\ncatastrophic forgetting problem, there has been little attention paid to the\nefficiency of these algorithms. Here, we study recent methods for incremental\nclass learning and illustrate that many are highly inefficient in terms of\ncompute, memory, and storage. Some methods even require more compute than\ntraining from scratch! We argue that for continual learning to have real-world\napplicability, the research community cannot ignore the resources used by these\nalgorithms. There is more to continual learning than mitigating catastrophic\nforgetting.",
          "arxiv_id": "2303.18171v2"
        },
        {
          "title": "Hybrid Learners Do Not Forget: A Brain-Inspired Neuro-Symbolic Approach to Continual Learning",
          "year": "2025-03",
          "abstract": "Continual learning is crucial for creating AI agents that can learn and\nimprove themselves autonomously. A primary challenge in continual learning is\nto learn new tasks without losing previously learned knowledge. Current\ncontinual learning methods primarily focus on enabling a neural network with\nmechanisms that mitigate forgetting effects. Inspired by the two distinct\nsystems in the human brain, System 1 and System 2, we propose a Neuro-Symbolic\nBrain-Inspired Continual Learning (NeSyBiCL) framework that incorporates two\nsubsystems to solve continual learning: A neural network model responsible for\nquickly adapting to the most recent task, together with a symbolic reasoner\nresponsible for retaining previously acquired knowledge from previous tasks.\nMoreover, we design an integration mechanism between these components to\nfacilitate knowledge transfer from the symbolic reasoner to the neural network.\nWe also introduce two compositional continual learning benchmarks and\ndemonstrate that NeSyBiCL is effective and leads to superior performance\ncompared to continual learning methods that merely rely on neural architectures\nto address forgetting.",
          "arxiv_id": "2503.12635v1"
        },
        {
          "title": "Probing Representation Forgetting in Supervised and Unsupervised Continual Learning",
          "year": "2022-03",
          "abstract": "Continual Learning research typically focuses on tackling the phenomenon of\ncatastrophic forgetting in neural networks. Catastrophic forgetting is\nassociated with an abrupt loss of knowledge previously learned by a model when\nthe task, or more broadly the data distribution, being trained on changes. In\nsupervised learning problems this forgetting, resulting from a change in the\nmodel's representation, is typically measured or observed by evaluating the\ndecrease in old task performance. However, a model's representation can change\nwithout losing knowledge about prior tasks. In this work we consider the\nconcept of representation forgetting, observed by using the difference in\nperformance of an optimal linear classifier before and after a new task is\nintroduced. Using this tool we revisit a number of standard continual learning\nbenchmarks and observe that, through this lens, model representations trained\nwithout any explicit control for forgetting often experience small\nrepresentation forgetting and can sometimes be comparable to methods which\nexplicitly control for forgetting, especially in longer task sequences. We also\nshow that representation forgetting can lead to new insights on the effect of\nmodel capacity and loss function used in continual learning. Based on our\nresults, we show that a simple yet competitive approach is to learn\nrepresentations continually with standard supervised contrastive learning while\nconstructing prototypes of class samples when queried on old samples.",
          "arxiv_id": "2203.13381v2"
        }
      ],
      "44": [
        {
          "title": "TSO: Self-Training with Scaled Preference Optimization",
          "year": "2024-08",
          "abstract": "Enhancing the conformity of large language models (LLMs) to human preferences\nremains an ongoing research challenge. Recently, offline approaches such as\nDirect Preference Optimization (DPO) have gained prominence as attractive\noptions due to offering effective improvement in simple, efficient, and stable\nwithout interactions with reward models. However, these offline preference\noptimization methods highly rely on the quality of pairwise preference samples.\nMeanwhile, numerous iterative methods require additional training of reward\nmodels to select positive and negative samples from the model's own generated\nresponses for preference learning. Furthermore, as LLMs' capabilities advance,\nit is quite challenging to continuously construct high-quality positive and\nnegative preference instances from the model's outputs due to the lack of\ndiversity. To tackle these challenges, we propose TSO, or Self-Training with\nScaled Preference Optimization, a framework for preference optimization that\nconducts self-training preference learning without training an additional\nreward model. TSO enhances the diversity of responses by constructing a model\nmatrix and incorporating human preference responses. Furthermore, TSO\nintroduces corrections for model preference errors through human and AI\nfeedback. Finally, TSO adopts iterative and dual clip reward strategies to\nupdate the reference model and its responses, adaptively adjusting preference\ndata and balancing the optimization process. Experimental results demonstrate\nthat TSO outperforms existing mainstream methods on various alignment\nevaluation benchmarks, providing practical insight into preference data\nconstruction and model training strategies in the alignment domain.",
          "arxiv_id": "2409.02118v1"
        },
        {
          "title": "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets",
          "year": "2024-10",
          "abstract": "A critical component of the current generation of language models is\npreference alignment, which aims to precisely control the model's behavior to\nmeet human needs and values. The most notable among such methods is\nReinforcement Learning with Human Feedback (RLHF) and its offline variant\nDirect Preference Optimization (DPO), both of which seek to maximize a reward\nmodel based on human preferences. In particular, DPO derives reward signals\ndirectly from the offline preference data, but in doing so overfits the reward\nsignals and generates suboptimal responses that may contain human biases in the\ndataset. In this work, we propose a practical application of a\ndiversity-seeking RL algorithm called GFlowNet-DPO (GDPO) in an offline\npreference alignment setting to curtail such challenges. Empirical results show\nGDPO can generate far more diverse responses than the baseline methods that are\nstill relatively aligned with human values in dialog generation and\nsummarization tasks.",
          "arxiv_id": "2410.15096v1"
        },
        {
          "title": "Preference as Reward, Maximum Preference Optimization with Importance Sampling",
          "year": "2023-12",
          "abstract": "Preference learning is a key technology for aligning language models with\nhuman values. Reinforcement Learning from Human Feedback (RLHF) is a\nmodel-based algorithm to optimize preference learning, which first fits a\nreward model for preference scores and then optimizes the generating policy\nwith an on-policy PPO algorithm to maximize the reward. The processing of RLHF\nis complex, time-consuming, and unstable. The Direct Preference Optimization\n(DPO) algorithm uses an off-policy algorithm to directly optimize the\ngenerating policy and eliminates the need for a reward model. DPO is more\ndata-efficient and stable. However, DPO has a drawback of overfitting to the\npreference data and ignoring the KL-regularization term when the preference is\ndeterministic. Identity mapping Preference Optimization(IPO) uses a\nroot-finding MSE loss to incorporate KL-regularization. However, both DPO and\nIPO fail to properly address the KL-regularization term because the support of\nthe preference distribution is not equal to the reference distribution. In this\npaper, we propose a simple and intuitive off-policy preference optimization\nalgorithm from an importance sampling view, which we call Maximum Preference\nOptimization (MPO). MPO incorporates the off-policy KL-regularization term,\nmaking regularization truly effective. MPO achieves the best of both worlds by\ncombining the objectives of RLHF and IPO while being an off-policy algorithm.\nFurthermore, MPO eliminates the need for a reward model and reference policy,\nsimplifying the learning process and reducing memory usage.",
          "arxiv_id": "2312.16430v5"
        }
      ],
      "45": [
        {
          "title": "Towards Learning-automation IoT Attack Detection through Reinforcement Learning",
          "year": "2020-06",
          "abstract": "As a massive number of the Internet of Things (IoT) devices are deployed, the\nsecurity and privacy issues in IoT arouse more and more attention. The IoT\nattacks are causing tremendous loss to the IoT networks and even threatening\nhuman safety. Compared to traditional networks, IoT networks have unique\ncharacteristics, which make the attack detection more challenging. First, the\nheterogeneity of platforms, protocols, software, and hardware exposes various\nvulnerabilities. Second, in addition to the traditional high-rate attacks, the\nlow-rate attacks are also extensively used by IoT attackers to obfuscate the\nlegitimate and malicious traffic. These low-rate attacks are challenging to\ndetect and can persist in the networks. Last, the attackers are evolving to be\nmore intelligent and can dynamically change their attack strategies based on\nthe environment feedback to avoid being detected, making it more challenging\nfor the defender to discover a consistent pattern to identify the attack.\n  In order to adapt to the new characteristics in IoT attacks, we propose a\nreinforcement learning-based attack detection model that can automatically\nlearn and recognize the transformation of the attack pattern. Therefore, we can\ncontinuously detect IoT attacks with less human intervention. In this paper, we\nexplore the crucial features of IoT traffics and utilize the entropy-based\nmetrics to detect both the high-rate and low-rate IoT attacks. Afterward, we\nleverage the reinforcement learning technique to continuously adjust the attack\ndetection threshold based on the detection feedback, which optimizes the\ndetection and the false alarm rate. We conduct extensive experiments over a\nreal IoT attack dataset and demonstrate the effectiveness of our IoT attack\ndetection framework.",
          "arxiv_id": "2006.15826v1"
        },
        {
          "title": "Gotham Dataset 2025: A Reproducible Large-Scale IoT Network Dataset for Intrusion Detection and Security Research",
          "year": "2025-02",
          "abstract": "In this paper, a dataset of IoT network traffic is presented. Our dataset was\ngenerated by utilising the Gotham testbed, an emulated large-scale Internet of\nThings (IoT) network designed to provide a realistic and heterogeneous\nenvironment for network security research. The testbed includes 78 emulated IoT\ndevices operating on various protocols, including MQTT, CoAP, and RTSP. Network\ntraffic was captured in Packet Capture (PCAP) format using tcpdump, and both\nbenign and malicious traffic were recorded. Malicious traffic was generated\nthrough scripted attacks, covering a variety of attack types, such as Denial of\nService (DoS), Telnet Brute Force, Network Scanning, CoAP Amplification, and\nvarious stages of Command and Control (C&C) communication. The data were\nsubsequently processed in Python for feature extraction using the Tshark tool,\nand the resulting data was converted to Comma Separated Values (CSV) format and\nlabelled. The data repository includes the raw network traffic in PCAP format\nand the processed labelled data in CSV format. Our dataset was collected in a\ndistributed manner, where network traffic was captured separately for each IoT\ndevice at the interface between the IoT gateway and the device. Our dataset was\ncollected in a distributed manner, where network traffic was separately\ncaptured for each IoT device at the interface between the IoT gateway and the\ndevice. With its diverse traffic patterns and attack scenarios, this dataset\nprovides a valuable resource for developing Intrusion Detection Systems and\nsecurity mechanisms tailored to complex, large-scale IoT environments. The\ndataset is publicly available at Zenodo.",
          "arxiv_id": "2502.03134v1"
        },
        {
          "title": "Intrusion Detection using Network Traffic Profiling and Machine Learning for IoT",
          "year": "2021-09",
          "abstract": "The rapid increase in the use of IoT devices brings many benefits to the\ndigital society, ranging from improved efficiency to higher productivity.\nHowever, the limited resources and the open nature of these devices make them\nvulnerable to various cyber threats. A single compromised device can have an\nimpact on the whole network and lead to major security and physical damages.\nThis paper explores the potential of using network profiling and machine\nlearning to secure IoT against cyber-attacks. The proposed anomaly-based\nintrusion detection solution dynamically and actively profiles and monitors all\nnetworked devices for the detection of IoT device tampering attempts as well as\nsuspicious network transactions. Any deviation from the defined profile is\nconsidered to be an attack and is subject to further analysis. Raw traffic is\nalso passed on to the machine learning classifier for examination and\nidentification of potential attacks. Performance assessment of the proposed\nmethodology is conducted on the Cyber-Trust testbed using normal and malicious\nnetwork traffic. The experimental results show that the proposed anomaly\ndetection system delivers promising results with an overall accuracy of 98.35%\nand 0.98% of false-positive alarms.",
          "arxiv_id": "2109.02544v1"
        }
      ],
      "46": [
        {
          "title": "State-of-the-Art Approaches to Enhancing Privacy Preservation of Machine Learning Datasets: A Survey",
          "year": "2024-02",
          "abstract": "This paper examines the evolving landscape of machine learning (ML) and its\nprofound impact across various sectors, with a special focus on the emerging\nfield of Privacy-preserving Machine Learning (PPML). As ML applications become\nincreasingly integral to industries like telecommunications, financial\ntechnology, and surveillance, they raise significant privacy concerns,\nnecessitating the development of PPML strategies. The paper highlights the\nunique challenges in safeguarding privacy within ML frameworks, which stem from\nthe diverse capabilities of potential adversaries, including their ability to\ninfer sensitive information from model outputs or training data.\n  We delve into the spectrum of threat models that characterize adversarial\nintentions, ranging from membership and attribute inference to data\nreconstruction. The paper emphasizes the importance of maintaining the\nconfidentiality and integrity of training data, outlining current research\nefforts that focus on refining training data to minimize privacy-sensitive\ninformation and enhancing data processing techniques to uphold privacy.\n  Through a comprehensive analysis of privacy leakage risks and countermeasures\nin both centralized and collaborative learning settings, this paper aims to\nprovide a thorough understanding of effective strategies for protecting ML\ntraining data against privacy intrusions. It explores the balance between data\nprivacy and model utility, shedding light on privacy-preserving techniques that\nleverage cryptographic methods, Differential Privacy, and Trusted Execution\nEnvironments. The discussion extends to the application of these techniques in\nsensitive domains, underscoring the critical role of PPML in ensuring the\nprivacy and security of ML systems.",
          "arxiv_id": "2404.16847v2"
        },
        {
          "title": "Attack-Aware Noise Calibration for Differential Privacy",
          "year": "2024-07",
          "abstract": "Differential privacy (DP) is a widely used approach for mitigating privacy\nrisks when training machine learning models on sensitive data. DP mechanisms\nadd noise during training to limit the risk of information leakage. The scale\nof the added noise is critical, as it determines the trade-off between privacy\nand utility. The standard practice is to select the noise scale to satisfy a\ngiven privacy budget $\\varepsilon$. This privacy budget is in turn interpreted\nin terms of operational attack risks, such as accuracy, sensitivity, and\nspecificity of inference attacks aimed to recover information about the\ntraining data records. We show that first calibrating the noise scale to a\nprivacy budget $\\varepsilon$, and then translating {\\epsilon} to attack risk\nleads to overly conservative risk assessments and unnecessarily low utility.\nInstead, we propose methods to directly calibrate the noise scale to a desired\nattack risk level, bypassing the step of choosing $\\varepsilon$. For a given\nnotion of attack risk, our approach significantly decreases noise scale,\nleading to increased utility at the same level of privacy. We empirically\ndemonstrate that calibrating noise to attack sensitivity/specificity, rather\nthan $\\varepsilon$, when training privacy-preserving ML models substantially\nimproves model accuracy for the same risk level. Our work provides a principled\nand practical way to improve the utility of privacy-preserving ML without\ncompromising on privacy. The code is available at\nhttps://github.com/Felipe-Gomez/riskcal",
          "arxiv_id": "2407.02191v2"
        },
        {
          "title": "On the utility and protection of optimization with differential privacy and classic regularization techniques",
          "year": "2022-09",
          "abstract": "Nowadays, owners and developers of deep learning models must consider\nstringent privacy-preservation rules of their training data, usually\ncrowd-sourced and retaining sensitive information. The most widely adopted\nmethod to enforce privacy guarantees of a deep learning model nowadays relies\non optimization techniques enforcing differential privacy. According to the\nliterature, this approach has proven to be a successful defence against several\nmodels' privacy attacks, but its downside is a substantial degradation of the\nmodels' performance. In this work, we compare the effectiveness of the\ndifferentially-private stochastic gradient descent (DP-SGD) algorithm against\nstandard optimization practices with regularization techniques. We analyze the\nresulting models' utility, training performance, and the effectiveness of\nmembership inference and model inversion attacks against the learned models.\nFinally, we discuss differential privacy's flaws and limits and empirically\ndemonstrate the often superior privacy-preserving properties of dropout and\nl2-regularization.",
          "arxiv_id": "2209.03175v1"
        }
      ],
      "47": [
        {
          "title": "Renewable energy integration and microgrid energy trading using multi-agent deep reinforcement learning",
          "year": "2021-11",
          "abstract": "In this paper, multi-agent reinforcement learning is used to control a hybrid\nenergy storage system working collaboratively to reduce the energy costs of a\nmicrogrid through maximising the value of renewable energy and trading. The\nagents must learn to control three different types of energy storage system\nsuited for short, medium, and long-term storage under fluctuating demand,\ndynamic wholesale energy prices, and unpredictable renewable energy generation.\nTwo case studies are considered: the first looking at how the energy storage\nsystems can better integrate renewable energy generation under dynamic pricing,\nand the second with how those same agents can be used alongside an aggregator\nagent to sell energy to self-interested external microgrids looking to reduce\ntheir own energy bills. This work found that the centralised learning with\ndecentralised execution of the multi-agent deep deterministic policy gradient\nand its state-of-the-art variants allowed the multi-agent methods to perform\nsignificantly better than the control from a single global agent. It was also\nfound that using separate reward functions in the multi-agent approach\nperformed much better than using a single control agent. Being able to trade\nwith the other microgrids, rather than just selling back to the utility grid,\nalso was found to greatly increase the grid's savings.",
          "arxiv_id": "2111.10898v2"
        },
        {
          "title": "Laxity-Aware Scalable Reinforcement Learning for HVAC Control",
          "year": "2023-06",
          "abstract": "Demand flexibility plays a vital role in maintaining grid balance, reducing\npeak demand, and saving customers' energy bills. Given their highly shiftable\nload and significant contribution to a building's energy consumption, Heating,\nVentilation, and Air Conditioning (HVAC) systems can provide valuable demand\nflexibility to the power systems by adjusting their energy consumption in\nresponse to electricity price and power system needs. To exploit this\nflexibility in both operation time and power, it is imperative to accurately\nmodel and aggregate the load flexibility of a large population of HVAC systems\nas well as designing effective control algorithms. In this paper, we tackle the\ncurse of dimensionality issue in modeling and control by utilizing the concept\nof laxity to quantify the emergency level of each HVAC operation request. We\nfurther propose a two-level approach to address energy optimization for a large\npopulation of HVAC systems. The lower level involves an aggregator to aggregate\nHVAC load laxity information and use least-laxity-first (LLF) rule to allocate\nreal-time power for individual HVAC systems based on the controller's total\npower. Due to the complex and uncertain nature of HVAC systems, we leverage a\nreinforcement learning (RL)-based controller to schedule the total power based\non the aggregated laxity information and electricity price. We evaluate the\ntemperature control and energy cost saving performance of a large-scale group\nof HVAC systems in both single-zone and multi-zone scenarios, under varying\nclimate and electricity market conditions. The experiment results indicate that\nproposed approach outperforms the centralized methods in the majority of test\nscenarios, and performs comparably to model-based method in some scenarios.",
          "arxiv_id": "2306.16619v1"
        },
        {
          "title": "Battery and Hydrogen Energy Storage Control in a Smart Energy Network with Flexible Energy Demand using Deep Reinforcement Learning",
          "year": "2022-08",
          "abstract": "Smart energy networks provide for an effective means to accommodate high\npenetrations of variable renewable energy sources like solar and wind, which\nare key for deep decarbonisation of energy production. However, given the\nvariability of the renewables as well as the energy demand, it is imperative to\ndevelop effective control and energy storage schemes to manage the variable\nenergy generation and achieve desired system economics and environmental goals.\nIn this paper, we introduce a hybrid energy storage system composed of battery\nand hydrogen energy storage to handle the uncertainties related to electricity\nprices, renewable energy production and consumption. We aim to improve\nrenewable energy utilisation and minimise energy costs and carbon emissions\nwhile ensuring energy reliability and stability within the network. To achieve\nthis, we propose a multi-agent deep deterministic policy gradient approach,\nwhich is a deep reinforcement learning-based control strategy to optimise the\nscheduling of the hybrid energy storage system and energy demand in real-time.\nThe proposed approach is model-free and does not require explicit knowledge and\nrigorous mathematical models of the smart energy network environment.\nSimulation results based on real-world data show that: (i) integration and\noptimised operation of the hybrid energy storage system and energy demand\nreduces carbon emissions by 78.69%, improves cost savings by 23.5% and\nrenewable energy utilisation by over 13.2% compared to other baseline models\nand (ii) the proposed algorithm outperforms the state-of-the-art self-learning\nalgorithms like deep-Q network.",
          "arxiv_id": "2208.12779v1"
        }
      ],
      "48": [
        {
          "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization",
          "year": "2023-02",
          "abstract": "Text summarization has been a crucial problem in natural language processing\n(NLP) for several decades. It aims to condense lengthy documents into shorter\nversions while retaining the most critical information. Various methods have\nbeen proposed for text summarization, including extractive and abstractive\nsummarization. The emergence of large language models (LLMs) like GPT3 and\nChatGPT has recently created significant interest in using these models for\ntext summarization tasks. Recent studies \\cite{goyal2022news,\nzhang2023benchmarking} have shown that LLMs-generated news summaries are\nalready on par with humans. However, the performance of LLMs for more practical\napplications like aspect or query-based summaries is underexplored. To fill\nthis gap, we conducted an evaluation of ChatGPT's performance on four widely\nused benchmark datasets, encompassing diverse summaries from Reddit posts, news\narticles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's\nperformance is comparable to traditional fine-tuning methods in terms of Rouge\nscores. Moreover, we highlight some unique differences between\nChatGPT-generated summaries and human references, providing valuable insights\ninto the superpower of ChatGPT for diverse text summarization tasks. Our\nfindings call for new directions in this area, and we plan to conduct further\nresearch to systematically examine the characteristics of ChatGPT-generated\nsummaries through extensive human evaluation.",
          "arxiv_id": "2302.08081v1"
        },
        {
          "title": "Bengali Abstractive News Summarization(BANS): A Neural Attention Approach",
          "year": "2020-12",
          "abstract": "Abstractive summarization is the process of generating novel sentences based\non the information extracted from the original text document while retaining\nthe context. Due to abstractive summarization's underlying complexities, most\nof the past research work has been done on the extractive summarization\napproach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq)\nmodel, abstractive summarization becomes more viable. Although a significant\nnumber of notable research has been done in the English language based on\nabstractive summarization, only a couple of works have been done on Bengali\nabstractive news summarization (BANS). In this article, we presented a seq2seq\nbased Long Short-Term Memory (LSTM) network model with attention at\nencoder-decoder. Our proposed system deploys a local attention-based model that\nproduces a long sequence of words with lucid and human-like generated sentences\nwith noteworthy information of the original document. We also prepared a\ndataset of more than 19k articles and corresponding human-written summaries\ncollected from bangla.bdnews24.com1 which is till now the most extensive\ndataset for Bengali news document summarization and publicly published in\nKaggle2. We evaluated our model qualitatively and quantitatively and compared\nit with other published results. It showed significant improvement in terms of\nhuman evaluation scores with state-of-the-art approaches for BANS.",
          "arxiv_id": "2012.01747v1"
        },
        {
          "title": "Event-Keyed Summarization",
          "year": "2024-02",
          "abstract": "We introduce event-keyed summarization (EKS), a novel task that marries\ntraditional summarization and document-level event extraction, with the goal of\ngenerating a contextualized summary for a specific event, given a document and\nan extracted event structure. We introduce a dataset for this task, MUCSUM,\nconsisting of summaries of all events in the classic MUC-4 dataset, along with\na set of baselines that comprises both pretrained LM standards in the\nsummarization literature, as well as larger frontier models. We show that\nablations that reduce EKS to traditional summarization or structure-to-text\nyield inferior summaries of target events and that MUCSUM is a robust benchmark\nfor this task. Lastly, we conduct a human evaluation of both reference and\nmodel summaries, and provide some detailed analysis of the results.",
          "arxiv_id": "2402.06973v1"
        }
      ],
      "49": [
        {
          "title": "Do Weibo platform experts perform better at predicting stock market?",
          "year": "2024-02",
          "abstract": "Sentiment analysis can be used for stock market prediction. However, existing\nresearch has not studied the impact of a user's financial background on\nsentiment-based forecasting of the stock market using artificial neural\nnetworks. In this work, a novel combination of neural networks is used for the\nassessment of sentiment-based stock market prediction, based on the financial\nbackground of the population that generated the sentiment. The state-of-the-art\nlanguage processing model Bidirectional Encoder Representations from\nTransformers (BERT) is used to classify the sentiment and a Long-Short Term\nMemory (LSTM) model is used for time-series based stock market prediction. For\nevaluation, the Weibo social networking platform is used as a sentiment data\ncollection source. Weibo users (and their comments respectively) are divided\ninto Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor\n(UFA) groups according to their background information, as collected by Weibo.\nThe Hong Kong Hang Seng index is used to extract historical stock market change\ndata. The results indicate that stock market prediction learned from the AFA\ngroup users is 39.67% more precise than that learned from the UFA group users\nand shows the highest accuracy (87%) when compared to existing approaches.",
          "arxiv_id": "2403.00772v1"
        },
        {
          "title": "DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture Fleeting Intraday Trading Opportunities",
          "year": "2021-12",
          "abstract": "Reinforcement learning (RL) techniques have shown great success in many\nchallenging quantitative trading tasks, such as portfolio management and\nalgorithmic trading. Especially, intraday trading is one of the most profitable\nand risky tasks because of the intraday behaviors of the financial market that\nreflect billions of rapidly fluctuating capitals. However, a vast majority of\nexisting RL methods focus on the relatively low frequency trading scenarios\n(e.g., day-level) and fail to capture the fleeting intraday investment\nopportunities due to two major challenges: 1) how to effectively train\nprofitable RL agents for intraday investment decision-making, which involves\nhigh-dimensional fine-grained action space; 2) how to learn meaningful\nmulti-modality market representation to understand the intraday behaviors of\nthe financial market at tick-level. Motivated by the efficient workflow of\nprofessional human intraday traders, we propose DeepScalper, a deep\nreinforcement learning framework for intraday trading to tackle the above\nchallenges. Specifically, DeepScalper includes four components: 1) a dueling\nQ-network with action branching to deal with the large action space of intraday\ntrading for efficient RL optimization; 2) a novel reward function with a\nhindsight bonus to encourage RL agents making trading decisions with a\nlong-term horizon of the entire trading day; 3) an encoder-decoder architecture\nto learn multi-modality temporal market embedding, which incorporates both\nmacro-level and micro-level market information; 4) a risk-aware auxiliary task\nto maintain a striking balance between maximizing profit and minimizing risk.\nThrough extensive experiments on real-world market data spanning over three\nyears on six financial futures, we demonstrate that DeepScalper significantly\noutperforms many state-of-the-art baselines in terms of four financial\ncriteria.",
          "arxiv_id": "2201.09058v3"
        },
        {
          "title": "Predicting Stock Prices with FinBERT-LSTM: Integrating News Sentiment Analysis",
          "year": "2024-07",
          "abstract": "The stock market's ascent typically mirrors the flourishing state of the\neconomy, whereas its decline is often an indicator of an economic downturn.\nTherefore, for a long time, significant correlation elements for predicting\ntrends in financial stock markets have been widely discussed, and people are\nbecoming increasingly interested in the task of financial text mining. The\ninherent instability of stock prices makes them acutely responsive to\nfluctuations within the financial markets. In this article, we use deep\nlearning networks, based on the history of stock prices and articles of\nfinancial, business, technical news that introduce market information to\npredict stock prices. We illustrate the enhancement of predictive precision by\nintegrating weighted news categories into the forecasting model. We developed a\npre-trained NLP model known as FinBERT, designed to discern the sentiments\nwithin financial texts. Subsequently, we advanced this model by incorporating\nthe sophisticated Long Short Term Memory (LSTM) architecture, thus constructing\nthe innovative FinBERT-LSTM model. This model utilizes news categories related\nto the stock market structure hierarchy, namely market, industry, and stock\nrelated news categories, combined with the stock market's stock price situation\nin the previous week for prediction. We selected NASDAQ-100 index stock data\nand trained the model on Benzinga news articles, and utilized Mean Absolute\nError (MAE), Mean Absolute Percentage Error (MAPE), and Accuracy as the key\nmetrics for the assessment and comparative analysis of the model's performance.\nThe results indicate that FinBERT-LSTM performs the best, followed by LSTM, and\nDNN model ranks third in terms of effectiveness.",
          "arxiv_id": "2407.16150v1"
        }
      ],
      "50": [
        {
          "title": "Towards an end-to-end artificial intelligence driven global weather forecasting system",
          "year": "2023-12",
          "abstract": "The weather forecasting system is important for science and society, and\nsignificant achievements have been made in applying artificial intelligence\n(AI) to medium-range weather forecasting. However, existing AI-based weather\nforecasting models rely on analysis or reanalysis products from traditional\nnumerical weather prediction (NWP) systems as initial conditions for making\npredictions. Initial states are typically generated by traditional data\nassimilation components, which are computational expensive and time-consuming.\nHere we present an AI-based data assimilation model, i.e., Adas, for global\nweather variables. By introducing the confidence matrix, Adas employs gated\nconvolution to handle sparse observations and gated cross-attention for\ncapturing the interactions between the background and observations. Further, we\ncombine Adas with the advanced AI-based forecasting model (i.e., FengWu) to\nconstruct the first end-to-end AI-based global weather forecasting system:\nFengWu-Adas. We demonstrate that Adas can assimilate global observations to\nproduce high-quality analysis, enabling the system operate stably for long\nterm. Moreover, we are the first to apply the methods to real-world scenarios,\nwhich is more challenging and has considerable practical application potential.\nWe have also achieved the forecasts based on the analyses generated by AI with\na skillful forecast lead time exceeding that of the IFS for the first time.",
          "arxiv_id": "2312.12462v3"
        },
        {
          "title": "FengWu-4DVar: Coupling the Data-driven Weather Forecasting Model with 4D Variational Assimilation",
          "year": "2023-12",
          "abstract": "Weather forecasting is a crucial yet highly challenging task. With the\nmaturity of Artificial Intelligence (AI), the emergence of data-driven weather\nforecasting models has opened up a new paradigm for the development of weather\nforecasting systems. Despite the significant successes that have been achieved\n(e.g., surpassing advanced traditional physical models for global medium-range\nforecasting), existing data-driven weather forecasting models still rely on the\nanalysis fields generated by the traditional assimilation and forecasting\nsystem, which hampers the significance of data-driven weather forecasting\nmodels regarding both computational cost and forecasting accuracy. In this\nwork, we explore the possibility of coupling the data-driven weather\nforecasting model with data assimilation by integrating the global AI weather\nforecasting model, FengWu, with one of the most popular assimilation\nalgorithms, Four-Dimensional Variational (4DVar) assimilation, and develop an\nAI-based cyclic weather forecasting system, FengWu-4DVar. FengWu-4DVar can\nincorporate observational data into the data-driven weather forecasting model\nand consider the temporal evolution of atmospheric dynamics to obtain accurate\nanalysis fields for making predictions in a cycling manner without the help of\nphysical models. Owning to the auto-differentiation ability of deep learning\nmodels, FengWu-4DVar eliminates the need of developing the cumbersome adjoint\nmodel, which is usually required in the traditional implementation of the 4DVar\nalgorithm. Experiments on the simulated observational dataset demonstrate that\nFengWu-4DVar is capable of generating reasonable analysis fields for making\naccurate and efficient iterative predictions.",
          "arxiv_id": "2312.12455v2"
        },
        {
          "title": "FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather Forecasting",
          "year": "2024-01",
          "abstract": "Kilometer-scale modeling of global atmosphere dynamics enables fine-grained\nweather forecasting and decreases the risk of disastrous weather and climate\nactivity. Therefore, building a kilometer-scale global forecast model is a\npersistent pursuit in the meteorology domain. Active international efforts have\nbeen made in past decades to improve the spatial resolution of numerical\nweather models. Nonetheless, developing the higher resolution numerical model\nremains a long-standing challenge due to the substantial consumption of\ncomputational resources. Recent advances in data-driven global weather\nforecasting models utilize reanalysis data for model training and have\ndemonstrated comparable or even higher forecasting skills than numerical\nmodels. However, they are all limited by the resolution of reanalysis data and\nincapable of generating higher-resolution forecasts. This work presents\nFengWu-GHR, the first data-driven global weather forecasting model running at\nthe 0.09$^{\\circ}$ horizontal resolution. FengWu-GHR introduces a novel\napproach that opens the door for operating ML-based high-resolution forecasts\nby inheriting prior knowledge from a pretrained low-resolution model. The\nhindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to\nthe IFS-HRES. Furthermore, evaluations on station observations and case studies\nof extreme events support the competitive operational forecasting skill of\nFengWu-GHR at the high resolution.",
          "arxiv_id": "2402.00059v1"
        }
      ],
      "51": [
        {
          "title": "Doubly Adversarial Federated Bandits",
          "year": "2023-01",
          "abstract": "We study a new non-stochastic federated multi-armed bandit problem with\nmultiple agents collaborating via a communication network. The losses of the\narms are assigned by an oblivious adversary that specifies the loss of each arm\nnot only for each time step but also for each agent, which we call ``doubly\nadversarial\". In this setting, different agents may choose the same arm in the\nsame time step but observe different feedback. The goal of each agent is to\nfind a globally best arm in hindsight that has the lowest cumulative loss\naveraged over all agents, which necessities the communication among agents. We\nprovide regret lower bounds for any federated bandit algorithm under different\nsettings, when agents have access to full-information feedback, or the bandit\nfeedback. For the bandit feedback setting, we propose a near-optimal federated\nbandit algorithm called FEDEXP3. Our algorithm gives a positive answer to an\nopen question proposed in Cesa-Bianchi et al. (2016): FEDEXP3 can guarantee a\nsub-linear regret without exchanging sequences of selected arm identities or\nloss sequences among agents. We also provide numerical evaluations of our\nalgorithm to validate our theoretical results and demonstrate its effectiveness\non synthetic and real-world datasets",
          "arxiv_id": "2301.09223v2"
        },
        {
          "title": "Complete Policy Regret Bounds for Tallying Bandits",
          "year": "2022-04",
          "abstract": "Policy regret is a well established notion of measuring the performance of an\nonline learning algorithm against an adaptive adversary. We study restrictions\non the adversary that enable efficient minimization of the \\emph{complete\npolicy regret}, which is the strongest possible version of policy regret. We\nidentify a gap in the current theoretical understanding of what sorts of\nrestrictions permit tractability in this challenging setting. To resolve this\ngap, we consider a generalization of the stochastic multi armed bandit, which\nwe call the \\emph{tallying bandit}. This is an online learning setting with an\n$m$-memory bounded adversary, where the average loss for playing an action is\nan unknown function of the number (or tally) of times that the action was\nplayed in the last $m$ timesteps. For tallying bandit problems with $K$ actions\nand time horizon $T$, we provide an algorithm that w.h.p achieves a complete\npolicy regret guarantee of $\\tilde{\\mathcal{O}}(mK\\sqrt{T})$, where the\n$\\tilde{\\mathcal{O}}$ notation hides only logarithmic factors. We additionally\nprove an $\\tilde\\Omega(\\sqrt{m K T})$ lower bound on the expected complete\npolicy regret of any tallying bandit algorithm, demonstrating the near\noptimality of our method.",
          "arxiv_id": "2204.11174v1"
        },
        {
          "title": "Influential Bandits: Pulling an Arm May Change the Environment",
          "year": "2025-04",
          "abstract": "While classical formulations of multi-armed bandit problems assume that each\narm's reward is independent and stationary, real-world applications often\ninvolve non-stationary environments and interdependencies between arms. In\nparticular, selecting one arm may influence the future rewards of other arms, a\nscenario not adequately captured by existing models such as rotting bandits or\nrestless bandits. To address this limitation, we propose the influential bandit\nproblem, which models inter-arm interactions through an unknown, symmetric,\npositive semi-definite interaction matrix that governs the dynamics of arm\nlosses. We formally define this problem and establish two regret lower bounds,\nincluding a superlinear $\\Omega(T^2 / \\log^2 T)$ bound for the standard LCB\nalgorithm (loss minimization version of UCB) and an algorithm-independent\n$\\Omega(T)$ bound, which highlight the inherent difficulty of the setting. We\nthen introduce a new algorithm based on a lower confidence bound (LCB)\nestimator tailored to the structure of the loss dynamics. Under mild\nassumptions, our algorithm achieves a regret of $O(KT \\log T)$, which is nearly\noptimal in terms of its dependence on the time horizon. The algorithm is simple\nto implement and computationally efficient. Empirical evaluations on both\nsynthetic and real-world datasets demonstrate the presence of inter-arm\ninfluence and confirm the superior performance of our method compared to\nconventional bandit algorithms.",
          "arxiv_id": "2504.08200v2"
        }
      ],
      "52": [
        {
          "title": "Leveraging Retrieval Augment Approach for Multimodal Emotion Recognition Under Missing Modalities",
          "year": "2024-09",
          "abstract": "Multimodal emotion recognition utilizes complete multimodal information and\nrobust multimodal joint representation to gain high performance. However, the\nideal condition of full modality integrity is often not applicable in reality\nand there always appears the situation that some modalities are missing. For\nexample, video, audio, or text data is missing due to sensor failure or network\nbandwidth problems, which presents a great challenge to MER research.\nTraditional methods extract useful information from the complete modalities and\nreconstruct the missing modalities to learn robust multimodal joint\nrepresentation. These methods have laid a solid foundation for research in this\nfield, and to a certain extent, alleviated the difficulty of multimodal emotion\nrecognition under missing modalities. However, relying solely on internal\nreconstruction and multimodal joint learning has its limitations, especially\nwhen the missing information is critical for emotion recognition. To address\nthis challenge, we propose a novel framework of Retrieval Augment for Missing\nModality Multimodal Emotion Recognition (RAMER), which introduces similar\nmultimodal emotion data to enhance the performance of emotion recognition under\nmissing modalities. By leveraging databases, that contain related multimodal\nemotion data, we can retrieve similar multimodal emotion information to fill in\nthe gaps left by missing modalities. Various experimental results demonstrate\nthat our framework is superior to existing state-of-the-art approaches in\nmissing modality MER tasks. Our whole project is publicly available on\nhttps://github.com/WooyoohL/Retrieval_Augment_MER.",
          "arxiv_id": "2410.02804v1"
        },
        {
          "title": "FAF: A novel multimodal emotion recognition approach integrating face, body and text",
          "year": "2022-11",
          "abstract": "Multimodal emotion analysis performed better in emotion recognition depending\non more comprehensive emotional clues and multimodal emotion dataset. In this\npaper, we developed a large multimodal emotion dataset, named \"HED\" dataset, to\nfacilitate the emotion recognition task, and accordingly propose a multimodal\nemotion recognition method. To promote recognition accuracy, \"Feature After\nFeature\" framework was used to explore crucial emotional information from the\naligned face, body and text samples. We employ various benchmarks to evaluate\nthe \"HED\" dataset and compare the performance with our method. The results show\nthat the five classification accuracy of the proposed multimodal fusion method\nis about 83.75%, and the performance is improved by 1.83%, 9.38%, and 21.62%\nrespectively compared with that of individual modalities. The complementarity\nbetween each channel is effectively used to improve the performance of emotion\nrecognition. We had also established a multimodal online emotion prediction\nplatform, aiming to provide free emotion prediction to more users.",
          "arxiv_id": "2211.15425v1"
        },
        {
          "title": "Emotion Correlation Mining Through Deep Learning Models on Natural Language Text",
          "year": "2020-07",
          "abstract": "Emotion analysis has been attracting researchers' attention. Most previous\nworks in the artificial intelligence field focus on recognizing emotion rather\nthan mining the reason why emotions are not or wrongly recognized. Correlation\namong emotions contributes to the failure of emotion recognition. In this\npaper, we try to fill the gap between emotion recognition and emotion\ncorrelation mining through natural language text from web news. Correlation\namong emotions, expressed as the confusion and evolution of emotion, is\nprimarily caused by human emotion cognitive bias. To mine emotion correlation\nfrom emotion recognition through text, three kinds of features and two deep\nneural network models are presented. The emotion confusion law is extracted\nthrough orthogonal basis. The emotion evolution law is evaluated from three\nperspectives, one-step shift, limited-step shifts, and shortest path transfer.\nThe method is validated using three datasets-the titles, the bodies, and the\ncomments of news articles, covering both objective and subjective texts in\nvarying lengths (long and short). The experimental results show that, in\nsubjective comments, emotions are easily mistaken as anger. Comments tend to\narouse emotion circulations of love-anger and sadness-anger. In objective news,\nit is easy to recognize text emotion as love and cause fear-joy circulation.\nThat means, journalists may try to attract attention using fear and joy words\nbut arouse the emotion love instead; After news release, netizens generate\nemotional comments to express their intense emotions, i.e., anger, sadness, and\nlove. These findings could provide insights for applications regarding\naffective interaction such as network public sentiment, social media\ncommunication, and human-computer interaction.",
          "arxiv_id": "2007.14071v1"
        }
      ],
      "53": [
        {
          "title": "A Close Look at Spatial Modeling: From Attention to Convolution",
          "year": "2022-12",
          "abstract": "Vision Transformers have shown great promise recently for many vision tasks\ndue to the insightful architecture design and attention mechanism. By\nrevisiting the self-attention responses in Transformers, we empirically observe\ntwo interesting issues. First, Vision Transformers present a queryirrelevant\nbehavior at deep layers, where the attention maps exhibit nearly consistent\ncontexts in global scope, regardless of the query patch position (also\nhead-irrelevant). Second, the attention maps are intrinsically sparse, few\ntokens dominate the attention weights; introducing the knowledge from ConvNets\nwould largely smooth the attention and enhance the performance. Motivated by\nabove observations, we generalize self-attention formulation to abstract a\nqueryirrelevant global context directly and further integrate the global\ncontext into convolutions. The resulting model, a Fully Convolutional Vision\nTransformer (i.e., FCViT), purely consists of convolutional layers and firmly\ninherits the merits of both attention mechanism and convolutions, including\ndynamic property, weight sharing, and short- and long-range feature modeling,\netc. Experimental results demonstrate the effectiveness of FCViT. With less\nthan 14M parameters, our FCViT-S12 outperforms related work ResT-Lite by 3.7%\ntop1 accuracy on ImageNet-1K. When scaling FCViT to larger models, we still\nperform better than previous state-of-the-art ConvNeXt with even fewer\nparameters. FCViT-based models also demonstrate promising transferability to\ndownstream tasks, like object detection, instance segmentation, and semantic\nsegmentation. Codes and models are made available at:\nhttps://github.com/ma-xu/FCViT.",
          "arxiv_id": "2212.12552v1"
        },
        {
          "title": "Plug n' Play: Channel Shuffle Module for Enhancing Tiny Vision Transformers",
          "year": "2023-10",
          "abstract": "Vision Transformers (ViTs) have demonstrated remarkable performance in\nvarious computer vision tasks. However, the high computational complexity\nhinders ViTs' applicability on devices with limited memory and computing\nresources. Although certain investigations have delved into the fusion of\nconvolutional layers with self-attention mechanisms to enhance the efficiency\nof ViTs, there remains a knowledge gap in constructing tiny yet effective ViTs\nsolely based on the self-attention mechanism. Furthermore, the straightforward\nstrategy of reducing the feature channels in a large but outperforming ViT\noften results in significant performance degradation despite improved\nefficiency. To address these challenges, we propose a novel channel shuffle\nmodule to improve tiny-size ViTs, showing the potential of pure self-attention\nmodels in environments with constrained computing resources. Inspired by the\nchannel shuffle design in ShuffleNetV2 \\cite{ma2018shufflenet}, our module\nexpands the feature channels of a tiny ViT and partitions the channels into two\ngroups: the \\textit{Attended} and \\textit{Idle} groups. Self-attention\ncomputations are exclusively employed on the designated \\textit{Attended}\ngroup, followed by a channel shuffle operation that facilitates information\nexchange between the two groups. By incorporating our module into a tiny ViT,\nwe can achieve superior performance while maintaining a comparable\ncomputational complexity to the vanilla model. Specifically, our proposed\nchannel shuffle module consistently improves the top-1 accuracy on the\nImageNet-1K dataset for various tiny ViT models by up to 2.8\\%, with the\nchanges in model complexity being less than 0.03 GMACs.",
          "arxiv_id": "2310.05642v1"
        },
        {
          "title": "The Linear Attention Resurrection in Vision Transformer",
          "year": "2025-01",
          "abstract": "Vision Transformers (ViTs) have recently taken computer vision by storm.\nHowever, the softmax attention underlying ViTs comes with a quadratic\ncomplexity in time and memory, hindering the application of ViTs to\nhigh-resolution images. We revisit the attention design and propose a linear\nattention method to address the limitation, which doesn't sacrifice ViT's core\nadvantage of capturing global representation like existing methods (e.g. local\nwindow attention of Swin). We further investigate the key difference between\nlinear attention and softmax attention. Our empirical results suggest that\nlinear attention lacks a fundamental property of concentrating the distribution\nof the attention matrix. Inspired by this observation, we introduce a local\nconcentration module to enhance linear attention. By incorporating enhanced\nlinear global attention and local window attention, we propose a new ViT\narchitecture, dubbed L$^2$ViT. Notably, L$^2$ViT can effectively capture both\nglobal interactions and local representations while enjoying linear\ncomputational complexity. Extensive experiments demonstrate the strong\nperformance of L$^2$ViT. On image classification, L$^2$ViT achieves 84.4% Top-1\naccuracy on ImageNet-1K without any extra training data or label. By further\npre-training on ImageNet-22k, it attains 87.0% when fine-tuned with resolution\n384$^2$. For downstream tasks, L$^2$ViT delivers favorable performance as a\nbackbone on object detection as well as semantic segmentation.",
          "arxiv_id": "2501.16182v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:20:41Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}