{
  "topics": {
    "data": {
      "0": {
        "name": "0_image_resolution_quality_Image",
        "keywords": [
          [
            "image",
            0.012452346053056926
          ],
          [
            "resolution",
            0.010170802331491714
          ],
          [
            "quality",
            0.009546705837598637
          ],
          [
            "Image",
            0.008503758547683447
          ],
          [
            "images",
            0.008500317376251123
          ],
          [
            "low",
            0.008144907675743742
          ],
          [
            "restoration",
            0.008084245974008275
          ],
          [
            "compression",
            0.007895388336300928
          ],
          [
            "super",
            0.007576942605740909
          ],
          [
            "super resolution",
            0.007299170899568569
          ]
        ],
        "count": 7177
      },
      "1": {
        "name": "1_pruning_training_networks_accuracy",
        "keywords": [
          [
            "pruning",
            0.009322540368701539
          ],
          [
            "training",
            0.00872646706087877
          ],
          [
            "networks",
            0.008650356303713165
          ],
          [
            "accuracy",
            0.008025828724379446
          ],
          [
            "ImageNet",
            0.007908834423647353
          ],
          [
            "learning",
            0.007572735741373181
          ],
          [
            "neural",
            0.007425695585062365
          ],
          [
            "performance",
            0.007332022112221409
          ],
          [
            "network",
            0.007134423471660611
          ],
          [
            "classification",
            0.006914037476613721
          ]
        ],
        "count": 5832
      },
      "2": {
        "name": "2_visual_language_reasoning_Language",
        "keywords": [
          [
            "visual",
            0.017103154795389028
          ],
          [
            "language",
            0.016465361304206297
          ],
          [
            "reasoning",
            0.014024868499514398
          ],
          [
            "Language",
            0.012442622886089121
          ],
          [
            "text",
            0.011825809391233334
          ],
          [
            "multimodal",
            0.011045651057174363
          ],
          [
            "VQA",
            0.010224142463868333
          ],
          [
            "vision",
            0.0099541845662091
          ],
          [
            "models",
            0.00989539077911173
          ],
          [
            "Models",
            0.009075542407533548
          ]
        ],
        "count": 4377
      },
      "3": {
        "name": "3_remote sensing_sensing_remote_imagery",
        "keywords": [
          [
            "remote sensing",
            0.014636110119580143
          ],
          [
            "sensing",
            0.01448331554118207
          ],
          [
            "remote",
            0.014441757712578933
          ],
          [
            "imagery",
            0.010361091776741285
          ],
          [
            "change",
            0.010280354981111904
          ],
          [
            "satellite",
            0.009794573861766851
          ],
          [
            "data",
            0.008992740423512228
          ],
          [
            "crop",
            0.008257868783044686
          ],
          [
            "images",
            0.007596567201392323
          ],
          [
            "segmentation",
            0.007368909895875921
          ]
        ],
        "count": 3156
      },
      "4": {
        "name": "4_diffusion_text_generation_Diffusion",
        "keywords": [
          [
            "diffusion",
            0.02256716497761443
          ],
          [
            "text",
            0.018724994250604508
          ],
          [
            "generation",
            0.01835030635302517
          ],
          [
            "Diffusion",
            0.01744296844341268
          ],
          [
            "diffusion models",
            0.015428197545645924
          ],
          [
            "image",
            0.014986945639612264
          ],
          [
            "models",
            0.013525433672166418
          ],
          [
            "text image",
            0.013483109088541438
          ],
          [
            "image generation",
            0.012863421304936893
          ],
          [
            "style",
            0.01230892619203552
          ]
        ],
        "count": 3064
      },
      "5": {
        "name": "5_adversarial_attacks_attack_robustness",
        "keywords": [
          [
            "adversarial",
            0.04833213685295604
          ],
          [
            "attacks",
            0.035876869612987916
          ],
          [
            "attack",
            0.028011253751878757
          ],
          [
            "robustness",
            0.01880017649106496
          ],
          [
            "Adversarial",
            0.01839123224813055
          ],
          [
            "adversarial examples",
            0.01447152630712815
          ],
          [
            "adversarial attacks",
            0.014432493197896138
          ],
          [
            "examples",
            0.01418183711958447
          ],
          [
            "perturbations",
            0.013371911383053625
          ],
          [
            "defense",
            0.012201011984761477
          ]
        ],
        "count": 2835
      },
      "6": {
        "name": "6_robot_navigation_manipulation_agent",
        "keywords": [
          [
            "robot",
            0.018955308354092437
          ],
          [
            "navigation",
            0.015146751649525496
          ],
          [
            "manipulation",
            0.014040477813347161
          ],
          [
            "agent",
            0.012398215683776175
          ],
          [
            "environments",
            0.011736259661684909
          ],
          [
            "robotic",
            0.011362983188078291
          ],
          [
            "world",
            0.011146303855689503
          ],
          [
            "tasks",
            0.010649627381652373
          ],
          [
            "object",
            0.010473508743557597
          ],
          [
            "policy",
            0.010282861628801584
          ]
        ],
        "count": 2786
      },
      "7": {
        "name": "7_action_video_action recognition_Action",
        "keywords": [
          [
            "action",
            0.04022079535940792
          ],
          [
            "video",
            0.019465513978882184
          ],
          [
            "action recognition",
            0.019404532009085863
          ],
          [
            "Action",
            0.01813946125941251
          ],
          [
            "temporal",
            0.018031594928337866
          ],
          [
            "recognition",
            0.016790536970828596
          ],
          [
            "actions",
            0.01342567150169399
          ],
          [
            "videos",
            0.01062541080448298
          ],
          [
            "skeleton",
            0.01002492661252908
          ],
          [
            "Recognition",
            0.009383672812203368
          ]
        ],
        "count": 2566
      },
      "8": {
        "name": "8_cancer_cell_breast_slide",
        "keywords": [
          [
            "cancer",
            0.020782990365570665
          ],
          [
            "cell",
            0.017185758780301614
          ],
          [
            "breast",
            0.015434371986391353
          ],
          [
            "slide",
            0.012803397815122372
          ],
          [
            "tissue",
            0.011427865626450814
          ],
          [
            "images",
            0.0105545636611369
          ],
          [
            "MIL",
            0.009429263157784635
          ],
          [
            "classification",
            0.00938915274406531
          ],
          [
            "segmentation",
            0.009322365991407974
          ],
          [
            "histopathology",
            0.008799019145934103
          ]
        ],
        "count": 2413
      },
      "9": {
        "name": "9_rendering_NeRF_Gaussian_view",
        "keywords": [
          [
            "rendering",
            0.029679069155788454
          ],
          [
            "NeRF",
            0.028037762194237845
          ],
          [
            "Gaussian",
            0.02687147973454012
          ],
          [
            "view",
            0.019952031652761628
          ],
          [
            "scene",
            0.018649231309210305
          ],
          [
            "Splatting",
            0.017885950381494594
          ],
          [
            "3D",
            0.015960578918647616
          ],
          [
            "view synthesis",
            0.01575839992453587
          ],
          [
            "scenes",
            0.015518005981857095
          ],
          [
            "3DGS",
            0.014599016875111283
          ]
        ],
        "count": 1990
      },
      "10": {
        "name": "10_3D_LiDAR_3D object_3D object detection",
        "keywords": [
          [
            "3D",
            0.02603703154707563
          ],
          [
            "LiDAR",
            0.025930298323107517
          ],
          [
            "3D object",
            0.019544391436069685
          ],
          [
            "3D object detection",
            0.017717178607394576
          ],
          [
            "object",
            0.016529069426365246
          ],
          [
            "detection",
            0.01639879271338719
          ],
          [
            "object detection",
            0.015835157661716198
          ],
          [
            "point",
            0.014006592906767344
          ],
          [
            "autonomous",
            0.011689973680405705
          ],
          [
            "driving",
            0.011634868096157637
          ]
        ],
        "count": 1953
      },
      "11": {
        "name": "11_driving_trajectory_traffic_prediction",
        "keywords": [
          [
            "driving",
            0.029885638403336627
          ],
          [
            "trajectory",
            0.020993288166801412
          ],
          [
            "traffic",
            0.02021145069266129
          ],
          [
            "prediction",
            0.016797923553690527
          ],
          [
            "autonomous",
            0.016781167556078166
          ],
          [
            "vehicle",
            0.014215774270375762
          ],
          [
            "autonomous driving",
            0.013481153832844733
          ],
          [
            "trajectories",
            0.013359844042960426
          ],
          [
            "trajectory prediction",
            0.011577063201854941
          ],
          [
            "vehicles",
            0.01027798065283799
          ]
        ],
        "count": 1870
      },
      "12": {
        "name": "12_video_Video_videos_temporal",
        "keywords": [
          [
            "video",
            0.04450604610061579
          ],
          [
            "Video",
            0.025944533770265506
          ],
          [
            "videos",
            0.015797650848687398
          ],
          [
            "temporal",
            0.014690460245181709
          ],
          [
            "understanding",
            0.013981602014454036
          ],
          [
            "language",
            0.01229693984040292
          ],
          [
            "video understanding",
            0.01209002360821414
          ],
          [
            "text",
            0.01082969641744625
          ],
          [
            "reasoning",
            0.01038309262322666
          ],
          [
            "retrieval",
            0.010025267073433907
          ]
        ],
        "count": 1864
      },
      "13": {
        "name": "13_face_deepfake_detection_fake",
        "keywords": [
          [
            "face",
            0.021852596340828506
          ],
          [
            "deepfake",
            0.020538286148219394
          ],
          [
            "detection",
            0.01671819557410676
          ],
          [
            "fake",
            0.013245921119341626
          ],
          [
            "Face",
            0.012706537491868406
          ],
          [
            "Deepfake",
            0.012340897038453829
          ],
          [
            "attacks",
            0.011212016957545628
          ],
          [
            "images",
            0.010423459080319298
          ],
          [
            "Detection",
            0.01039638477261408
          ],
          [
            "attack",
            0.00972037839472165
          ]
        ],
        "count": 1744
      },
      "14": {
        "name": "14_text_document_OCR_recognition",
        "keywords": [
          [
            "text",
            0.02881775179100129
          ],
          [
            "document",
            0.022968796210084515
          ],
          [
            "OCR",
            0.017293691465713166
          ],
          [
            "recognition",
            0.017078408410465984
          ],
          [
            "documents",
            0.015181729206684078
          ],
          [
            "handwritten",
            0.01325308021262768
          ],
          [
            "character",
            0.012936966226863748
          ],
          [
            "Text",
            0.012386679136157083
          ],
          [
            "characters",
            0.010951772568156598
          ],
          [
            "text recognition",
            0.010944776626985448
          ]
        ],
        "count": 1729
      },
      "15": {
        "name": "15_segmentation_medical_image segmentation_medical image",
        "keywords": [
          [
            "segmentation",
            0.03160058088637997
          ],
          [
            "medical",
            0.028076273624902906
          ],
          [
            "image segmentation",
            0.022364127843888734
          ],
          [
            "medical image",
            0.021632743635279568
          ],
          [
            "medical image segmentation",
            0.01982768759131387
          ],
          [
            "Medical",
            0.013961495212448612
          ],
          [
            "Segmentation",
            0.013035760977391142
          ],
          [
            "image",
            0.00970646284944141
          ],
          [
            "supervised",
            0.009459937124547192
          ],
          [
            "Net",
            0.008875285951367799
          ]
        ],
        "count": 1728
      },
      "16": {
        "name": "16_depth_depth estimation_estimation_Depth",
        "keywords": [
          [
            "depth",
            0.051942160830640106
          ],
          [
            "depth estimation",
            0.0227985387497031
          ],
          [
            "estimation",
            0.02238305979680713
          ],
          [
            "Depth",
            0.021094369663798348
          ],
          [
            "stereo",
            0.017848197350558887
          ],
          [
            "flow",
            0.017518083072655816
          ],
          [
            "monocular",
            0.012274582229762307
          ],
          [
            "optical flow",
            0.010087959746929198
          ],
          [
            "monocular depth",
            0.010045754812249251
          ],
          [
            "Stereo",
            0.00964240700547703
          ]
        ],
        "count": 1715
      },
      "17": {
        "name": "17_anomaly_detection_defect_Anomaly",
        "keywords": [
          [
            "anomaly",
            0.029855864613029248
          ],
          [
            "detection",
            0.02035346389791253
          ],
          [
            "defect",
            0.018844316795678592
          ],
          [
            "Anomaly",
            0.01615793160933393
          ],
          [
            "anomalies",
            0.012712737273349699
          ],
          [
            "industrial",
            0.012287487633063377
          ],
          [
            "inspection",
            0.01190077929410124
          ],
          [
            "defects",
            0.011531088812557913
          ],
          [
            "Detection",
            0.010762303393457769
          ],
          [
            "normal",
            0.01057118859388373
          ]
        ],
        "count": 1709
      },
      "18": {
        "name": "18_object_object detection_detection_Object",
        "keywords": [
          [
            "object",
            0.027048483417515614
          ],
          [
            "object detection",
            0.02395506603461271
          ],
          [
            "detection",
            0.022830674594087257
          ],
          [
            "Object",
            0.016929806946828527
          ],
          [
            "detectors",
            0.014074101152309648
          ],
          [
            "Detection",
            0.013655391628464674
          ],
          [
            "COCO",
            0.01241529132041456
          ],
          [
            "objects",
            0.010763217750596086
          ],
          [
            "detector",
            0.009985096374643397
          ],
          [
            "AP",
            0.008903864142703735
          ]
        ],
        "count": 1509
      },
      "19": {
        "name": "19_segmentation_semantic segmentation_semantic_Segmentation",
        "keywords": [
          [
            "segmentation",
            0.03000257766753189
          ],
          [
            "semantic segmentation",
            0.019580310388049606
          ],
          [
            "semantic",
            0.01899359276518325
          ],
          [
            "Segmentation",
            0.016415355428780525
          ],
          [
            "Semantic",
            0.01314968083434213
          ],
          [
            "pixel",
            0.009319809570744875
          ],
          [
            "instance",
            0.0087333390693564
          ],
          [
            "level",
            0.00841583598858113
          ],
          [
            "class",
            0.007802118792883161
          ],
          [
            "supervised",
            0.0077752559975513775
          ]
        ],
        "count": 1508
      },
      "20": {
        "name": "20_pose_human_pose estimation_human pose",
        "keywords": [
          [
            "pose",
            0.03947555116540004
          ],
          [
            "human",
            0.03002984212484009
          ],
          [
            "pose estimation",
            0.024652088977185058
          ],
          [
            "human pose",
            0.022030989233731223
          ],
          [
            "body",
            0.02179535174330985
          ],
          [
            "estimation",
            0.021737273307337896
          ],
          [
            "3D",
            0.020901680072677585
          ],
          [
            "Pose",
            0.01920896724750462
          ],
          [
            "Human",
            0.018549250200317044
          ],
          [
            "3D human",
            0.017686558256646903
          ]
        ],
        "count": 1244
      },
      "21": {
        "name": "21_GAN_GANs_latent_Generative",
        "keywords": [
          [
            "GAN",
            0.02645431761813839
          ],
          [
            "GANs",
            0.02643575947277914
          ],
          [
            "latent",
            0.01576752490416224
          ],
          [
            "Generative",
            0.014836491281539693
          ],
          [
            "image",
            0.014113936549122488
          ],
          [
            "generative",
            0.013761255458139996
          ],
          [
            "translation",
            0.013232746698470601
          ],
          [
            "generator",
            0.012764884921958985
          ],
          [
            "Adversarial",
            0.010458707410191385
          ],
          [
            "images",
            0.01024018343587415
          ]
        ],
        "count": 1227
      },
      "22": {
        "name": "22_domain_target_source_adaptation",
        "keywords": [
          [
            "domain",
            0.04980340742217982
          ],
          [
            "target",
            0.02895527020830574
          ],
          [
            "source",
            0.028755487622289314
          ],
          [
            "adaptation",
            0.026909251554010968
          ],
          [
            "Domain",
            0.026875702255618514
          ],
          [
            "Adaptation",
            0.021224949650559842
          ],
          [
            "domains",
            0.020792232049693483
          ],
          [
            "domain adaptation",
            0.01946151079378964
          ],
          [
            "target domain",
            0.018662703829485712
          ],
          [
            "Domain Adaptation",
            0.018181219536223526
          ]
        ],
        "count": 1155
      },
      "23": {
        "name": "23_facial_emotion_expression_Facial",
        "keywords": [
          [
            "facial",
            0.033403881607456616
          ],
          [
            "emotion",
            0.03105145165232778
          ],
          [
            "expression",
            0.02549078607623782
          ],
          [
            "Facial",
            0.02074317820852639
          ],
          [
            "recognition",
            0.018363572337665637
          ],
          [
            "emotions",
            0.01620499667133209
          ],
          [
            "AU",
            0.016133034798088534
          ],
          [
            "expressions",
            0.015727115015271645
          ],
          [
            "Emotion",
            0.015431782286362597
          ],
          [
            "emotion recognition",
            0.01540253986710925
          ]
        ],
        "count": 1147
      },
      "24": {
        "name": "24_tracking_Tracking_MOT_object tracking",
        "keywords": [
          [
            "tracking",
            0.05978460527898232
          ],
          [
            "Tracking",
            0.029099851527087276
          ],
          [
            "MOT",
            0.022322316457249018
          ],
          [
            "object tracking",
            0.02229266826651357
          ],
          [
            "object",
            0.019415097427439992
          ],
          [
            "Object Tracking",
            0.01896481176221824
          ],
          [
            "trackers",
            0.018914174104525436
          ],
          [
            "tracker",
            0.015978138635090233
          ],
          [
            "Object",
            0.013359117107406676
          ],
          [
            "target",
            0.012460949785445995
          ]
        ],
        "count": 1120
      },
      "25": {
        "name": "25_MRI_reconstruction_CT_imaging",
        "keywords": [
          [
            "MRI",
            0.030399513766767108
          ],
          [
            "reconstruction",
            0.028851084870365012
          ],
          [
            "CT",
            0.01850710082026805
          ],
          [
            "imaging",
            0.013621039329891987
          ],
          [
            "MR",
            0.01293459932663659
          ],
          [
            "denoising",
            0.00999950888008549
          ],
          [
            "resolution",
            0.009784971129284696
          ],
          [
            "image",
            0.009454920321289908
          ],
          [
            "deep",
            0.00938525877703161
          ],
          [
            "quality",
            0.009242608711236815
          ]
        ],
        "count": 1107
      },
      "26": {
        "name": "26_SLAM_localization_odometry_Visual",
        "keywords": [
          [
            "SLAM",
            0.05107031938593499
          ],
          [
            "localization",
            0.0139830509261505
          ],
          [
            "odometry",
            0.013107779990053692
          ],
          [
            "Visual",
            0.01200645153412668
          ],
          [
            "Place",
            0.011681333376616218
          ],
          [
            "LiDAR",
            0.01025421040975373
          ],
          [
            "mapping",
            0.010080508187932434
          ],
          [
            "camera",
            0.010052882520554772
          ],
          [
            "map",
            0.009957214332800134
          ],
          [
            "visual",
            0.009276340525502214
          ]
        ],
        "count": 1102
      },
      "27": {
        "name": "27_video_video generation_generation_Video",
        "keywords": [
          [
            "video",
            0.047253061982557326
          ],
          [
            "video generation",
            0.03055998592734816
          ],
          [
            "generation",
            0.02535290443113269
          ],
          [
            "Video",
            0.022125881941426662
          ],
          [
            "videos",
            0.02128554667856421
          ],
          [
            "motion",
            0.01836134271976283
          ],
          [
            "editing",
            0.014811070964144458
          ],
          [
            "diffusion",
            0.013629244578216
          ],
          [
            "text",
            0.013246759656374117
          ],
          [
            "temporal",
            0.012629743907326517
          ]
        ],
        "count": 1099
      },
      "28": {
        "name": "28_chest_CT_ray_lung",
        "keywords": [
          [
            "chest",
            0.028794713568301697
          ],
          [
            "CT",
            0.02186982443140734
          ],
          [
            "ray",
            0.02176767794184559
          ],
          [
            "lung",
            0.019358186309468665
          ],
          [
            "diagnosis",
            0.014661936701573428
          ],
          [
            "disease",
            0.014367205301862094
          ],
          [
            "patients",
            0.011950327646327365
          ],
          [
            "classification",
            0.009975883808973306
          ],
          [
            "rays",
            0.009849834635124365
          ],
          [
            "ray images",
            0.009813305194380947
          ]
        ],
        "count": 1051
      },
      "29": {
        "name": "29_Re_person_Person_identification",
        "keywords": [
          [
            "Re",
            0.039051426712050155
          ],
          [
            "person",
            0.038545800783853425
          ],
          [
            "Person",
            0.02802252946551266
          ],
          [
            "identification",
            0.027980962037896212
          ],
          [
            "ID",
            0.025850671710423975
          ],
          [
            "person identification",
            0.017860737392271203
          ],
          [
            "Identification",
            0.014158064551035248
          ],
          [
            "features",
            0.011022052487683618
          ],
          [
            "modality",
            0.010197363435437133
          ],
          [
            "feature",
            0.009333401970379455
          ]
        ],
        "count": 1049
      },
      "30": {
        "name": "30_3D_generation_view_diffusion",
        "keywords": [
          [
            "3D",
            0.04510832492155652
          ],
          [
            "generation",
            0.023035829736111752
          ],
          [
            "view",
            0.01923928600095272
          ],
          [
            "diffusion",
            0.018474508273493083
          ],
          [
            "text",
            0.015094951704856967
          ],
          [
            "3D generation",
            0.013708396847843076
          ],
          [
            "2D",
            0.011929812750839658
          ],
          [
            "scene",
            0.011501790718935152
          ],
          [
            "editing",
            0.011047928793362773
          ],
          [
            "quality",
            0.010656748500813102
          ]
        ],
        "count": 1013
      },
      "31": {
        "name": "31_explanations_explanation_concepts_XAI",
        "keywords": [
          [
            "explanations",
            0.02925095831007286
          ],
          [
            "explanation",
            0.01832059165065534
          ],
          [
            "concepts",
            0.013502426803069668
          ],
          [
            "XAI",
            0.013433068553912799
          ],
          [
            "concept",
            0.012575944020995754
          ],
          [
            "interpretability",
            0.011780567939201566
          ],
          [
            "attribution",
            0.011516232249454276
          ],
          [
            "CAM",
            0.011274972927358504
          ],
          [
            "saliency",
            0.009960651183020792
          ],
          [
            "methods",
            0.009467008260916268
          ]
        ],
        "count": 986
      },
      "32": {
        "name": "32_retinal_OCT_Retinal_images",
        "keywords": [
          [
            "retinal",
            0.038232819981565566
          ],
          [
            "OCT",
            0.026354666531192384
          ],
          [
            "Retinal",
            0.013676647595482158
          ],
          [
            "images",
            0.012092927197733925
          ],
          [
            "vessel",
            0.012017510517038558
          ],
          [
            "disease",
            0.011452064416258617
          ],
          [
            "segmentation",
            0.011283632426637462
          ],
          [
            "diseases",
            0.010852690840618228
          ],
          [
            "diagnosis",
            0.010801473124903392
          ],
          [
            "vessels",
            0.008582049363792008
          ]
        ],
        "count": 957
      },
      "33": {
        "name": "33_registration_image registration_Registration_deformation",
        "keywords": [
          [
            "registration",
            0.04826903033836503
          ],
          [
            "image registration",
            0.020693183095433478
          ],
          [
            "Registration",
            0.012793516400749947
          ],
          [
            "deformation",
            0.010417635206237376
          ],
          [
            "CT",
            0.009679885918285813
          ],
          [
            "anatomical",
            0.009672928869734565
          ],
          [
            "medical",
            0.009309760471766553
          ],
          [
            "image",
            0.00921769137547228
          ],
          [
            "bone",
            0.008993729539348
          ],
          [
            "deep",
            0.008403809546818573
          ]
        ],
        "count": 906
      },
      "34": {
        "name": "34_registration_point_matching_correspondences",
        "keywords": [
          [
            "registration",
            0.028637906648613987
          ],
          [
            "point",
            0.025729866384655402
          ],
          [
            "matching",
            0.025352814233254767
          ],
          [
            "correspondences",
            0.01816588035717399
          ],
          [
            "cloud registration",
            0.015982970093502604
          ],
          [
            "cloud",
            0.014653214682542305
          ],
          [
            "point cloud registration",
            0.014014649368948409
          ],
          [
            "point cloud",
            0.013942374310899229
          ],
          [
            "Registration",
            0.012112240688883561
          ],
          [
            "Point",
            0.010685434457124842
          ]
        ],
        "count": 891
      },
      "35": {
        "name": "35_shot_shot learning_Few_Shot",
        "keywords": [
          [
            "shot",
            0.04004864628375139
          ],
          [
            "shot learning",
            0.026916134676219363
          ],
          [
            "Few",
            0.020866520644090336
          ],
          [
            "Shot",
            0.019685498564674667
          ],
          [
            "classes",
            0.018239396597338524
          ],
          [
            "meta",
            0.0169428947233692
          ],
          [
            "learning",
            0.016353259421035844
          ],
          [
            "class",
            0.015163403989786066
          ],
          [
            "FSL",
            0.01488634198115288
          ],
          [
            "Learning",
            0.013791556559086184
          ]
        ],
        "count": 890
      },
      "36": {
        "name": "36_point_point cloud_cloud_Point",
        "keywords": [
          [
            "point",
            0.04665882954062883
          ],
          [
            "point cloud",
            0.03330128261357497
          ],
          [
            "cloud",
            0.03306515785040339
          ],
          [
            "Point",
            0.02397863144398123
          ],
          [
            "3D",
            0.021624698239901437
          ],
          [
            "clouds",
            0.016991299642565117
          ],
          [
            "point clouds",
            0.016836297888662616
          ],
          [
            "segmentation",
            0.013797471164682407
          ],
          [
            "3D point",
            0.013451246881227702
          ],
          [
            "Cloud",
            0.01229764284686307
          ]
        ],
        "count": 840
      },
      "37": {
        "name": "37_medical_reports_report_radiology",
        "keywords": [
          [
            "medical",
            0.035630469501670654
          ],
          [
            "reports",
            0.022219110949649026
          ],
          [
            "report",
            0.02027918052245925
          ],
          [
            "radiology",
            0.016679725640028606
          ],
          [
            "clinical",
            0.016013850406718554
          ],
          [
            "Medical",
            0.014539360787179606
          ],
          [
            "language",
            0.01446920941126521
          ],
          [
            "VQA",
            0.010729343225821076
          ],
          [
            "Med",
            0.010207627968231341
          ],
          [
            "Language",
            0.00982863869789732
          ]
        ],
        "count": 838
      },
      "38": {
        "name": "38_forgetting_continual_continual learning_incremental",
        "keywords": [
          [
            "forgetting",
            0.03231626760645057
          ],
          [
            "continual",
            0.028831815434872256
          ],
          [
            "continual learning",
            0.026759913063782476
          ],
          [
            "incremental",
            0.02530649247850585
          ],
          [
            "Continual",
            0.02231374313774325
          ],
          [
            "learning",
            0.021738387345297126
          ],
          [
            "catastrophic forgetting",
            0.0200732670026075
          ],
          [
            "catastrophic",
            0.01984952136692009
          ],
          [
            "incremental learning",
            0.01755094673714805
          ],
          [
            "new",
            0.017386245996889912
          ]
        ],
        "count": 834
      },
      "39": {
        "name": "39_surgical_surgery_Surgical_instrument",
        "keywords": [
          [
            "surgical",
            0.06804661659175605
          ],
          [
            "surgery",
            0.02285000714273415
          ],
          [
            "Surgical",
            0.02179837043648557
          ],
          [
            "instrument",
            0.013776588054560094
          ],
          [
            "endoscopic",
            0.013711088968244066
          ],
          [
            "instruments",
            0.010079572515629635
          ],
          [
            "videos",
            0.009259668130859856
          ],
          [
            "depth",
            0.009104825677309065
          ],
          [
            "Surgery",
            0.007840721983422281
          ],
          [
            "segmentation",
            0.007694570964088765
          ]
        ],
        "count": 807
      },
      "40": {
        "name": "40_event_SNNs_SNN_Spiking",
        "keywords": [
          [
            "event",
            0.0477515045033294
          ],
          [
            "SNNs",
            0.035076856803041304
          ],
          [
            "SNN",
            0.029541748206491174
          ],
          [
            "Spiking",
            0.028122056385618113
          ],
          [
            "Event",
            0.021118558256362104
          ],
          [
            "spike",
            0.01957916741283898
          ],
          [
            "neuromorphic",
            0.019355345321484266
          ],
          [
            "cameras",
            0.01595149455503755
          ],
          [
            "events",
            0.013835328297225669
          ],
          [
            "temporal",
            0.013242757455336011
          ]
        ],
        "count": 745
      },
      "41": {
        "name": "41_spectral_hyperspectral_spatial_classification",
        "keywords": [
          [
            "spectral",
            0.04789609872835919
          ],
          [
            "hyperspectral",
            0.04433336384631126
          ],
          [
            "spatial",
            0.019803003874495965
          ],
          [
            "classification",
            0.012701755669238167
          ],
          [
            "Spectral",
            0.011838191340032854
          ],
          [
            "bands",
            0.010033185049300716
          ],
          [
            "resolution",
            0.008881097349599945
          ],
          [
            "information",
            0.008710174048295976
          ],
          [
            "Image",
            0.00759028144094112
          ],
          [
            "spectral information",
            0.007376031712186211
          ]
        ],
        "count": 735
      },
      "42": {
        "name": "42_underwater_species_animal_fish",
        "keywords": [
          [
            "underwater",
            0.026976303663638238
          ],
          [
            "species",
            0.02275400144164354
          ],
          [
            "animal",
            0.01690649786057645
          ],
          [
            "fish",
            0.014992163745177485
          ],
          [
            "detection",
            0.012551966051512178
          ],
          [
            "monitoring",
            0.011969411328952245
          ],
          [
            "dataset",
            0.009603364487834196
          ],
          [
            "animals",
            0.00913634030326172
          ],
          [
            "identification",
            0.009075650919951057
          ],
          [
            "data",
            0.007845729116928334
          ]
        ],
        "count": 688
      },
      "43": {
        "name": "43_face_face recognition_recognition_Face",
        "keywords": [
          [
            "face",
            0.05924968274426677
          ],
          [
            "face recognition",
            0.03517645032644094
          ],
          [
            "recognition",
            0.02840049222291517
          ],
          [
            "Face",
            0.027595934564268093
          ],
          [
            "facial",
            0.015994214964773715
          ],
          [
            "faces",
            0.012906246569590024
          ],
          [
            "Recognition",
            0.01259997629904122
          ],
          [
            "gender",
            0.011160994704422807
          ],
          [
            "bias",
            0.010920631622036655
          ],
          [
            "face detection",
            0.009277287211917767
          ]
        ],
        "count": 686
      },
      "44": {
        "name": "44_face_facial_identity_3D",
        "keywords": [
          [
            "face",
            0.03733003844898615
          ],
          [
            "facial",
            0.02929274394450935
          ],
          [
            "identity",
            0.017297188747582348
          ],
          [
            "3D",
            0.01709806552110454
          ],
          [
            "makeup",
            0.013882540841457134
          ],
          [
            "Face",
            0.013403281186996626
          ],
          [
            "faces",
            0.01279423550623341
          ],
          [
            "expression",
            0.011228378487092856
          ],
          [
            "latent",
            0.01055914501267691
          ],
          [
            "head",
            0.010135607932591483
          ]
        ],
        "count": 625
      },
      "45": {
        "name": "45_human_body_3D_clothing",
        "keywords": [
          [
            "human",
            0.032163704297654964
          ],
          [
            "body",
            0.024568522267566766
          ],
          [
            "3D",
            0.02135110489298994
          ],
          [
            "clothing",
            0.01743821882304513
          ],
          [
            "rendering",
            0.017281594338346147
          ],
          [
            "avatars",
            0.015829515488730895
          ],
          [
            "view",
            0.014132274916952242
          ],
          [
            "Human",
            0.013344325402861016
          ],
          [
            "pose",
            0.013268081990590458
          ],
          [
            "geometry",
            0.01311568748919941
          ]
        ],
        "count": 599
      },
      "46": {
        "name": "46_prompt_CLIP_prompts_Prompt",
        "keywords": [
          [
            "prompt",
            0.03041049481738081
          ],
          [
            "CLIP",
            0.020815032054556842
          ],
          [
            "prompts",
            0.01972449456764794
          ],
          [
            "Prompt",
            0.018466038555337835
          ],
          [
            "shot",
            0.01804235483530114
          ],
          [
            "tuning",
            0.01681553251859864
          ],
          [
            "language",
            0.013470776851565848
          ],
          [
            "downstream",
            0.012304978345628327
          ],
          [
            "prompt tuning",
            0.011878976867649953
          ],
          [
            "generalization",
            0.01152727997871967
          ]
        ],
        "count": 583
      },
      "47": {
        "name": "47_pose_6D_estimation_object pose",
        "keywords": [
          [
            "pose",
            0.05694358752603442
          ],
          [
            "6D",
            0.04460677773064548
          ],
          [
            "estimation",
            0.03140183442299154
          ],
          [
            "object pose",
            0.027522220309515306
          ],
          [
            "object",
            0.027067512454220385
          ],
          [
            "Pose",
            0.025855314220517234
          ],
          [
            "Estimation",
            0.016660564675930415
          ],
          [
            "objects",
            0.015422471360343632
          ],
          [
            "RGB",
            0.0134772536179032
          ],
          [
            "Object",
            0.01288039651941514
          ]
        ],
        "count": 523
      },
      "48": {
        "name": "48_segmentation_vessel_cardiac_vascular",
        "keywords": [
          [
            "segmentation",
            0.02376778180689443
          ],
          [
            "vessel",
            0.02326029340262778
          ],
          [
            "cardiac",
            0.020927839696256036
          ],
          [
            "vascular",
            0.014799446339055064
          ],
          [
            "vessels",
            0.012446745254743973
          ],
          [
            "clinical",
            0.011972085427883677
          ],
          [
            "left",
            0.009954709770915874
          ],
          [
            "blood",
            0.009660759412033012
          ],
          [
            "LA",
            0.008480923496545304
          ],
          [
            "MRI",
            0.008206532335237073
          ]
        ],
        "count": 520
      },
      "49": {
        "name": "49_VOS_video_segmentation_Video",
        "keywords": [
          [
            "VOS",
            0.029216430964172344
          ],
          [
            "video",
            0.0254589295519626
          ],
          [
            "segmentation",
            0.02359180244375385
          ],
          [
            "Video",
            0.022086013632202545
          ],
          [
            "VIS",
            0.02098032159426583
          ],
          [
            "Segmentation",
            0.01968291477994234
          ],
          [
            "Object Segmentation",
            0.01838992275054465
          ],
          [
            "object",
            0.01834684701081431
          ],
          [
            "object segmentation",
            0.01785554644944087
          ],
          [
            "frame",
            0.017475089714591437
          ]
        ],
        "count": 492
      },
      "50": {
        "name": "50_tumor_brain_brain tumor_MRI",
        "keywords": [
          [
            "tumor",
            0.06089975800219983
          ],
          [
            "brain",
            0.04383319217515492
          ],
          [
            "brain tumor",
            0.0320629551895179
          ],
          [
            "MRI",
            0.030704847632186862
          ],
          [
            "Brain",
            0.02650958166446098
          ],
          [
            "Tumor",
            0.02191195167185234
          ],
          [
            "tumors",
            0.021612768509399072
          ],
          [
            "segmentation",
            0.021129477508996353
          ],
          [
            "treatment",
            0.011931566731035377
          ],
          [
            "Segmentation",
            0.010882443681053967
          ]
        ],
        "count": 489
      },
      "51": {
        "name": "51_motion_motions_Motion_motion generation",
        "keywords": [
          [
            "motion",
            0.08160098468486106
          ],
          [
            "motions",
            0.03179608597655428
          ],
          [
            "Motion",
            0.02851236766265396
          ],
          [
            "motion generation",
            0.028458003402534408
          ],
          [
            "human motion",
            0.027319110173796835
          ],
          [
            "human",
            0.02676712737986784
          ],
          [
            "generation",
            0.020371575886967478
          ],
          [
            "text",
            0.016521592169146157
          ],
          [
            "Human",
            0.013244603322461693
          ],
          [
            "sequences",
            0.011859472871838939
          ]
        ],
        "count": 480
      },
      "52": {
        "name": "52_talking_audio_lip_facial",
        "keywords": [
          [
            "talking",
            0.05165221475526088
          ],
          [
            "audio",
            0.04340905501778608
          ],
          [
            "lip",
            0.03932482404350629
          ],
          [
            "facial",
            0.03597681619316192
          ],
          [
            "head",
            0.03021203143685293
          ],
          [
            "speech",
            0.02398692041541282
          ],
          [
            "Talking",
            0.02345782985058243
          ],
          [
            "face",
            0.020638184904834895
          ],
          [
            "animation",
            0.016999377254697867
          ],
          [
            "motion",
            0.016735047985171597
          ]
        ],
        "count": 471
      },
      "53": {
        "name": "53_brain_Alzheimer_AD_disease",
        "keywords": [
          [
            "brain",
            0.048143058692622316
          ],
          [
            "Alzheimer",
            0.03282621114184828
          ],
          [
            "AD",
            0.028953703716796838
          ],
          [
            "disease",
            0.022936875461944967
          ],
          [
            "MRI",
            0.020323753574066575
          ],
          [
            "age",
            0.016850931389629127
          ],
          [
            "diagnosis",
            0.016681137506702522
          ],
          [
            "functional",
            0.01494865934960767
          ],
          [
            "Disease",
            0.014473155295969625
          ],
          [
            "Brain",
            0.012419256979040085
          ]
        ],
        "count": 466
      },
      "54": {
        "name": "54_surface_mesh_shape_3D",
        "keywords": [
          [
            "surface",
            0.03301407461321188
          ],
          [
            "mesh",
            0.025589339492070377
          ],
          [
            "shape",
            0.025096895169702368
          ],
          [
            "3D",
            0.020307780773314535
          ],
          [
            "point",
            0.01993016901332215
          ],
          [
            "surfaces",
            0.01937307620076197
          ],
          [
            "implicit",
            0.017980544123543166
          ],
          [
            "meshes",
            0.016999003631735003
          ],
          [
            "reconstruction",
            0.016956813340300803
          ],
          [
            "shapes",
            0.016577296969385796
          ]
        ],
        "count": 462
      },
      "55": {
        "name": "55_OOD_distribution_detection_Out",
        "keywords": [
          [
            "OOD",
            0.09704710611304747
          ],
          [
            "distribution",
            0.029898994846313927
          ],
          [
            "detection",
            0.027416350742513775
          ],
          [
            "Out",
            0.027271411203275676
          ],
          [
            "ID",
            0.0237083597526123
          ],
          [
            "samples",
            0.019128642909435615
          ],
          [
            "Distribution",
            0.01900536659534123
          ],
          [
            "OOD samples",
            0.015445990590809002
          ],
          [
            "Detection",
            0.011645772136731665
          ],
          [
            "class",
            0.011531543580885527
          ]
        ],
        "count": 435
      },
      "56": {
        "name": "56_audio_sound_visual_music",
        "keywords": [
          [
            "audio",
            0.07725235554853432
          ],
          [
            "sound",
            0.04143967171528513
          ],
          [
            "visual",
            0.0273881724913512
          ],
          [
            "music",
            0.026187190071712493
          ],
          [
            "Audio",
            0.025716959657284523
          ],
          [
            "Sound",
            0.014473861812645726
          ],
          [
            "sounds",
            0.013604503328231252
          ],
          [
            "Music",
            0.011964318927747958
          ],
          [
            "Visual",
            0.011786727517356602
          ],
          [
            "video",
            0.009596214448832905
          ]
        ],
        "count": 433
      },
      "57": {
        "name": "57_skin_lesion_Skin_lesions",
        "keywords": [
          [
            "skin",
            0.07221597579925616
          ],
          [
            "lesion",
            0.03053985779825369
          ],
          [
            "Skin",
            0.028944521905702354
          ],
          [
            "lesions",
            0.02039601058356462
          ],
          [
            "cancer",
            0.017630192871719068
          ],
          [
            "diagnosis",
            0.016680837341128515
          ],
          [
            "classification",
            0.011745816667518512
          ],
          [
            "Lesion",
            0.011244786518430158
          ],
          [
            "clinical",
            0.009410437605288877
          ],
          [
            "images",
            0.009287194621383861
          ]
        ],
        "count": 427
      },
      "58": {
        "name": "58_medical_CT_images_synthetic",
        "keywords": [
          [
            "medical",
            0.02400404441849429
          ],
          [
            "CT",
            0.0179543403201667
          ],
          [
            "images",
            0.016999968970035962
          ],
          [
            "synthetic",
            0.015025011258811529
          ],
          [
            "MRI",
            0.013823647497467707
          ],
          [
            "imaging",
            0.013463878274302446
          ],
          [
            "GAN",
            0.013219051164672597
          ],
          [
            "image",
            0.012820472593259327
          ],
          [
            "generative",
            0.011879105901402634
          ],
          [
            "synthesis",
            0.011559579035518339
          ]
        ],
        "count": 426
      },
      "59": {
        "name": "59_anomaly_Anomaly_detection_video",
        "keywords": [
          [
            "anomaly",
            0.04143211559018138
          ],
          [
            "Anomaly",
            0.025712680464898316
          ],
          [
            "detection",
            0.021682754323133345
          ],
          [
            "video",
            0.0201418246435749
          ],
          [
            "anomalies",
            0.01849098943940179
          ],
          [
            "Video",
            0.01751444057978677
          ],
          [
            "surveillance",
            0.016756030709189317
          ],
          [
            "Detection",
            0.01637023302270126
          ],
          [
            "normal",
            0.015135803299390434
          ],
          [
            "abnormal",
            0.014247815657638009
          ]
        ],
        "count": 414
      },
      "60": {
        "name": "60_domain_adaptation_target_source",
        "keywords": [
          [
            "domain",
            0.04684352107277301
          ],
          [
            "adaptation",
            0.02725662486282556
          ],
          [
            "target",
            0.025872080451591783
          ],
          [
            "source",
            0.02366793246949775
          ],
          [
            "semantic segmentation",
            0.0233121510091788
          ],
          [
            "UDA",
            0.022995023348176817
          ],
          [
            "Domain",
            0.022205197698466836
          ],
          [
            "domain adaptation",
            0.021276026262632974
          ],
          [
            "target domain",
            0.020728200419170083
          ],
          [
            "semantic",
            0.020064987198750907
          ]
        ],
        "count": 410
      },
      "61": {
        "name": "61_inpainting_Inpainting_image inpainting_image",
        "keywords": [
          [
            "inpainting",
            0.0720698066510964
          ],
          [
            "Inpainting",
            0.029129939076718344
          ],
          [
            "image inpainting",
            0.02789710074025616
          ],
          [
            "image",
            0.01667900938975993
          ],
          [
            "missing",
            0.012332246461078814
          ],
          [
            "regions",
            0.011236596418398602
          ],
          [
            "Image",
            0.010928965069470348
          ],
          [
            "mask",
            0.00930059180681338
          ],
          [
            "diffusion",
            0.00927001121753303
          ],
          [
            "video",
            0.008097090130676565
          ]
        ],
        "count": 396
      },
      "62": {
        "name": "62_student_teacher_distillation_KD",
        "keywords": [
          [
            "student",
            0.060215149223312045
          ],
          [
            "teacher",
            0.05855918500370383
          ],
          [
            "distillation",
            0.05272881152579062
          ],
          [
            "KD",
            0.0394284695333001
          ],
          [
            "knowledge",
            0.037968805603472426
          ],
          [
            "Knowledge",
            0.03518907565819119
          ],
          [
            "Distillation",
            0.03231014910916174
          ],
          [
            "knowledge distillation",
            0.02597668620952535
          ],
          [
            "student model",
            0.015565199121522712
          ],
          [
            "teacher model",
            0.012344078058413145
          ]
        ],
        "count": 396
      },
      "63": {
        "name": "63_sign_Sign_language_Language",
        "keywords": [
          [
            "sign",
            0.09766004400048695
          ],
          [
            "Sign",
            0.07096518597001203
          ],
          [
            "language",
            0.043648187294438194
          ],
          [
            "Language",
            0.03393846081913797
          ],
          [
            "SLR",
            0.018237950451233588
          ],
          [
            "signs",
            0.01772458928305423
          ],
          [
            "recognition",
            0.01681215406390603
          ],
          [
            "translation",
            0.015809847824724336
          ],
          [
            "Recognition",
            0.01277231297582308
          ],
          [
            "hand",
            0.01142008096637919
          ]
        ],
        "count": 391
      },
      "64": {
        "name": "64_FL_Federated_clients_federated",
        "keywords": [
          [
            "FL",
            0.060039013257042306
          ],
          [
            "Federated",
            0.054021378702775937
          ],
          [
            "clients",
            0.045911106364319644
          ],
          [
            "federated",
            0.03428131475487602
          ],
          [
            "client",
            0.03268435626647381
          ],
          [
            "privacy",
            0.021288230511395688
          ],
          [
            "local",
            0.01939426494482006
          ],
          [
            "data",
            0.019350579095636155
          ],
          [
            "Learning",
            0.018062875936250746
          ],
          [
            "server",
            0.01790798847514696
          ]
        ],
        "count": 378
      },
      "65": {
        "name": "65_watermarks_copyright_diffusion_content",
        "keywords": [
          [
            "watermarks",
            0.020374089529963692
          ],
          [
            "copyright",
            0.018338701866087992
          ],
          [
            "diffusion",
            0.015723688417862886
          ],
          [
            "content",
            0.015209815824312462
          ],
          [
            "protection",
            0.014437109190823872
          ],
          [
            "models",
            0.013547782102407948
          ],
          [
            "attacks",
            0.013508948246830103
          ],
          [
            "Diffusion",
            0.013365035185475396
          ],
          [
            "image",
            0.013276707612405148
          ],
          [
            "diffusion models",
            0.013140785303512805
          ]
        ],
        "count": 368
      },
      "66": {
        "name": "66_speech_audio_speaker_lip",
        "keywords": [
          [
            "speech",
            0.06635699879724534
          ],
          [
            "audio",
            0.04175188850724152
          ],
          [
            "speaker",
            0.036317409970819316
          ],
          [
            "lip",
            0.028862795505488748
          ],
          [
            "Speech",
            0.027822303705720725
          ],
          [
            "visual",
            0.02402310017554121
          ],
          [
            "Audio",
            0.016417534760255325
          ],
          [
            "speech recognition",
            0.01608489906023644
          ],
          [
            "reading",
            0.01356876907239255
          ],
          [
            "Visual",
            0.013083035626263109
          ]
        ],
        "count": 364
      }
    },
    "correlations": [
      [
        1.0,
        -0.7342764066225682,
        -0.728994684204084,
        -0.7354339178476739,
        -0.47643354913870783,
        -0.731672713913526,
        -0.7311080721964716,
        -0.7541231084820317,
        -0.7454609757541275,
        -0.7368078874194413,
        -0.7394857576223501,
        -0.7497000753854064,
        -0.7289498386925338,
        -0.7422006571986942,
        -0.7220455922940773,
        -0.7308682631537855,
        -0.7266232941572759,
        -0.7507631019906427,
        -0.7293016220883404,
        -0.7307403952964522,
        -0.74033661250998,
        -0.41850549133947545,
        -0.7220646373911122,
        -0.7483872503903385,
        -0.7538726554669193,
        -0.6941017220466126,
        -0.7591482879463181,
        -0.7146987025291238,
        -0.753316786841282,
        -0.7461022834330915,
        -0.7260190151798622,
        -0.7533257259976395,
        -0.7559974210822298,
        -0.7476887017480097,
        -0.7418482639299573,
        -0.7407043321379003,
        -0.7458842850051633,
        -0.7338866977123973,
        -0.749356242678225,
        -0.7624943033930562,
        -0.7571825288850886,
        -0.7267595362678765,
        -0.7483333006030758,
        -0.7349187518518958,
        -0.7370299879851925,
        -0.7409793641803004,
        -0.721305321146817,
        -0.7399101335486191,
        -0.7481984692918849,
        -0.7353378934211985,
        -0.7482132038051701,
        -0.7382786423300329,
        -0.7513555256086208,
        -0.7504144833089774,
        -0.7379742438469263,
        -0.7466130534895965,
        -0.753321864262543,
        -0.7544863149794386,
        -0.6683672212040669,
        -0.7474902655523674,
        -0.7194871189958332,
        -0.7352697045507495,
        -0.734187093529445,
        -0.751603730448994,
        -0.7631792046485086,
        -0.716448156770519,
        -0.7545950544769605
      ],
      [
        -0.7342764066225682,
        1.0,
        -0.754426567154215,
        -0.7548106670813406,
        -0.7341089664447318,
        -0.717045613638842,
        -0.7486650712382269,
        -0.7566697878578827,
        -0.751432670851774,
        -0.7527797900386226,
        -0.7470546399989433,
        -0.7472379886033131,
        -0.7556081099813413,
        -0.7541220355680336,
        -0.7543049737676939,
        -0.7465495056781333,
        -0.7435251663104949,
        -0.756466209137189,
        -0.7400396819391786,
        -0.7458459772708019,
        -0.7505442593320442,
        -0.7445413010802331,
        -0.7383333691424308,
        -0.7563429659892825,
        -0.756951154398753,
        -0.7442539261709926,
        -0.7632791752429519,
        -0.7539305774203624,
        -0.7533079565474528,
        -0.7567276535091743,
        -0.7505092309674415,
        -0.7233096900546057,
        -0.760005254572771,
        -0.7568297276626513,
        -0.7509550357864268,
        -0.745078158557013,
        -0.7478679438578216,
        -0.745535752390815,
        -0.7397537035469783,
        -0.7628562640414553,
        -0.7455103507860151,
        -0.7553859645805477,
        -0.7555102294182061,
        -0.7509686689168299,
        -0.7565157235635382,
        -0.7481991010009394,
        -0.7450005800511064,
        -0.7521046411369596,
        -0.7566275146436713,
        -0.75176404764838,
        -0.7523849691973004,
        -0.7562652109793754,
        -0.7571520078032965,
        -0.7501171405228724,
        -0.7459058546131399,
        -0.7425025868667237,
        -0.7582669300253028,
        -0.7579490698839484,
        -0.7337191878049123,
        -0.7561545086042516,
        -0.7407214897663656,
        -0.7616017228322687,
        -0.7439924863612097,
        -0.7547451073549788,
        -0.7606423546566102,
        -0.7552799082085518,
        -0.7576203605008557
      ],
      [
        -0.728994684204084,
        -0.754426567154215,
        1.0,
        -0.7435429827325029,
        -0.6375667026363141,
        -0.7362950932801744,
        -0.6926142170542026,
        -0.7204594116570906,
        -0.7569224618680385,
        -0.7513773483454866,
        -0.7355146659642613,
        -0.7298210795295144,
        -0.6679932789097396,
        -0.749253308922432,
        -0.61149983509177,
        -0.7370463024497367,
        -0.7400748239649076,
        -0.7521257595649816,
        -0.7301220185716721,
        -0.7411582535675079,
        -0.7376919890217808,
        -0.7291628867184814,
        -0.7101916896180924,
        -0.7396217140846354,
        -0.7524364246158881,
        -0.7572359427039795,
        -0.7608056105914308,
        -0.6962274682634422,
        -0.7503616242687268,
        -0.7431117256538335,
        -0.7184647944940694,
        -0.7313983404049129,
        -0.7617755515184887,
        -0.7630446901034849,
        -0.7463686549009052,
        -0.6784722900447213,
        -0.7495169878936314,
        -0.47186732684149724,
        -0.7397965997236379,
        -0.7607514018597954,
        -0.7531412561171789,
        -0.7575487559329546,
        -0.7533773801197703,
        -0.7320031122513702,
        -0.7439538850424752,
        -0.7267646974551389,
        -0.26892305968251107,
        -0.7471174083603795,
        -0.7589813197298312,
        -0.7366100826788208,
        -0.7572211633722163,
        -0.7425256067654992,
        -0.7431092370963863,
        -0.7519885602973173,
        -0.7487146090869257,
        -0.7418727340561188,
        -0.6630842850864289,
        -0.7588071194197136,
        -0.7208475691870136,
        -0.7445819935789249,
        -0.7172705610298974,
        -0.7628130179607673,
        -0.7210435468317289,
        -0.5967881267700725,
        -0.760917837293617,
        -0.7260130127684747,
        -0.6679370898828358
      ],
      [
        -0.7354339178476739,
        -0.7548106670813406,
        -0.7435429827325029,
        1.0,
        -0.739171021322341,
        -0.7562648322676161,
        -0.7513294491003539,
        -0.7557278150215572,
        -0.7593099152561715,
        -0.7544059689853038,
        -0.7288514797429454,
        -0.7479635154133486,
        -0.7579020162599541,
        -0.745660117767474,
        -0.7580202871871071,
        -0.7345191403084427,
        -0.7480300210538929,
        -0.7435259459046644,
        -0.7205843331855354,
        -0.7026765170555417,
        -0.749855287072049,
        -0.7438226878533863,
        -0.7279126260185378,
        -0.7578463981228999,
        -0.7519739730771073,
        -0.7481784940066281,
        -0.7627595817284272,
        -0.755626648252584,
        -0.7574579870646679,
        -0.7539508602342927,
        -0.7501472780604612,
        -0.7609826420151915,
        -0.7643526210297953,
        -0.7516419109018818,
        -0.7460097375170569,
        -0.7470139037408952,
        -0.7362877313499745,
        -0.7495100694605308,
        -0.7551568543876845,
        -0.7646550518631353,
        -0.754721696602782,
        -0.6736121605283079,
        -0.7333485694932143,
        -0.7502959202083135,
        -0.75620816286769,
        -0.7519706130578337,
        -0.7366337859990876,
        -0.7497813665102803,
        -0.7505028961477764,
        -0.7467953217730592,
        -0.761433344720839,
        -0.7579337758093883,
        -0.7584939697605001,
        -0.7617941501876355,
        -0.7379704080963134,
        -0.7474089811869062,
        -0.76064192781005,
        -0.7617349105766538,
        -0.7327507679409555,
        -0.7426551191807742,
        -0.7120851035387452,
        -0.7610471407916539,
        -0.7481689759687531,
        -0.7598336970189419,
        -0.7594340702650458,
        -0.7572274256219271,
        -0.7614481727454392
      ],
      [
        -0.47643354913870783,
        -0.7341089664447318,
        -0.6375667026363141,
        -0.739171021322341,
        1.0,
        -0.7026813565114355,
        -0.7237986679197648,
        -0.7405495140118746,
        -0.7431708980513478,
        -0.7200404685298691,
        -0.7208169020455032,
        -0.7279007096358496,
        -0.7078386228344635,
        -0.72465450769141,
        -0.5015129757044537,
        -0.717894350213422,
        -0.7213144463793618,
        -0.7401368008122062,
        -0.7137563065731183,
        -0.7185520297729,
        -0.716609922592617,
        -0.4632610592460138,
        -0.6951808054044546,
        -0.7338827320522252,
        -0.7485368389824341,
        -0.712014858178442,
        -0.7602908709482343,
        -0.5578686969098876,
        -0.7459651260423146,
        -0.7338979528641283,
        -0.4761478396246477,
        -0.7381812090017354,
        -0.7550820306944666,
        -0.7542715921781167,
        -0.7293238687729846,
        -0.7033725111341376,
        -0.732266706265128,
        -0.6934116656897819,
        -0.7366055144012724,
        -0.7607653687645239,
        -0.7548630197790447,
        -0.7431833402277208,
        -0.7449987870011912,
        -0.7165480774894464,
        -0.7050504779069541,
        -0.710641373947623,
        -0.59564127130268,
        -0.7236569450934962,
        -0.7457419215926431,
        -0.724622226327416,
        -0.7407302521179336,
        -0.6995412050187337,
        -0.7324955696063338,
        -0.74030291106903,
        -0.7184867567644897,
        -0.7295385633804979,
        -0.7344890579823731,
        -0.7500007215190783,
        -0.6468527291113495,
        -0.7383439707536659,
        -0.6967275844699654,
        -0.7236693069670346,
        -0.707088781056189,
        -0.7296781431740889,
        -0.7598237603548343,
        -0.25791459273218187,
        -0.7357121498783725
      ],
      [
        -0.731672713913526,
        -0.717045613638842,
        -0.7362950932801744,
        -0.7562648322676161,
        -0.7026813565114355,
        1.0,
        -0.7411280686645163,
        -0.7529123454992563,
        -0.7556684679806794,
        -0.7516591779770158,
        -0.7311902822363558,
        -0.7331033568430224,
        -0.7498739724131314,
        -0.6880220249859125,
        -0.7451295811116476,
        -0.7429603304988475,
        -0.7451420124405959,
        -0.744344926879019,
        -0.7252153072026816,
        -0.7426863153089436,
        -0.7440829802991642,
        -0.6887986827555852,
        -0.7122223108936131,
        -0.745977468293621,
        -0.7522968829772383,
        -0.7439539973058873,
        -0.760585111011196,
        -0.7419437668775876,
        -0.753443321771383,
        -0.7445015379901299,
        -0.7399881676876625,
        -0.7405873118417217,
        -0.7606033220422137,
        -0.7579918667372492,
        -0.7443920726535357,
        -0.7443422435586743,
        -0.7419570275532401,
        -0.7394024708636904,
        -0.7483068633651946,
        -0.763077431925927,
        -0.7538272411589697,
        -0.753756156072132,
        -0.7557531193476787,
        -0.7094012251420438,
        -0.7239694690227677,
        -0.7461380737428809,
        -0.7258781749633312,
        -0.7459550700860462,
        -0.7566162989252724,
        -0.7520365317533951,
        -0.7540674212948644,
        -0.7506602481434463,
        -0.7485944374684073,
        -0.75178657624308,
        -0.7434204954562882,
        -0.7331350471114113,
        -0.751867843159449,
        -0.7580337840720754,
        -0.72092720441439,
        -0.7422799604385226,
        -0.7140007996994395,
        -0.7571474151585872,
        -0.7336832980835056,
        -0.7397193960977566,
        -0.7448159411473962,
        -0.6743172090960707,
        -0.7527637760601865
      ],
      [
        -0.7311080721964716,
        -0.7486650712382269,
        -0.6926142170542026,
        -0.7513294491003539,
        -0.7237986679197648,
        -0.7411280686645163,
        1.0,
        -0.6810918239442387,
        -0.7601321199396812,
        -0.7255683879805985,
        -0.7095951653525789,
        -0.6941139891148291,
        -0.7313889758121488,
        -0.7403424296741237,
        -0.745217286297181,
        -0.7501854840373019,
        -0.7191297773243229,
        -0.7464114004320082,
        -0.7200216633935173,
        -0.7381898524787593,
        -0.7034447192036802,
        -0.736580282084989,
        -0.7224833957442202,
        -0.7523851617939998,
        -0.7331288073827805,
        -0.7460011403972571,
        -0.7294699860931599,
        -0.7226152905230371,
        -0.7578446033253674,
        -0.7441969370388222,
        -0.7117466378592987,
        -0.7571885731514297,
        -0.7633910894109537,
        -0.7553038991852101,
        -0.7294396180435586,
        -0.7295422735935635,
        -0.7245767205564575,
        -0.7389942875120173,
        -0.7363789143385755,
        -0.7564974285154291,
        -0.7533275694789748,
        -0.7582300095581422,
        -0.746596768933079,
        -0.74079158818419,
        -0.7448123813698575,
        -0.7105515197824945,
        -0.6997819462970902,
        -0.700507636962534,
        -0.7557349547684957,
        -0.7306862774532039,
        -0.7599992411342721,
        -0.7140839502249992,
        -0.7506048163549363,
        -0.7583997480168996,
        -0.7255252057293995,
        -0.744042888437155,
        -0.7429454691808626,
        -0.7618828545044372,
        -0.7302420042054958,
        -0.7426369565832998,
        -0.7242524201677953,
        -0.7613332845562625,
        -0.7406049327608044,
        -0.7386396415102536,
        -0.7607022399835688,
        -0.7475414498764431,
        -0.7460286540587001
      ],
      [
        -0.7541231084820317,
        -0.7566697878578827,
        -0.7204594116570906,
        -0.7557278150215572,
        -0.7405495140118746,
        -0.7529123454992563,
        -0.6810918239442387,
        1.0,
        -0.7616793733809986,
        -0.7536160240522132,
        -0.7401711632461676,
        -0.7259750278328001,
        -0.5391611683433379,
        -0.7473153811340224,
        -0.705411831658026,
        -0.7540409601762195,
        -0.7419543326298327,
        -0.7505841749283608,
        -0.7386198298330835,
        -0.7479154439482842,
        -0.7083600740087888,
        -0.7518060593834145,
        -0.7367974702753608,
        -0.679636642731656,
        -0.742453205029125,
        -0.7592764268534016,
        -0.7601804944747852,
        -0.5777840787535815,
        -0.7583890061560611,
        -0.7357452530372894,
        -0.7408897434114363,
        -0.7553681433976016,
        -0.763299063897767,
        -0.7614246217951774,
        -0.7474878309106867,
        -0.7257305468117263,
        -0.748152523374804,
        -0.7447115238863851,
        -0.7499920148182466,
        -0.7459266748202877,
        -0.7376704455938428,
        -0.7535048843733139,
        -0.7533871374739072,
        -0.6853893764728307,
        -0.73757685967387,
        -0.7128525289243941,
        -0.7194719206016236,
        -0.7287267176057066,
        -0.7572033279921713,
        -0.663444560061199,
        -0.7589377534061714,
        -0.6815834079524574,
        -0.7330521365302397,
        -0.7563833073734051,
        -0.7528608156980993,
        -0.7542260918954078,
        -0.7388857658969266,
        -0.7621747221258459,
        -0.752966936920392,
        -0.6918255246450995,
        -0.7381840150328802,
        -0.763065549209619,
        -0.7400215621677818,
        -0.7413750558379303,
        -0.7629310545717386,
        -0.7538702831590421,
        -0.7417846255608076
      ],
      [
        -0.7454609757541275,
        -0.751432670851774,
        -0.7569224618680385,
        -0.7593099152561715,
        -0.7431708980513478,
        -0.7556684679806794,
        -0.7601321199396812,
        -0.7616793733809986,
        1.0,
        -0.7610075726066801,
        -0.7495320396842049,
        -0.7536081854352947,
        -0.7611637344381321,
        -0.7450917358009719,
        -0.756987793039216,
        -0.686048674212036,
        -0.7580574355971322,
        -0.7410078325714147,
        -0.7431853278438851,
        -0.7103003795870761,
        -0.7522796539731547,
        -0.7414809310132697,
        -0.7339743342596965,
        -0.7437559187754798,
        -0.7531104151139514,
        -0.7225888876410256,
        -0.7640651591679155,
        -0.7595518300365436,
        -0.7358522830464309,
        -0.7522042274401488,
        -0.7505106656022127,
        -0.7530877958614564,
        -0.7517477896042621,
        -0.7380630679856328,
        -0.7528181164652117,
        -0.7510783391988292,
        -0.7554056311900701,
        -0.6832436369355043,
        -0.7555276755362685,
        -0.7481244777057392,
        -0.7609628469201652,
        -0.7565357319329333,
        -0.7572786404943037,
        -0.7552216422062705,
        -0.7592144962067238,
        -0.749571019375164,
        -0.7474950988077249,
        -0.7583932950084262,
        -0.7115110213066456,
        -0.7260415326535112,
        -0.6238842974210064,
        -0.7575757732857519,
        -0.76108883338402,
        -0.7204664362762152,
        -0.7532693910105683,
        -0.7422183639524642,
        -0.7619767454200681,
        -0.5249442327268055,
        -0.6742330908904084,
        -0.7364825450640693,
        -0.7290228517602193,
        -0.7638553320932661,
        -0.7465017636082052,
        -0.7621690159840064,
        -0.7580646058376797,
        -0.7595089244848445,
        -0.7632143293655158
      ],
      [
        -0.7368078874194413,
        -0.7527797900386226,
        -0.7513773483454866,
        -0.7544059689853038,
        -0.7200404685298691,
        -0.7516591779770158,
        -0.7255683879805985,
        -0.7536160240522132,
        -0.7610075726066801,
        1.0,
        -0.7021634771240569,
        -0.726056058889822,
        -0.7388922023757745,
        -0.755181136185204,
        -0.7490169767576926,
        -0.7543284233223085,
        -0.6793035153655542,
        -0.7592544079835875,
        -0.7405518996158743,
        -0.745096398791336,
        -0.6932951016526787,
        -0.7336382129607885,
        -0.7426502074734584,
        -0.7514480812118611,
        -0.7373107833026666,
        -0.6936746920325512,
        -0.7299388131634383,
        -0.7211421715143304,
        -0.7557201290669151,
        -0.7523364449234438,
        -0.48578508461140835,
        -0.7618289006355647,
        -0.7631694419209325,
        -0.7527594963777076,
        -0.7144307006226795,
        -0.74426272541624,
        -0.6999619448305181,
        -0.7549701118339901,
        -0.7542402897862002,
        -0.7541079943543941,
        -0.753515620161123,
        -0.7523236570353429,
        -0.7511297223516002,
        -0.7445198697057194,
        -0.7257074032653255,
        -0.5763919229254498,
        -0.7432559137227281,
        -0.6932634533589941,
        -0.7589090339142947,
        -0.7348759946713432,
        -0.7622729545491523,
        -0.7079992040696852,
        -0.7405335689829805,
        -0.760553358313321,
        -0.6360994294094442,
        -0.7540489365048373,
        -0.7559119294475138,
        -0.7613131390030397,
        -0.7289428742585984,
        -0.7564353468013628,
        -0.7412461886341009,
        -0.7435827314983793,
        -0.743852275736808,
        -0.7584927513258026,
        -0.7621302456630688,
        -0.7375762000038026,
        -0.757291559700388
      ],
      [
        -0.7394857576223501,
        -0.7470546399989433,
        -0.7355146659642613,
        -0.7288514797429454,
        -0.7208169020455032,
        -0.7311902822363558,
        -0.7095951653525789,
        -0.7401711632461676,
        -0.7495320396842049,
        -0.7021634771240569,
        1.0,
        -0.441685912747772,
        -0.7378089408743664,
        -0.6368143250463683,
        -0.7448313939717979,
        -0.7215948939005736,
        -0.6626578290214347,
        -0.6241543709783122,
        -0.08601100867472822,
        -0.6660256356169979,
        -0.6719153948965217,
        -0.7348122335054926,
        -0.7089819313471393,
        -0.7514271495936203,
        -0.6562796250012617,
        -0.7331969993125762,
        -0.7196886829962506,
        -0.7276439467369982,
        -0.7486696465243996,
        -0.7410087647342477,
        -0.5551382478595965,
        -0.7552609271299271,
        -0.757536383460544,
        -0.737305066370866,
        -0.5606653007057153,
        -0.7318969191631642,
        -0.45267517827894754,
        -0.7395234279824298,
        -0.7451866775560205,
        -0.760310191210237,
        -0.7474169553943791,
        -0.745970999378136,
        -0.6229157032002408,
        -0.739825685914182,
        -0.7095049908766962,
        -0.6798878472761021,
        -0.725510871747341,
        -0.641958780026803,
        -0.7363594629482967,
        -0.6398734061922335,
        -0.7477481233534151,
        -0.7078207345469976,
        -0.7459759386278422,
        -0.7455929248095798,
        -0.5732610336963774,
        -0.6696524932624024,
        -0.7543237351222974,
        -0.754923869743765,
        -0.7254618966839874,
        -0.633929680973107,
        -0.6802604568517889,
        -0.7568638901515403,
        -0.7189571984985796,
        -0.7493584079762002,
        -0.7604241675160883,
        -0.7488986601651484,
        -0.7564329268308214
      ],
      [
        -0.7497000753854064,
        -0.7472379886033131,
        -0.7298210795295144,
        -0.7479635154133486,
        -0.7279007096358496,
        -0.7331033568430224,
        -0.6941139891148291,
        -0.7259750278328001,
        -0.7536081854352947,
        -0.726056058889822,
        -0.441685912747772,
        1.0,
        -0.7288738462372036,
        -0.7441236521512511,
        -0.7521568414456767,
        -0.7427983483200997,
        -0.7076475914186909,
        -0.7361639277606928,
        -0.6746676698974207,
        -0.7110059300536444,
        -0.7234482607717598,
        -0.7400865160693465,
        -0.7262520982349272,
        -0.7502749990173696,
        -0.7053894444245257,
        -0.748432736098958,
        -0.7375127938865751,
        -0.7152311123372355,
        -0.7545589021880337,
        -0.7395484075065486,
        -0.7109134429821624,
        -0.7487264521641586,
        -0.7606852866685612,
        -0.7527610549956542,
        -0.7210245393686461,
        -0.7491988589960489,
        -0.7035493948062385,
        -0.7420755216323793,
        -0.750113499165195,
        -0.7611376740443982,
        -0.7437891312493442,
        -0.7548328376076257,
        -0.7429713108078821,
        -0.7424348397311004,
        -0.7450687456932489,
        -0.7326342959043963,
        -0.7358557578556841,
        -0.7248290276007026,
        -0.7488830839853426,
        -0.7316749733635608,
        -0.755056242661248,
        -0.6637616594606153,
        -0.7493369855322507,
        -0.7424947382509463,
        -0.7245525303341852,
        -0.7414450357099782,
        -0.7553347928380774,
        -0.7600510910143862,
        -0.7383618263496974,
        -0.7285107290841447,
        -0.7089806394593142,
        -0.7609776287042334,
        -0.7346144769111511,
        -0.7423619450066943,
        -0.760471335172682,
        -0.7513991555144146,
        -0.756901155850786
      ],
      [
        -0.7289498386925338,
        -0.7556081099813413,
        -0.6679932789097396,
        -0.7579020162599541,
        -0.7078386228344635,
        -0.7498739724131314,
        -0.7313889758121488,
        -0.5391611683433379,
        -0.7611637344381321,
        -0.7388922023757745,
        -0.7378089408743664,
        -0.7288738462372036,
        1.0,
        -0.7302652336562527,
        -0.7170610835709261,
        -0.7509233308599695,
        -0.725289655654735,
        -0.7403571532915989,
        -0.7304481164091022,
        -0.7427057519800628,
        -0.7229375443451183,
        -0.7398616805039219,
        -0.7304087461609465,
        -0.7284057875187367,
        -0.7152349461811345,
        -0.7473102598534017,
        -0.7594977749324353,
        0.21816913106783056,
        -0.7564341137527271,
        -0.7289616301934305,
        -0.7199009585099908,
        -0.7574920683850045,
        -0.7641128795935916,
        -0.7590054174733987,
        -0.7431448786844874,
        -0.7318636068151796,
        -0.7475556047972735,
        -0.7299789477188394,
        -0.7525277012288747,
        -0.7443778395687617,
        -0.7148820352388094,
        -0.7510151912104243,
        -0.7497137273166029,
        -0.7302475845512829,
        -0.7258934027609035,
        -0.7234137625928228,
        -0.7042079439847214,
        -0.7312650846904888,
        -0.7520440819012857,
        -0.47472882018651447,
        -0.7598519975937901,
        -0.6524140474328608,
        -0.6941288454077724,
        -0.7590939553657429,
        -0.7493036544000318,
        -0.7522840507018101,
        -0.6886644330456264,
        -0.7604087703877942,
        -0.7488955346762165,
        -0.5978354818380555,
        -0.7316047711060685,
        -0.7497142180826919,
        -0.7383343461663019,
        -0.7198796974597603,
        -0.7615823288701951,
        -0.7229838050572155,
        -0.6996037579505368
      ],
      [
        -0.7422006571986942,
        -0.7541220355680336,
        -0.749253308922432,
        -0.745660117767474,
        -0.72465450769141,
        -0.6880220249859125,
        -0.7403424296741237,
        -0.7473153811340224,
        -0.7450917358009719,
        -0.755181136185204,
        -0.6368143250463683,
        -0.7441236521512511,
        -0.7302652336562527,
        1.0,
        -0.742623840855066,
        -0.7453741193497481,
        -0.7477317578181801,
        -0.52587986272989,
        -0.6069729661015719,
        -0.7460867585883404,
        -0.735063416172695,
        -0.7028441303769858,
        -0.7248689054602803,
        -0.6557947482083653,
        -0.7443511670294161,
        -0.7485237418068037,
        -0.7617176869541389,
        -0.7209013732184238,
        -0.7394077757951676,
        -0.7220598967439,
        -0.7390064515386637,
        -0.748993854564834,
        -0.7523922570748834,
        -0.7565312954579061,
        -0.7510673061646009,
        -0.7465045460634814,
        -0.7484815403225064,
        -0.7433459933759282,
        -0.7512468492591586,
        -0.7615511544320575,
        -0.7573385493177498,
        -0.7506274615619764,
        -0.7186524226138651,
        -0.19283419974221078,
        -0.2679721360910262,
        -0.7360837332345934,
        -0.7433864808406015,
        -0.7390748094431389,
        -0.7481132718454635,
        -0.7409482043421376,
        -0.7499368054187591,
        -0.7448755082110776,
        -0.6352252282179577,
        -0.7477301679327292,
        -0.7442919924155988,
        -0.6195386021689114,
        -0.7325281713971926,
        -0.7373490415202444,
        -0.7193565557350979,
        -0.542474933214629,
        -0.7296955452489033,
        -0.756017493933264,
        -0.7438058707041566,
        -0.7564353586771941,
        -0.7567596762673467,
        -0.7011741955004464,
        -0.7311141394229365
      ],
      [
        -0.7220455922940773,
        -0.7543049737676939,
        -0.61149983509177,
        -0.7580202871871071,
        -0.5015129757044537,
        -0.7451295811116476,
        -0.745217286297181,
        -0.705411831658026,
        -0.756987793039216,
        -0.7490169767576926,
        -0.7448313939717979,
        -0.7521568414456767,
        -0.7170610835709261,
        -0.742623840855066,
        1.0,
        -0.7460752783483395,
        -0.751078446480921,
        -0.7471862435288341,
        -0.7316432476282335,
        -0.7396999076137043,
        -0.7436688238032474,
        -0.7167574408025195,
        -0.7257558831951985,
        -0.6899937881071575,
        -0.7553447216672238,
        -0.7550752132183695,
        -0.7605923329419662,
        -0.7006790492744942,
        -0.7531553433433793,
        -0.7336314311607968,
        -0.7062808213290179,
        -0.7539567491358168,
        -0.7626424526275163,
        -0.7601831606128127,
        -0.7464588583237959,
        -0.7205699886655157,
        -0.7524949353042565,
        -0.7122415166728951,
        -0.7527539309656828,
        -0.7630950603841389,
        -0.7590207101378886,
        -0.7596495511489414,
        -0.7558892305004237,
        -0.664053687136822,
        -0.736527011436364,
        -0.7383278113152975,
        -0.6484948569108522,
        -0.7505737868230762,
        -0.7565506458286064,
        -0.7426658213073261,
        -0.7606161568052208,
        -0.7405015003871728,
        -0.744131012013036,
        -0.7598140352523225,
        -0.7440627937388211,
        -0.7511083014244371,
        -0.7385115357877634,
        -0.7605621854104022,
        -0.719619258576928,
        -0.7467178192307057,
        -0.7268864900609455,
        -0.754010813570857,
        -0.7351796325371499,
        -0.7127669118864415,
        -0.7623147667298529,
        -0.6827470484047471,
        -0.7335966505775693
      ],
      [
        -0.7308682631537855,
        -0.7465495056781333,
        -0.7370463024497367,
        -0.7345191403084427,
        -0.717894350213422,
        -0.7429603304988475,
        -0.7501854840373019,
        -0.7540409601762195,
        -0.686048674212036,
        -0.7543284233223085,
        -0.7215948939005736,
        -0.7427983483200997,
        -0.7509233308599695,
        -0.7453741193497481,
        -0.7460752783483395,
        1.0,
        -0.7406889905299323,
        -0.7313493531119821,
        -0.7228489388378512,
        -0.2961764727980256,
        -0.7397409092425735,
        -0.720277875228456,
        -0.6945198656072832,
        -0.7537939914739137,
        -0.7456976160376383,
        -0.6722020962120432,
        -0.7618987017430687,
        -0.7449454099165067,
        -0.6897374165460087,
        -0.7488330956696161,
        -0.7225944057072344,
        -0.7466602297391371,
        -0.7206584514121994,
        -0.7168671691445991,
        -0.7278842700065129,
        -0.7241386340497615,
        -0.7303832983957141,
        -0.16529949564374358,
        -0.7461531247864921,
        -0.7347281140118893,
        -0.7598389345348502,
        -0.7496794805009157,
        -0.7479138361770579,
        -0.7473471445405968,
        -0.7498384727456939,
        -0.7359164178790314,
        -0.7087371084066065,
        -0.7460701596408965,
        -0.42809255316710737,
        -0.5484023352222327,
        -0.5643753523599582,
        -0.7493746424509387,
        -0.7516171004832117,
        -0.6736785329267276,
        -0.7261700187991762,
        -0.7374008656029398,
        -0.7577240968806804,
        -0.6751175280590447,
        -0.3354765518821481,
        -0.7338891033532762,
        -0.5775321116367693,
        -0.7570084317320982,
        -0.7279569349334195,
        -0.7564154759452866,
        -0.7405864042332373,
        -0.7510250113876001,
        -0.7585977712210095
      ],
      [
        -0.7266232941572759,
        -0.7435251663104949,
        -0.7400748239649076,
        -0.7480300210538929,
        -0.7213144463793618,
        -0.7451420124405959,
        -0.7191297773243229,
        -0.7419543326298327,
        -0.7580574355971322,
        -0.6793035153655542,
        -0.6626578290214347,
        -0.7076475914186909,
        -0.725289655654735,
        -0.7477317578181801,
        -0.751078446480921,
        -0.7406889905299323,
        1.0,
        -0.7497884331621864,
        -0.7125252864584086,
        -0.7170065657215989,
        -0.6539520762698339,
        -0.7363910250566323,
        -0.7266971221970435,
        -0.7515507327768061,
        -0.7282299236073535,
        -0.7270309322257505,
        -0.7262983757954016,
        -0.7182873665063214,
        -0.7569608544896609,
        -0.7496022286002115,
        -0.6791558956278461,
        -0.7596254428083988,
        -0.7600895756762418,
        -0.7478478247210423,
        -0.6935648986168174,
        -0.7419865759618428,
        -0.7051529933409948,
        -0.7465915513770525,
        -0.752364882925971,
        -0.7463378197281798,
        -0.7368597939891256,
        -0.7494470196637621,
        -0.7411834595080707,
        -0.7439386391241183,
        -0.7430636941333479,
        -0.7192527628094938,
        -0.7341527715807596,
        -0.6321251212800255,
        -0.7508257544897653,
        -0.7218837520249508,
        -0.7555913926279845,
        -0.6863824212812332,
        -0.7502752758009889,
        -0.755726897572972,
        -0.6886464603324275,
        -0.7531021989555533,
        -0.7536754609824923,
        -0.7593758578458802,
        -0.7287911777059998,
        -0.7468419798065802,
        -0.71564648526253,
        -0.7480876974164707,
        -0.7352180955637931,
        -0.7571291977926906,
        -0.7623523158784253,
        -0.7437555575713803,
        -0.7562665410213443
      ],
      [
        -0.7507631019906427,
        -0.756466209137189,
        -0.7521257595649816,
        -0.7435259459046644,
        -0.7401368008122062,
        -0.744344926879019,
        -0.7464114004320082,
        -0.7505841749283608,
        -0.7410078325714147,
        -0.7592544079835875,
        -0.6241543709783122,
        -0.7361639277606928,
        -0.7403571532915989,
        -0.52587986272989,
        -0.7471862435288341,
        -0.7313493531119821,
        -0.7497884331621864,
        1.0,
        -0.5749133612683635,
        -0.7314216673973966,
        -0.7474530401076352,
        -0.7421283745251436,
        -0.7390169773635422,
        -0.7545373395578314,
        -0.7441897211109701,
        -0.7387768688997078,
        -0.7616914792050691,
        -0.7398056230467669,
        -0.7427957857345986,
        -0.7428710985716929,
        -0.7467268609695373,
        -0.7550798294115598,
        -0.7496928042241374,
        -0.7548410150990429,
        -0.7485315457621418,
        -0.7394283090973153,
        -0.7440892344519446,
        -0.7334467820823677,
        -0.7527692163818729,
        -0.7601083553048278,
        -0.7528389283660444,
        -0.7503508188886536,
        -0.7122845139990919,
        -0.7448740613136864,
        -0.7476680209094397,
        -0.7477025954604327,
        -0.7442559189344864,
        -0.7503760979351478,
        -0.738561594650732,
        -0.7356430072709095,
        -0.7383552857476334,
        -0.7484662296770688,
        -0.7509499724044801,
        -0.6945889155408216,
        -0.726733385379706,
        -0.6100297081518185,
        -0.7554399447356628,
        -0.7455181491983512,
        -0.7242946551024076,
        0.24153156922702376,
        -0.7339875288665518,
        -0.7574612821802504,
        -0.7454349307282633,
        -0.7605183890230311,
        -0.7612586721152914,
        -0.7540755873006706,
        -0.7564212950923297
      ],
      [
        -0.7293016220883404,
        -0.7400396819391786,
        -0.7301220185716721,
        -0.7205843331855354,
        -0.7137563065731183,
        -0.7252153072026816,
        -0.7200216633935173,
        -0.7386198298330835,
        -0.7431853278438851,
        -0.7405518996158743,
        -0.08601100867472822,
        -0.6746676698974207,
        -0.7304481164091022,
        -0.6069729661015719,
        -0.7316432476282335,
        -0.7228489388378512,
        -0.7125252864584086,
        -0.5749133612683635,
        1.0,
        -0.6745355845454102,
        -0.7220983768640864,
        -0.728894223961863,
        -0.7067550273232459,
        -0.7494244697485688,
        -0.6562737971304805,
        -0.7479558945822657,
        -0.7548439899388508,
        -0.727105757576649,
        -0.7409839215689988,
        -0.7344301413051896,
        -0.7144024653608033,
        -0.7521909392464612,
        -0.7566757753100274,
        -0.753073287888387,
        -0.7137057111153342,
        -0.7139645695069238,
        -0.7016246206718029,
        -0.7348385846898287,
        -0.738155047826522,
        -0.76064181256698,
        -0.7423037259141583,
        -0.7485070563065754,
        -0.5802027377130129,
        -0.7361228128927235,
        -0.745166722647775,
        -0.7313802718102655,
        -0.7114481293537982,
        -0.6589908771596797,
        -0.7412974183673171,
        -0.6207499808657858,
        -0.7497764345177094,
        -0.7329436045130492,
        -0.7375848823854089,
        -0.7478130311934819,
        -0.7158337088777096,
        -0.654792991285152,
        -0.7503118175400776,
        -0.7501130036103558,
        -0.7149783028290096,
        -0.5604913985834565,
        -0.6895544659424022,
        -0.7570000405853752,
        -0.715168825848413,
        -0.7482371707191254,
        -0.7585419681366888,
        -0.7495369221649526,
        -0.7526011436317956
      ],
      [
        -0.7307403952964522,
        -0.7458459772708019,
        -0.7411582535675079,
        -0.7026765170555417,
        -0.7185520297729,
        -0.7426863153089436,
        -0.7381898524787593,
        -0.7479154439482842,
        -0.7103003795870761,
        -0.745096398791336,
        -0.6660256356169979,
        -0.7110059300536444,
        -0.7427057519800628,
        -0.7460867585883404,
        -0.7396999076137043,
        -0.2961764727980256,
        -0.7170065657215989,
        -0.7314216673973966,
        -0.6745355845454102,
        1.0,
        -0.7358957360850658,
        -0.7250156645235886,
        -0.6878188376560034,
        -0.7543380552137545,
        -0.7387372664613769,
        -0.7188867965631769,
        -0.7566228599369789,
        -0.7389007711375292,
        -0.7371587711368233,
        -0.7464164748698612,
        -0.7176619300894154,
        -0.7563272849162577,
        -0.7412165175947976,
        -0.7471631949483095,
        -0.7101518488868139,
        -0.7064477882979434,
        -0.6871034351854073,
        -0.6909942869979693,
        -0.7356493662861084,
        -0.7421748104416653,
        -0.7554557080303699,
        -0.7450972806443301,
        -0.7350611912561511,
        -0.7470468603286111,
        -0.7499772259763882,
        -0.735535997216545,
        -0.709939524581916,
        -0.7374096405142401,
        -0.46939621113646635,
        -0.3761356892152332,
        -0.633813531978668,
        -0.7464715647188114,
        -0.7472205401956986,
        -0.7283708264347242,
        -0.7090098088735075,
        -0.738987423073165,
        -0.7561551249762337,
        -0.7284836863288429,
        -0.6716946395752788,
        -0.7366894403540132,
        -0.2373264176266262,
        -0.7573491433470149,
        -0.7204018688331626,
        -0.7545622894218281,
        -0.7584865458925776,
        -0.7505997405330007,
        -0.7576485086853657
      ],
      [
        -0.74033661250998,
        -0.7505442593320442,
        -0.7376919890217808,
        -0.749855287072049,
        -0.716609922592617,
        -0.7440829802991642,
        -0.7034447192036802,
        -0.7083600740087888,
        -0.7522796539731547,
        -0.6932951016526787,
        -0.6719153948965217,
        -0.7234482607717598,
        -0.7229375443451183,
        -0.735063416172695,
        -0.7436688238032474,
        -0.7397409092425735,
        -0.6539520762698339,
        -0.7474530401076352,
        -0.7220983768640864,
        -0.7358957360850658,
        1.0,
        -0.7280929922371098,
        -0.7278518353977945,
        -0.7276354060291614,
        -0.7099651455140499,
        -0.7216030699091105,
        -0.722568476270158,
        -0.7071383822966965,
        -0.7523256437828956,
        -0.6836567776448965,
        -0.6197646389153415,
        -0.7556178783647651,
        -0.7598758769731947,
        -0.7378177539750082,
        -0.694596623079233,
        -0.7416558700618883,
        -0.6898921866810053,
        -0.7387560901947581,
        -0.7531838069371352,
        -0.7498244413799338,
        -0.7505432517362169,
        -0.7561466086225723,
        -0.7392466616340767,
        -0.7174144252424752,
        -0.6802935842004094,
        -0.08700467652995744,
        -0.7366208706847591,
        0.2444127903973542,
        -0.745565494531081,
        -0.7221103593326443,
        -0.7446078026469901,
        -0.5733885221791352,
        -0.7214552953124798,
        -0.7450428419780419,
        -0.6449534127536272,
        -0.7491631560489076,
        -0.7450220507809562,
        -0.7525393203076896,
        -0.721616689544377,
        -0.7395566558582459,
        -0.7254985587423798,
        -0.7548671679329564,
        -0.7397317068297671,
        -0.7444445891692465,
        -0.7606881753741797,
        -0.7403673205264766,
        -0.7471460554761236
      ],
      [
        -0.41850549133947545,
        -0.7445413010802331,
        -0.7291628867184814,
        -0.7438226878533863,
        -0.4632610592460138,
        -0.6887986827555852,
        -0.736580282084989,
        -0.7518060593834145,
        -0.7414809310132697,
        -0.7336382129607885,
        -0.7348122335054926,
        -0.7400865160693465,
        -0.7398616805039219,
        -0.7028441303769858,
        -0.7167574408025195,
        -0.720277875228456,
        -0.7363910250566323,
        -0.7421283745251436,
        -0.728894223961863,
        -0.7250156645235886,
        -0.7280929922371098,
        1.0,
        -0.6939566753910924,
        -0.7324166021859544,
        -0.7541444852630816,
        -0.7030993110198136,
        -0.760751982772006,
        -0.708466277261685,
        -0.7474560964307903,
        -0.7404123856383402,
        -0.6901833406363802,
        -0.744911228199041,
        -0.7513521073933305,
        -0.7520206735861834,
        -0.735281738964974,
        -0.7266405292754792,
        -0.7419999156079315,
        -0.719612981972527,
        -0.7409926784717351,
        -0.7604508564696351,
        -0.7571282690700838,
        -0.7387051164950282,
        -0.7494789685176468,
        -0.7113155556297043,
        -0.6879976030042623,
        -0.7267834079936593,
        -0.715044493336802,
        -0.7304465675926977,
        -0.7431294145333327,
        -0.7377510786744732,
        -0.738256245051676,
        -0.7349913394743628,
        -0.7363377586923935,
        -0.7380067788123399,
        -0.7247732838143279,
        -0.7342384064243916,
        -0.7480725751128279,
        -0.7480480423348002,
        -0.6052162175276234,
        -0.7401718466662512,
        -0.6950417102071539,
        -0.7351418243953862,
        -0.7306742723215579,
        -0.750716847494743,
        -0.758047903310829,
        -0.6825412615801288,
        -0.7497516393692696
      ],
      [
        -0.7220646373911122,
        -0.7383333691424308,
        -0.7101916896180924,
        -0.7279126260185378,
        -0.6951808054044546,
        -0.7122223108936131,
        -0.7224833957442202,
        -0.7367974702753608,
        -0.7339743342596965,
        -0.7426502074734584,
        -0.7089819313471393,
        -0.7262520982349272,
        -0.7304087461609465,
        -0.7248689054602803,
        -0.7257558831951985,
        -0.6945198656072832,
        -0.7266971221970435,
        -0.7390169773635422,
        -0.7067550273232459,
        -0.6878188376560034,
        -0.7278518353977945,
        -0.6939566753910924,
        1.0,
        -0.7371540059386918,
        -0.7355566941223799,
        -0.7251996226775748,
        -0.7571218041673022,
        -0.7231927126474493,
        -0.7430164579834309,
        -0.7205420341499518,
        -0.7242113499611944,
        -0.7484329625972495,
        -0.749061120396888,
        -0.750125832755624,
        -0.7283582658771234,
        -0.6886963348831653,
        -0.7323200247433856,
        -0.6985343146468923,
        -0.7213442216463193,
        -0.7541648397035088,
        -0.7536898531471463,
        -0.7415973547555168,
        -0.7393960907024746,
        -0.7200865098227096,
        -0.7245544926336274,
        -0.7349298924926436,
        -0.6793078521951248,
        -0.7297581629991912,
        -0.733790200298708,
        -0.7259339518342116,
        -0.735893743337112,
        -0.735972054020217,
        -0.7401318509309194,
        -0.7415873814867585,
        -0.7287802149389573,
        -0.7171734860031254,
        -0.7450622762220354,
        -0.7487631212611245,
        -0.6868026855022562,
        -0.7359709251543561,
        0.3376784692762258,
        -0.7591561100460112,
        -0.7101286744498012,
        -0.7451244433512826,
        -0.7490417454719493,
        -0.7332360392971822,
        -0.7450574112234047
      ],
      [
        -0.7483872503903385,
        -0.7563429659892825,
        -0.7396217140846354,
        -0.7578463981228999,
        -0.7338827320522252,
        -0.745977468293621,
        -0.7523851617939998,
        -0.679636642731656,
        -0.7437559187754798,
        -0.7514480812118611,
        -0.7514271495936203,
        -0.7502749990173696,
        -0.7284057875187367,
        -0.6557947482083653,
        -0.6899937881071575,
        -0.7537939914739137,
        -0.7515507327768061,
        -0.7545373395578314,
        -0.7494244697485688,
        -0.7543380552137545,
        -0.7276354060291614,
        -0.7324166021859544,
        -0.7371540059386918,
        1.0,
        -0.7521834056004553,
        -0.7553593222646009,
        -0.7612353377899843,
        -0.7243733560141857,
        -0.757071752665117,
        -0.7281693987164615,
        -0.7385086005781122,
        -0.7568566933859666,
        -0.7635738915697268,
        -0.7583280531781695,
        -0.7543845471893678,
        -0.748941911372436,
        -0.7532373454515982,
        -0.7508038280225753,
        -0.7557675644750812,
        -0.7608933025729885,
        -0.7568859140276811,
        -0.7566352276778691,
        -0.7568309205605945,
        -0.5105501797242112,
        -0.26102110245175425,
        -0.7199576938523373,
        -0.740386615089891,
        -0.7378564873838225,
        -0.7590739647749276,
        -0.744756393487181,
        -0.7548639167124456,
        -0.732716189825722,
        -0.37288080442079824,
        -0.7521537328782831,
        -0.7482395758391924,
        -0.7524944896478982,
        -0.7021193504070293,
        -0.742858937226861,
        -0.7413790789420266,
        -0.7478193512372684,
        -0.7390039946792486,
        -0.7576349512394971,
        -0.7439961033738944,
        -0.7384421017635052,
        -0.7613130888753394,
        -0.7476287522690299,
        -0.6886605291962568
      ],
      [
        -0.7538726554669193,
        -0.756951154398753,
        -0.7524364246158881,
        -0.7519739730771073,
        -0.7485368389824341,
        -0.7522968829772383,
        -0.7331288073827805,
        -0.742453205029125,
        -0.7531104151139514,
        -0.7373107833026666,
        -0.6562796250012617,
        -0.7053894444245257,
        -0.7152349461811345,
        -0.7443511670294161,
        -0.7553447216672238,
        -0.7456976160376383,
        -0.7282299236073535,
        -0.7441897211109701,
        -0.6562737971304805,
        -0.7387372664613769,
        -0.7099651455140499,
        -0.7541444852630816,
        -0.7355566941223799,
        -0.7521834056004553,
        1.0,
        -0.7501114826501165,
        -0.7233980587081057,
        -0.7145417281992508,
        -0.7571831909026685,
        -0.7231889661196069,
        -0.7319486147420817,
        -0.7621906378475802,
        -0.7615250967981352,
        -0.7472825807744986,
        -0.7240360874943486,
        -0.7513673293594447,
        -0.7272786562596422,
        -0.7512309759584004,
        -0.7564571970602866,
        -0.7427148573400154,
        -0.7281085768597675,
        -0.7572093242498452,
        -0.7254687046839565,
        -0.7483783522151272,
        -0.7474704416222835,
        -0.7309840439637905,
        -0.7486289927766119,
        -0.6533976084801085,
        -0.7436272799258685,
        -0.6319724696560141,
        -0.7558058483112275,
        -0.6687817953516613,
        -0.7474226475196497,
        -0.7565626598844832,
        -0.7303547153545025,
        -0.752210384594594,
        -0.7536035231789133,
        -0.7596180832717403,
        -0.7490860656732645,
        -0.7354767483262998,
        -0.7364269756790289,
        -0.7622974775556365,
        -0.7489837347877393,
        -0.7566395735188862,
        -0.7634172932686152,
        -0.7594546364889658,
        -0.7557623075711526
      ],
      [
        -0.6941017220466126,
        -0.7442539261709926,
        -0.7572359427039795,
        -0.7481784940066281,
        -0.712014858178442,
        -0.7439539973058873,
        -0.7460011403972571,
        -0.7592764268534016,
        -0.7225888876410256,
        -0.6936746920325512,
        -0.7331969993125762,
        -0.748432736098958,
        -0.7473102598534017,
        -0.7485237418068037,
        -0.7550752132183695,
        -0.6722020962120432,
        -0.7270309322257505,
        -0.7387768688997078,
        -0.7479558945822657,
        -0.7188867965631769,
        -0.7216030699091105,
        -0.7030993110198136,
        -0.7251996226775748,
        -0.7553593222646009,
        -0.7501114826501165,
        1.0,
        -0.7535379321539388,
        -0.7354153909752578,
        -0.7095278596755201,
        -0.7533215909410855,
        -0.6580674386398804,
        -0.7603187032344624,
        -0.75701460395678,
        -0.7125459019969262,
        -0.724225909575321,
        -0.748259103087447,
        -0.7265024352303294,
        -0.681755995614403,
        -0.7534037514062306,
        -0.7432319502936646,
        -0.7520404023927314,
        -0.7323212778007526,
        -0.7580612380022411,
        -0.746284484922497,
        -0.7354969533118099,
        -0.7067296182820011,
        -0.7442088806111167,
        -0.7336199185106882,
        -0.654384744182259,
        -0.7279442287282617,
        -0.4575517345563196,
        -0.7183228729476092,
        -0.7506339114558737,
        -0.5585907975203264,
        -0.6385233390460566,
        -0.7465457985376438,
        -0.7579206236481824,
        -0.7243497749422629,
        -0.5822391176361353,
        -0.738886665741253,
        -0.7211660578405323,
        -0.7501768224120329,
        -0.743934712151828,
        -0.7600038264802464,
        -0.7538223733170945,
        -0.7423116218405292,
        -0.7584060676930429
      ],
      [
        -0.7591482879463181,
        -0.7632791752429519,
        -0.7608056105914308,
        -0.7627595817284272,
        -0.7602908709482343,
        -0.760585111011196,
        -0.7294699860931599,
        -0.7601804944747852,
        -0.7640651591679155,
        -0.7299388131634383,
        -0.7196886829962506,
        -0.7375127938865751,
        -0.7594977749324353,
        -0.7617176869541389,
        -0.7605923329419662,
        -0.7618987017430687,
        -0.7262983757954016,
        -0.7616914792050691,
        -0.7548439899388508,
        -0.7566228599369789,
        -0.722568476270158,
        -0.760751982772006,
        -0.7571218041673022,
        -0.7612353377899843,
        -0.7233980587081057,
        -0.7535379321539388,
        1.0,
        -0.7587437151436609,
        -0.7646024715841017,
        -0.760655792475273,
        -0.750209160227104,
        -0.7635200910525559,
        -0.7653828243293814,
        -0.7477793742902017,
        -0.7092621350129933,
        -0.7624838609738382,
        -0.7269188426559366,
        -0.7619246438807221,
        -0.7601756929161856,
        -0.7602793570974644,
        -0.7515810629093271,
        -0.7639914746013445,
        -0.7551770279110537,
        -0.7540821652545184,
        -0.7624112766073816,
        -0.7567052888274206,
        -0.7611641224759698,
        -0.7085563869179061,
        -0.7639792124623523,
        -0.757433092691959,
        -0.7639517070640147,
        -0.7382724689733449,
        -0.7631523973039807,
        -0.76409327446434,
        -0.7420875090138384,
        -0.761720250665018,
        -0.7591691979347168,
        -0.7650968781828955,
        -0.7571589802464256,
        -0.7621246937201679,
        -0.7558227968272455,
        -0.7630263602091464,
        -0.756691128369793,
        -0.7636846413377698,
        -0.7644827210297525,
        -0.763691192101257,
        -0.7609361429460206
      ],
      [
        -0.7146987025291238,
        -0.7539305774203624,
        -0.6962274682634422,
        -0.755626648252584,
        -0.5578686969098876,
        -0.7419437668775876,
        -0.7226152905230371,
        -0.5777840787535815,
        -0.7595518300365436,
        -0.7211421715143304,
        -0.7276439467369982,
        -0.7152311123372355,
        0.21816913106783056,
        -0.7209013732184238,
        -0.7006790492744942,
        -0.7449454099165067,
        -0.7182873665063214,
        -0.7398056230467669,
        -0.727105757576649,
        -0.7389007711375292,
        -0.7071383822966965,
        -0.708466277261685,
        -0.7231927126474493,
        -0.7243733560141857,
        -0.7145417281992508,
        -0.7354153909752578,
        -0.7587437151436609,
        1.0,
        -0.7544788967561284,
        -0.7296735906193371,
        -0.575370458633448,
        -0.756899290789633,
        -0.7628412943842275,
        -0.7555599664573774,
        -0.7387865684563769,
        -0.7260523540287283,
        -0.7406802455569395,
        -0.7288783153047346,
        -0.7517827111403437,
        -0.7450433194094604,
        -0.7300285016723871,
        -0.7484770241985468,
        -0.7489862858646472,
        -0.7221843421722887,
        -0.7057504888484576,
        -0.705205956790668,
        -0.6974213837324545,
        -0.7193494597493406,
        -0.7485466276253053,
        -0.4959901257442271,
        -0.7568136841766502,
        -0.5415961451824947,
        -0.6812901734851159,
        -0.7559983318775836,
        -0.7369532936950733,
        -0.7484637209963361,
        -0.6901485338813091,
        -0.7592626865302665,
        -0.7325137669675263,
        -0.6149478423332556,
        -0.7251008278835984,
        -0.7394010868025244,
        -0.7327657491418536,
        -0.7318831854495469,
        -0.7609443513662411,
        -0.6160951134020997,
        -0.7007916195951047
      ],
      [
        -0.753316786841282,
        -0.7533079565474528,
        -0.7503616242687268,
        -0.7574579870646679,
        -0.7459651260423146,
        -0.753443321771383,
        -0.7578446033253674,
        -0.7583890061560611,
        -0.7358522830464309,
        -0.7557201290669151,
        -0.7486696465243996,
        -0.7545589021880337,
        -0.7564341137527271,
        -0.7394077757951676,
        -0.7531553433433793,
        -0.6897374165460087,
        -0.7569608544896609,
        -0.7427957857345986,
        -0.7409839215689988,
        -0.7371587711368233,
        -0.7523256437828956,
        -0.7474560964307903,
        -0.7430164579834309,
        -0.757071752665117,
        -0.7571831909026685,
        -0.7095278596755201,
        -0.7646024715841017,
        -0.7544788967561284,
        1.0,
        -0.7510257591352628,
        -0.7507742383004693,
        -0.7514124128178267,
        -0.7563211810835878,
        -0.7505611488734287,
        -0.7550074791844142,
        -0.7507648568464154,
        -0.7554445272382828,
        -0.646290331753427,
        -0.7567732256468426,
        -0.7561314632761781,
        -0.7569353098082616,
        -0.7606519394477173,
        -0.7584875476186517,
        -0.7414369512250515,
        -0.7474764662550992,
        -0.7509475812904556,
        -0.7468642389841351,
        -0.7565142937940048,
        -0.7319360283793523,
        -0.7434810289574794,
        -0.7402094464379219,
        -0.7585776270015638,
        -0.7557989637341882,
        -0.7170608783198688,
        -0.7530903047718,
        -0.7482149406987219,
        -0.7600203771606213,
        -0.728112100141016,
        -0.6645067103016331,
        -0.7359350119647445,
        -0.7413388461882596,
        -0.7627801604815958,
        -0.7501098739322365,
        -0.7601321719585943,
        -0.7512811732510034,
        -0.7586296264189981,
        -0.7607430106690242
      ],
      [
        -0.7461022834330915,
        -0.7567276535091743,
        -0.7431117256538335,
        -0.7539508602342927,
        -0.7338979528641283,
        -0.7445015379901299,
        -0.7441969370388222,
        -0.7357452530372894,
        -0.7522042274401488,
        -0.7523364449234438,
        -0.7410087647342477,
        -0.7395484075065486,
        -0.7289616301934305,
        -0.7220598967439,
        -0.7336314311607968,
        -0.7488330956696161,
        -0.7496022286002115,
        -0.7428710985716929,
        -0.7344301413051896,
        -0.7464164748698612,
        -0.6836567776448965,
        -0.7404123856383402,
        -0.7205420341499518,
        -0.7281693987164615,
        -0.7231889661196069,
        -0.7533215909410855,
        -0.760655792475273,
        -0.7296735906193371,
        -0.7510257591352628,
        1.0,
        -0.7423028029258392,
        -0.7567834281244448,
        -0.759261775722094,
        -0.7581442139625727,
        -0.740587609342034,
        -0.744476610071896,
        -0.750974065907402,
        -0.746080972560508,
        -0.7497305708592652,
        -0.7596367088725748,
        -0.7579012118945172,
        -0.7525087503926879,
        -0.7280430140865459,
        -0.7050205500323254,
        -0.6970142187309507,
        -0.6872359655824982,
        -0.73752086615491,
        -0.7077288343745873,
        -0.7510939101429039,
        -0.7395313686746952,
        -0.7531145235570215,
        -0.7328642315828062,
        -0.7313675242811156,
        -0.7455295246161653,
        -0.7451491746953284,
        -0.7191883112146675,
        -0.7495708626411011,
        -0.7496179337163507,
        -0.7293630171789807,
        -0.7343212388402305,
        -0.721921315286385,
        -0.7586938392348559,
        -0.7430130053143039,
        -0.7575671469657133,
        -0.7585633698972217,
        -0.7492401420899164,
        -0.7486674436390831
      ],
      [
        -0.7260190151798622,
        -0.7505092309674415,
        -0.7184647944940694,
        -0.7501472780604612,
        -0.4761478396246477,
        -0.7399881676876625,
        -0.7117466378592987,
        -0.7408897434114363,
        -0.7505106656022127,
        -0.48578508461140835,
        -0.5551382478595965,
        -0.7109134429821624,
        -0.7199009585099908,
        -0.7390064515386637,
        -0.7062808213290179,
        -0.7225944057072344,
        -0.6791558956278461,
        -0.7467268609695373,
        -0.7144024653608033,
        -0.7176619300894154,
        -0.6197646389153415,
        -0.6901833406363802,
        -0.7242113499611944,
        -0.7385086005781122,
        -0.7319486147420817,
        -0.6580674386398804,
        -0.750209160227104,
        -0.575370458633448,
        -0.7507742383004693,
        -0.7423028029258392,
        1.0,
        -0.7595858884096638,
        -0.7594409890149011,
        -0.7423206715066587,
        -0.6704028228937411,
        -0.7327313016936277,
        -0.6280083811436379,
        -0.7232436965135126,
        -0.7495315467389316,
        -0.7541217795320587,
        -0.7562713639988741,
        -0.7480647537605561,
        -0.7493379283905691,
        -0.7233631434432881,
        -0.6445304308558291,
        -0.577333931612554,
        -0.7002012211803504,
        -0.6711916399298489,
        -0.7349036757290093,
        -0.7161733171672893,
        -0.7405776561471777,
        -0.6692494487577606,
        -0.7264002193937491,
        -0.742931706816369,
        -0.5313973388681363,
        -0.7456626529812825,
        -0.7471157533435218,
        -0.7535120187463202,
        -0.6965369018836132,
        -0.744481721399776,
        -0.7185851582129201,
        -0.7329415798825327,
        -0.7250216052398368,
        -0.7472351578695805,
        -0.7624063630580034,
        -0.5517572279204828,
        -0.7501039726013695
      ],
      [
        -0.7533257259976395,
        -0.7233096900546057,
        -0.7313983404049129,
        -0.7609826420151915,
        -0.7381812090017354,
        -0.7405873118417217,
        -0.7571885731514297,
        -0.7553681433976016,
        -0.7530877958614564,
        -0.7618289006355647,
        -0.7552609271299271,
        -0.7487264521641586,
        -0.7574920683850045,
        -0.748993854564834,
        -0.7539567491358168,
        -0.7466602297391371,
        -0.7596254428083988,
        -0.7550798294115598,
        -0.7521909392464612,
        -0.7563272849162577,
        -0.7556178783647651,
        -0.744911228199041,
        -0.7484329625972495,
        -0.7568566933859666,
        -0.7621906378475802,
        -0.7603187032344624,
        -0.7635200910525559,
        -0.756899290789633,
        -0.7514124128178267,
        -0.7567834281244448,
        -0.7595858884096638,
        1.0,
        -0.7584338933100496,
        -0.7630029543432739,
        -0.7607310075703562,
        -0.7528174808868209,
        -0.7588890255792806,
        -0.7295316291697762,
        -0.7569750691503256,
        -0.7639934186440024,
        -0.7597570246223864,
        -0.761078732971675,
        -0.7571382656048801,
        -0.7535471975605628,
        -0.7563740360242155,
        -0.7480440091397145,
        -0.7445843614134726,
        -0.7610283252424062,
        -0.7605756646536574,
        -0.759084005027348,
        -0.7576807720332273,
        -0.7603905462966853,
        -0.7584696493752479,
        -0.7517062263294788,
        -0.7590839907296694,
        -0.7544241209289743,
        -0.7562779315795394,
        -0.7477691629865583,
        -0.7410127098240878,
        -0.7545476197548351,
        -0.7523606350877576,
        -0.7629287670356958,
        -0.7542579799503168,
        -0.7571026329211187,
        -0.7641210375195138,
        -0.754458227258533,
        -0.7573521954936915
      ],
      [
        -0.7559974210822298,
        -0.760005254572771,
        -0.7617755515184887,
        -0.7643526210297953,
        -0.7550820306944666,
        -0.7606033220422137,
        -0.7633910894109537,
        -0.763299063897767,
        -0.7517477896042621,
        -0.7631694419209325,
        -0.757536383460544,
        -0.7606852866685612,
        -0.7641128795935916,
        -0.7523922570748834,
        -0.7626424526275163,
        -0.7206584514121994,
        -0.7600895756762418,
        -0.7496928042241374,
        -0.7566757753100274,
        -0.7412165175947976,
        -0.7598758769731947,
        -0.7513521073933305,
        -0.749061120396888,
        -0.7635738915697268,
        -0.7615250967981352,
        -0.75701460395678,
        -0.7653828243293814,
        -0.7628412943842275,
        -0.7563211810835878,
        -0.759261775722094,
        -0.7594409890149011,
        -0.7584338933100496,
        1.0,
        -0.7481661343328957,
        -0.7580673855300046,
        -0.7608787121733466,
        -0.7612748143270452,
        -0.7238277315452984,
        -0.7619008137222354,
        -0.758319856206287,
        -0.7598883050279484,
        -0.7598926871051958,
        -0.7611202362299729,
        -0.7579745304473341,
        -0.759959059105867,
        -0.7603299297004562,
        -0.7594632298891465,
        -0.7614493971236187,
        -0.6094590990664808,
        -0.7467802904446802,
        -0.7485995641540817,
        -0.7582242144683071,
        -0.762980025104574,
        -0.717179417573601,
        -0.7584089329924943,
        -0.756556936919054,
        -0.7625349642824987,
        -0.7298813338782046,
        -0.7224942548506248,
        -0.7494885844724775,
        -0.7461955226384314,
        -0.763658597824179,
        -0.7553585110301906,
        -0.7652629236988052,
        -0.7606336230538462,
        -0.7625560580372814,
        -0.7630442133068983
      ],
      [
        -0.7476887017480097,
        -0.7568297276626513,
        -0.7630446901034849,
        -0.7516419109018818,
        -0.7542715921781167,
        -0.7579918667372492,
        -0.7553038991852101,
        -0.7614246217951774,
        -0.7380630679856328,
        -0.7527594963777076,
        -0.737305066370866,
        -0.7527610549956542,
        -0.7590054174733987,
        -0.7565312954579061,
        -0.7601831606128127,
        -0.7168671691445991,
        -0.7478478247210423,
        -0.7548410150990429,
        -0.753073287888387,
        -0.7471631949483095,
        -0.7378177539750082,
        -0.7520206735861834,
        -0.750125832755624,
        -0.7583280531781695,
        -0.7472825807744986,
        -0.7125459019969262,
        -0.7477793742902017,
        -0.7555599664573774,
        -0.7505611488734287,
        -0.7581442139625727,
        -0.7423206715066587,
        -0.7630029543432739,
        -0.7481661343328957,
        1.0,
        -0.3953790502423622,
        -0.7596548850588786,
        -0.6880811139044942,
        -0.7268699329282682,
        -0.7589966994227977,
        -0.7386474149011908,
        -0.7628512110723604,
        -0.752863899244222,
        -0.7564407035870075,
        -0.753960430879505,
        -0.7525200901504021,
        -0.7516312422891903,
        -0.7616582114173422,
        -0.7348955671055508,
        -0.7179336337369773,
        -0.7507314342666594,
        -0.709962390135586,
        -0.7380386534087352,
        -0.7589001667844639,
        -0.7157484675087668,
        -0.7181666753072224,
        -0.7592432950216864,
        -0.7634015364952511,
        -0.7507873487539647,
        -0.7095318859102151,
        -0.754954459107978,
        -0.7482944326661554,
        -0.7623441319366517,
        -0.7573685662389926,
        -0.7630817122147213,
        -0.7641706761629257,
        -0.760175735715261,
        -0.7634581030282219
      ],
      [
        -0.7418482639299573,
        -0.7509550357864268,
        -0.7463686549009052,
        -0.7460097375170569,
        -0.7293238687729846,
        -0.7443920726535357,
        -0.7294396180435586,
        -0.7474878309106867,
        -0.7528181164652117,
        -0.7144307006226795,
        -0.5606653007057153,
        -0.7210245393686461,
        -0.7431448786844874,
        -0.7510673061646009,
        -0.7464588583237959,
        -0.7278842700065129,
        -0.6935648986168174,
        -0.7485315457621418,
        -0.7137057111153342,
        -0.7101518488868139,
        -0.694596623079233,
        -0.735281738964974,
        -0.7283582658771234,
        -0.7543845471893678,
        -0.7240360874943486,
        -0.724225909575321,
        -0.7092621350129933,
        -0.7387865684563769,
        -0.7550074791844142,
        -0.740587609342034,
        -0.6704028228937411,
        -0.7607310075703562,
        -0.7580673855300046,
        -0.3953790502423622,
        1.0,
        -0.7376237144734353,
        -0.05906392735682755,
        -0.7401063557466618,
        -0.7523666624727178,
        -0.7518219000624973,
        -0.7566656879675255,
        -0.7455538074676182,
        -0.749444234458484,
        -0.7436759050219683,
        -0.7398906948785323,
        -0.726255348578988,
        -0.7373907067943712,
        -0.6843421514640982,
        -0.7385027767086558,
        -0.7206552703303549,
        -0.7414043652373683,
        -0.7211512894115427,
        -0.7525101062522429,
        -0.7444252643707168,
        -0.46903663842868076,
        -0.7484358976812506,
        -0.7556061837960332,
        -0.7577834928290903,
        -0.7179863415232972,
        -0.748360724158099,
        -0.7158862735008391,
        -0.7580724170039201,
        -0.737638375688972,
        -0.756306830261576,
        -0.7630141209460765,
        -0.7497593841177119,
        -0.7577556986543431
      ],
      [
        -0.7407043321379003,
        -0.745078158557013,
        -0.6784722900447213,
        -0.7470139037408952,
        -0.7033725111341376,
        -0.7443422435586743,
        -0.7295422735935635,
        -0.7257305468117263,
        -0.7510783391988292,
        -0.74426272541624,
        -0.7318969191631642,
        -0.7491988589960489,
        -0.7318636068151796,
        -0.7465045460634814,
        -0.7205699886655157,
        -0.7241386340497615,
        -0.7419865759618428,
        -0.7394283090973153,
        -0.7139645695069238,
        -0.7064477882979434,
        -0.7416558700618883,
        -0.7266405292754792,
        -0.6886963348831653,
        -0.748941911372436,
        -0.7513673293594447,
        -0.748259103087447,
        -0.7624838609738382,
        -0.7260523540287283,
        -0.7507648568464154,
        -0.744476610071896,
        -0.7327313016936277,
        -0.7528174808868209,
        -0.7608787121733466,
        -0.7596548850588786,
        -0.7376237144734353,
        1.0,
        -0.7411422835278234,
        -0.7079112440741699,
        -0.6880505545908209,
        -0.7600438796016424,
        -0.7598050726998982,
        -0.755068219382611,
        -0.7449628717992216,
        -0.7390641165326667,
        -0.7446853296600544,
        -0.7425920444306633,
        -0.47896771813110867,
        -0.742047752151107,
        -0.7510051713250779,
        -0.7275320921545403,
        -0.7528766322832966,
        -0.7492991534652198,
        -0.7422554788748705,
        -0.7499093617527466,
        -0.7431005800789345,
        -0.7318786079459847,
        -0.7483862786480642,
        -0.7563986448064527,
        -0.7197404317118694,
        -0.7369919083201955,
        -0.6852550435692788,
        -0.759800615648291,
        -0.7232677067279414,
        -0.7396591207721839,
        -0.7585894752780683,
        -0.7405387039051421,
        -0.7497046885387983
      ],
      [
        -0.7458842850051633,
        -0.7478679438578216,
        -0.7495169878936314,
        -0.7362877313499745,
        -0.732266706265128,
        -0.7419570275532401,
        -0.7245767205564575,
        -0.748152523374804,
        -0.7554056311900701,
        -0.6999619448305181,
        -0.45267517827894754,
        -0.7035493948062385,
        -0.7475556047972735,
        -0.7484815403225064,
        -0.7524949353042565,
        -0.7303832983957141,
        -0.7051529933409948,
        -0.7440892344519446,
        -0.7016246206718029,
        -0.6871034351854073,
        -0.6898921866810053,
        -0.7419999156079315,
        -0.7323200247433856,
        -0.7532373454515982,
        -0.7272786562596422,
        -0.7265024352303294,
        -0.7269188426559366,
        -0.7406802455569395,
        -0.7554445272382828,
        -0.750974065907402,
        -0.6280083811436379,
        -0.7588890255792806,
        -0.7612748143270452,
        -0.6880811139044942,
        -0.05906392735682755,
        -0.7411422835278234,
        1.0,
        -0.7490323453741128,
        -0.7509867100503909,
        -0.75686608519158,
        -0.7524609404201863,
        -0.7451197975946184,
        -0.7478752691305304,
        -0.7457344161723072,
        -0.7207922866206926,
        -0.6965139697588618,
        -0.740237193003746,
        -0.7024644646008342,
        -0.7391980977787872,
        -0.7183544837644081,
        -0.7512220531585588,
        -0.7234093287827523,
        -0.753035368132811,
        -0.7538989135130396,
        -0.3730517431546634,
        -0.7470627051114525,
        -0.756649791051135,
        -0.7590422377106977,
        -0.7325842241498497,
        -0.7438819376480887,
        -0.7101034743069959,
        -0.7570404217182491,
        -0.7401499572776018,
        -0.7568992538651336,
        -0.7604072928357006,
        -0.7510843379838557,
        -0.7578837814551702
      ],
      [
        -0.7338866977123973,
        -0.745535752390815,
        -0.47186732684149724,
        -0.7495100694605308,
        -0.6934116656897819,
        -0.7394024708636904,
        -0.7389942875120173,
        -0.7447115238863851,
        -0.6832436369355043,
        -0.7549701118339901,
        -0.7395234279824298,
        -0.7420755216323793,
        -0.7299789477188394,
        -0.7433459933759282,
        -0.7122415166728951,
        -0.16529949564374358,
        -0.7465915513770525,
        -0.7334467820823677,
        -0.7348385846898287,
        -0.6909942869979693,
        -0.7387560901947581,
        -0.719612981972527,
        -0.6985343146468923,
        -0.7508038280225753,
        -0.7512309759584004,
        -0.681755995614403,
        -0.7619246438807221,
        -0.7288783153047346,
        -0.646290331753427,
        -0.746080972560508,
        -0.7232436965135126,
        -0.7295316291697762,
        -0.7238277315452984,
        -0.7268699329282682,
        -0.7401063557466618,
        -0.7079112440741699,
        -0.7490323453741128,
        1.0,
        -0.7432335908113261,
        -0.7385041078239045,
        -0.7560639230793462,
        -0.7553972359705534,
        -0.754129455607119,
        -0.7408996001674513,
        -0.7451741124541762,
        -0.7334356354815277,
        -0.5452067179679101,
        -0.7492616238119832,
        -0.6770170419469341,
        -0.7212511222233438,
        -0.6678923474697074,
        -0.749404211614424,
        -0.7482459452107972,
        -0.6724882812511388,
        -0.7418294368043588,
        -0.7352016683929178,
        -0.7481664647227518,
        -0.680870167915322,
        -0.3314223174729344,
        -0.7255101713520373,
        -0.6955817369097651,
        -0.759542460413422,
        -0.727355021541082,
        -0.6985404611794731,
        -0.7398026170497296,
        -0.7428364682823319,
        -0.7469743354133016
      ],
      [
        -0.749356242678225,
        -0.7397537035469783,
        -0.7397965997236379,
        -0.7551568543876845,
        -0.7366055144012724,
        -0.7483068633651946,
        -0.7363789143385755,
        -0.7499920148182466,
        -0.7555276755362685,
        -0.7542402897862002,
        -0.7451866775560205,
        -0.750113499165195,
        -0.7525277012288747,
        -0.7512468492591586,
        -0.7527539309656828,
        -0.7461531247864921,
        -0.752364882925971,
        -0.7527692163818729,
        -0.738155047826522,
        -0.7356493662861084,
        -0.7531838069371352,
        -0.7409926784717351,
        -0.7213442216463193,
        -0.7557675644750812,
        -0.7564571970602866,
        -0.7534037514062306,
        -0.7601756929161856,
        -0.7517827111403437,
        -0.7567732256468426,
        -0.7497305708592652,
        -0.7495315467389316,
        -0.7569750691503256,
        -0.7619008137222354,
        -0.7589966994227977,
        -0.7523666624727178,
        -0.6880505545908209,
        -0.7509867100503909,
        -0.7432335908113261,
        1.0,
        -0.7626517043279164,
        -0.7587193024862116,
        -0.7589435401530483,
        -0.7567641352349155,
        -0.7470088594983568,
        -0.7525771463455199,
        -0.7515271410786453,
        -0.7116573620743007,
        -0.7538577101653372,
        -0.7576971418088374,
        -0.7500360573441052,
        -0.7505095958280106,
        -0.7565570585418586,
        -0.7539693600043131,
        -0.747320169310129,
        -0.7498739418867891,
        -0.7416329770176291,
        -0.7554134093199498,
        -0.7604235997883133,
        -0.7397148287060791,
        -0.7517937920093938,
        -0.7197405485069119,
        -0.7641684706203746,
        -0.6991001997101434,
        -0.7585089556148469,
        -0.7470087630754745,
        -0.753503798279364,
        -0.7569524001248127
      ],
      [
        -0.7624943033930562,
        -0.7628562640414553,
        -0.7607514018597954,
        -0.7646550518631353,
        -0.7607653687645239,
        -0.763077431925927,
        -0.7564974285154291,
        -0.7459266748202877,
        -0.7481244777057392,
        -0.7541079943543941,
        -0.760310191210237,
        -0.7611376740443982,
        -0.7443778395687617,
        -0.7615511544320575,
        -0.7630950603841389,
        -0.7347281140118893,
        -0.7463378197281798,
        -0.7601083553048278,
        -0.76064181256698,
        -0.7421748104416653,
        -0.7498244413799338,
        -0.7604508564696351,
        -0.7541648397035088,
        -0.7608933025729885,
        -0.7427148573400154,
        -0.7432319502936646,
        -0.7602793570974644,
        -0.7450433194094604,
        -0.7561314632761781,
        -0.7596367088725748,
        -0.7541217795320587,
        -0.7639934186440024,
        -0.758319856206287,
        -0.7386474149011908,
        -0.7518219000624973,
        -0.7600438796016424,
        -0.75686608519158,
        -0.7385041078239045,
        -0.7626517043279164,
        1.0,
        -0.7636907664380999,
        -0.7591181150384552,
        -0.7640124099621273,
        -0.75897702664805,
        -0.7600010423638297,
        -0.7562204081007908,
        -0.757157857301039,
        -0.7500410942997173,
        -0.7365325057042884,
        -0.7419100365080052,
        -0.7310781697951391,
        -0.7520300651523597,
        -0.7596542340977788,
        -0.7516178532316456,
        -0.7510294556099519,
        -0.7613158116627837,
        -0.7624022649506144,
        -0.7551110424148044,
        -0.7443094081848591,
        -0.7596321705822299,
        -0.7496155162788649,
        -0.7623412147391218,
        -0.758444963628732,
        -0.7644732017474127,
        -0.759726466037062,
        -0.7630983440699473,
        -0.7625640180760036
      ],
      [
        -0.7571825288850886,
        -0.7455103507860151,
        -0.7531412561171789,
        -0.754721696602782,
        -0.7548630197790447,
        -0.7538272411589697,
        -0.7533275694789748,
        -0.7376704455938428,
        -0.7609628469201652,
        -0.753515620161123,
        -0.7474169553943791,
        -0.7437891312493442,
        -0.7148820352388094,
        -0.7573385493177498,
        -0.7590207101378886,
        -0.7598389345348502,
        -0.7368597939891256,
        -0.7528389283660444,
        -0.7423037259141583,
        -0.7554557080303699,
        -0.7505432517362169,
        -0.7571282690700838,
        -0.7536898531471463,
        -0.7568859140276811,
        -0.7281085768597675,
        -0.7520404023927314,
        -0.7515810629093271,
        -0.7300285016723871,
        -0.7569353098082616,
        -0.7579012118945172,
        -0.7562713639988741,
        -0.7597570246223864,
        -0.7598883050279484,
        -0.7628512110723604,
        -0.7566656879675255,
        -0.7598050726998982,
        -0.7524609404201863,
        -0.7560639230793462,
        -0.7587193024862116,
        -0.7636907664380999,
        1.0,
        -0.7579824559355621,
        -0.7574965423490327,
        -0.7560361711402164,
        -0.7584268450718445,
        -0.7560257815536826,
        -0.7570019185044834,
        -0.7515123900163229,
        -0.7614187702008393,
        -0.7475255977088195,
        -0.7524450873767998,
        -0.6995494649862888,
        -0.7531732946094962,
        -0.7475394537827673,
        -0.7565105960773024,
        -0.7590032408089382,
        -0.7466085958383185,
        -0.7626167536745977,
        -0.7541706006463584,
        -0.7411306140735648,
        -0.7537398248201366,
        -0.7641680251780476,
        -0.7510381912311371,
        -0.7574029377866328,
        -0.762552130501009,
        -0.7635400697160112,
        -0.7481125660729816
      ],
      [
        -0.7267595362678765,
        -0.7553859645805477,
        -0.7575487559329546,
        -0.6736121605283079,
        -0.7431833402277208,
        -0.753756156072132,
        -0.7582300095581422,
        -0.7535048843733139,
        -0.7565357319329333,
        -0.7523236570353429,
        -0.745970999378136,
        -0.7548328376076257,
        -0.7510151912104243,
        -0.7506274615619764,
        -0.7596495511489414,
        -0.7496794805009157,
        -0.7494470196637621,
        -0.7503508188886536,
        -0.7485070563065754,
        -0.7450972806443301,
        -0.7561466086225723,
        -0.7387051164950282,
        -0.7415973547555168,
        -0.7566352276778691,
        -0.7572093242498452,
        -0.7323212778007526,
        -0.7639914746013445,
        -0.7484770241985468,
        -0.7606519394477173,
        -0.7525087503926879,
        -0.7480647537605561,
        -0.761078732971675,
        -0.7598926871051958,
        -0.752863899244222,
        -0.7455538074676182,
        -0.755068219382611,
        -0.7451197975946184,
        -0.7553972359705534,
        -0.7589435401530483,
        -0.7591181150384552,
        -0.7579824559355621,
        1.0,
        -0.7506749900776596,
        -0.751497399191732,
        -0.7543815053007283,
        -0.7566014612406889,
        -0.7538742704376151,
        -0.7563138346583078,
        -0.7555911980684081,
        -0.7507157024813254,
        -0.7553092976015532,
        -0.7538405921097069,
        -0.7580968309798806,
        -0.7540797879237022,
        -0.7381364119209515,
        -0.7553444768372055,
        -0.7575191414443802,
        -0.7597975735522888,
        -0.7399506920247773,
        -0.7484426901268646,
        -0.7395558745182897,
        -0.7560070856669634,
        -0.7537223416780021,
        -0.7616680457028691,
        -0.7646105754374212,
        -0.7559774336222593,
        -0.7582742231268205
      ],
      [
        -0.7483333006030758,
        -0.7555102294182061,
        -0.7533773801197703,
        -0.7333485694932143,
        -0.7449987870011912,
        -0.7557531193476787,
        -0.746596768933079,
        -0.7533871374739072,
        -0.7572786404943037,
        -0.7511297223516002,
        -0.6229157032002408,
        -0.7429713108078821,
        -0.7497137273166029,
        -0.7186524226138651,
        -0.7558892305004237,
        -0.7479138361770579,
        -0.7411834595080707,
        -0.7122845139990919,
        -0.5802027377130129,
        -0.7350611912561511,
        -0.7392466616340767,
        -0.7494789685176468,
        -0.7393960907024746,
        -0.7568309205605945,
        -0.7254687046839565,
        -0.7580612380022411,
        -0.7551770279110537,
        -0.7489862858646472,
        -0.7584875476186517,
        -0.7280430140865459,
        -0.7493379283905691,
        -0.7571382656048801,
        -0.7611202362299729,
        -0.7564407035870075,
        -0.749444234458484,
        -0.7449628717992216,
        -0.7478752691305304,
        -0.754129455607119,
        -0.7567641352349155,
        -0.7640124099621273,
        -0.7574965423490327,
        -0.7506749900776596,
        1.0,
        -0.7463396739845531,
        -0.7556492159226178,
        -0.7484683155821925,
        -0.7471027495875256,
        -0.7347970705556485,
        -0.7530651102377409,
        -0.7358975901873459,
        -0.7587381807548285,
        -0.7488454074498093,
        -0.7559036392361709,
        -0.7548309807190654,
        -0.7485372696724553,
        -0.7304961431729304,
        -0.754814927534901,
        -0.7611726146547858,
        -0.7359718584048038,
        -0.7133587961793393,
        -0.7360124921126929,
        -0.7641288071440233,
        -0.7480500740436486,
        -0.7599711125837866,
        -0.7607591502805965,
        -0.7595337956055759,
        -0.7586292524318522
      ],
      [
        -0.7349187518518958,
        -0.7509686689168299,
        -0.7320031122513702,
        -0.7502959202083135,
        -0.7165480774894464,
        -0.7094012251420438,
        -0.74079158818419,
        -0.6853893764728307,
        -0.7552216422062705,
        -0.7445198697057194,
        -0.739825685914182,
        -0.7424348397311004,
        -0.7302475845512829,
        -0.19283419974221078,
        -0.664053687136822,
        -0.7473471445405968,
        -0.7439386391241183,
        -0.7448740613136864,
        -0.7361228128927235,
        -0.7470468603286111,
        -0.7174144252424752,
        -0.7113155556297043,
        -0.7200865098227096,
        -0.5105501797242112,
        -0.7483783522151272,
        -0.746284484922497,
        -0.7540821652545184,
        -0.7221843421722887,
        -0.7414369512250515,
        -0.7050205500323254,
        -0.7233631434432881,
        -0.7535471975605628,
        -0.7579745304473341,
        -0.753960430879505,
        -0.7436759050219683,
        -0.7390641165326667,
        -0.7457344161723072,
        -0.7408996001674513,
        -0.7470088594983568,
        -0.75897702664805,
        -0.7560361711402164,
        -0.751497399191732,
        -0.7463396739845531,
        1.0,
        0.12769894204530804,
        -0.7177741007683636,
        -0.732507797394524,
        -0.7263734027738291,
        -0.7552125134158016,
        -0.7440360846770536,
        -0.7553704954156413,
        -0.7378088373490278,
        -0.5823704578653472,
        -0.7525989415354242,
        -0.7334770367348384,
        -0.7462878525153738,
        -0.7354606306106113,
        -0.7390452700936752,
        -0.7272587099099733,
        -0.7354762333787737,
        -0.7259805126724459,
        -0.7517435217105028,
        -0.7356398335675804,
        -0.7460118598577871,
        -0.7545653903654119,
        -0.7382927574266519,
        -0.724279127621879
      ],
      [
        -0.7370299879851925,
        -0.7565157235635382,
        -0.7439538850424752,
        -0.75620816286769,
        -0.7050504779069541,
        -0.7239694690227677,
        -0.7448123813698575,
        -0.73757685967387,
        -0.7592144962067238,
        -0.7257074032653255,
        -0.7095049908766962,
        -0.7450687456932489,
        -0.7258934027609035,
        -0.2679721360910262,
        -0.736527011436364,
        -0.7498384727456939,
        -0.7430636941333479,
        -0.7476680209094397,
        -0.745166722647775,
        -0.7499772259763882,
        -0.6802935842004094,
        -0.6879976030042623,
        -0.7245544926336274,
        -0.26102110245175425,
        -0.7474704416222835,
        -0.7354969533118099,
        -0.7624112766073816,
        -0.7057504888484576,
        -0.7474764662550992,
        -0.6970142187309507,
        -0.6445304308558291,
        -0.7563740360242155,
        -0.759959059105867,
        -0.7525200901504021,
        -0.7398906948785323,
        -0.7446853296600544,
        -0.7207922866206926,
        -0.7451741124541762,
        -0.7525771463455199,
        -0.7600010423638297,
        -0.7584268450718445,
        -0.7543815053007283,
        -0.7556492159226178,
        0.12769894204530804,
        1.0,
        -0.6643232599236203,
        -0.7363846421036218,
        -0.715222311243082,
        -0.7542154232776239,
        -0.7436093822201413,
        -0.7559529965496687,
        -0.7252809532546154,
        -0.3419547254948465,
        -0.7527295130345728,
        -0.6980698327376289,
        -0.7485061086338981,
        -0.7224550567896297,
        -0.7341897936549888,
        -0.7284527429705965,
        -0.7396503416509677,
        -0.7298043794226913,
        -0.7455388637584162,
        -0.7406420019437451,
        -0.7487351930511388,
        -0.757680083096093,
        -0.7302797958439706,
        -0.7041299964766643
      ],
      [
        -0.7409793641803004,
        -0.7481991010009394,
        -0.7267646974551389,
        -0.7519706130578337,
        -0.710641373947623,
        -0.7461380737428809,
        -0.7105515197824945,
        -0.7128525289243941,
        -0.749571019375164,
        -0.5763919229254498,
        -0.6798878472761021,
        -0.7326342959043963,
        -0.7234137625928228,
        -0.7360837332345934,
        -0.7383278113152975,
        -0.7359164178790314,
        -0.7192527628094938,
        -0.7477025954604327,
        -0.7313802718102655,
        -0.735535997216545,
        -0.08700467652995744,
        -0.7267834079936593,
        -0.7349298924926436,
        -0.7199576938523373,
        -0.7309840439637905,
        -0.7067296182820011,
        -0.7567052888274206,
        -0.705205956790668,
        -0.7509475812904556,
        -0.6872359655824982,
        -0.577333931612554,
        -0.7480440091397145,
        -0.7603299297004562,
        -0.7516312422891903,
        -0.726255348578988,
        -0.7425920444306633,
        -0.6965139697588618,
        -0.7334356354815277,
        -0.7515271410786453,
        -0.7562204081007908,
        -0.7560257815536826,
        -0.7566014612406889,
        -0.7484683155821925,
        -0.7177741007683636,
        -0.6643232599236203,
        1.0,
        -0.7323060246657923,
        -0.5984845044332578,
        -0.7436528293827491,
        -0.7298104195617764,
        -0.7407897975749769,
        -0.5799472521888455,
        -0.711310151240376,
        -0.7407632863458412,
        -0.6136235061896786,
        -0.7504670767976392,
        -0.7422671966833827,
        -0.7451204246238566,
        -0.720023974734969,
        -0.7423661064338161,
        -0.731931345181607,
        -0.7525764983584031,
        -0.7417971484354036,
        -0.7478116199854856,
        -0.7622450632288513,
        -0.7375110941308116,
        -0.74460380903499
      ],
      [
        -0.721305321146817,
        -0.7450005800511064,
        -0.26892305968251107,
        -0.7366337859990876,
        -0.59564127130268,
        -0.7258781749633312,
        -0.6997819462970902,
        -0.7194719206016236,
        -0.7474950988077249,
        -0.7432559137227281,
        -0.725510871747341,
        -0.7358557578556841,
        -0.7042079439847214,
        -0.7433864808406015,
        -0.6484948569108522,
        -0.7087371084066065,
        -0.7341527715807596,
        -0.7442559189344864,
        -0.7114481293537982,
        -0.709939524581916,
        -0.7366208706847591,
        -0.715044493336802,
        -0.6793078521951248,
        -0.740386615089891,
        -0.7486289927766119,
        -0.7442088806111167,
        -0.7611641224759698,
        -0.6974213837324545,
        -0.7468642389841351,
        -0.73752086615491,
        -0.7002012211803504,
        -0.7445843614134726,
        -0.7594632298891465,
        -0.7616582114173422,
        -0.7373907067943712,
        -0.47896771813110867,
        -0.740237193003746,
        -0.5452067179679101,
        -0.7116573620743007,
        -0.757157857301039,
        -0.7570019185044834,
        -0.7538742704376151,
        -0.7471027495875256,
        -0.732507797394524,
        -0.7363846421036218,
        -0.7323060246657923,
        1.0,
        -0.7411568274906263,
        -0.7496230004494828,
        -0.7240027908920883,
        -0.7494427206743872,
        -0.7408222163850775,
        -0.7435196467530176,
        -0.7496165220447549,
        -0.7413841012878246,
        -0.7303717544377071,
        -0.7389316531102887,
        -0.755427666768729,
        -0.7045528351449493,
        -0.7383896133585752,
        -0.6841333275259052,
        -0.7549022971184367,
        -0.713851618477908,
        -0.6784117196006715,
        -0.7554534978227908,
        -0.6908273191500652,
        -0.7396185659106423
      ],
      [
        -0.7399101335486191,
        -0.7521046411369596,
        -0.7471174083603795,
        -0.7497813665102803,
        -0.7236569450934962,
        -0.7459550700860462,
        -0.700507636962534,
        -0.7287267176057066,
        -0.7583932950084262,
        -0.6932634533589941,
        -0.641958780026803,
        -0.7248290276007026,
        -0.7312650846904888,
        -0.7390748094431389,
        -0.7505737868230762,
        -0.7460701596408965,
        -0.6321251212800255,
        -0.7503760979351478,
        -0.6589908771596797,
        -0.7374096405142401,
        0.2444127903973542,
        -0.7304465675926977,
        -0.7297581629991912,
        -0.7378564873838225,
        -0.6533976084801085,
        -0.7336199185106882,
        -0.7085563869179061,
        -0.7193494597493406,
        -0.7565142937940048,
        -0.7077288343745873,
        -0.6711916399298489,
        -0.7610283252424062,
        -0.7614493971236187,
        -0.7348955671055508,
        -0.6843421514640982,
        -0.742047752151107,
        -0.7024644646008342,
        -0.7492616238119832,
        -0.7538577101653372,
        -0.7500410942997173,
        -0.7515123900163229,
        -0.7563138346583078,
        -0.7347970705556485,
        -0.7263734027738291,
        -0.715222311243082,
        -0.5984845044332578,
        -0.7411568274906263,
        1.0,
        -0.7510256461904399,
        -0.659011255565856,
        -0.7537868241432941,
        -0.6700163478044945,
        -0.7302850535654242,
        -0.754277221945416,
        -0.6775638399934214,
        -0.7494440812226475,
        -0.7490104293171966,
        -0.7577318391906303,
        -0.7291260931350995,
        -0.7418942031682583,
        -0.728630999093703,
        -0.7547107639393275,
        -0.7438664593169215,
        -0.7465185056816042,
        -0.7621253429804027,
        -0.7435832554393884,
        -0.7519685054289819
      ],
      [
        -0.7481984692918849,
        -0.7566275146436713,
        -0.7589813197298312,
        -0.7505028961477764,
        -0.7457419215926431,
        -0.7566162989252724,
        -0.7557349547684957,
        -0.7572033279921713,
        -0.7115110213066456,
        -0.7589090339142947,
        -0.7363594629482967,
        -0.7488830839853426,
        -0.7520440819012857,
        -0.7481132718454635,
        -0.7565506458286064,
        -0.42809255316710737,
        -0.7508257544897653,
        -0.738561594650732,
        -0.7412974183673171,
        -0.46939621113646635,
        -0.745565494531081,
        -0.7431294145333327,
        -0.733790200298708,
        -0.7590739647749276,
        -0.7436272799258685,
        -0.654384744182259,
        -0.7639792124623523,
        -0.7485466276253053,
        -0.7319360283793523,
        -0.7510939101429039,
        -0.7349036757290093,
        -0.7605756646536574,
        -0.6094590990664808,
        -0.7179336337369773,
        -0.7385027767086558,
        -0.7510051713250779,
        -0.7391980977787872,
        -0.6770170419469341,
        -0.7576971418088374,
        -0.7365325057042884,
        -0.7614187702008393,
        -0.7555911980684081,
        -0.7530651102377409,
        -0.7552125134158016,
        -0.7542154232776239,
        -0.7436528293827491,
        -0.7496230004494828,
        -0.7510256461904399,
        1.0,
        -0.5323968493227176,
        -0.5707238072855954,
        -0.7334687684354583,
        -0.7569669876800247,
        -0.6893856649643331,
        -0.7269230239590536,
        -0.7484918184662892,
        -0.7601891541859086,
        -0.7129058315403823,
        -0.6685338822822051,
        -0.7385199071915113,
        -0.6482909592201576,
        -0.7617394416719079,
        -0.747433485984226,
        -0.7616868024719841,
        -0.7586572408806576,
        -0.7583591387110313,
        -0.760866159875522
      ],
      [
        -0.7353378934211985,
        -0.75176404764838,
        -0.7366100826788208,
        -0.7467953217730592,
        -0.724622226327416,
        -0.7520365317533951,
        -0.7306862774532039,
        -0.663444560061199,
        -0.7260415326535112,
        -0.7348759946713432,
        -0.6398734061922335,
        -0.7316749733635608,
        -0.47472882018651447,
        -0.7409482043421376,
        -0.7426658213073261,
        -0.5484023352222327,
        -0.7218837520249508,
        -0.7356430072709095,
        -0.6207499808657858,
        -0.3761356892152332,
        -0.7221103593326443,
        -0.7377510786744732,
        -0.7259339518342116,
        -0.744756393487181,
        -0.6319724696560141,
        -0.7279442287282617,
        -0.757433092691959,
        -0.4959901257442271,
        -0.7434810289574794,
        -0.7395313686746952,
        -0.7161733171672893,
        -0.759084005027348,
        -0.7467802904446802,
        -0.7507314342666594,
        -0.7206552703303549,
        -0.7275320921545403,
        -0.7183544837644081,
        -0.7212511222233438,
        -0.7500360573441052,
        -0.7419100365080052,
        -0.7475255977088195,
        -0.7507157024813254,
        -0.7358975901873459,
        -0.7440360846770536,
        -0.7436093822201413,
        -0.7298104195617764,
        -0.7240027908920883,
        -0.659011255565856,
        -0.5323968493227176,
        1.0,
        -0.6610195652631671,
        -0.6884122407512026,
        -0.7332512855780651,
        -0.7420667326265791,
        -0.718589297419058,
        -0.7473018500776021,
        -0.738281618806257,
        -0.7387380344900529,
        -0.7098815569016095,
        -0.647343062454816,
        -0.6539011039949273,
        -0.7523761078924373,
        -0.7394372813268157,
        -0.7493511536802746,
        -0.7625492407009272,
        -0.7469276997469018,
        -0.7431701655307685
      ],
      [
        -0.7482132038051701,
        -0.7523849691973004,
        -0.7572211633722163,
        -0.761433344720839,
        -0.7407302521179336,
        -0.7540674212948644,
        -0.7599992411342721,
        -0.7589377534061714,
        -0.6238842974210064,
        -0.7622729545491523,
        -0.7477481233534151,
        -0.755056242661248,
        -0.7598519975937901,
        -0.7499368054187591,
        -0.7606161568052208,
        -0.5643753523599582,
        -0.7555913926279845,
        -0.7383552857476334,
        -0.7497764345177094,
        -0.633813531978668,
        -0.7446078026469901,
        -0.738256245051676,
        -0.735893743337112,
        -0.7548639167124456,
        -0.7558058483112275,
        -0.4575517345563196,
        -0.7639517070640147,
        -0.7568136841766502,
        -0.7402094464379219,
        -0.7531145235570215,
        -0.7405776561471777,
        -0.7576807720332273,
        -0.7485995641540817,
        -0.709962390135586,
        -0.7414043652373683,
        -0.7528766322832966,
        -0.7512220531585588,
        -0.6678923474697074,
        -0.7505095958280106,
        -0.7310781697951391,
        -0.7524450873767998,
        -0.7553092976015532,
        -0.7587381807548285,
        -0.7553704954156413,
        -0.7559529965496687,
        -0.7407897975749769,
        -0.7494427206743872,
        -0.7537868241432941,
        -0.5707238072855954,
        -0.6610195652631671,
        1.0,
        -0.7451850938308315,
        -0.7555876697179874,
        -0.03422139615733305,
        -0.7417685986368114,
        -0.7474478262426469,
        -0.7563276163840388,
        -0.6807435842636053,
        -0.6459598898720221,
        -0.7364905847194905,
        -0.6937375900806595,
        -0.7577803868896854,
        -0.7473723332230735,
        -0.7617584879797511,
        -0.7510109344381883,
        -0.7559644728955368,
        -0.7566306271931887
      ],
      [
        -0.7382786423300329,
        -0.7562652109793754,
        -0.7425256067654992,
        -0.7579337758093883,
        -0.6995412050187337,
        -0.7506602481434463,
        -0.7140839502249992,
        -0.6815834079524574,
        -0.7575757732857519,
        -0.7079992040696852,
        -0.7078207345469976,
        -0.6637616594606153,
        -0.6524140474328608,
        -0.7448755082110776,
        -0.7405015003871728,
        -0.7493746424509387,
        -0.6863824212812332,
        -0.7484662296770688,
        -0.7329436045130492,
        -0.7464715647188114,
        -0.5733885221791352,
        -0.7349913394743628,
        -0.735972054020217,
        -0.732716189825722,
        -0.6687817953516613,
        -0.7183228729476092,
        -0.7382724689733449,
        -0.5415961451824947,
        -0.7585776270015638,
        -0.7328642315828062,
        -0.6692494487577606,
        -0.7603905462966853,
        -0.7582242144683071,
        -0.7380386534087352,
        -0.7211512894115427,
        -0.7492991534652198,
        -0.7234093287827523,
        -0.749404211614424,
        -0.7565570585418586,
        -0.7520300651523597,
        -0.6995494649862888,
        -0.7538405921097069,
        -0.7488454074498093,
        -0.7378088373490278,
        -0.7252809532546154,
        -0.5799472521888455,
        -0.7408222163850775,
        -0.6700163478044945,
        -0.7334687684354583,
        -0.6884122407512026,
        -0.7451850938308315,
        1.0,
        -0.7057306331745912,
        -0.7488910772064343,
        -0.7151519278397322,
        -0.75441616280573,
        -0.7265399349794337,
        -0.7586510514543402,
        -0.7392202841111029,
        -0.7296586881130935,
        -0.7366637666456346,
        -0.7567263372325886,
        -0.7472526333178237,
        -0.750910992339889,
        -0.7630721607868756,
        -0.7342513509118276,
        -0.7266957577689299
      ],
      [
        -0.7513555256086208,
        -0.7571520078032965,
        -0.7431092370963863,
        -0.7584939697605001,
        -0.7324955696063338,
        -0.7485944374684073,
        -0.7506048163549363,
        -0.7330521365302397,
        -0.76108883338402,
        -0.7405335689829805,
        -0.7459759386278422,
        -0.7493369855322507,
        -0.6941288454077724,
        -0.6352252282179577,
        -0.744131012013036,
        -0.7516171004832117,
        -0.7502752758009889,
        -0.7509499724044801,
        -0.7375848823854089,
        -0.7472205401956986,
        -0.7214552953124798,
        -0.7363377586923935,
        -0.7401318509309194,
        -0.37288080442079824,
        -0.7474226475196497,
        -0.7506339114558737,
        -0.7631523973039807,
        -0.6812901734851159,
        -0.7557989637341882,
        -0.7313675242811156,
        -0.7264002193937491,
        -0.7584696493752479,
        -0.762980025104574,
        -0.7589001667844639,
        -0.7525101062522429,
        -0.7422554788748705,
        -0.753035368132811,
        -0.7482459452107972,
        -0.7539693600043131,
        -0.7596542340977788,
        -0.7531732946094962,
        -0.7580968309798806,
        -0.7559036392361709,
        -0.5823704578653472,
        -0.3419547254948465,
        -0.711310151240376,
        -0.7435196467530176,
        -0.7302850535654242,
        -0.7569669876800247,
        -0.7332512855780651,
        -0.7555876697179874,
        -0.7057306331745912,
        1.0,
        -0.7548314111926517,
        -0.7429227674436112,
        -0.750743237624838,
        -0.2590484974849703,
        -0.7460772508417925,
        -0.7450386548640038,
        -0.7392301219750468,
        -0.7410607583545985,
        -0.7560221706653342,
        -0.744331830184872,
        -0.7438492989503696,
        -0.7616147915250953,
        -0.7477251047664224,
        -0.23196071971498122
      ],
      [
        -0.7504144833089774,
        -0.7501171405228724,
        -0.7519885602973173,
        -0.7617941501876355,
        -0.74030291106903,
        -0.75178657624308,
        -0.7583997480168996,
        -0.7563833073734051,
        -0.7204664362762152,
        -0.760553358313321,
        -0.7455929248095798,
        -0.7424947382509463,
        -0.7590939553657429,
        -0.7477301679327292,
        -0.7598140352523225,
        -0.6736785329267276,
        -0.755726897572972,
        -0.6945889155408216,
        -0.7478130311934819,
        -0.7283708264347242,
        -0.7450428419780419,
        -0.7380067788123399,
        -0.7415873814867585,
        -0.7521537328782831,
        -0.7565626598844832,
        -0.5585907975203264,
        -0.76409327446434,
        -0.7559983318775836,
        -0.7170608783198688,
        -0.7455295246161653,
        -0.742931706816369,
        -0.7517062263294788,
        -0.717179417573601,
        -0.7157484675087668,
        -0.7444252643707168,
        -0.7499093617527466,
        -0.7538989135130396,
        -0.6724882812511388,
        -0.747320169310129,
        -0.7516178532316456,
        -0.7475394537827673,
        -0.7540797879237022,
        -0.7548309807190654,
        -0.7525989415354242,
        -0.7527295130345728,
        -0.7407632863458412,
        -0.7496165220447549,
        -0.754277221945416,
        -0.6893856649643331,
        -0.7420667326265791,
        -0.03422139615733305,
        -0.7488910772064343,
        -0.7548314111926517,
        1.0,
        -0.7441055502474818,
        -0.745356495794638,
        -0.7530185459620742,
        -0.7013072997452372,
        -0.6704145802754038,
        -0.6915592048736556,
        -0.7379629605786193,
        -0.7590022683063731,
        -0.7503895375281533,
        -0.7610097954764645,
        -0.7535591616548292,
        -0.756366290924082,
        -0.7530435060953713
      ],
      [
        -0.7379742438469263,
        -0.7459058546131399,
        -0.7487146090869257,
        -0.7379704080963134,
        -0.7184867567644897,
        -0.7434204954562882,
        -0.7255252057293995,
        -0.7528608156980993,
        -0.7532693910105683,
        -0.6360994294094442,
        -0.5732610336963774,
        -0.7245525303341852,
        -0.7493036544000318,
        -0.7442919924155988,
        -0.7440627937388211,
        -0.7261700187991762,
        -0.6886464603324275,
        -0.726733385379706,
        -0.7158337088777096,
        -0.7090098088735075,
        -0.6449534127536272,
        -0.7247732838143279,
        -0.7287802149389573,
        -0.7482395758391924,
        -0.7303547153545025,
        -0.6385233390460566,
        -0.7420875090138384,
        -0.7369532936950733,
        -0.7530903047718,
        -0.7451491746953284,
        -0.5313973388681363,
        -0.7590839907296694,
        -0.7584089329924943,
        -0.7181666753072224,
        -0.46903663842868076,
        -0.7431005800789345,
        -0.3730517431546634,
        -0.7418294368043588,
        -0.7498739418867891,
        -0.7510294556099519,
        -0.7565105960773024,
        -0.7381364119209515,
        -0.7485372696724553,
        -0.7334770367348384,
        -0.6980698327376289,
        -0.6136235061896786,
        -0.7413841012878246,
        -0.6775638399934214,
        -0.7269230239590536,
        -0.718589297419058,
        -0.7417685986368114,
        -0.7151519278397322,
        -0.7429227674436112,
        -0.7441055502474818,
        1.0,
        -0.7468187549553997,
        -0.7547257154645738,
        -0.7527567576077352,
        -0.7136895066422554,
        -0.7420625905419397,
        -0.7180286654958641,
        -0.7539790612846671,
        -0.7438719636825208,
        -0.7563810802373168,
        -0.7626183929694317,
        -0.747722962481824,
        -0.7579804503872991
      ],
      [
        -0.7466130534895965,
        -0.7425025868667237,
        -0.7418727340561188,
        -0.7474089811869062,
        -0.7295385633804979,
        -0.7331350471114113,
        -0.744042888437155,
        -0.7542260918954078,
        -0.7422183639524642,
        -0.7540489365048373,
        -0.6696524932624024,
        -0.7414450357099782,
        -0.7522840507018101,
        -0.6195386021689114,
        -0.7511083014244371,
        -0.7374008656029398,
        -0.7531021989555533,
        -0.6100297081518185,
        -0.654792991285152,
        -0.738987423073165,
        -0.7491631560489076,
        -0.7342384064243916,
        -0.7171734860031254,
        -0.7524944896478982,
        -0.752210384594594,
        -0.7465457985376438,
        -0.761720250665018,
        -0.7484637209963361,
        -0.7482149406987219,
        -0.7191883112146675,
        -0.7456626529812825,
        -0.7544241209289743,
        -0.756556936919054,
        -0.7592432950216864,
        -0.7484358976812506,
        -0.7318786079459847,
        -0.7470627051114525,
        -0.7352016683929178,
        -0.7416329770176291,
        -0.7613158116627837,
        -0.7590032408089382,
        -0.7553444768372055,
        -0.7304961431729304,
        -0.7462878525153738,
        -0.7485061086338981,
        -0.7504670767976392,
        -0.7303717544377071,
        -0.7494440812226475,
        -0.7484918184662892,
        -0.7473018500776021,
        -0.7474478262426469,
        -0.75441616280573,
        -0.750743237624838,
        -0.745356495794638,
        -0.7468187549553997,
        1.0,
        -0.7577612859141702,
        -0.7466238056887649,
        -0.7258877662914067,
        -0.6226789840972011,
        -0.7207189195303919,
        -0.7596160135485542,
        -0.7474325944260279,
        -0.757718131628281,
        -0.7593760206037253,
        -0.7492323666433216,
        -0.7571476709050328
      ],
      [
        -0.753321864262543,
        -0.7582669300253028,
        -0.6630842850864289,
        -0.76064192781005,
        -0.7344890579823731,
        -0.751867843159449,
        -0.7429454691808626,
        -0.7388857658969266,
        -0.7619767454200681,
        -0.7559119294475138,
        -0.7543237351222974,
        -0.7553347928380774,
        -0.6886644330456264,
        -0.7325281713971926,
        -0.7385115357877634,
        -0.7577240968806804,
        -0.7536754609824923,
        -0.7554399447356628,
        -0.7503118175400776,
        -0.7561551249762337,
        -0.7450220507809562,
        -0.7480725751128279,
        -0.7450622762220354,
        -0.7021193504070293,
        -0.7536035231789133,
        -0.7579206236481824,
        -0.7591691979347168,
        -0.6901485338813091,
        -0.7600203771606213,
        -0.7495708626411011,
        -0.7471157533435218,
        -0.7562779315795394,
        -0.7625349642824987,
        -0.7634015364952511,
        -0.7556061837960332,
        -0.7483862786480642,
        -0.756649791051135,
        -0.7481664647227518,
        -0.7554134093199498,
        -0.7624022649506144,
        -0.7466085958383185,
        -0.7575191414443802,
        -0.754814927534901,
        -0.7354606306106113,
        -0.7224550567896297,
        -0.7422671966833827,
        -0.7389316531102887,
        -0.7490104293171966,
        -0.7601891541859086,
        -0.738281618806257,
        -0.7563276163840388,
        -0.7265399349794337,
        -0.2590484974849703,
        -0.7530185459620742,
        -0.7547257154645738,
        -0.7577612859141702,
        1.0,
        -0.7609167316279251,
        -0.75154924453257,
        -0.7444839226027884,
        -0.7468496225899346,
        -0.7614070336411658,
        -0.7492797300102381,
        -0.7441083003156101,
        -0.7621241972093209,
        -0.7468994961827494,
        0.02661949154841849
      ],
      [
        -0.7544863149794386,
        -0.7579490698839484,
        -0.7588071194197136,
        -0.7617349105766538,
        -0.7500007215190783,
        -0.7580337840720754,
        -0.7618828545044372,
        -0.7621747221258459,
        -0.5249442327268055,
        -0.7613131390030397,
        -0.754923869743765,
        -0.7600510910143862,
        -0.7604087703877942,
        -0.7373490415202444,
        -0.7605621854104022,
        -0.6751175280590447,
        -0.7593758578458802,
        -0.7455181491983512,
        -0.7501130036103558,
        -0.7284836863288429,
        -0.7525393203076896,
        -0.7480480423348002,
        -0.7487631212611245,
        -0.742858937226861,
        -0.7596180832717403,
        -0.7243497749422629,
        -0.7650968781828955,
        -0.7592626865302665,
        -0.728112100141016,
        -0.7496179337163507,
        -0.7535120187463202,
        -0.7477691629865583,
        -0.7298813338782046,
        -0.7507873487539647,
        -0.7577834928290903,
        -0.7563986448064527,
        -0.7590422377106977,
        -0.680870167915322,
        -0.7604235997883133,
        -0.7551110424148044,
        -0.7626167536745977,
        -0.7597975735522888,
        -0.7611726146547858,
        -0.7390452700936752,
        -0.7341897936549888,
        -0.7451204246238566,
        -0.755427666768729,
        -0.7577318391906303,
        -0.7129058315403823,
        -0.7387380344900529,
        -0.6807435842636053,
        -0.7586510514543402,
        -0.7460772508417925,
        -0.7013072997452372,
        -0.7527567576077352,
        -0.7466238056887649,
        -0.7609167316279251,
        1.0,
        -0.6894853771322457,
        -0.743053662702253,
        -0.7427556222984563,
        -0.7607456840053942,
        -0.752544996301628,
        -0.7610812734226649,
        -0.7557299411236184,
        -0.7599740850719253,
        -0.7626044340934561
      ],
      [
        -0.6683672212040669,
        -0.7337191878049123,
        -0.7208475691870136,
        -0.7327507679409555,
        -0.6468527291113495,
        -0.72092720441439,
        -0.7302420042054958,
        -0.752966936920392,
        -0.6742330908904084,
        -0.7289428742585984,
        -0.7254618966839874,
        -0.7383618263496974,
        -0.7488955346762165,
        -0.7193565557350979,
        -0.719619258576928,
        -0.3354765518821481,
        -0.7287911777059998,
        -0.7242946551024076,
        -0.7149783028290096,
        -0.6716946395752788,
        -0.721616689544377,
        -0.6052162175276234,
        -0.6868026855022562,
        -0.7413790789420266,
        -0.7490860656732645,
        -0.5822391176361353,
        -0.7571589802464256,
        -0.7325137669675263,
        -0.6645067103016331,
        -0.7293630171789807,
        -0.6965369018836132,
        -0.7410127098240878,
        -0.7224942548506248,
        -0.7095318859102151,
        -0.7179863415232972,
        -0.7197404317118694,
        -0.7325842241498497,
        -0.3314223174729344,
        -0.7397148287060791,
        -0.7443094081848591,
        -0.7541706006463584,
        -0.7399506920247773,
        -0.7359718584048038,
        -0.7272587099099733,
        -0.7284527429705965,
        -0.720023974734969,
        -0.7045528351449493,
        -0.7291260931350995,
        -0.6685338822822051,
        -0.7098815569016095,
        -0.6459598898720221,
        -0.7392202841111029,
        -0.7450386548640038,
        -0.6704145802754038,
        -0.7136895066422554,
        -0.7258877662914067,
        -0.75154924453257,
        -0.6894853771322457,
        1.0,
        -0.7235066956850936,
        -0.6785078342835071,
        -0.7526805220810411,
        -0.7243698015930538,
        -0.7537003302675764,
        -0.7487188100585251,
        -0.7049356819273946,
        -0.7534940341193499
      ],
      [
        -0.7474902655523674,
        -0.7561545086042516,
        -0.7445819935789249,
        -0.7426551191807742,
        -0.7383439707536659,
        -0.7422799604385226,
        -0.7426369565832998,
        -0.6918255246450995,
        -0.7364825450640693,
        -0.7564353468013628,
        -0.633929680973107,
        -0.7285107290841447,
        -0.5978354818380555,
        -0.542474933214629,
        -0.7467178192307057,
        -0.7338891033532762,
        -0.7468419798065802,
        0.24153156922702376,
        -0.5604913985834565,
        -0.7366894403540132,
        -0.7395566558582459,
        -0.7401718466662512,
        -0.7359709251543561,
        -0.7478193512372684,
        -0.7354767483262998,
        -0.738886665741253,
        -0.7621246937201679,
        -0.6149478423332556,
        -0.7359350119647445,
        -0.7343212388402305,
        -0.744481721399776,
        -0.7545476197548351,
        -0.7494885844724775,
        -0.754954459107978,
        -0.748360724158099,
        -0.7369919083201955,
        -0.7438819376480887,
        -0.7255101713520373,
        -0.7517937920093938,
        -0.7596321705822299,
        -0.7411306140735648,
        -0.7484426901268646,
        -0.7133587961793393,
        -0.7354762333787737,
        -0.7396503416509677,
        -0.7423661064338161,
        -0.7383896133585752,
        -0.7418942031682583,
        -0.7385199071915113,
        -0.647343062454816,
        -0.7364905847194905,
        -0.7296586881130935,
        -0.7392301219750468,
        -0.6915592048736556,
        -0.7420625905419397,
        -0.6226789840972011,
        -0.7444839226027884,
        -0.743053662702253,
        -0.7235066956850936,
        1.0,
        -0.7329634620575849,
        -0.7558742082898482,
        -0.7422824423624654,
        -0.755895303290124,
        -0.7609634879181257,
        -0.7478861996671307,
        -0.7468634312894984
      ],
      [
        -0.7194871189958332,
        -0.7407214897663656,
        -0.7172705610298974,
        -0.7120851035387452,
        -0.6967275844699654,
        -0.7140007996994395,
        -0.7242524201677953,
        -0.7381840150328802,
        -0.7290228517602193,
        -0.7412461886341009,
        -0.6802604568517889,
        -0.7089806394593142,
        -0.7316047711060685,
        -0.7296955452489033,
        -0.7268864900609455,
        -0.5775321116367693,
        -0.71564648526253,
        -0.7339875288665518,
        -0.6895544659424022,
        -0.2373264176266262,
        -0.7254985587423798,
        -0.6950417102071539,
        0.3376784692762258,
        -0.7390039946792486,
        -0.7364269756790289,
        -0.7211660578405323,
        -0.7558227968272455,
        -0.7251008278835984,
        -0.7413388461882596,
        -0.721921315286385,
        -0.7185851582129201,
        -0.7523606350877576,
        -0.7461955226384314,
        -0.7482944326661554,
        -0.7158862735008391,
        -0.6852550435692788,
        -0.7101034743069959,
        -0.6955817369097651,
        -0.7197405485069119,
        -0.7496155162788649,
        -0.7537398248201366,
        -0.7395558745182897,
        -0.7360124921126929,
        -0.7259805126724459,
        -0.7298043794226913,
        -0.731931345181607,
        -0.6841333275259052,
        -0.728630999093703,
        -0.6482909592201576,
        -0.6539011039949273,
        -0.6937375900806595,
        -0.7366637666456346,
        -0.7410607583545985,
        -0.7379629605786193,
        -0.7180286654958641,
        -0.7207189195303919,
        -0.7468496225899346,
        -0.7427556222984563,
        -0.6785078342835071,
        -0.7329634620575849,
        1.0,
        -0.7578210427173024,
        -0.7050427320992494,
        -0.7468494878264512,
        -0.7521197903662258,
        -0.7351970485485232,
        -0.7483216076849732
      ],
      [
        -0.7352697045507495,
        -0.7616017228322687,
        -0.7628130179607673,
        -0.7610471407916539,
        -0.7236693069670346,
        -0.7571474151585872,
        -0.7613332845562625,
        -0.763065549209619,
        -0.7638553320932661,
        -0.7435827314983793,
        -0.7568638901515403,
        -0.7609776287042334,
        -0.7497142180826919,
        -0.756017493933264,
        -0.754010813570857,
        -0.7570084317320982,
        -0.7480876974164707,
        -0.7574612821802504,
        -0.7570000405853752,
        -0.7573491433470149,
        -0.7548671679329564,
        -0.7351418243953862,
        -0.7591561100460112,
        -0.7576349512394971,
        -0.7622974775556365,
        -0.7501768224120329,
        -0.7630263602091464,
        -0.7394010868025244,
        -0.7627801604815958,
        -0.7586938392348559,
        -0.7329415798825327,
        -0.7629287670356958,
        -0.763658597824179,
        -0.7623441319366517,
        -0.7580724170039201,
        -0.759800615648291,
        -0.7570404217182491,
        -0.759542460413422,
        -0.7641684706203746,
        -0.7623412147391218,
        -0.7641680251780476,
        -0.7560070856669634,
        -0.7641288071440233,
        -0.7517435217105028,
        -0.7455388637584162,
        -0.7525764983584031,
        -0.7549022971184367,
        -0.7547107639393275,
        -0.7617394416719079,
        -0.7523761078924373,
        -0.7577803868896854,
        -0.7567263372325886,
        -0.7560221706653342,
        -0.7590022683063731,
        -0.7539790612846671,
        -0.7596160135485542,
        -0.7614070336411658,
        -0.7607456840053942,
        -0.7526805220810411,
        -0.7558742082898482,
        -0.7578210427173024,
        1.0,
        -0.7612121105081622,
        -0.763702334847619,
        -0.7651792323067329,
        -0.7298431178505752,
        -0.7621241516833273
      ],
      [
        -0.734187093529445,
        -0.7439924863612097,
        -0.7210435468317289,
        -0.7481689759687531,
        -0.707088781056189,
        -0.7336832980835056,
        -0.7406049327608044,
        -0.7400215621677818,
        -0.7465017636082052,
        -0.743852275736808,
        -0.7189571984985796,
        -0.7346144769111511,
        -0.7383343461663019,
        -0.7438058707041566,
        -0.7351796325371499,
        -0.7279569349334195,
        -0.7352180955637931,
        -0.7454349307282633,
        -0.715168825848413,
        -0.7204018688331626,
        -0.7397317068297671,
        -0.7306742723215579,
        -0.7101286744498012,
        -0.7439961033738944,
        -0.7489837347877393,
        -0.743934712151828,
        -0.756691128369793,
        -0.7327657491418536,
        -0.7501098739322365,
        -0.7430130053143039,
        -0.7250216052398368,
        -0.7542579799503168,
        -0.7553585110301906,
        -0.7573685662389926,
        -0.737638375688972,
        -0.7232677067279414,
        -0.7401499572776018,
        -0.727355021541082,
        -0.6991001997101434,
        -0.758444963628732,
        -0.7510381912311371,
        -0.7537223416780021,
        -0.7480500740436486,
        -0.7356398335675804,
        -0.7406420019437451,
        -0.7417971484354036,
        -0.713851618477908,
        -0.7438664593169215,
        -0.747433485984226,
        -0.7394372813268157,
        -0.7473723332230735,
        -0.7472526333178237,
        -0.744331830184872,
        -0.7503895375281533,
        -0.7438719636825208,
        -0.7474325944260279,
        -0.7492797300102381,
        -0.752544996301628,
        -0.7243698015930538,
        -0.7422824423624654,
        -0.7050427320992494,
        -0.7612121105081622,
        1.0,
        -0.7532389336040426,
        -0.7484017978991904,
        -0.7341571050402165,
        -0.7508341212975707
      ],
      [
        -0.751603730448994,
        -0.7547451073549788,
        -0.5967881267700725,
        -0.7598336970189419,
        -0.7296781431740889,
        -0.7397193960977566,
        -0.7386396415102536,
        -0.7413750558379303,
        -0.7621690159840064,
        -0.7584927513258026,
        -0.7493584079762002,
        -0.7423619450066943,
        -0.7198796974597603,
        -0.7564353586771941,
        -0.7127669118864415,
        -0.7564154759452866,
        -0.7571291977926906,
        -0.7605183890230311,
        -0.7482371707191254,
        -0.7545622894218281,
        -0.7444445891692465,
        -0.750716847494743,
        -0.7451244433512826,
        -0.7384421017635052,
        -0.7566395735188862,
        -0.7600038264802464,
        -0.7636846413377698,
        -0.7318831854495469,
        -0.7601321719585943,
        -0.7575671469657133,
        -0.7472351578695805,
        -0.7571026329211187,
        -0.7652629236988052,
        -0.7630817122147213,
        -0.756306830261576,
        -0.7396591207721839,
        -0.7568992538651336,
        -0.6985404611794731,
        -0.7585089556148469,
        -0.7644732017474127,
        -0.7574029377866328,
        -0.7616680457028691,
        -0.7599711125837866,
        -0.7460118598577871,
        -0.7487351930511388,
        -0.7478116199854856,
        -0.6784117196006715,
        -0.7465185056816042,
        -0.7616868024719841,
        -0.7493511536802746,
        -0.7617584879797511,
        -0.750910992339889,
        -0.7438492989503696,
        -0.7610097954764645,
        -0.7563810802373168,
        -0.757718131628281,
        -0.7441083003156101,
        -0.7610812734226649,
        -0.7537003302675764,
        -0.755895303290124,
        -0.7468494878264512,
        -0.763702334847619,
        -0.7532389336040426,
        1.0,
        -0.7633578661867217,
        -0.7544639671185488,
        -0.7301398620611139
      ],
      [
        -0.7631792046485086,
        -0.7606423546566102,
        -0.760917837293617,
        -0.7594340702650458,
        -0.7598237603548343,
        -0.7448159411473962,
        -0.7607022399835688,
        -0.7629310545717386,
        -0.7580646058376797,
        -0.7621302456630688,
        -0.7604241675160883,
        -0.760471335172682,
        -0.7615823288701951,
        -0.7567596762673467,
        -0.7623147667298529,
        -0.7405864042332373,
        -0.7623523158784253,
        -0.7612586721152914,
        -0.7585419681366888,
        -0.7584865458925776,
        -0.7606881753741797,
        -0.758047903310829,
        -0.7490417454719493,
        -0.7613130888753394,
        -0.7634172932686152,
        -0.7538223733170945,
        -0.7644827210297525,
        -0.7609443513662411,
        -0.7512811732510034,
        -0.7585633698972217,
        -0.7624063630580034,
        -0.7641210375195138,
        -0.7606336230538462,
        -0.7641706761629257,
        -0.7630141209460765,
        -0.7585894752780683,
        -0.7604072928357006,
        -0.7398026170497296,
        -0.7470087630754745,
        -0.759726466037062,
        -0.762552130501009,
        -0.7646105754374212,
        -0.7607591502805965,
        -0.7545653903654119,
        -0.757680083096093,
        -0.7622450632288513,
        -0.7554534978227908,
        -0.7621253429804027,
        -0.7586572408806576,
        -0.7625492407009272,
        -0.7510109344381883,
        -0.7630721607868756,
        -0.7616147915250953,
        -0.7535591616548292,
        -0.7626183929694317,
        -0.7593760206037253,
        -0.7621241972093209,
        -0.7557299411236184,
        -0.7487188100585251,
        -0.7609634879181257,
        -0.7521197903662258,
        -0.7651792323067329,
        -0.7484017978991904,
        -0.7633578661867217,
        1.0,
        -0.7603074026474821,
        -0.7621878958390307
      ],
      [
        -0.716448156770519,
        -0.7552799082085518,
        -0.7260130127684747,
        -0.7572274256219271,
        -0.25791459273218187,
        -0.6743172090960707,
        -0.7475414498764431,
        -0.7538702831590421,
        -0.7595089244848445,
        -0.7375762000038026,
        -0.7488986601651484,
        -0.7513991555144146,
        -0.7229838050572155,
        -0.7011741955004464,
        -0.6827470484047471,
        -0.7510250113876001,
        -0.7437555575713803,
        -0.7540755873006706,
        -0.7495369221649526,
        -0.7505997405330007,
        -0.7403673205264766,
        -0.6825412615801288,
        -0.7332360392971822,
        -0.7476287522690299,
        -0.7594546364889658,
        -0.7423116218405292,
        -0.763691192101257,
        -0.6160951134020997,
        -0.7586296264189981,
        -0.7492401420899164,
        -0.5517572279204828,
        -0.754458227258533,
        -0.7625560580372814,
        -0.760175735715261,
        -0.7497593841177119,
        -0.7405387039051421,
        -0.7510843379838557,
        -0.7428364682823319,
        -0.753503798279364,
        -0.7630983440699473,
        -0.7635400697160112,
        -0.7559774336222593,
        -0.7595337956055759,
        -0.7382927574266519,
        -0.7302797958439706,
        -0.7375110941308116,
        -0.6908273191500652,
        -0.7435832554393884,
        -0.7583591387110313,
        -0.7469276997469018,
        -0.7559644728955368,
        -0.7342513509118276,
        -0.7477251047664224,
        -0.756366290924082,
        -0.747722962481824,
        -0.7492323666433216,
        -0.7468994961827494,
        -0.7599740850719253,
        -0.7049356819273946,
        -0.7478861996671307,
        -0.7351970485485232,
        -0.7298431178505752,
        -0.7341571050402165,
        -0.7544639671185488,
        -0.7603074026474821,
        1.0,
        -0.7480699816057884
      ],
      [
        -0.7545950544769605,
        -0.7576203605008557,
        -0.6679370898828358,
        -0.7614481727454392,
        -0.7357121498783725,
        -0.7527637760601865,
        -0.7460286540587001,
        -0.7417846255608076,
        -0.7632143293655158,
        -0.757291559700388,
        -0.7564329268308214,
        -0.756901155850786,
        -0.6996037579505368,
        -0.7311141394229365,
        -0.7335966505775693,
        -0.7585977712210095,
        -0.7562665410213443,
        -0.7564212950923297,
        -0.7526011436317956,
        -0.7576485086853657,
        -0.7471460554761236,
        -0.7497516393692696,
        -0.7450574112234047,
        -0.6886605291962568,
        -0.7557623075711526,
        -0.7584060676930429,
        -0.7609361429460206,
        -0.7007916195951047,
        -0.7607430106690242,
        -0.7486674436390831,
        -0.7501039726013695,
        -0.7573521954936915,
        -0.7630442133068983,
        -0.7634581030282219,
        -0.7577556986543431,
        -0.7497046885387983,
        -0.7578837814551702,
        -0.7469743354133016,
        -0.7569524001248127,
        -0.7625640180760036,
        -0.7481125660729816,
        -0.7582742231268205,
        -0.7586292524318522,
        -0.724279127621879,
        -0.7041299964766643,
        -0.74460380903499,
        -0.7396185659106423,
        -0.7519685054289819,
        -0.760866159875522,
        -0.7431701655307685,
        -0.7566306271931887,
        -0.7266957577689299,
        -0.23196071971498122,
        -0.7530435060953713,
        -0.7579804503872991,
        -0.7571476709050328,
        0.02661949154841849,
        -0.7626044340934561,
        -0.7534940341193499,
        -0.7468634312894984,
        -0.7483216076849732,
        -0.7621241516833273,
        -0.7508341212975707,
        -0.7301398620611139,
        -0.7621878958390307,
        -0.7480699816057884,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        55,
        11,
        6,
        8,
        17,
        24,
        9,
        17,
        17,
        3,
        3,
        17,
        15,
        2,
        18,
        18,
        28,
        5,
        26,
        25,
        8,
        30,
        35,
        13,
        18,
        13,
        9,
        7,
        4,
        21,
        6,
        7,
        6,
        15,
        11,
        15,
        17,
        3,
        12,
        9,
        11,
        12,
        1,
        18,
        3,
        8,
        2,
        24,
        6,
        8,
        6,
        16,
        2,
        10,
        5,
        3,
        9,
        9,
        18,
        4,
        4,
        5,
        9,
        2,
        1,
        0,
        2
      ],
      "2020-02": [
        85,
        26,
        5,
        7,
        10,
        59,
        10,
        26,
        11,
        6,
        8,
        23,
        15,
        2,
        15,
        18,
        22,
        3,
        39,
        34,
        9,
        32,
        33,
        18,
        20,
        16,
        5,
        8,
        16,
        26,
        6,
        2,
        2,
        8,
        5,
        21,
        29,
        7,
        9,
        4,
        5,
        10,
        4,
        44,
        2,
        5,
        3,
        20,
        5,
        5,
        11,
        22,
        3,
        12,
        14,
        4,
        13,
        3,
        21,
        11,
        3,
        6,
        13,
        4,
        2,
        4,
        5
      ],
      "2020-03": [
        100,
        15,
        8,
        15,
        12,
        60,
        20,
        35,
        14,
        8,
        11,
        35,
        21,
        5,
        23,
        23,
        62,
        3,
        68,
        48,
        19,
        39,
        52,
        14,
        34,
        22,
        7,
        10,
        21,
        26,
        14,
        8,
        5,
        12,
        16,
        42,
        47,
        9,
        15,
        16,
        14,
        16,
        2,
        56,
        9,
        11,
        6,
        38,
        2,
        14,
        13,
        26,
        3,
        10,
        32,
        11,
        7,
        13,
        22,
        8,
        12,
        6,
        29,
        12,
        2,
        2,
        10
      ],
      "2020-04": [
        109,
        10,
        16,
        16,
        10,
        52,
        19,
        26,
        13,
        4,
        16,
        31,
        17,
        5,
        25,
        26,
        48,
        1,
        46,
        30,
        25,
        28,
        51,
        22,
        33,
        16,
        7,
        7,
        53,
        35,
        9,
        9,
        9,
        15,
        7,
        34,
        51,
        11,
        18,
        4,
        10,
        22,
        9,
        58,
        6,
        14,
        4,
        36,
        5,
        5,
        13,
        31,
        5,
        10,
        22,
        5,
        10,
        15,
        25,
        12,
        14,
        6,
        20,
        9,
        2,
        2,
        8
      ],
      "2020-05": [
        87,
        10,
        4,
        13,
        12,
        34,
        10,
        19,
        18,
        1,
        7,
        22,
        17,
        1,
        21,
        28,
        31,
        1,
        44,
        31,
        10,
        29,
        31,
        17,
        20,
        19,
        6,
        11,
        26,
        27,
        5,
        6,
        14,
        13,
        4,
        32,
        35,
        9,
        8,
        3,
        15,
        14,
        11,
        29,
        9,
        8,
        8,
        20,
        7,
        3,
        11,
        26,
        5,
        17,
        13,
        4,
        19,
        10,
        24,
        11,
        8,
        6,
        15,
        4,
        4,
        3,
        11
      ],
      "2020-06": [
        132,
        14,
        9,
        21,
        12,
        63,
        9,
        37,
        10,
        10,
        12,
        23,
        25,
        3,
        20,
        31,
        49,
        0,
        58,
        42,
        9,
        44,
        38,
        16,
        41,
        16,
        5,
        12,
        37,
        26,
        14,
        15,
        9,
        11,
        13,
        27,
        52,
        13,
        22,
        6,
        12,
        18,
        9,
        50,
        7,
        8,
        4,
        25,
        1,
        3,
        13,
        23,
        2,
        16,
        29,
        13,
        19,
        9,
        23,
        17,
        5,
        10,
        24,
        8,
        0,
        2,
        10
      ],
      "2020-07": [
        117,
        19,
        7,
        9,
        18,
        70,
        14,
        47,
        32,
        11,
        12,
        45,
        32,
        3,
        24,
        39,
        63,
        2,
        75,
        56,
        24,
        48,
        64,
        22,
        44,
        23,
        9,
        13,
        34,
        36,
        11,
        9,
        13,
        7,
        11,
        44,
        62,
        19,
        22,
        17,
        14,
        25,
        6,
        52,
        11,
        14,
        3,
        40,
        9,
        11,
        18,
        42,
        1,
        13,
        28,
        9,
        27,
        11,
        14,
        14,
        6,
        10,
        36,
        10,
        4,
        1,
        14
      ],
      "2020-08": [
        99,
        18,
        3,
        12,
        8,
        39,
        10,
        51,
        15,
        15,
        12,
        27,
        30,
        1,
        19,
        11,
        46,
        1,
        65,
        35,
        25,
        38,
        56,
        26,
        42,
        29,
        6,
        11,
        25,
        38,
        9,
        5,
        8,
        17,
        9,
        45,
        61,
        11,
        16,
        7,
        9,
        16,
        3,
        44,
        3,
        14,
        4,
        19,
        9,
        7,
        13,
        42,
        2,
        17,
        19,
        4,
        5,
        13,
        19,
        10,
        4,
        6,
        22,
        9,
        5,
        1,
        7
      ],
      "2020-09": [
        119,
        15,
        5,
        14,
        12,
        46,
        9,
        23,
        16,
        3,
        11,
        25,
        20,
        3,
        27,
        12,
        33,
        2,
        50,
        29,
        14,
        28,
        38,
        23,
        24,
        17,
        12,
        10,
        36,
        31,
        6,
        9,
        9,
        6,
        9,
        31,
        40,
        12,
        13,
        4,
        12,
        18,
        6,
        38,
        4,
        9,
        4,
        12,
        5,
        4,
        12,
        18,
        4,
        22,
        16,
        6,
        9,
        11,
        18,
        7,
        7,
        8,
        24,
        6,
        6,
        1,
        5
      ],
      "2020-10": [
        113,
        12,
        11,
        12,
        14,
        75,
        11,
        33,
        23,
        8,
        4,
        38,
        24,
        3,
        22,
        20,
        46,
        4,
        55,
        32,
        15,
        36,
        50,
        27,
        28,
        27,
        13,
        9,
        44,
        25,
        8,
        8,
        16,
        12,
        7,
        29,
        55,
        18,
        16,
        8,
        10,
        18,
        9,
        43,
        5,
        11,
        7,
        31,
        6,
        9,
        21,
        26,
        5,
        14,
        21,
        6,
        13,
        14,
        22,
        8,
        11,
        6,
        21,
        19,
        2,
        0,
        12
      ],
      "2020-11": [
        92,
        24,
        9,
        16,
        12,
        50,
        20,
        41,
        20,
        11,
        13,
        29,
        28,
        2,
        22,
        21,
        57,
        2,
        64,
        40,
        19,
        50,
        29,
        21,
        38,
        35,
        15,
        9,
        35,
        29,
        14,
        8,
        12,
        23,
        8,
        35,
        57,
        8,
        16,
        3,
        9,
        14,
        7,
        33,
        9,
        14,
        8,
        35,
        5,
        6,
        23,
        47,
        4,
        22,
        30,
        5,
        10,
        9,
        19,
        17,
        4,
        12,
        21,
        9,
        1,
        2,
        12
      ],
      "2020-12": [
        102,
        7,
        6,
        18,
        11,
        72,
        16,
        36,
        17,
        18,
        22,
        34,
        25,
        6,
        27,
        22,
        47,
        7,
        67,
        42,
        23,
        38,
        55,
        22,
        34,
        17,
        3,
        11,
        33,
        39,
        13,
        10,
        13,
        18,
        13,
        27,
        81,
        8,
        23,
        6,
        10,
        27,
        8,
        34,
        5,
        8,
        4,
        33,
        7,
        15,
        24,
        28,
        4,
        14,
        37,
        7,
        14,
        6,
        32,
        12,
        11,
        13,
        31,
        10,
        5,
        2,
        10
      ],
      "2021-01": [
        69,
        11,
        5,
        13,
        8,
        51,
        15,
        24,
        11,
        6,
        5,
        23,
        23,
        4,
        15,
        25,
        27,
        6,
        53,
        32,
        7,
        24,
        26,
        14,
        28,
        20,
        6,
        12,
        26,
        21,
        6,
        7,
        3,
        12,
        7,
        34,
        30,
        12,
        14,
        4,
        7,
        13,
        6,
        40,
        4,
        11,
        5,
        17,
        7,
        6,
        18,
        26,
        1,
        10,
        15,
        8,
        8,
        15,
        15,
        5,
        9,
        9,
        15,
        6,
        4,
        1,
        4
      ],
      "2021-02": [
        77,
        10,
        4,
        8,
        15,
        45,
        6,
        21,
        12,
        4,
        6,
        24,
        15,
        4,
        12,
        24,
        28,
        2,
        50,
        37,
        3,
        25,
        31,
        9,
        13,
        16,
        9,
        10,
        30,
        17,
        5,
        9,
        1,
        7,
        4,
        32,
        25,
        12,
        11,
        5,
        9,
        11,
        7,
        29,
        5,
        10,
        3,
        21,
        7,
        2,
        16,
        21,
        3,
        9,
        16,
        3,
        12,
        5,
        21,
        3,
        5,
        4,
        24,
        4,
        1,
        0,
        4
      ],
      "2021-03": [
        135,
        11,
        10,
        15,
        15,
        85,
        18,
        36,
        14,
        22,
        24,
        28,
        42,
        3,
        23,
        41,
        81,
        3,
        74,
        65,
        24,
        53,
        75,
        31,
        49,
        27,
        21,
        11,
        36,
        42,
        12,
        10,
        5,
        17,
        16,
        53,
        65,
        15,
        28,
        13,
        17,
        33,
        13,
        72,
        6,
        6,
        9,
        44,
        4,
        8,
        17,
        48,
        3,
        20,
        42,
        12,
        14,
        9,
        33,
        17,
        17,
        10,
        46,
        11,
        7,
        4,
        4
      ],
      "2021-04": [
        93,
        11,
        10,
        15,
        16,
        43,
        20,
        42,
        10,
        18,
        14,
        37,
        40,
        4,
        29,
        25,
        65,
        8,
        74,
        56,
        24,
        47,
        65,
        21,
        41,
        17,
        8,
        12,
        42,
        53,
        20,
        11,
        5,
        10,
        7,
        47,
        60,
        11,
        22,
        5,
        11,
        18,
        6,
        55,
        10,
        13,
        3,
        44,
        3,
        10,
        8,
        54,
        12,
        15,
        28,
        8,
        24,
        14,
        31,
        31,
        8,
        12,
        37,
        10,
        6,
        3,
        10
      ],
      "2021-05": [
        101,
        14,
        3,
        18,
        19,
        55,
        18,
        30,
        19,
        7,
        9,
        34,
        29,
        7,
        41,
        30,
        34,
        1,
        56,
        37,
        15,
        33,
        48,
        22,
        35,
        29,
        8,
        6,
        37,
        31,
        14,
        9,
        4,
        9,
        9,
        38,
        44,
        10,
        16,
        4,
        20,
        15,
        6,
        50,
        6,
        15,
        7,
        29,
        4,
        7,
        9,
        35,
        0,
        16,
        16,
        8,
        9,
        11,
        12,
        17,
        8,
        4,
        27,
        12,
        3,
        4,
        2
      ],
      "2021-06": [
        137,
        22,
        9,
        13,
        9,
        58,
        13,
        37,
        21,
        17,
        14,
        36,
        34,
        3,
        39,
        34,
        39,
        7,
        79,
        57,
        21,
        58,
        60,
        18,
        30,
        36,
        10,
        13,
        22,
        30,
        10,
        8,
        8,
        19,
        14,
        40,
        46,
        13,
        25,
        6,
        12,
        26,
        15,
        54,
        5,
        9,
        6,
        21,
        3,
        16,
        12,
        26,
        2,
        14,
        29,
        14,
        17,
        16,
        25,
        14,
        11,
        12,
        32,
        6,
        10,
        3,
        14
      ],
      "2021-07": [
        88,
        8,
        6,
        14,
        9,
        41,
        12,
        36,
        15,
        10,
        9,
        21,
        27,
        2,
        25,
        33,
        51,
        2,
        55,
        51,
        23,
        41,
        64,
        27,
        26,
        24,
        6,
        11,
        33,
        25,
        15,
        7,
        11,
        17,
        9,
        36,
        63,
        11,
        18,
        11,
        7,
        15,
        5,
        47,
        3,
        8,
        6,
        26,
        6,
        8,
        21,
        17,
        10,
        15,
        17,
        17,
        14,
        9,
        26,
        14,
        15,
        3,
        39,
        15,
        8,
        1,
        6
      ],
      "2021-08": [
        116,
        10,
        4,
        14,
        18,
        52,
        21,
        25,
        15,
        10,
        14,
        25,
        26,
        1,
        21,
        24,
        56,
        6,
        81,
        54,
        13,
        43,
        66,
        13,
        35,
        22,
        10,
        10,
        21,
        32,
        20,
        7,
        7,
        11,
        9,
        33,
        62,
        9,
        14,
        3,
        8,
        21,
        16,
        42,
        11,
        16,
        11,
        36,
        5,
        9,
        14,
        33,
        7,
        16,
        37,
        10,
        9,
        8,
        27,
        16,
        14,
        9,
        32,
        3,
        7,
        4,
        8
      ],
      "2021-09": [
        104,
        11,
        10,
        11,
        15,
        37,
        24,
        27,
        14,
        7,
        10,
        27,
        35,
        3,
        22,
        33,
        30,
        6,
        52,
        35,
        19,
        34,
        56,
        19,
        16,
        23,
        14,
        9,
        28,
        33,
        12,
        10,
        14,
        13,
        16,
        23,
        35,
        8,
        13,
        11,
        21,
        14,
        10,
        36,
        9,
        6,
        6,
        33,
        10,
        3,
        23,
        22,
        2,
        14,
        19,
        13,
        13,
        9,
        22,
        8,
        9,
        9,
        19,
        6,
        3,
        0,
        12
      ],
      "2021-10": [
        130,
        22,
        4,
        10,
        22,
        62,
        23,
        43,
        18,
        11,
        13,
        32,
        35,
        1,
        23,
        36,
        47,
        4,
        82,
        45,
        20,
        35,
        63,
        22,
        33,
        28,
        5,
        9,
        33,
        32,
        21,
        9,
        13,
        12,
        10,
        42,
        51,
        11,
        22,
        8,
        18,
        17,
        7,
        61,
        9,
        10,
        10,
        29,
        5,
        5,
        8,
        27,
        5,
        21,
        21,
        17,
        20,
        13,
        30,
        12,
        13,
        12,
        36,
        14,
        7,
        1,
        12
      ],
      "2021-11": [
        105,
        15,
        11,
        18,
        14,
        70,
        21,
        31,
        18,
        21,
        11,
        31,
        32,
        2,
        31,
        23,
        38,
        3,
        43,
        52,
        14,
        34,
        43,
        28,
        38,
        25,
        6,
        13,
        23,
        31,
        6,
        12,
        5,
        11,
        9,
        32,
        60,
        13,
        21,
        5,
        16,
        28,
        11,
        45,
        9,
        16,
        10,
        28,
        7,
        11,
        24,
        32,
        5,
        11,
        30,
        14,
        19,
        10,
        23,
        14,
        12,
        12,
        39,
        8,
        7,
        0,
        9
      ],
      "2021-12": [
        109,
        10,
        9,
        12,
        26,
        57,
        15,
        41,
        24,
        30,
        18,
        23,
        41,
        1,
        24,
        29,
        56,
        4,
        84,
        59,
        22,
        48,
        41,
        19,
        31,
        20,
        6,
        8,
        21,
        31,
        16,
        7,
        4,
        15,
        9,
        51,
        67,
        8,
        25,
        3,
        14,
        18,
        9,
        46,
        13,
        15,
        19,
        36,
        5,
        14,
        12,
        39,
        4,
        13,
        24,
        13,
        15,
        10,
        21,
        16,
        10,
        10,
        31,
        7,
        9,
        4,
        6
      ],
      "2022-01": [
        91,
        13,
        10,
        22,
        13,
        44,
        9,
        14,
        19,
        9,
        8,
        18,
        30,
        1,
        21,
        26,
        29,
        1,
        54,
        29,
        10,
        25,
        31,
        25,
        24,
        30,
        8,
        14,
        25,
        28,
        10,
        5,
        11,
        5,
        8,
        21,
        41,
        13,
        16,
        1,
        8,
        14,
        7,
        41,
        5,
        6,
        13,
        10,
        6,
        2,
        18,
        29,
        7,
        15,
        28,
        6,
        12,
        5,
        20,
        4,
        13,
        6,
        33,
        13,
        5,
        0,
        8
      ],
      "2022-02": [
        77,
        19,
        6,
        10,
        14,
        52,
        18,
        23,
        23,
        13,
        12,
        22,
        17,
        8,
        13,
        30,
        35,
        2,
        48,
        43,
        8,
        30,
        34,
        10,
        30,
        28,
        13,
        6,
        16,
        17,
        7,
        8,
        13,
        17,
        12,
        22,
        47,
        10,
        15,
        9,
        11,
        11,
        4,
        35,
        2,
        7,
        9,
        19,
        0,
        8,
        10,
        33,
        3,
        11,
        20,
        9,
        18,
        9,
        18,
        13,
        6,
        15,
        29,
        9,
        7,
        1,
        9
      ],
      "2022-03": [
        139,
        19,
        15,
        17,
        27,
        71,
        32,
        42,
        29,
        29,
        30,
        45,
        42,
        10,
        44,
        56,
        85,
        5,
        98,
        93,
        24,
        47,
        77,
        41,
        46,
        43,
        12,
        18,
        23,
        33,
        11,
        15,
        10,
        24,
        22,
        59,
        103,
        16,
        34,
        12,
        31,
        31,
        11,
        76,
        9,
        7,
        27,
        59,
        10,
        15,
        21,
        66,
        11,
        17,
        43,
        19,
        29,
        17,
        33,
        29,
        14,
        7,
        52,
        10,
        17,
        1,
        9
      ],
      "2022-04": [
        112,
        10,
        11,
        20,
        17,
        56,
        22,
        42,
        21,
        21,
        18,
        35,
        39,
        2,
        24,
        22,
        54,
        7,
        79,
        54,
        24,
        39,
        59,
        22,
        34,
        26,
        9,
        16,
        11,
        32,
        8,
        9,
        5,
        7,
        13,
        45,
        53,
        18,
        25,
        5,
        7,
        20,
        9,
        47,
        6,
        19,
        17,
        37,
        4,
        5,
        15,
        38,
        5,
        17,
        31,
        14,
        22,
        14,
        26,
        8,
        19,
        9,
        27,
        16,
        13,
        0,
        12
      ],
      "2022-05": [
        107,
        15,
        13,
        19,
        17,
        55,
        11,
        30,
        17,
        26,
        18,
        39,
        28,
        6,
        25,
        24,
        48,
        5,
        59,
        42,
        16,
        36,
        40,
        18,
        36,
        35,
        5,
        17,
        18,
        25,
        16,
        7,
        5,
        11,
        10,
        27,
        67,
        14,
        21,
        6,
        13,
        22,
        10,
        35,
        8,
        18,
        20,
        33,
        7,
        6,
        10,
        27,
        5,
        14,
        25,
        13,
        16,
        17,
        32,
        9,
        15,
        13,
        36,
        12,
        9,
        1,
        16
      ],
      "2022-06": [
        114,
        13,
        15,
        13,
        28,
        65,
        27,
        43,
        28,
        22,
        21,
        37,
        39,
        5,
        21,
        48,
        51,
        5,
        67,
        43,
        11,
        26,
        52,
        14,
        27,
        28,
        7,
        14,
        30,
        23,
        19,
        23,
        12,
        19,
        4,
        42,
        45,
        22,
        25,
        7,
        23,
        11,
        14,
        40,
        1,
        7,
        20,
        34,
        8,
        8,
        26,
        38,
        10,
        19,
        34,
        24,
        12,
        10,
        23,
        18,
        9,
        8,
        29,
        1,
        14,
        5,
        10
      ],
      "2022-07": [
        137,
        21,
        14,
        18,
        17,
        65,
        24,
        43,
        22,
        30,
        23,
        40,
        52,
        8,
        35,
        40,
        73,
        9,
        89,
        60,
        21,
        49,
        72,
        30,
        38,
        31,
        13,
        19,
        38,
        47,
        22,
        16,
        15,
        13,
        11,
        48,
        82,
        9,
        27,
        9,
        27,
        19,
        10,
        46,
        9,
        19,
        22,
        46,
        11,
        12,
        12,
        52,
        7,
        19,
        28,
        24,
        27,
        14,
        20,
        21,
        18,
        9,
        42,
        10,
        12,
        3,
        10
      ],
      "2022-08": [
        100,
        10,
        9,
        21,
        29,
        58,
        18,
        36,
        20,
        20,
        16,
        23,
        33,
        6,
        30,
        36,
        51,
        7,
        55,
        33,
        19,
        25,
        59,
        26,
        29,
        27,
        8,
        15,
        22,
        25,
        13,
        7,
        9,
        15,
        14,
        30,
        50,
        11,
        17,
        6,
        11,
        7,
        9,
        46,
        6,
        10,
        12,
        34,
        9,
        8,
        26,
        30,
        10,
        14,
        16,
        20,
        13,
        18,
        17,
        12,
        17,
        12,
        33,
        8,
        7,
        2,
        7
      ],
      "2022-09": [
        99,
        9,
        13,
        15,
        35,
        50,
        18,
        46,
        14,
        24,
        28,
        31,
        24,
        5,
        28,
        39,
        44,
        3,
        61,
        35,
        8,
        28,
        64,
        21,
        26,
        29,
        14,
        11,
        18,
        37,
        14,
        15,
        11,
        6,
        13,
        33,
        73,
        17,
        25,
        6,
        19,
        15,
        14,
        29,
        10,
        13,
        11,
        26,
        11,
        9,
        15,
        41,
        4,
        26,
        32,
        21,
        8,
        8,
        21,
        22,
        8,
        9,
        43,
        11,
        8,
        2,
        8
      ],
      "2022-10": [
        120,
        19,
        14,
        25,
        57,
        76,
        36,
        29,
        11,
        33,
        17,
        34,
        43,
        5,
        31,
        39,
        67,
        3,
        72,
        70,
        28,
        38,
        67,
        26,
        33,
        32,
        20,
        22,
        23,
        27,
        21,
        11,
        12,
        10,
        9,
        70,
        94,
        25,
        34,
        4,
        21,
        27,
        5,
        46,
        6,
        13,
        35,
        50,
        5,
        11,
        14,
        43,
        7,
        26,
        36,
        24,
        21,
        11,
        37,
        21,
        9,
        11,
        56,
        10,
        12,
        2,
        11
      ],
      "2022-11": [
        123,
        13,
        20,
        14,
        67,
        64,
        23,
        35,
        29,
        39,
        32,
        41,
        39,
        5,
        39,
        31,
        49,
        7,
        72,
        65,
        18,
        40,
        69,
        17,
        41,
        34,
        7,
        15,
        30,
        37,
        25,
        19,
        6,
        17,
        14,
        60,
        71,
        23,
        34,
        7,
        16,
        23,
        11,
        56,
        10,
        18,
        27,
        51,
        5,
        6,
        25,
        41,
        7,
        26,
        34,
        23,
        28,
        15,
        29,
        32,
        15,
        13,
        58,
        12,
        15,
        0,
        8
      ],
      "2022-12": [
        96,
        11,
        20,
        25,
        42,
        53,
        19,
        23,
        16,
        38,
        18,
        31,
        28,
        3,
        24,
        22,
        35,
        6,
        75,
        62,
        18,
        26,
        58,
        17,
        20,
        19,
        7,
        18,
        13,
        26,
        19,
        12,
        8,
        11,
        8,
        33,
        57,
        17,
        19,
        7,
        16,
        8,
        10,
        36,
        9,
        18,
        28,
        31,
        3,
        8,
        13,
        32,
        10,
        11,
        27,
        16,
        26,
        9,
        24,
        23,
        9,
        11,
        35,
        6,
        10,
        4,
        14
      ],
      "2023-01": [
        67,
        11,
        13,
        9,
        33,
        39,
        11,
        21,
        15,
        23,
        18,
        23,
        24,
        1,
        16,
        29,
        41,
        2,
        46,
        40,
        11,
        24,
        57,
        10,
        20,
        19,
        6,
        6,
        18,
        16,
        12,
        10,
        3,
        12,
        6,
        30,
        50,
        10,
        18,
        5,
        15,
        8,
        3,
        24,
        3,
        6,
        11,
        11,
        4,
        8,
        16,
        21,
        6,
        12,
        16,
        7,
        11,
        10,
        26,
        18,
        5,
        9,
        20,
        12,
        7,
        1,
        9
      ],
      "2023-02": [
        79,
        8,
        12,
        11,
        58,
        41,
        21,
        30,
        13,
        22,
        24,
        21,
        17,
        2,
        20,
        31,
        36,
        6,
        46,
        36,
        17,
        27,
        37,
        14,
        26,
        20,
        6,
        9,
        14,
        20,
        9,
        7,
        9,
        11,
        7,
        35,
        51,
        21,
        20,
        8,
        20,
        15,
        9,
        38,
        10,
        10,
        14,
        30,
        8,
        6,
        18,
        24,
        6,
        15,
        12,
        17,
        16,
        12,
        24,
        17,
        5,
        11,
        37,
        8,
        8,
        1,
        12
      ],
      "2023-03": [
        158,
        25,
        37,
        29,
        100,
        96,
        27,
        46,
        38,
        74,
        48,
        52,
        59,
        7,
        39,
        57,
        75,
        11,
        100,
        85,
        39,
        50,
        83,
        44,
        42,
        42,
        15,
        28,
        25,
        39,
        52,
        14,
        11,
        19,
        26,
        70,
        119,
        24,
        50,
        13,
        38,
        27,
        13,
        48,
        17,
        16,
        70,
        60,
        8,
        16,
        19,
        65,
        17,
        23,
        58,
        39,
        37,
        17,
        33,
        31,
        17,
        12,
        69,
        14,
        19,
        7,
        15
      ],
      "2023-04": [
        101,
        13,
        21,
        25,
        67,
        52,
        23,
        38,
        40,
        54,
        38,
        35,
        30,
        10,
        26,
        70,
        62,
        4,
        79,
        54,
        30,
        39,
        59,
        17,
        29,
        34,
        10,
        22,
        18,
        32,
        27,
        13,
        11,
        11,
        10,
        49,
        84,
        21,
        40,
        8,
        21,
        29,
        8,
        55,
        5,
        22,
        45,
        33,
        4,
        10,
        23,
        54,
        11,
        11,
        35,
        11,
        15,
        14,
        31,
        20,
        16,
        17,
        44,
        7,
        16,
        4,
        6
      ],
      "2023-05": [
        142,
        20,
        50,
        25,
        126,
        71,
        15,
        37,
        30,
        40,
        21,
        44,
        45,
        2,
        57,
        46,
        56,
        7,
        75,
        54,
        12,
        38,
        56,
        18,
        28,
        37,
        11,
        24,
        13,
        24,
        23,
        10,
        14,
        20,
        17,
        54,
        84,
        15,
        27,
        8,
        19,
        19,
        16,
        56,
        8,
        24,
        53,
        31,
        11,
        15,
        25,
        39,
        12,
        23,
        31,
        26,
        36,
        15,
        36,
        23,
        18,
        15,
        65,
        19,
        13,
        5,
        22
      ],
      "2023-06": [
        96,
        10,
        34,
        35,
        103,
        73,
        27,
        25,
        27,
        34,
        19,
        35,
        45,
        3,
        37,
        61,
        52,
        6,
        66,
        54,
        17,
        23,
        43,
        19,
        32,
        31,
        9,
        19,
        9,
        26,
        26,
        13,
        9,
        14,
        8,
        54,
        73,
        26,
        30,
        12,
        18,
        25,
        9,
        41,
        7,
        14,
        49,
        29,
        8,
        6,
        19,
        50,
        11,
        26,
        34,
        25,
        35,
        13,
        32,
        16,
        6,
        12,
        47,
        9,
        20,
        4,
        15
      ],
      "2023-07": [
        96,
        14,
        29,
        30,
        65,
        64,
        26,
        39,
        27,
        30,
        22,
        48,
        33,
        7,
        33,
        66,
        50,
        7,
        69,
        62,
        18,
        40,
        56,
        24,
        23,
        40,
        3,
        16,
        14,
        23,
        21,
        18,
        13,
        25,
        14,
        45,
        73,
        29,
        20,
        16,
        23,
        25,
        15,
        42,
        8,
        12,
        35,
        27,
        7,
        11,
        27,
        36,
        13,
        24,
        25,
        27,
        25,
        28,
        36,
        25,
        14,
        14,
        41,
        3,
        14,
        4,
        8
      ],
      "2023-08": [
        119,
        9,
        43,
        18,
        78,
        59,
        35,
        43,
        27,
        45,
        21,
        43,
        38,
        3,
        53,
        56,
        46,
        8,
        86,
        71,
        30,
        33,
        80,
        35,
        57,
        34,
        13,
        26,
        22,
        35,
        32,
        10,
        14,
        21,
        14,
        45,
        70,
        24,
        40,
        14,
        27,
        19,
        16,
        68,
        5,
        18,
        38,
        45,
        9,
        15,
        31,
        58,
        13,
        27,
        46,
        30,
        28,
        23,
        23,
        29,
        20,
        8,
        52,
        19,
        19,
        4,
        14
      ],
      "2023-09": [
        119,
        6,
        37,
        30,
        74,
        59,
        22,
        39,
        21,
        50,
        34,
        51,
        42,
        8,
        29,
        57,
        60,
        14,
        76,
        72,
        24,
        34,
        71,
        17,
        42,
        32,
        17,
        23,
        18,
        28,
        39,
        19,
        12,
        15,
        16,
        52,
        63,
        16,
        31,
        12,
        36,
        17,
        9,
        54,
        5,
        14,
        42,
        39,
        8,
        14,
        20,
        44,
        10,
        33,
        36,
        24,
        31,
        21,
        25,
        16,
        12,
        12,
        48,
        12,
        8,
        4,
        21
      ],
      "2023-10": [
        110,
        7,
        67,
        34,
        84,
        75,
        28,
        28,
        30,
        56,
        36,
        55,
        29,
        10,
        30,
        53,
        62,
        4,
        64,
        66,
        13,
        22,
        72,
        19,
        41,
        33,
        8,
        23,
        17,
        39,
        31,
        14,
        8,
        12,
        9,
        38,
        58,
        26,
        28,
        4,
        17,
        23,
        13,
        41,
        3,
        14,
        51,
        40,
        10,
        9,
        19,
        51,
        7,
        31,
        20,
        30,
        40,
        14,
        27,
        23,
        15,
        13,
        51,
        10,
        16,
        4,
        9
      ],
      "2023-11": [
        107,
        8,
        76,
        25,
        126,
        70,
        15,
        44,
        32,
        76,
        18,
        54,
        49,
        7,
        40,
        68,
        56,
        9,
        74,
        56,
        29,
        29,
        60,
        25,
        35,
        36,
        11,
        38,
        23,
        40,
        43,
        20,
        18,
        22,
        13,
        51,
        71,
        39,
        33,
        11,
        24,
        28,
        18,
        54,
        6,
        23,
        41,
        43,
        9,
        11,
        22,
        53,
        10,
        39,
        48,
        22,
        32,
        22,
        23,
        23,
        8,
        17,
        48,
        11,
        20,
        11,
        13
      ],
      "2023-12": [
        91,
        12,
        69,
        27,
        158,
        78,
        25,
        36,
        30,
        113,
        31,
        55,
        56,
        5,
        40,
        48,
        58,
        7,
        77,
        62,
        29,
        38,
        64,
        32,
        38,
        31,
        11,
        50,
        11,
        44,
        62,
        17,
        7,
        6,
        13,
        53,
        98,
        29,
        30,
        23,
        22,
        28,
        11,
        52,
        19,
        28,
        74,
        50,
        8,
        9,
        10,
        91,
        21,
        21,
        35,
        22,
        24,
        17,
        32,
        20,
        14,
        24,
        62,
        8,
        22,
        6,
        11
      ],
      "2024-01": [
        106,
        7,
        43,
        31,
        71,
        61,
        25,
        31,
        32,
        45,
        23,
        46,
        33,
        9,
        26,
        44,
        49,
        7,
        70,
        53,
        15,
        17,
        57,
        22,
        32,
        20,
        10,
        33,
        17,
        31,
        32,
        13,
        5,
        16,
        11,
        33,
        57,
        26,
        15,
        10,
        25,
        31,
        9,
        44,
        7,
        17,
        39,
        24,
        7,
        5,
        24,
        46,
        9,
        24,
        23,
        15,
        22,
        10,
        21,
        19,
        10,
        15,
        33,
        10,
        7,
        1,
        12
      ],
      "2024-02": [
        83,
        7,
        86,
        21,
        99,
        79,
        17,
        28,
        18,
        56,
        20,
        52,
        29,
        5,
        35,
        36,
        45,
        8,
        67,
        49,
        14,
        24,
        61,
        23,
        34,
        32,
        21,
        23,
        16,
        32,
        25,
        13,
        10,
        17,
        14,
        35,
        49,
        24,
        25,
        8,
        21,
        19,
        3,
        39,
        3,
        10,
        43,
        33,
        8,
        9,
        19,
        41,
        13,
        21,
        26,
        23,
        14,
        9,
        42,
        15,
        7,
        16,
        34,
        13,
        10,
        4,
        10
      ],
      "2024-03": [
        130,
        6,
        103,
        25,
        180,
        89,
        40,
        50,
        49,
        124,
        48,
        64,
        48,
        7,
        37,
        91,
        95,
        7,
        94,
        72,
        32,
        39,
        84,
        41,
        62,
        43,
        23,
        50,
        26,
        41,
        70,
        18,
        9,
        17,
        19,
        63,
        96,
        55,
        69,
        21,
        34,
        37,
        17,
        60,
        14,
        21,
        88,
        72,
        11,
        11,
        33,
        103,
        18,
        41,
        38,
        42,
        43,
        17,
        38,
        36,
        15,
        26,
        66,
        9,
        14,
        7,
        15
      ],
      "2024-04": [
        120,
        10,
        74,
        36,
        122,
        58,
        24,
        42,
        35,
        84,
        25,
        44,
        51,
        8,
        48,
        56,
        57,
        7,
        79,
        55,
        22,
        24,
        53,
        42,
        27,
        26,
        15,
        24,
        17,
        30,
        41,
        16,
        7,
        22,
        5,
        55,
        97,
        34,
        31,
        12,
        30,
        29,
        23,
        47,
        6,
        27,
        60,
        41,
        5,
        13,
        26,
        80,
        11,
        23,
        25,
        24,
        21,
        19,
        28,
        28,
        12,
        19,
        56,
        21,
        24,
        7,
        14
      ],
      "2024-05": [
        108,
        12,
        89,
        49,
        137,
        77,
        29,
        37,
        34,
        91,
        37,
        57,
        51,
        4,
        40,
        46,
        52,
        8,
        62,
        61,
        19,
        25,
        71,
        27,
        48,
        49,
        15,
        38,
        20,
        40,
        49,
        23,
        14,
        30,
        14,
        46,
        64,
        44,
        30,
        14,
        22,
        30,
        16,
        44,
        10,
        19,
        71,
        21,
        3,
        9,
        27,
        86,
        9,
        30,
        30,
        27,
        36,
        15,
        29,
        22,
        7,
        23,
        61,
        21,
        20,
        7,
        5
      ],
      "2024-06": [
        114,
        14,
        122,
        46,
        164,
        61,
        35,
        36,
        36,
        82,
        27,
        50,
        86,
        12,
        40,
        63,
        68,
        6,
        71,
        51,
        15,
        30,
        55,
        19,
        41,
        38,
        8,
        62,
        12,
        32,
        45,
        28,
        13,
        21,
        9,
        39,
        53,
        39,
        25,
        13,
        25,
        31,
        21,
        44,
        8,
        11,
        70,
        41,
        13,
        15,
        29,
        50,
        7,
        25,
        37,
        23,
        29,
        12,
        30,
        29,
        11,
        21,
        48,
        13,
        18,
        8,
        14
      ],
      "2024-07": [
        137,
        10,
        95,
        27,
        128,
        76,
        38,
        55,
        41,
        69,
        40,
        55,
        52,
        5,
        40,
        77,
        50,
        15,
        96,
        78,
        25,
        28,
        66,
        53,
        47,
        43,
        17,
        46,
        15,
        35,
        58,
        19,
        15,
        24,
        18,
        52,
        94,
        41,
        51,
        25,
        38,
        41,
        11,
        46,
        13,
        22,
        71,
        40,
        9,
        16,
        20,
        73,
        7,
        38,
        46,
        33,
        45,
        23,
        37,
        31,
        13,
        25,
        73,
        20,
        31,
        3,
        6
      ],
      "2024-08": [
        102,
        12,
        71,
        23,
        94,
        75,
        19,
        42,
        45,
        64,
        43,
        48,
        52,
        9,
        36,
        72,
        48,
        12,
        78,
        54,
        15,
        28,
        53,
        30,
        37,
        37,
        12,
        39,
        11,
        29,
        38,
        17,
        10,
        16,
        10,
        62,
        67,
        34,
        26,
        17,
        27,
        28,
        21,
        52,
        4,
        16,
        57,
        41,
        6,
        16,
        20,
        53,
        20,
        28,
        29,
        19,
        21,
        14,
        40,
        27,
        10,
        15,
        55,
        19,
        13,
        2,
        14
      ],
      "2024-09": [
        105,
        11,
        85,
        27,
        108,
        65,
        25,
        35,
        44,
        81,
        35,
        63,
        52,
        11,
        40,
        70,
        89,
        14,
        85,
        53,
        13,
        31,
        67,
        34,
        51,
        39,
        16,
        40,
        12,
        37,
        43,
        8,
        16,
        14,
        11,
        49,
        84,
        30,
        31,
        21,
        40,
        26,
        22,
        61,
        6,
        19,
        71,
        45,
        14,
        10,
        38,
        70,
        13,
        37,
        39,
        27,
        31,
        24,
        35,
        27,
        8,
        21,
        66,
        17,
        12,
        4,
        28
      ],
      "2024-10": [
        131,
        8,
        149,
        35,
        187,
        74,
        37,
        49,
        42,
        111,
        28,
        54,
        71,
        11,
        45,
        68,
        60,
        12,
        89,
        52,
        18,
        26,
        53,
        40,
        54,
        41,
        18,
        73,
        17,
        43,
        45,
        8,
        19,
        24,
        11,
        54,
        90,
        48,
        33,
        24,
        26,
        20,
        15,
        42,
        6,
        11,
        77,
        34,
        12,
        10,
        28,
        67,
        14,
        39,
        31,
        27,
        52,
        16,
        36,
        29,
        9,
        14,
        49,
        17,
        25,
        4,
        9
      ],
      "2024-11": [
        116,
        6,
        105,
        52,
        165,
        73,
        31,
        36,
        45,
        108,
        26,
        57,
        59,
        7,
        41,
        49,
        57,
        12,
        77,
        52,
        23,
        29,
        60,
        27,
        59,
        43,
        14,
        73,
        20,
        26,
        47,
        16,
        11,
        17,
        11,
        40,
        74,
        53,
        33,
        13,
        23,
        31,
        15,
        42,
        4,
        18,
        70,
        38,
        10,
        11,
        39,
        72,
        18,
        35,
        23,
        35,
        30,
        20,
        28,
        24,
        7,
        26,
        42,
        17,
        27,
        4,
        6
      ],
      "2024-12": [
        129,
        7,
        149,
        34,
        207,
        87,
        40,
        45,
        31,
        127,
        32,
        85,
        77,
        13,
        51,
        69,
        81,
        15,
        88,
        58,
        19,
        25,
        92,
        26,
        52,
        40,
        16,
        109,
        17,
        51,
        62,
        17,
        7,
        17,
        12,
        62,
        90,
        45,
        39,
        14,
        42,
        30,
        25,
        57,
        19,
        18,
        100,
        44,
        9,
        9,
        22,
        99,
        18,
        39,
        36,
        24,
        47,
        13,
        27,
        39,
        13,
        29,
        66,
        14,
        15,
        8,
        17
      ],
      "2025-01": [
        76,
        3,
        102,
        31,
        96,
        59,
        20,
        36,
        33,
        61,
        26,
        43,
        55,
        11,
        27,
        52,
        57,
        8,
        55,
        55,
        21,
        22,
        38,
        30,
        34,
        41,
        15,
        46,
        20,
        31,
        33,
        14,
        16,
        18,
        8,
        47,
        69,
        30,
        19,
        20,
        32,
        22,
        16,
        43,
        4,
        11,
        43,
        28,
        6,
        12,
        28,
        50,
        11,
        26,
        26,
        24,
        13,
        20,
        37,
        18,
        9,
        18,
        32,
        14,
        12,
        5,
        14
      ],
      "2025-02": [
        100,
        9,
        130,
        25,
        130,
        66,
        25,
        44,
        37,
        53,
        17,
        60,
        50,
        11,
        31,
        56,
        51,
        8,
        55,
        41,
        8,
        15,
        45,
        34,
        38,
        34,
        13,
        74,
        18,
        28,
        31,
        15,
        11,
        21,
        9,
        44,
        80,
        40,
        29,
        22,
        30,
        29,
        25,
        37,
        9,
        13,
        54,
        39,
        15,
        11,
        27,
        52,
        11,
        32,
        22,
        21,
        20,
        18,
        25,
        22,
        3,
        11,
        45,
        11,
        13,
        9,
        19
      ],
      "2025-03": [
        149,
        5,
        186,
        62,
        208,
        68,
        41,
        66,
        53,
        163,
        44,
        104,
        105,
        17,
        54,
        81,
        111,
        10,
        86,
        63,
        24,
        38,
        82,
        46,
        67,
        63,
        25,
        111,
        16,
        45,
        71,
        18,
        20,
        26,
        18,
        61,
        106,
        37,
        60,
        27,
        49,
        50,
        30,
        58,
        13,
        30,
        99,
        63,
        13,
        11,
        33,
        117,
        28,
        43,
        55,
        25,
        58,
        26,
        32,
        42,
        17,
        33,
        80,
        21,
        27,
        12,
        34
      ],
      "2025-04": [
        123,
        9,
        130,
        54,
        140,
        72,
        31,
        38,
        32,
        79,
        26,
        57,
        85,
        8,
        43,
        47,
        58,
        9,
        71,
        36,
        25,
        17,
        64,
        33,
        51,
        36,
        14,
        59,
        11,
        27,
        45,
        14,
        17,
        20,
        6,
        44,
        88,
        34,
        23,
        20,
        37,
        34,
        22,
        43,
        5,
        16,
        63,
        41,
        14,
        18,
        16,
        82,
        15,
        30,
        33,
        16,
        33,
        17,
        25,
        31,
        9,
        22,
        47,
        14,
        13,
        5,
        15
      ],
      "2025-05": [
        134,
        7,
        234,
        60,
        216,
        77,
        44,
        54,
        37,
        83,
        25,
        77,
        88,
        16,
        58,
        67,
        75,
        12,
        63,
        59,
        29,
        27,
        81,
        47,
        56,
        60,
        18,
        100,
        21,
        42,
        51,
        12,
        14,
        26,
        10,
        52,
        88,
        61,
        43,
        23,
        41,
        55,
        25,
        60,
        5,
        19,
        102,
        47,
        10,
        5,
        25,
        97,
        15,
        45,
        30,
        28,
        50,
        18,
        27,
        52,
        13,
        15,
        76,
        22,
        20,
        9,
        30
      ],
      "2025-06": [
        119,
        6,
        194,
        44,
        150,
        72,
        27,
        74,
        33,
        104,
        29,
        79,
        111,
        9,
        42,
        62,
        64,
        18,
        70,
        52,
        15,
        18,
        63,
        34,
        47,
        39,
        19,
        97,
        13,
        29,
        58,
        24,
        12,
        21,
        12,
        54,
        77,
        59,
        30,
        36,
        40,
        46,
        12,
        47,
        7,
        18,
        81,
        36,
        12,
        18,
        21,
        103,
        14,
        35,
        40,
        29,
        41,
        22,
        35,
        35,
        9,
        26,
        62,
        15,
        21,
        8,
        16
      ],
      "2025-07": [
        98,
        7,
        147,
        54,
        121,
        62,
        44,
        39,
        41,
        83,
        26,
        96,
        66,
        13,
        51,
        66,
        77,
        17,
        76,
        51,
        19,
        15,
        65,
        47,
        55,
        50,
        13,
        54,
        14,
        48,
        55,
        21,
        16,
        22,
        13,
        52,
        69,
        54,
        36,
        26,
        22,
        51,
        26,
        64,
        12,
        20,
        93,
        43,
        19,
        15,
        37,
        90,
        14,
        37,
        31,
        27,
        35,
        23,
        26,
        29,
        9,
        23,
        53,
        18,
        16,
        7,
        14
      ],
      "2025-08": [
        117,
        5,
        160,
        27,
        133,
        75,
        40,
        56,
        43,
        110,
        30,
        66,
        73,
        18,
        42,
        57,
        67,
        13,
        90,
        47,
        27,
        19,
        74,
        41,
        50,
        62,
        7,
        69,
        16,
        39,
        47,
        12,
        20,
        20,
        16,
        49,
        66,
        51,
        45,
        30,
        52,
        48,
        21,
        60,
        11,
        27,
        103,
        47,
        14,
        16,
        56,
        91,
        20,
        31,
        24,
        35,
        46,
        31,
        29,
        50,
        13,
        18,
        58,
        17,
        19,
        7,
        15
      ],
      "2025-09": [
        54,
        1,
        63,
        18,
        54,
        27,
        22,
        28,
        16,
        31,
        11,
        28,
        23,
        4,
        16,
        31,
        30,
        8,
        40,
        30,
        6,
        7,
        21,
        16,
        28,
        20,
        6,
        21,
        11,
        16,
        27,
        3,
        15,
        8,
        8,
        24,
        31,
        20,
        7,
        11,
        16,
        26,
        15,
        16,
        3,
        5,
        35,
        21,
        5,
        2,
        21,
        34,
        2,
        21,
        18,
        9,
        12,
        14,
        9,
        19,
        2,
        12,
        30,
        6,
        10,
        2,
        2
      ]
    },
    "papers": {
      "0": [
        {
          "title": "A Single-Frame and Multi-Frame Cascaded Image Super-Resolution Method",
          "year": "2024-12",
          "abstract": "The objective of image super-resolution is to reconstruct a high-resolution\n(HR) image with the prior knowledge from one or several low-resolution (LR)\nimages. However, in the real world, due to the limited complementary\ninformation, the performance of both single-frame and multi-frame\nsuper-resolution reconstruction degrades rapidly as the magnification\nincreases. In this paper, we propose a novel two-step image super resolution\nmethod concatenating multi-frame super-resolution (MFSR) with single-frame\nsuper-resolution (SFSR), to progressively upsample images to the desired\nresolution. The proposed method consisting of an L0-norm constrained\nreconstruction scheme and an enhanced residual back-projection network,\nintegrating the flexibility of the variational modelbased method and the\nfeature learning capacity of the deep learning-based method. To verify the\neffectiveness of the proposed algorithm, extensive experiments with both\nsimulated and real world sequences were implemented. The experimental results\nshow that the proposed method yields superior performance in both objective and\nperceptual quality measurements. The average PSNRs of the cascade model in set5\nand set14 are 33.413 dB and 29.658 dB respectively, which are 0.76 dB and 0.621\ndB more than the baseline method. In addition, the experiment indicates that\nthis cascade model can be robustly applied to different SFSR and MFSR methods.",
          "arxiv_id": "2412.09846v1"
        },
        {
          "title": "Degradation-Guided Meta-Restoration Network for Blind Super-Resolution",
          "year": "2022-07",
          "abstract": "Blind super-resolution (SR) aims to recover high-quality visual textures from\na low-resolution (LR) image, which is usually degraded by down-sampling blur\nkernels and additive noises. This task is extremely difficult due to the\nchallenges of complicated image degradations in the real-world. Existing SR\napproaches either assume a predefined blur kernel or a fixed noise, which\nlimits these approaches in challenging cases. In this paper, we propose a\nDegradation-guided Meta-restoration network for blind Super-Resolution (DMSR)\nthat facilitates image restoration for real cases. DMSR consists of a\ndegradation extractor and meta-restoration modules. The extractor estimates the\ndegradations in LR inputs and guides the meta-restoration modules to predict\nrestoration parameters for different degradations on-the-fly. DMSR is jointly\noptimized by a novel degradation consistency loss and reconstruction losses.\nThrough such an optimization, DMSR outperforms SOTA by a large margin on three\nwidely-used benchmarks. A user study including 16 subjects further validates\nthe superiority of DMSR in real-world blind SR tasks.",
          "arxiv_id": "2207.00943v1"
        },
        {
          "title": "Blind Image Super-Resolution with Spatial Context Hallucination",
          "year": "2020-09",
          "abstract": "Deep convolution neural networks (CNNs) play a critical role in single image\nsuper-resolution (SISR) since the amazing improvement of high performance\ncomputing. However, most of the super-resolution (SR) methods only focus on\nrecovering bicubic degradation. Reconstructing high-resolution (HR) images from\nrandomly blurred and noisy low-resolution (LR) images is still a challenging\nproblem. In this paper, we propose a novel Spatial Context Hallucination\nNetwork (SCHN) for blind super-resolution without knowing the degradation\nkernel. We find that when the blur kernel is unknown, separate deblurring and\nsuper-resolution could limit the performance because of the accumulation of\nerror. Thus, we integrate denoising, deblurring and super-resolution within one\nframework to avoid such a problem. We train our model on two high quality\ndatasets, DIV2K and Flickr2K. Our method performs better than state-of-the-art\nmethods when input images are corrupted with random blur and noise.",
          "arxiv_id": "2009.12461v1"
        }
      ],
      "1": [
        {
          "title": "Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization",
          "year": "2022-06",
          "abstract": "Neural architecture search (NAS) and network pruning are widely studied\nefficient AI techniques, but not yet perfect. NAS performs exhaustive candidate\narchitecture search, incurring tremendous search cost. Though (structured)\npruning can simply shrink model dimension, it remains unclear how to decide the\nper-layer sparsity automatically and optimally. In this work, we revisit the\nproblem of layer-width optimization and propose Pruning-as-Search (PaS), an\nend-to-end channel pruning method to search out desired sub-network\nautomatically and efficiently. Specifically, we add a depth-wise binary\nconvolution to learn pruning policies directly through gradient descent. By\ncombining the structural reparameterization and PaS, we successfully searched\nout a new family of VGG-like and lightweight networks, which enable the\nflexibility of arbitrary width with respect to each layer instead of each\nstage. Experimental results show that our proposed architecture outperforms\nprior arts by around $1.0\\%$ top-1 accuracy under similar inference speed on\nImageNet-1000 classification task. Furthermore, we demonstrate the\neffectiveness of our width search on complex tasks including instance\nsegmentation and image translation. Code and models are released.",
          "arxiv_id": "2206.01198v1"
        },
        {
          "title": "DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search",
          "year": "2020-03",
          "abstract": "Efficient search is a core issue in Neural Architecture Search (NAS). It is\ndifficult for conventional NAS algorithms to directly search the architectures\non large-scale tasks like ImageNet. In general, the cost of GPU hours for NAS\ngrows with regard to training dataset size and candidate set size. One common\nway is searching on a smaller proxy dataset (e.g., CIFAR-10) and then\ntransferring to the target task (e.g., ImageNet). These architectures optimized\non proxy data are not guaranteed to be optimal on the target task. Another\ncommon way is learning with a smaller candidate set, which may require expert\nknowledge and indeed betrays the essence of NAS. In this paper, we present\nDA-NAS that can directly search the architecture for large-scale target tasks\nwhile allowing a large candidate set in a more efficient manner. Our method is\nbased on an interesting observation that the learning speed for blocks in deep\nneural networks is related to the difficulty of recognizing distinct\ncategories. We carefully design a progressive data adapted pruning strategy for\nefficient architecture search. It will quickly trim low performed blocks on a\nsubset of target dataset (e.g., easy classes), and then gradually find the best\nblocks on the whole target dataset. At this time, the original candidate set\nbecomes as compact as possible, providing a faster search in the target task.\nExperiments on ImageNet verify the effectiveness of our approach. It is 2x\nfaster than previous methods while the accuracy is currently state-of-the-art,\nat 76.2% under small FLOPs constraint. It supports an argument search space\n(i.e., more candidate blocks) to efficiently search the best-performing\narchitecture.",
          "arxiv_id": "2003.12563v1"
        },
        {
          "title": "Towards Self-supervised and Weight-preserving Neural Architecture Search",
          "year": "2022-06",
          "abstract": "Neural architecture search (NAS) algorithms save tremendous labor from human\nexperts. Recent advancements further reduce the computational overhead to an\naffordable level. However, it is still cumbersome to deploy the NAS techniques\nin real-world applications due to the fussy procedures and the supervised\nlearning paradigm. In this work, we propose the self-supervised and\nweight-preserving neural architecture search (SSWP-NAS) as an extension of the\ncurrent NAS framework by allowing the self-supervision and retaining the\nconcomitant weights discovered during the search stage. As such, we simplify\nthe workflow of NAS to a one-stage and proxy-free procedure. Experiments show\nthat the architectures searched by the proposed framework achieve\nstate-of-the-art accuracy on CIFAR-10, CIFAR-100, and ImageNet datasets without\nusing manual labels. Moreover, we show that employing the concomitant weights\nas initialization consistently outperforms the random initialization and the\ntwo-stage weight pre-training method by a clear margin under semi-supervised\nlearning scenarios. Codes are publicly available at\nhttps://github.com/LzVv123456/SSWP-NAS.",
          "arxiv_id": "2206.04125v1"
        }
      ],
      "2": [
        {
          "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
          "year": "2024-06",
          "abstract": "Understanding the mechanisms of information storage and transfer in\nTransformer-based models is important for driving model understanding progress.\nRecent work has studied these mechanisms for Large Language Models (LLMs),\nrevealing insights on how information is stored in a model's parameters and how\ninformation flows to and from these parameters in response to specific prompts.\nHowever, these studies have not yet been extended to Multi-modal Large Language\nModels (MLLMs). Given their expanding capabilities and real-world use, we start\nby studying one aspect of these models -- how MLLMs process information in a\nfactual visual question answering task. We use a constraint-based formulation\nwhich views a visual question as having a set of visual or textual constraints\nthat the model's generated answer must satisfy to be correct (e.g. What movie\ndirected by the director in this photo has won a Golden Globe?). Under this\nsetting, we contribute i) a method that extends causal information tracing from\npure language to the multi-modal setting, and ii) VQA-Constraints, a test-bed\nof 9.7K visual questions annotated with constraints. We use these tools to\nstudy two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show\nthat these MLLMs rely on MLP and self-attention blocks in much earlier layers\nfor information storage, compared to LLMs whose mid-layer MLPs are more\nimportant. We also show that a consistent small subset of visual tokens output\nby the vision encoder are responsible for transferring information from the\nimage to these causal blocks. We validate these mechanisms by introducing\nMultEdit, a model-editing algorithm that can correct errors and insert new\nlong-tailed information into MLLMs by targeting these causal blocks.",
          "arxiv_id": "2406.04236v1"
        },
        {
          "title": "EasyARC: Evaluating Vision Language Models on True Visual Reasoning",
          "year": "2025-06",
          "abstract": "Building on recent advances in language-based reasoning models, we explore\nmultimodal reasoning that integrates vision and text. Existing multimodal\nbenchmarks primarily test visual extraction combined with text-based reasoning,\nlacking true visual reasoning with more complex interactions between vision and\nlanguage. Inspired by the ARC challenge, we introduce EasyARC, a\nvision-language benchmark requiring multi-image, multi-step reasoning, and\nself-correction. EasyARC is procedurally generated, fully verifiable, and\nscalable, making it ideal for reinforcement learning (RL) pipelines. The\ngenerators incorporate progressive difficulty levels, enabling structured\nevaluation across task types and complexities. We benchmark state-of-the-art\nvision-language models and analyze their failure modes. We argue that EasyARC\nsets a new standard for evaluating true reasoning and test-time scaling\ncapabilities in vision-language models. We open-source our benchmark dataset\nand evaluation code.",
          "arxiv_id": "2506.11595v1"
        },
        {
          "title": "Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models",
          "year": "2024-05",
          "abstract": "Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models",
          "arxiv_id": "2405.13872v2"
        }
      ],
      "3": [
        {
          "title": "Transformers in Remote Sensing: A Survey",
          "year": "2022-09",
          "abstract": "Deep learning-based algorithms have seen a massive popularity in different\nareas of remote sensing image analysis over the past decade. Recently,\ntransformers-based architectures, originally introduced in natural language\nprocessing, have pervaded computer vision field where the self-attention\nmechanism has been utilized as a replacement to the popular convolution\noperator for capturing long-range dependencies. Inspired by recent advances in\ncomputer vision, remote sensing community has also witnessed an increased\nexploration of vision transformers for a diverse set of tasks. Although a\nnumber of surveys have focused on transformers in computer vision in general,\nto the best of our knowledge we are the first to present a systematic review of\nrecent advances based on transformers in remote sensing. Our survey covers more\nthan 60 recent transformers-based methods for different remote sensing problems\nin sub-areas of remote sensing: very high-resolution (VHR), hyperspectral (HSI)\nand synthetic aperture radar (SAR) imagery. We conclude the survey by\ndiscussing different challenges and open issues of transformers in remote\nsensing. Additionally, we intend to frequently update and maintain the latest\ntransformers in remote sensing papers with their respective code at:\nhttps://github.com/VIROBO-15/Transformer-in-Remote-Sensing",
          "arxiv_id": "2209.01206v1"
        },
        {
          "title": "LSKNet: A Foundation Lightweight Backbone for Remote Sensing",
          "year": "2024-03",
          "abstract": "Remote sensing images pose distinct challenges for downstream tasks due to\ntheir inherent complexity. While a considerable amount of research has been\ndedicated to remote sensing classification, object detection and semantic\nsegmentation, most of these studies have overlooked the valuable prior\nknowledge embedded within remote sensing scenarios. Such prior knowledge can be\nuseful because remote sensing objects may be mistakenly recognized without\nreferencing a sufficiently long-range context, which can vary for different\nobjects. This paper considers these priors and proposes a lightweight Large\nSelective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its\nlarge spatial receptive field to better model the ranging context of various\nobjects in remote sensing scenarios. To our knowledge, large and selective\nkernel mechanisms have not been previously explored in remote sensing images.\nWithout bells and whistles, our lightweight LSKNet sets new state-of-the-art\nscores on standard remote sensing classification, object detection and semantic\nsegmentation benchmarks. Our comprehensive analysis further validated the\nsignificance of the identified priors and the effectiveness of LSKNet. The code\nis available at https://github.com/zcablii/LSKNet.",
          "arxiv_id": "2403.11735v6"
        },
        {
          "title": "A Survey on Remote Sensing Foundation Models: From Vision to Multimodality",
          "year": "2025-03",
          "abstract": "The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.",
          "arxiv_id": "2503.22081v1"
        }
      ],
      "4": [
        {
          "title": "GlyphDiffusion: Text Generation as Image Generation",
          "year": "2023-04",
          "abstract": "Diffusion models have become a new generative paradigm for text generation.\nConsidering the discrete categorical nature of text, in this paper, we propose\nGlyphDiffusion, a novel diffusion approach for text generation via text-guided\nimage generation. Our key idea is to render the target text as a glyph image\ncontaining visual language content. In this way, conditional text generation\ncan be cast as a glyph image generation task, and it is then natural to apply\ncontinuous diffusion models to discrete texts. Specially, we utilize a cascaded\narchitecture (ie a base and a super-resolution diffusion model) to generate\nhigh-fidelity glyph images, conditioned on the input text. Furthermore, we\ndesign a text grounding module to transform and refine the visual language\ncontent from generated glyph images into the final texts. In experiments over\nfour conditional text generation tasks and two classes of metrics (ie quality\nand diversity), GlyphDiffusion can achieve comparable or even better results\nthan several baselines, including pretrained language models. Our model also\nmakes significant improvements compared to the recent diffusion model.",
          "arxiv_id": "2304.12519v2"
        },
        {
          "title": "RealCompo: Balancing Realism and Compositionality Improves Text-to-Image Diffusion Models",
          "year": "2024-02",
          "abstract": "Diffusion models have achieved remarkable advancements in text-to-image\ngeneration. However, existing models still have many difficulties when faced\nwith multiple-object compositional generation. In this paper, we propose\nRealCompo, a new training-free and transferred-friendly text-to-image\ngeneration framework, which aims to leverage the respective advantages of\ntext-to-image models and spatial-aware image diffusion models (e.g., layout,\nkeypoints and segmentation maps) to enhance both realism and compositionality\nof the generated images. An intuitive and novel balancer is proposed to\ndynamically balance the strengths of the two models in denoising process,\nallowing plug-and-play use of any model without extra training. Extensive\nexperiments show that our RealCompo consistently outperforms state-of-the-art\ntext-to-image models and spatial-aware image diffusion models in\nmultiple-object compositional generation while keeping satisfactory realism and\ncompositionality of the generated images. Notably, our RealCompo can be\nseamlessly extended with a wide range of spatial-aware image diffusion models\nand stylized diffusion models. Our code is available at:\nhttps://github.com/YangLing0818/RealCompo",
          "arxiv_id": "2402.12908v3"
        },
        {
          "title": "Self-correcting LLM-controlled Diffusion Models",
          "year": "2023-11",
          "abstract": "Text-to-image generation has witnessed significant progress with the advent\nof diffusion models. Despite the ability to generate photorealistic images,\ncurrent text-to-image diffusion models still often struggle to accurately\ninterpret and follow complex input text prompts. In contrast to existing models\nthat aim to generate images only with their best effort, we introduce\nSelf-correcting LLM-controlled Diffusion (SLD). SLD is a framework that\ngenerates an image from the input prompt, assesses its alignment with the\nprompt, and performs self-corrections on the inaccuracies in the generated\nimage. Steered by an LLM controller, SLD turns text-to-image generation into an\niterative closed-loop process, ensuring correctness in the resulting image. SLD\nis not only training-free but can also be seamlessly integrated with diffusion\nmodels behind API access, such as DALL-E 3, to further boost the performance of\nstate-of-the-art diffusion models. Experimental results show that our approach\ncan rectify a majority of incorrect generations, particularly in generative\nnumeracy, attribute binding, and spatial relationships. Furthermore, by simply\nadjusting the instructions to the LLM, SLD can perform image editing tasks,\nbridging the gap between text-to-image generation and image editing pipelines.\nWe will make our code available for future research and applications.",
          "arxiv_id": "2311.16090v1"
        }
      ],
      "5": [
        {
          "title": "A Multi-objective Memetic Algorithm for Auto Adversarial Attack Optimization Design",
          "year": "2022-08",
          "abstract": "The phenomenon of adversarial examples has been revealed in variant\nscenarios. Recent studies show that well-designed adversarial defense\nstrategies can improve the robustness of deep learning models against\nadversarial examples. However, with the rapid development of defense\ntechnologies, it also tends to be more difficult to evaluate the robustness of\nthe defensed model due to the weak performance of existing manually designed\nadversarial attacks. To address the challenge, given the defensed model, the\nefficient adversarial attack with less computational burden and lower robust\naccuracy is needed to be further exploited. Therefore, we propose a\nmulti-objective memetic algorithm for auto adversarial attack optimization\ndesign, which realizes the automatical search for the near-optimal adversarial\nattack towards defensed models. Firstly, the more general mathematical model of\nauto adversarial attack optimization design is constructed, where the search\nspace includes not only the attacker operations, magnitude, iteration number,\nand loss functions but also the connection ways of multiple adversarial\nattacks. In addition, we develop a multi-objective memetic algorithm combining\nNSGA-II and local search to solve the optimization problem. Finally, to\ndecrease the evaluation cost during the search, we propose a representative\ndata selection strategy based on the sorting of cross entropy loss values of\neach images output by models. Experiments on CIFAR10, CIFAR100, and ImageNet\ndatasets show the effectiveness of our proposed method.",
          "arxiv_id": "2208.06984v1"
        },
        {
          "title": "Fuzziness-tuned: Improving the Transferability of Adversarial Examples",
          "year": "2023-03",
          "abstract": "With the development of adversarial attacks, adversairal examples have been\nwidely used to enhance the robustness of the training models on deep neural\nnetworks. Although considerable efforts of adversarial attacks on improving the\ntransferability of adversarial examples have been developed, the attack success\nrate of the transfer-based attacks on the surrogate model is much higher than\nthat on victim model under the low attack strength (e.g., the attack strength\n$\\epsilon=8/255$). In this paper, we first systematically investigated this\nissue and found that the enormous difference of attack success rates between\nthe surrogate model and victim model is caused by the existence of a special\narea (known as fuzzy domain in our paper), in which the adversarial examples in\nthe area are classified wrongly by the surrogate model while correctly by the\nvictim model. Then, to eliminate such enormous difference of attack success\nrates for improving the transferability of generated adversarial examples, a\nfuzziness-tuned method consisting of confidence scaling mechanism and\ntemperature scaling mechanism is proposed to ensure the generated adversarial\nexamples can effectively skip out of the fuzzy domain. The confidence scaling\nmechanism and the temperature scaling mechanism can collaboratively tune the\nfuzziness of the generated adversarial examples through adjusting the gradient\ndescent weight of fuzziness and stabilizing the update direction, respectively.\nSpecifically, the proposed fuzziness-tuned method can be effectively integrated\nwith existing adversarial attacks to further improve the transferability of\nadverarial examples without changing the time complexity. Extensive experiments\ndemonstrated that fuzziness-tuned method can effectively enhance the\ntransferability of adversarial examples in the latest transfer-based attacks.",
          "arxiv_id": "2303.10078v1"
        },
        {
          "title": "Visually Adversarial Attacks and Defenses in the Physical World: A Survey",
          "year": "2022-11",
          "abstract": "Although Deep Neural Networks (DNNs) have been widely applied in various\nreal-world scenarios, they are vulnerable to adversarial examples. The current\nadversarial attacks in computer vision can be divided into digital attacks and\nphysical attacks according to their different attack forms. Compared with\ndigital attacks, which generate perturbations in the digital pixels, physical\nattacks are more practical in the real world. Owing to the serious security\nproblem caused by physically adversarial examples, many works have been\nproposed to evaluate the physically adversarial robustness of DNNs in the past\nyears. In this paper, we summarize a survey versus the current physically\nadversarial attacks and physically adversarial defenses in computer vision. To\nestablish a taxonomy, we organize the current physical attacks from attack\ntasks, attack forms, and attack methods, respectively. Thus, readers can have a\nsystematic knowledge of this topic from different aspects. For the physical\ndefenses, we establish the taxonomy from pre-processing, in-processing, and\npost-processing for the DNN models to achieve full coverage of the adversarial\ndefenses. Based on the above survey, we finally discuss the challenges of this\nresearch field and further outlook on the future direction.",
          "arxiv_id": "2211.01671v5"
        }
      ],
      "6": [
        {
          "title": "H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation",
          "year": "2025-07",
          "abstract": "Imitation learning for robotic manipulation faces a fundamental challenge:\nthe scarcity of large-scale, high-quality robot demonstration data. Recent\nrobotic foundation models often pre-train on cross-embodiment robot datasets to\nincrease data scale, while they face significant limitations as the diverse\nmorphologies and action spaces across different robot embodiments make unified\ntraining challenging. In this paper, we present H-RDT (Human to Robotics\nDiffusion Transformer), a novel approach that leverages human manipulation data\nto enhance robot manipulation capabilities. Our key insight is that large-scale\negocentric human manipulation videos with paired 3D hand pose annotations\nprovide rich behavioral priors that capture natural manipulation strategies and\ncan benefit robotic policy learning. We introduce a two-stage training\nparadigm: (1) pre-training on large-scale egocentric human manipulation data,\nand (2) cross-embodiment fine-tuning on robot-specific data with modular action\nencoders and decoders. Built on a diffusion transformer architecture with 2B\nparameters, H-RDT uses flow matching to model complex action distributions.\nExtensive evaluations encompassing both simulation and real-world experiments,\nsingle-task and multitask scenarios, as well as few-shot learning and\nrobustness assessments, demonstrate that H-RDT outperforms training from\nscratch and existing state-of-the-art methods, including Pi0 and RDT, achieving\nsignificant improvements of 13.9% and 40.5% over training from scratch in\nsimulation and real-world experiments, respectively. The results validate our\ncore hypothesis that human manipulation data can serve as a powerful foundation\nfor learning bimanual robotic manipulation policies.",
          "arxiv_id": "2507.23523v2"
        },
        {
          "title": "WorldEval: World Model as Real-World Robot Policies Evaluator",
          "year": "2025-05",
          "abstract": "The field of robotics has made significant strides toward developing\ngeneralist robot manipulation policies. However, evaluating these policies in\nreal-world scenarios remains time-consuming and challenging, particularly as\nthe number of tasks scales and environmental conditions change. In this work,\nwe demonstrate that world models can serve as a scalable, reproducible, and\nreliable proxy for real-world robot policy evaluation. A key challenge is\ngenerating accurate policy videos from world models that faithfully reflect the\nrobot actions. We observe that directly inputting robot actions or using\nhigh-dimensional encoding methods often fails to generate action-following\nvideos. To address this, we propose Policy2Vec, a simple yet effective approach\nto turn a video generation model into a world simulator that follows latent\naction to generate the robot video. We then introduce WorldEval, an automated\npipeline designed to evaluate real-world robot policies entirely online.\nWorldEval effectively ranks various robot policies and individual checkpoints\nwithin a single policy, and functions as a safety detector to prevent dangerous\nactions by newly developed robot models. Through comprehensive paired\nevaluations of manipulation policies in real-world environments, we demonstrate\na strong correlation between policy performance in WorldEval and real-world\nscenarios. Furthermore, our method significantly outperforms popular methods\nsuch as real-to-sim approach.",
          "arxiv_id": "2505.19017v1"
        },
        {
          "title": "Learning Video-Conditioned Policies for Unseen Manipulation Tasks",
          "year": "2023-05",
          "abstract": "The ability to specify robot commands by a non-expert user is critical for\nbuilding generalist agents capable of solving a large variety of tasks. One\nconvenient way to specify the intended robot goal is by a video of a person\ndemonstrating the target task. While prior work typically aims to imitate human\ndemonstrations performed in robot environments, here we focus on a more\nrealistic and challenging setup with demonstrations recorded in natural and\ndiverse human environments. We propose Video-conditioned Policy learning (ViP),\na data-driven approach that maps human demonstrations of previously unseen\ntasks to robot manipulation skills. To this end, we learn our policy to\ngenerate appropriate actions given current scene observations and a video of\nthe target task. To encourage generalization to new tasks, we avoid particular\ntasks during training and learn our policy from unlabelled robot trajectories\nand corresponding robot videos. Both robot and human videos in our framework\nare represented by video embeddings pre-trained for human action recognition.\nAt test time we first translate human videos to robot videos in the common\nvideo embedding space, and then use resulting embeddings to condition our\npolicies. Notably, our approach enables robot control by human demonstrations\nin a zero-shot manner, i.e., without using robot trajectories paired with human\ninstructions during training. We validate our approach on a set of challenging\nmulti-task robot manipulation environments and outperform state of the art. Our\nmethod also demonstrates excellent performance in a new challenging zero-shot\nsetup where no paired data is used during training.",
          "arxiv_id": "2305.06289v1"
        }
      ],
      "7": [
        {
          "title": "Semi-Supervised Few-Shot Atomic Action Recognition",
          "year": "2020-11",
          "abstract": "Despite excellent progress has been made, the performance on action\nrecognition still heavily relies on specific datasets, which are difficult to\nextend new action classes due to labor-intensive labeling. Moreover, the high\ndiversity in Spatio-temporal appearance requires robust and representative\naction feature aggregation and attention. To address the above issues, we focus\non atomic actions and propose a novel model for semi-supervised few-shot atomic\naction recognition. Our model features unsupervised and contrastive video\nembedding, loose action alignment, multi-head feature comparison, and\nattention-based aggregation, together of which enables action recognition with\nonly a few training examples through extracting more representative features\nand allowing flexibility in spatial and temporal alignment and variations in\nthe action. Experiments show that our model can attain high accuracy on\nrepresentative atomic action datasets outperforming their respective\nstate-of-the-art classification accuracy in full supervision setting.",
          "arxiv_id": "2011.08410v1"
        },
        {
          "title": "View-invariant action recognition",
          "year": "2020-09",
          "abstract": "Human action recognition is an important problem in computer vision. It has a\nwide range of applications in surveillance, human-computer interaction,\naugmented reality, video indexing, and retrieval. The varying pattern of\nspatio-temporal appearance generated by human action is key for identifying the\nperformed action. We have seen a lot of research exploring this dynamics of\nspatio-temporal appearance for learning a visual representation of human\nactions. However, most of the research in action recognition is focused on some\ncommon viewpoints, and these approaches do not perform well when there is a\nchange in viewpoint. Human actions are performed in a 3-dimensional environment\nand are projected to a 2-dimensional space when captured as a video from a\ngiven viewpoint. Therefore, an action will have a different spatio-temporal\nappearance from different viewpoints. The research in view-invariant action\nrecognition addresses this problem and focuses on recognizing human actions\nfrom unseen viewpoints.",
          "arxiv_id": "2009.00638v1"
        },
        {
          "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization",
          "year": "2021-05",
          "abstract": "Temporal action localization (TAL) is an important and challenging problem in\nvideo understanding. However, most existing TAL benchmarks are built upon the\ncoarse granularity of action classes, which exhibits two major limitations in\nthis task. First, coarse-level actions can make the localization models overfit\nin high-level context information, and ignore the atomic action details in the\nvideo. Second, the coarse action classes often lead to the ambiguous\nannotations of temporal boundaries, which are inappropriate for temporal action\nlocalization. To tackle these problems, we develop a novel large-scale and\nfine-grained video dataset, coined as FineAction, for temporal action\nlocalization. In total, FineAction contains 103K temporal instances of 106\naction categories, annotated in 17K untrimmed videos. Compared to the existing\nTAL datasets, our FineAction takes distinct characteristics of fine action\nclasses with rich diversity, dense annotations of multiple instances, and\nco-occurring actions of different classes, which introduces new opportunities\nand challenges for temporal action localization. To benchmark FineAction, we\nsystematically investigate the performance of several popular temporal\nlocalization methods on it, and deeply analyze the influence of fine-grained\ninstances in temporal action localization. As a minor contribution, we present\na simple baseline approach for handling the fine-grained action detection,\nwhich achieves an mAP of 13.17% on our FineAction. We believe that FineAction\ncan advance research of temporal action localization and beyond.",
          "arxiv_id": "2105.11107v3"
        }
      ],
      "8": [
        {
          "title": "SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced Classification in Pathology",
          "year": "2023-03",
          "abstract": "Multiple Instance learning (MIL) models have been extensively used in\npathology to predict biomarkers and risk-stratify patients from gigapixel-sized\nimages. Machine learning problems in medical imaging often deal with rare\ndiseases, making it important for these models to work in a label-imbalanced\nsetting. In pathology images, there is another level of imbalance, where given\na positively labeled Whole Slide Image (WSI), only a fraction of pixels within\nit contribute to the positive label. This compounds the severity of imbalance\nand makes imbalanced classification in pathology challenging. Furthermore,\nthese imbalances can occur in out-of-distribution (OOD) datasets when the\nmodels are deployed in the real-world. We leverage the idea that decoupling\nfeature and classifier learning can lead to improved decision boundaries for\nlabel imbalanced datasets. To this end, we investigate the integration of\nsupervised contrastive learning with multiple instance learning (SC-MIL).\nSpecifically, we propose a joint-training MIL framework in the presence of\nlabel imbalance that progressively transitions from learning bag-level\nrepresentations to optimal classifier learning. We perform experiments with\ndifferent imbalance settings for two well-studied problems in cancer pathology:\nsubtyping of non-small cell lung cancer and subtyping of renal cell carcinoma.\nSC-MIL provides large and consistent improvements over other techniques on both\nin-distribution (ID) and OOD held-out sets across multiple imbalanced settings.",
          "arxiv_id": "2303.13405v2"
        },
        {
          "title": "Classification of Breast Tumours Based on Histopathology Images Using Deep Features and Ensemble of Gradient Boosting Methods",
          "year": "2022-09",
          "abstract": "Breast cancer is the most common cancer among women worldwide. Early-stage\ndiagnosis of breast cancer can significantly improve the efficiency of\ntreatment. Computer-aided diagnosis (CAD) systems are widely adopted in this\nissue due to their reliability, accuracy and affordability. There are different\nimaging techniques for a breast cancer diagnosis; one of the most accurate ones\nis histopathology which is used in this paper. Deep feature transfer learning\nis used as the main idea of the proposed CAD system's feature extractor.\nAlthough 16 different pre-trained networks have been tested in this study, our\nmain focus is on the classification phase. The Inception-ResNet-v2 which has\nboth residual and inception networks profits together has shown the best\nfeature extraction capability in the case of breast cancer histopathology\nimages among all tested CNNs. In the classification phase, the ensemble of\nCatBoost, XGBoost and LightGBM has provided the best average accuracy. The\nBreakHis dataset was used to evaluate the proposed method. BreakHis contains\n7909 histopathology images (2,480 benign and 5,429 malignant) in four\nmagnification factors. The proposed method's accuracy (IRv2-CXL) using 70% of\nBreakHis dataset as training data in 40x, 100x, 200x and 400x magnification is\n96.82%, 95.84%, 97.01% and 96.15%, respectively. Most studies on automated\nbreast cancer detection have focused on feature extraction, which made us\nattend to the classification phase. IRv2-CXL has shown better or comparable\nresults in all magnifications due to using the soft voting ensemble method\nwhich could combine the advantages of CatBoost, XGBoost and LightGBM together.",
          "arxiv_id": "2209.01380v1"
        },
        {
          "title": "HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images",
          "year": "2022-05",
          "abstract": "Ultrasonography is an important routine examination for breast cancer\ndiagnosis, due to its non-invasive, radiation-free and low-cost properties.\nHowever, the diagnostic accuracy of breast cancer is still limited due to its\ninherent limitations. It would be a tremendous success if we can precisely\ndiagnose breast cancer by breast ultrasound images (BUS). Many learning-based\ncomputer-aided diagnostic methods have been proposed to achieve breast cancer\ndiagnosis/lesion classification. However, most of them require a pre-define ROI\nand then classify the lesion inside the ROI. Conventional classification\nbackbones, such as VGG16 and ResNet50, can achieve promising classification\nresults with no ROI requirement. But these models lack interpretability, thus\nrestricting their use in clinical practice. In this study, we propose a novel\nROI-free model for breast cancer diagnosis in ultrasound images with\ninterpretable feature representations. We leverage the anatomical prior\nknowledge that malignant and benign tumors have different spatial relationships\nbetween different tissue layers, and propose a HoVer-Transformer to formulate\nthis prior knowledge. The proposed HoVer-Trans block extracts the inter- and\nintra-layer spatial information horizontally and vertically. We conduct and\nrelease an open dataset GDPH&SYSUCC for breast cancer diagnosis in BUS. The\nproposed model is evaluated in three datasets by comparing with four CNN-based\nmodels and two vision transformer models via five-fold cross validation. It\nachieves state-of-the-art classification performance with the best model\ninterpretability. In the meanwhile, our proposed model outperforms two senior\nsonographers on the breast cancer diagnosis when only one BUS image is given.",
          "arxiv_id": "2205.08390v2"
        }
      ],
      "9": [
        {
          "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting",
          "year": "2025-07",
          "abstract": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework\nleverages the inherent continuous spatial representation of NeRF to mitigate\nseveral limitations of 3DGS, including sensitivity to Gaussian initialization,\nlimited spatial awareness, and weak inter-Gaussian correlations, thereby\nenhancing its performance. In NeRF-GS, we revisit the design of 3DGS and\nprogressively align its spatial features with NeRF, enabling both\nrepresentations to be optimized within the same scene through shared 3D spatial\ninformation. We further address the formal distinctions between the two\napproaches by optimizing residual vectors for both implicit features and\nGaussian positions to enhance the personalized capabilities of 3DGS.\nExperimental results on benchmark datasets show that NeRF-GS surpasses existing\nmethods and achieves state-of-the-art performance. This outcome confirms that\nNeRF and 3DGS are complementary rather than competing, offering new insights\ninto hybrid approaches that combine 3DGS and NeRF for efficient 3D scene\nrepresentation.",
          "arxiv_id": "2507.23374v1"
        },
        {
          "title": "2D Gaussian Splatting for Geometrically Accurate Radiance Fields",
          "year": "2024-03",
          "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\nreconstruction, achieving high quality novel view synthesis and fast rendering\nspeed without baking. However, 3DGS fails to accurately represent surfaces due\nto the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\nSplatting (2DGS), a novel approach to model and reconstruct geometrically\naccurate radiance fields from multi-view images. Our key idea is to collapse\nthe 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D\nGaussians, 2D Gaussians provide view-consistent geometry while modeling\nsurfaces intrinsically. To accurately recover thin surfaces and achieve stable\noptimization, we introduce a perspective-correct 2D splatting process utilizing\nray-splat intersection and rasterization. Additionally, we incorporate depth\ndistortion and normal consistency terms to further enhance the quality of the\nreconstructions. We demonstrate that our differentiable renderer allows for\nnoise-free and detailed geometry reconstruction while maintaining competitive\nappearance quality, fast training speed, and real-time rendering.",
          "arxiv_id": "2403.17888v3"
        },
        {
          "title": "Recent Advances in 3D Gaussian Splatting",
          "year": "2024-03",
          "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the\nrendering speed of novel view synthesis. Unlike neural implicit representations\nlike Neural Radiance Fields (NeRF) that represent a 3D scene with position and\nviewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of\nGaussian ellipsoids to model the scene so that efficient rendering can be\naccomplished by rasterizing Gaussian ellipsoids into images. Apart from the\nfast rendering speed, the explicit representation of 3D Gaussian Splatting\nfacilitates editing tasks like dynamic reconstruction, geometry editing, and\nphysical simulation. Considering the rapid change and growing number of works\nin this field, we present a literature review of recent 3D Gaussian Splatting\nmethods, which can be roughly classified into 3D reconstruction, 3D editing,\nand other downstream applications by functionality. Traditional point-based\nrendering methods and the rendering formulation of 3D Gaussian Splatting are\nalso illustrated for a better understanding of this technique. This survey aims\nto help beginners get into this field quickly and provide experienced\nresearchers with a comprehensive overview, which can stimulate the future\ndevelopment of the 3D Gaussian Splatting representation.",
          "arxiv_id": "2403.11134v2"
        }
      ],
      "10": [
        {
          "title": "Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D Object Detection",
          "year": "2021-03",
          "abstract": "The ability to accurately detect and localize objects is recognized as being\nthe most important for the perception of self-driving cars. From 2D to 3D\nobject detection, the most difficult is to determine the distance from the\nego-vehicle to objects. Expensive technology like LiDAR can provide a precise\nand accurate depth information, so most studies have tended to focus on this\nsensor showing a performance gap between LiDAR-based methods and camera-based\nmethods. Although many authors have investigated how to fuse LiDAR with RGB\ncameras, as far as we know there are no studies to fuse LiDAR and stereo in a\ndeep neural network for the 3D object detection task. This paper presents\nSLS-Fusion, a new approach to fuse data from 4-beam LiDAR and a stereo camera\nvia a neural network for depth estimation to achieve better dense depth maps\nand thereby improves 3D object detection performance. Since 4-beam LiDAR is\ncheaper than the well-known 64-beam LiDAR, this approach is also classified as\na low-cost sensors-based method. Through evaluation on the KITTI benchmark, it\nis shown that the proposed method significantly improves depth estimation\nperformance compared to a baseline method. Also, when applying it to 3D object\ndetection, a new state of the art on low-cost sensor based method is achieved.",
          "arxiv_id": "2103.03977v3"
        },
        {
          "title": "RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection",
          "year": "2025-05",
          "abstract": "Accurate, fast, and reliable 3D perception is essential for autonomous\ndriving. Recently, bird's-eye view (BEV)-based perception approaches have\nemerged as superior alternatives to perspective-based solutions, offering\nenhanced spatial understanding and more natural outputs for planning. Existing\nBEV-based 3D object detection methods, typically adhering to angle-based\nrepresentation, directly estimate the size and orientation of rotated bounding\nboxes. We observe that BEV-based 3D object detection is analogous to aerial\noriented object detection, where angle-based methods are recognized for being\naffected by discontinuities in their loss functions. Drawing inspiration from\nthis domain, we propose Restricted Quadrilateral Representation to define 3D\nregression targets. RQR3D regresses the smallest horizontal bounding box\nencapsulating the oriented box, along with the offsets between the corners of\nthese two boxes, thereby transforming the oriented object detection problem\ninto a keypoint regression task. RQR3D is compatible with any 3D object\ndetection approach. We employ RQR3D within an anchor-free single-stage object\ndetection method and introduce an objectness head to address class imbalance\nproblem. Furthermore, we introduce a simplified radar fusion backbone that\neliminates the need for voxel grouping and processes the BEV-mapped point cloud\nwith standard 2D convolutions, rather than sparse convolutions. Extensive\nevaluations on the nuScenes dataset demonstrate that RQR3D achieves\nstate-of-the-art performance in camera-radar 3D object detection, outperforming\nthe previous best method by +4% in NDS and +2.4% in mAP, and significantly\nreducing the translation and orientation errors, which are crucial for safe\nautonomous driving. These consistent gains highlight the robustness, precision,\nand real-world readiness of our approach.",
          "arxiv_id": "2505.17732v1"
        },
        {
          "title": "3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View Spatial Feature Fusion for 3D Object Detection",
          "year": "2020-04",
          "abstract": "In this paper, we propose a new deep architecture for fusing camera and LiDAR\nsensors for 3D object detection. Because the camera and LiDAR sensor signals\nhave different characteristics and distributions, fusing these two modalities\nis expected to improve both the accuracy and robustness of 3D object detection.\nOne of the challenges presented by the fusion of cameras and LiDAR is that the\nspatial feature maps obtained from each modality are represented by\nsignificantly different views in the camera and world coordinates; hence, it is\nnot an easy task to combine two heterogeneous feature maps without loss of\ninformation. To address this problem, we propose a method called 3D-CVF that\ncombines the camera and LiDAR features using the cross-view spatial feature\nfusion strategy. First, the method employs auto-calibrated projection, to\ntransform the 2D camera features to a smooth spatial feature map with the\nhighest correspondence to the LiDAR features in the bird's eye view (BEV)\ndomain. Then, a gated feature fusion network is applied to use the spatial\nattention maps to mix the camera and LiDAR features appropriately according to\nthe region. Next, camera-LiDAR feature fusion is also achieved in the\nsubsequent proposal refinement stage. The camera feature is used from the 2D\ncamera-view domain via 3D RoI grid pooling and fused with the BEV feature for\nproposal refinement. Our evaluations, conducted on the KITTI and nuScenes 3D\nobject detection datasets demonstrate that the camera-LiDAR fusion offers\nsignificant performance gain over single modality and that the proposed 3D-CVF\nachieves state-of-the-art performance in the KITTI benchmark.",
          "arxiv_id": "2004.12636v2"
        }
      ],
      "11": [
        {
          "title": "A Framework for a Capability-driven Evaluation of Scenario Understanding for Multimodal Large Language Models in Autonomous Driving",
          "year": "2025-03",
          "abstract": "Multimodal large language models (MLLMs) hold the potential to enhance\nautonomous driving by combining domain-independent world knowledge with\ncontext-specific language guidance. Their integration into autonomous driving\nsystems shows promising results in isolated proof-of-concept applications,\nwhile their performance is evaluated on selective singular aspects of\nperception, reasoning, or planning. To leverage their full potential a\nsystematic framework for evaluating MLLMs in the context of autonomous driving\nis required. This paper proposes a holistic framework for a capability-driven\nevaluation of MLLMs in autonomous driving. The framework structures scenario\nunderstanding along the four core capability dimensions semantic, spatial,\ntemporal, and physical. They are derived from the general requirements of\nautonomous driving systems, human driver cognition, and language-based\nreasoning. It further organises the domain into context layers, processing\nmodalities, and downstream tasks such as language-based interaction and\ndecision-making. To illustrate the framework's applicability, two exemplary\ntraffic scenarios are analysed, grounding the proposed dimensions in realistic\ndriving situations. The framework provides a foundation for the structured\nevaluation of MLLMs' potential for scenario understanding in autonomous\ndriving.",
          "arxiv_id": "2503.11400v1"
        },
        {
          "title": "ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving",
          "year": "2024-12",
          "abstract": "Trajectory prediction of agents is crucial for the safety of autonomous\nvehicles, whereas previous approaches usually rely on sufficiently\nlong-observed trajectory to predict the future trajectory of the agents.\nHowever, in real-world scenarios, it is not realistic to collect adequate\nobserved locations for moving agents, leading to the collapse of most\nprediction models. For instance, when a moving car suddenly appears and is very\nclose to an autonomous vehicle because of the obstruction, it is quite\nnecessary for the autonomous vehicle to quickly and accurately predict the\nfuture trajectories of the car with limited observed trajectory locations. In\nlight of this, we focus on investigating the task of instantaneous trajectory\nprediction, i.e., two observed locations are available during inference. To\nthis end, we propose a general and plug-and-play instantaneous trajectory\nprediction approach, called ITPNet. Specifically, we propose a backward\nforecasting mechanism to reversely predict the latent feature representations\nof unobserved historical trajectories of the agent based on its two observed\nlocations and then leverage them as complementary information for future\ntrajectory prediction. Meanwhile, due to the inevitable existence of noise and\nredundancy in the predicted latent feature representations, we further devise a\nNoise Redundancy Reduction Former, aiming at to filter out noise and redundancy\nfrom unobserved trajectories and integrate the filtered features and observed\nfeatures into a compact query for future trajectory predictions. In essence,\nITPNet can be naturally compatible with existing trajectory prediction models,\nenabling them to gracefully handle the case of instantaneous trajectory\nprediction. Extensive experiments on the Argoverse and nuScenes datasets\ndemonstrate ITPNet outperforms the baselines, and its efficacy with different\ntrajectory prediction models.",
          "arxiv_id": "2412.07369v1"
        },
        {
          "title": "ScePT: Scene-consistent, Policy-based Trajectory Predictions for Planning",
          "year": "2022-06",
          "abstract": "Trajectory prediction is a critical functionality of autonomous systems that\nshare environments with uncontrolled agents, one prominent example being\nself-driving vehicles. Currently, most prediction methods do not enforce scene\nconsistency, i.e., there are a substantial amount of self-collisions between\npredicted trajectories of different agents in the scene. Moreover, many\napproaches generate individual trajectory predictions per agent instead of\njoint trajectory predictions of the whole scene, which makes downstream\nplanning difficult. In this work, we present ScePT, a policy planning-based\ntrajectory prediction model that generates accurate, scene-consistent\ntrajectory predictions suitable for autonomous system motion planning. It\nexplicitly enforces scene consistency and learns an agent interaction policy\nthat can be used for conditional prediction. Experiments on multiple real-world\npedestrians and autonomous vehicle datasets show that ScePT} matches current\nstate-of-the-art prediction accuracy with significantly improved scene\nconsistency. We also demonstrate ScePT's ability to work with a downstream\ncontingency planner.",
          "arxiv_id": "2206.13387v1"
        }
      ],
      "12": [
        {
          "title": "Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning",
          "year": "2025-08",
          "abstract": "The video reasoning ability of multimodal large language models (MLLMs) is\ncrucial for downstream tasks like video question answering and temporal\ngrounding. While recent approaches have explored text-based chain-of-thought\n(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal\ninteraction and increased hallucination, especially with longer videos or\nreasoning chains. To address these challenges, we propose Video Intelligence\nvia Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning\nframework. With a visual toolbox, the model can densely sample new video frames\non demand and generate multimodal CoT for precise long video reasoning. We\nobserve that temporal grounding and question answering are mutually beneficial\nfor video understanding tasks. Therefore, we construct two high-quality\nmulti-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and\nMTVR-RL-110k for reinforcement learning. Moreover, we propose a\nDifficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to\nmitigate difficulty imbalance in multi-task reinforcement learning. Extensive\nexperiments on 11 challenging video understanding benchmarks demonstrate the\nadvanced reasoning ability of VITAL, outperforming existing methods in video\nquestion answering and temporal grounding tasks, especially in long video\nscenarios. Code is available at\nhttps://zhang9302002.github.io/thinkingwithvideos-page/.",
          "arxiv_id": "2508.04416v2"
        },
        {
          "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding",
          "year": "2025-07",
          "abstract": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.",
          "arxiv_id": "2507.13353v1"
        },
        {
          "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
          "year": "2025-04",
          "abstract": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.",
          "arxiv_id": "2504.10443v1"
        }
      ],
      "13": [
        {
          "title": "Face Reconstruction from Face Embeddings using Adapter to a Face Foundation Model",
          "year": "2024-11",
          "abstract": "Face recognition systems extract embedding vectors from face images and use\nthese embeddings to verify or identify individuals. Face reconstruction attack\n(also known as template inversion) refers to reconstructing face images from\nface embeddings and using the reconstructed face image to enter a face\nrecognition system. In this paper, we propose to use a face foundation model to\nreconstruct face images from the embeddings of a blackbox face recognition\nmodel. The foundation model is trained with 42M images to generate face images\nfrom the facial embeddings of a fixed face recognition model. We propose to use\nan adapter to translate target embeddings into the embedding space of the\nfoundation model. The generated images are evaluated on different face\nrecognition models and different datasets, demonstrating the effectiveness of\nour method to translate embeddings of different face recognition models. We\nalso evaluate the transferability of reconstructed face images when attacking\ndifferent face recognition models. Our experimental results show that our\nreconstructed face images outperform previous reconstruction attacks against\nface recognition models.",
          "arxiv_id": "2411.03960v1"
        },
        {
          "title": "Is Face Recognition Safe from Realizable Attacks?",
          "year": "2022-10",
          "abstract": "Face recognition is a popular form of biometric authentication and due to its\nwidespread use, attacks have become more common as well. Recent studies show\nthat Face Recognition Systems are vulnerable to attacks and can lead to\nerroneous identification of faces. Interestingly, most of these attacks are\nwhite-box, or they are manipulating facial images in ways that are not\nphysically realizable. In this paper, we propose an attack scheme where the\nattacker can generate realistic synthesized face images with subtle\nperturbations and physically realize that onto his face to attack black-box\nface recognition systems. Comprehensive experiments and analyses show that\nsubtle perturbations realized on attackers face can create successful attacks\non state-of-the-art face recognition systems in black-box settings. Our study\nexposes the underlying vulnerability posed by the Face Recognition Systems\nagainst realizable black-box attacks.",
          "arxiv_id": "2210.08178v1"
        },
        {
          "title": "Exploring Decision-based Black-box Attacks on Face Forgery Detection",
          "year": "2023-10",
          "abstract": "Face forgery generation technologies generate vivid faces, which have raised\npublic concerns about security and privacy. Many intelligent systems, such as\nelectronic payment and identity verification, rely on face forgery detection.\nAlthough face forgery detection has successfully distinguished fake faces,\nrecent studies have demonstrated that face forgery detectors are very\nvulnerable to adversarial examples. Meanwhile, existing attacks rely on network\narchitectures or training datasets instead of the predicted labels, which leads\nto a gap in attacking deployed applications. To narrow this gap, we first\nexplore the decision-based attacks on face forgery detection. However, applying\nexisting decision-based attacks directly suffers from perturbation\ninitialization failure and low image quality. First, we propose cross-task\nperturbation to handle initialization failures by utilizing the high\ncorrelation of face features on different tasks. Then, inspired by using\nfrequency cues by face forgery detection, we propose the frequency\ndecision-based attack. We add perturbations in the frequency domain and then\nconstrain the visual quality in the spatial domain. Finally, extensive\nexperiments demonstrate that our method achieves state-of-the-art attack\nperformance on FaceForensics++, CelebDF, and industrial APIs, with high query\nefficiency and guaranteed image quality. Further, the fake faces by our method\ncan pass face forgery detection and face recognition, which exposes the\nsecurity problems of face forgery detectors.",
          "arxiv_id": "2310.12017v1"
        }
      ],
      "14": [
        {
          "title": "ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting",
          "year": "2024-03",
          "abstract": "In recent years, text-image joint pre-training techniques have shown\npromising results in various tasks. However, in Optical Character Recognition\n(OCR) tasks, aligning text instances with their corresponding text regions in\nimages poses a challenge, as it requires effective alignment between text and\nOCR-Text (referring to the text in images as OCR-Text to distinguish from the\ntext in natural language) rather than a holistic understanding of the overall\nimage content. In this paper, we propose a new pre-training method called\nOCR-Text Destylization Modeling (ODM) that transfers diverse styles of text\nfound in images to a uniform style based on the text prompt. With ODM, we\nachieve better alignment between text and OCR-Text and enable pre-trained\nmodels to adapt to the complex and diverse styles of scene text detection and\nspotting tasks. Additionally, we have designed a new labeling generation method\nspecifically for ODM and combined it with our proposed Text-Controller module\nto address the challenge of annotation costs in OCR tasks, allowing a larger\namount of unlabeled data to participate in pre-training. Extensive experiments\non multiple public datasets demonstrate that our method significantly improves\nperformance and outperforms current pre-training methods in scene text\ndetection and spotting tasks. Code is available at\nhttps://github.com/PriNing/ODM.",
          "arxiv_id": "2403.00303v2"
        },
        {
          "title": "TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text",
          "year": "2021-05",
          "abstract": "A crucial component for the scene text based reasoning required for TextVQA\nand TextCaps datasets involve detecting and recognizing text present in the\nimages using an optical character recognition (OCR) system. The current systems\nare crippled by the unavailability of ground truth text annotations for these\ndatasets as well as lack of scene text detection and recognition datasets on\nreal images disallowing the progress in the field of OCR and evaluation of\nscene text based reasoning in isolation from OCR systems. In this work, we\npropose TextOCR, an arbitrary-shaped scene text detection and recognition with\n900k annotated words collected on real images from TextVQA dataset. We show\nthat current state-of-the-art text-recognition (OCR) models fail to perform\nwell on TextOCR and that training on TextOCR helps achieve state-of-the-art\nperformance on multiple other OCR datasets as well. We use a TextOCR trained\nOCR model to create PixelM4C model which can do scene text based reasoning on\nan image in an end-to-end fashion, allowing us to revisit several design\nchoices to achieve new state-of-the-art performance on TextVQA dataset.",
          "arxiv_id": "2105.05486v1"
        },
        {
          "title": "Enhancement of text recognition for hanja handwritten documents of Ancient Korea",
          "year": "2024-12",
          "abstract": "We implemented a high-performance optical character recognition model for\nclassical handwritten documents using data augmentation with highly variable\ncropping within the document region. Optical character recognition in\nhandwritten documents, especially classical documents, has been a challenging\ntopic in many countries and research organizations due to its difficulty.\nAlthough many researchers have conducted research on this topic, the quality of\nclassical texts over time and the unique stylistic characteristics of various\nauthors have made it difficult, and it is clear that the recognition of hanja\nhandwritten documents is a meaningful and special challenge, especially since\nhanja, which has been developed by reflecting the vocabulary, semantic, and\nsyntactic features of the Joseon Dynasty, is different from classical Chinese\ncharacters. To study this challenge, we used 1100 cursive documents, which are\nsmall in size, and augmented 100 documents per document by cropping a randomly\nsized region within each document for training, and trained them using a\ntwo-stage object detection model, High resolution neural network (HRNet), and\napplied the resulting model to achieve a high inference recognition rate of 90%\nfor cursive documents. Through this study, we also confirmed that the\nperformance of OCR is affected by the simplified characters, variants, variant\ncharacters, common characters, and alternators of Chinese characters that are\ndifficult to see in other studies, and we propose that the results of this\nstudy can be applied to optical character recognition of modern documents in\nmultiple languages as well as other typefaces in classical documents.",
          "arxiv_id": "2412.10647v1"
        }
      ],
      "15": [
        {
          "title": "A Recent Survey of Vision Transformers for Medical Image Segmentation",
          "year": "2023-12",
          "abstract": "Medical image segmentation plays a crucial role in various healthcare\napplications, enabling accurate diagnosis, treatment planning, and disease\nmonitoring. Traditionally, convolutional neural networks (CNNs) dominated this\ndomain, excelling at local feature extraction. However, their limitations in\ncapturing long-range dependencies across image regions pose challenges for\nsegmenting complex, interconnected structures often encountered in medical\ndata. In recent years, Vision Transformers (ViTs) have emerged as a promising\ntechnique for addressing the challenges in medical image segmentation. Their\nmulti-scale attention mechanism enables effective modeling of long-range\ndependencies between distant structures, crucial for segmenting organs or\nlesions spanning the image. Additionally, ViTs' ability to discern subtle\npattern heterogeneity allows for the precise delineation of intricate\nboundaries and edges, a critical aspect of accurate medical image segmentation.\nHowever, they do lack image-related inductive bias and translational\ninvariance, potentially impacting their performance. Recently, researchers have\ncome up with various ViT-based approaches that incorporate CNNs in their\narchitectures, known as Hybrid Vision Transformers (HVTs) to capture local\ncorrelation in addition to the global information in the images. This survey\npaper provides a detailed review of the recent advancements in ViTs and HVTs\nfor medical image segmentation. Along with the categorization of ViT and\nHVT-based medical image segmentation approaches, we also present a detailed\noverview of their real-time applications in several medical image modalities.\nThis survey may serve as a valuable resource for researchers, healthcare\npractitioners, and students in understanding the state-of-the-art approaches\nfor ViT-based medical image segmentation.",
          "arxiv_id": "2312.00634v2"
        },
        {
          "title": "Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation",
          "year": "2023-04",
          "abstract": "The Segment Anything Model (SAM) has recently gained popularity in the field\nof image segmentation due to its impressive capabilities in various\nsegmentation tasks and its prompt-based interface. However, recent studies and\nindividual experiments have shown that SAM underperforms in medical image\nsegmentation, since the lack of the medical specific knowledge. This raises the\nquestion of how to enhance SAM's segmentation capability for medical images. In\nthis paper, instead of fine-tuning the SAM model, we propose the Medical SAM\nAdapter (Med-SA), which incorporates domain-specific medical knowledge into the\nsegmentation model using a light yet effective adaptation technique. In Med-SA,\nwe propose Space-Depth Transpose (SD-Trans) to adapt 2D SAM to 3D medical\nimages and Hyper-Prompting Adapter (HyP-Adpt) to achieve prompt-conditioned\nadaptation. We conduct comprehensive evaluation experiments on 17 medical image\nsegmentation tasks across various image modalities. Med-SA outperforms several\nstate-of-the-art (SOTA) medical image segmentation methods, while updating only\n2\\% of the parameters. Our code is released at\nhttps://github.com/KidsWithTokens/Medical-SAM-Adapter.",
          "arxiv_id": "2304.12620v7"
        },
        {
          "title": "PyMIC: A deep learning toolkit for annotation-efficient medical image segmentation",
          "year": "2022-08",
          "abstract": "Background and Objective: Open-source deep learning toolkits are one of the\ndriving forces for developing medical image segmentation models. Existing\ntoolkits mainly focus on fully supervised segmentation and require full and\naccurate pixel-level annotations that are time-consuming and difficult to\nacquire for segmentation tasks, which makes learning from imperfect labels\nhighly desired for reducing the annotation cost. We aim to develop a new deep\nlearning toolkit to support annotation-efficient learning for medical image\nsegmentation.\n  Methods: Our proposed toolkit named PyMIC is a modular deep learning library\nfor medical image segmentation tasks. In addition to basic components that\nsupport development of high-performance models for fully supervised\nsegmentation, it contains several advanced components tailored for learning\nfrom imperfect annotations, such as loading annotated and unannounced images,\nloss functions for unannotated, partially or inaccurately annotated images, and\ntraining procedures for co-learning between multiple networks, etc. PyMIC\nsupports development of semi-supervised, weakly supervised and noise-robust\nlearning methods for medical image segmentation.\n  Results: We present several illustrative medical image segmentation tasks\nbased on PyMIC: (1) Achieving competitive performance on fully supervised\nlearning; (2) Semi-supervised cardiac structure segmentation with only 10%\ntraining images annotated; (3) Weakly supervised segmentation using scribble\nannotations; and (4) Learning from noisy labels for chest radiograph\nsegmentation.\n  Conclusions: The PyMIC toolkit is easy to use and facilitates efficient\ndevelopment of medical image segmentation models with imperfect annotations. It\nis modular and flexible, which enables researchers to develop high-performance\nmodels with low annotation cost. The source code is available at:\nhttps://github.com/HiLab-git/PyMIC.",
          "arxiv_id": "2208.09350v2"
        }
      ],
      "16": [
        {
          "title": "RealMonoDepth: Self-Supervised Monocular Depth Estimation for General Scenes",
          "year": "2020-04",
          "abstract": "We present a generalised self-supervised learning approach for monocular\nestimation of the real depth across scenes with diverse depth ranges from\n1--100s of meters. Existing supervised methods for monocular depth estimation\nrequire accurate depth measurements for training. This limitation has led to\nthe introduction of self-supervised methods that are trained on stereo image\npairs with a fixed camera baseline to estimate disparity which is transformed\nto depth given known calibration. Self-supervised approaches have demonstrated\nimpressive results but do not generalise to scenes with different depth ranges\nor camera baselines. In this paper, we introduce RealMonoDepth a\nself-supervised monocular depth estimation approach which learns to estimate\nthe real scene depth for a diverse range of indoor and outdoor scenes. A novel\nloss function with respect to the true scene depth based on relative depth\nscaling and warping is proposed. This allows self-supervised training of a\nsingle network with multiple data sets for scenes with diverse depth ranges\nfrom both stereo pair and in the wild moving camera data sets. A comprehensive\nperformance evaluation across five benchmark data sets demonstrates that\nRealMonoDepth provides a single trained network which generalises depth\nestimation across indoor and outdoor scenes, consistently outperforming\nprevious self-supervised approaches.",
          "arxiv_id": "2004.06267v1"
        },
        {
          "title": "Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning",
          "year": "2022-08",
          "abstract": "Self-supervised monocular methods can efficiently learn depth information of\nweakly textured surfaces or reflective objects. However, the depth accuracy is\nlimited due to the inherent ambiguity in monocular geometric modeling. In\ncontrast, multi-frame depth estimation methods improve the depth accuracy\nthanks to the success of Multi-View Stereo (MVS), which directly makes use of\ngeometric constraints. Unfortunately, MVS often suffers from texture-less\nregions, non-Lambertian surfaces, and moving objects, especially in real-world\nvideo sequences without known camera motion and depth supervision. Therefore,\nwe propose MOVEDepth, which exploits the MOnocular cues and VElocity guidance\nto improve multi-frame Depth learning. Unlike existing methods that enforce\nconsistency between MVS depth and monocular depth, MOVEDepth boosts multi-frame\ndepth learning by directly addressing the inherent problems of MVS. The key of\nour approach is to utilize monocular depth as a geometric priority to construct\nMVS cost volume, and adjust depth candidates of cost volume under the guidance\nof predicted camera velocity. We further fuse monocular depth and MVS depth by\nlearning uncertainty in the cost volume, which results in a robust depth\nestimation against ambiguity in multi-view geometry. Extensive experiments show\nMOVEDepth achieves state-of-the-art performance: Compared with Monodepth2 and\nPackNet, our method relatively improves the depth accuracy by 20\\% and 19.8\\%\non the KITTI benchmark. MOVEDepth also generalizes to the more challenging DDAD\nbenchmark, relatively outperforming ManyDepth by 7.2\\%. The code is available\nat https://github.com/JeffWang987/MOVEDepth.",
          "arxiv_id": "2208.09170v1"
        },
        {
          "title": "Non-learning Stereo-aided Depth Completion under Mis-projection via Selective Stereo Matching",
          "year": "2022-10",
          "abstract": "We propose a non-learning depth completion method for a sparse depth map\ncaptured using a light detection and ranging (LiDAR) sensor guided by a pair of\nstereo images. Generally, conventional stereo-aided depth completion methods\nhave two limiations. (i) They assume the given sparse depth map is accurately\naligned to the input image, whereas the alignment is difficult to achieve in\npractice. (ii) They have limited accuracy in the long range because the depth\nis estimated by pixel disparity. To solve the abovementioned limitations, we\npropose selective stereo matching (SSM) that searches the most appropriate\ndepth value for each image pixel from its neighborly projected LiDAR points\nbased on an energy minimization framework. This depth selection approach can\nhandle any type of mis-projection. Moreover, SSM has an advantage in terms of\nlong-range depth accuracy because it directly uses the LiDAR measurement rather\nthan the depth acquired from the stereo. SSM is a discrete process; thus, we\napply variational smoothing with binary anisotropic diffusion tensor (B-ADT) to\ngenerate a continuous depth map while preserving depth discontinuity across\nobject boundaries. Experimentally, compared with the previous state-of-the-art\nstereo-aided depth completion, the proposed method reduced the mean absolute\nerror (MAE) of the depth estimation to 0.65 times and demonstrated\napproximately twice more accurate estimation in the long range. Moreover, under\nvarious LiDAR-camera calibration errors, the proposed method reduced the depth\nestimation MAE to 0.34-0.93 times from previous depth completion methods.",
          "arxiv_id": "2210.01436v1"
        }
      ],
      "17": [
        {
          "title": "DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection",
          "year": "2024-10",
          "abstract": "Synthesizing anomaly samples has proven to be an effective strategy for\nself-supervised 2D industrial anomaly detection. However, this approach has\nbeen rarely explored in multi-modality anomaly detection, particularly\ninvolving 3D and RGB images. In this paper, we propose a novel dual-modality\naugmentation method for 3D anomaly synthesis, which is simple and capable of\nmimicking the characteristics of 3D defects. Incorporating with our anomaly\nsynthesis method, we introduce a reconstruction-based discriminative anomaly\ndetection network, in which a dual-modal discriminator is employed to fuse the\noriginal and reconstructed embedding of two modalities for anomaly detection.\nAdditionally, we design an augmentation dropout mechanism to enhance the\ngeneralizability of the discriminator. Extensive experiments show that our\nmethod outperforms the state-of-the-art methods on detection precision and\nachieves competitive segmentation performance on both MVTec 3D-AD and\nEyescandies datasets.",
          "arxiv_id": "2410.09821v2"
        },
        {
          "title": "FABLE : Fabric Anomaly Detection Automation Process",
          "year": "2023-06",
          "abstract": "Unsupervised anomaly in industry has been a concerning topic and a stepping\nstone for high performance industrial automation process. The vast majority of\nindustry-oriented methods focus on learning from good samples to detect anomaly\nnotwithstanding some specific industrial scenario requiring even less specific\ntraining and therefore a generalization for anomaly detection. The obvious use\ncase is the fabric anomaly detection, where we have to deal with a really wide\nrange of colors and types of textile and a stoppage of the production line for\ntraining could not be considered. In this paper, we propose an automation\nprocess for industrial fabric texture defect detection with a\nspecificity-learning process during the domain-generalized anomaly detection.\nCombining the ability to generalize and the learning process offer a fast and\nprecise anomaly detection and segmentation. The main contributions of this\npaper are the following: A domain-generalization texture anomaly detection\nmethod achieving the state-of-the-art performances, a fast specific training on\ngood samples extracted by the proposed method, a self-evaluation method based\non custom defect creation and an automatic detection of already seen fabric to\nprevent re-training.",
          "arxiv_id": "2306.10089v1"
        },
        {
          "title": "DRAEM -- A discriminatively trained reconstruction embedding for surface anomaly detection",
          "year": "2021-08",
          "abstract": "Visual surface anomaly detection aims to detect local image regions that\nsignificantly deviate from normal appearance. Recent surface anomaly detection\nmethods rely on generative models to accurately reconstruct the normal areas\nand to fail on anomalies. These methods are trained only on anomaly-free\nimages, and often require hand-crafted post-processing steps to localize the\nanomalies, which prohibits optimizing the feature extraction for maximal\ndetection capability. In addition to reconstructive approach, we cast surface\nanomaly detection primarily as a discriminative problem and propose a\ndiscriminatively trained reconstruction anomaly embedding model (DRAEM). The\nproposed method learns a joint representation of an anomalous image and its\nanomaly-free reconstruction, while simultaneously learning a decision boundary\nbetween normal and anomalous examples. The method enables direct anomaly\nlocalization without the need for additional complicated post-processing of the\nnetwork output and can be trained using simple and general anomaly simulations.\nOn the challenging MVTec anomaly detection dataset, DRAEM outperforms the\ncurrent state-of-the-art unsupervised methods by a large margin and even\ndelivers detection performance close to the fully-supervised methods on the\nwidely used DAGM surface-defect detection dataset, while substantially\noutperforming them in localization accuracy.",
          "arxiv_id": "2108.07610v2"
        }
      ],
      "18": [
        {
          "title": "OW-DETR: Open-world Detection Transformer",
          "year": "2021-12",
          "abstract": "Open-world object detection (OWOD) is a challenging computer vision problem,\nwhere the task is to detect a known set of object categories while\nsimultaneously identifying unknown objects. Additionally, the model must\nincrementally learn new classes that become known in the next training\nepisodes. Distinct from standard object detection, the OWOD setting poses\nsignificant challenges for generating quality candidate proposals on\npotentially unknown objects, separating the unknown objects from the background\nand detecting diverse unknown objects. Here, we introduce a novel end-to-end\ntransformer-based framework, OW-DETR, for open-world object detection. The\nproposed OW-DETR comprises three dedicated components namely, attention-driven\npseudo-labeling, novelty classification and objectness scoring to explicitly\naddress the aforementioned OWOD challenges. Our OW-DETR explicitly encodes\nmulti-scale contextual information, possesses less inductive bias, enables\nknowledge transfer from known classes to the unknown class and can better\ndiscriminate between unknown objects and background. Comprehensive experiments\nare performed on two benchmarks: MS-COCO and PASCAL VOC. The extensive\nablations reveal the merits of our proposed contributions. Further, our model\noutperforms the recently introduced OWOD approach, ORE, with absolute gains\nranging from 1.8% to 3.3% in terms of unknown recall on MS-COCO. In the case of\nincremental object detection, OW-DETR outperforms the state-of-the-art for all\nsettings on PASCAL VOC. Our code is available at\nhttps://github.com/akshitac8/OW-DETR.",
          "arxiv_id": "2112.01513v3"
        },
        {
          "title": "PROB: Probabilistic Objectness for Open World Object Detection",
          "year": "2022-12",
          "abstract": "Open World Object Detection (OWOD) is a new and challenging computer vision\ntask that bridges the gap between classic object detection (OD) benchmarks and\nobject detection in the real world. In addition to detecting and classifying\nseen/labeled objects, OWOD algorithms are expected to detect novel/unknown\nobjects - which can be classified and incrementally learned. In standard OD,\nobject proposals not overlapping with a labeled object are automatically\nclassified as background. Therefore, simply applying OD methods to OWOD fails\nas unknown objects would be predicted as background. The challenge of detecting\nunknown objects stems from the lack of supervision in distinguishing unknown\nobjects and background object proposals. Previous OWOD methods have attempted\nto overcome this issue by generating supervision using pseudo-labeling -\nhowever, unknown object detection has remained low. Probabilistic/generative\nmodels may provide a solution for this challenge. Herein, we introduce a novel\nprobabilistic framework for objectness estimation, where we alternate between\nprobability distribution estimation and objectness likelihood maximization of\nknown objects in the embedded feature space - ultimately allowing us to\nestimate the objectness probability of different proposals. The resulting\nProbabilistic Objectness transformer-based open-world detector, PROB,\nintegrates our framework into traditional object detection models, adapting\nthem for the open-world setting. Comprehensive experiments on OWOD benchmarks\nshow that PROB outperforms all existing OWOD methods in both unknown object\ndetection ($\\sim 2\\times$ unknown recall) and known object detection ($\\sim\n10\\%$ mAP). Our code will be made available upon publication at\nhttps://github.com/orrzohar/PROB.",
          "arxiv_id": "2212.01424v1"
        },
        {
          "title": "Generative Region-Language Pretraining for Open-Ended Object Detection",
          "year": "2024-03",
          "abstract": "In recent research, significant attention has been devoted to the\nopen-vocabulary object detection task, aiming to generalize beyond the limited\nnumber of classes labeled during training and detect objects described by\narbitrary category names at inference. Compared with conventional object\ndetection, open vocabulary object detection largely extends the object\ndetection categories. However, it relies on calculating the similarity between\nimage regions and a set of arbitrary category names with a pretrained\nvision-and-language model. This implies that, despite its open-set nature, the\ntask still needs the predefined object categories during the inference stage.\nThis raises the question: What if we do not have exact knowledge of object\ncategories during inference? In this paper, we call such a new setting as\ngenerative open-ended object detection, which is a more general and practical\nproblem. To address it, we formulate object detection as a generative problem\nand propose a simple framework named GenerateU, which can detect dense objects\nand generate their names in a free-form way. Particularly, we employ Deformable\nDETR as a region proposal generator with a language model translating visual\nregions to object names. To assess the free-form object detection task, we\nintroduce an evaluation method designed to quantitatively measure the\nperformance of generative outcomes. Extensive experiments demonstrate strong\nzero-shot detection performance of our GenerateU. For example, on the LVIS\ndataset, our GenerateU achieves comparable results to the open-vocabulary\nobject detection method GLIP, even though the category names are not seen by\nGenerateU during inference. Code is available at: https://\ngithub.com/FoundationVision/GenerateU .",
          "arxiv_id": "2403.10191v1"
        }
      ],
      "19": [
        {
          "title": "Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation",
          "year": "2022-03",
          "abstract": "Semantic segmentation with limited annotations, such as weakly supervised\nsemantic segmentation (WSSS) and semi-supervised semantic segmentation (SSSS),\nis a challenging task that has attracted much attention recently. Most leading\nWSSS methods employ a sophisticated multi-stage training strategy to estimate\npseudo-labels as precise as possible, but they suffer from high model\ncomplexity. In contrast, there exists another research line that trains a\nsingle network with image-level labels in one training cycle. However, such a\nsingle-stage strategy often performs poorly because of the compounding effect\ncaused by inaccurate pseudo-label estimation. To address this issue, this paper\npresents a Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and\nSSSS. The SLRNet uses cross-view self-supervision, that is, it simultaneously\npredicts several complementary attentive LR representations from different\nviews of an image to learn precise pseudo-labels. Specifically, we reformulate\nthe LR representation learning as a collective matrix factorization problem and\noptimize it jointly with the network learning in an end-to-end manner. The\nresulting LR representation deprecates noisy information while capturing stable\nsemantics across different views, making it robust to the input variations,\nthereby reducing overfitting to self-supervision errors. The SLRNet can provide\na unified single-stage framework for various label-efficient semantic\nsegmentation settings: 1) WSSS with image-level labeled data, 2) SSSS with a\nfew pixel-level labeled data, and 3) SSSS with a few pixel-level labeled data\nand many image-level labeled data. Extensive experiments on the Pascal VOC\n2012, COCO, and L2ID datasets demonstrate that our SLRNet outperforms both\nstate-of-the-art WSSS and SSSS methods with a variety of different settings,\nproving its good generalizability and efficacy.",
          "arxiv_id": "2203.10278v1"
        },
        {
          "title": "Unsupervised Universal Image Segmentation",
          "year": "2023-12",
          "abstract": "Several unsupervised image segmentation approaches have been proposed which\neliminate the need for dense manually-annotated segmentation masks; current\nmodels separately handle either semantic segmentation (e.g., STEGO) or\nclass-agnostic instance segmentation (e.g., CutLER), but not both (i.e.,\npanoptic segmentation). We propose an Unsupervised Universal Segmentation model\n(U2Seg) adept at performing various image segmentation tasks -- instance,\nsemantic and panoptic -- using a novel unified framework. U2Seg generates\npseudo semantic labels for these segmentation tasks via leveraging\nself-supervised models followed by clustering; each cluster represents\ndifferent semantic and/or instance membership of pixels. We then self-train the\nmodel on these pseudo semantic labels, yielding substantial performance gains\nover specialized methods tailored to each task: a +2.6 AP$^{\\text{box}}$ boost\nvs. CutLER in unsupervised instance segmentation on COCO and a +7.0 PixelAcc\nincrease (vs. STEGO) in unsupervised semantic segmentation on COCOStuff.\nMoreover, our method sets up a new baseline for unsupervised panoptic\nsegmentation, which has not been previously explored. U2Seg is also a strong\npretrained model for few-shot segmentation, surpassing CutLER by +5.0\nAP$^{\\text{mask}}$ when trained on a low-data regime, e.g., only 1% COCO\nlabels. We hope our simple yet effective method can inspire more research on\nunsupervised universal image segmentation.",
          "arxiv_id": "2312.17243v1"
        },
        {
          "title": "LarvSeg: Exploring Image Classification Data For Large Vocabulary Semantic Segmentation via Category-wise Attentive Classifier",
          "year": "2025-01",
          "abstract": "Scaling up the vocabulary of semantic segmentation models is extremely\nchallenging because annotating large-scale mask labels is labour-intensive and\ntime-consuming. Recently, language-guided segmentation models have been\nproposed to address this challenge. However, their performance drops\nsignificantly when applied to out-of-distribution categories. In this paper, we\npropose a new large vocabulary semantic segmentation framework, called LarvSeg.\nDifferent from previous works, LarvSeg leverages image classification data to\nscale the vocabulary of semantic segmentation models as large-vocabulary\nclassification datasets usually contain balanced categories and are much easier\nto obtain. However, for classification tasks, the category is image-level,\nwhile for segmentation we need to predict the label at pixel level. To address\nthis issue, we first propose a general baseline framework to incorporate\nimage-level supervision into the training process of a pixel-level segmentation\nmodel, making the trained network perform semantic segmentation on newly\nintroduced categories in the classification data. We then observe that a model\ntrained on segmentation data can group pixel features of categories beyond the\ntraining vocabulary. Inspired by this finding, we design a category-wise\nattentive classifier to apply supervision to the precise regions of\ncorresponding categories to improve the model performance. Extensive\nexperiments demonstrate that LarvSeg significantly improves the large\nvocabulary semantic segmentation performance, especially in the categories\nwithout mask labels. For the first time, we provide a 21K-category semantic\nsegmentation model with the help of ImageNet21K. The code is available at\nhttps://github.com/HaojunYu1998/large_voc_seg.",
          "arxiv_id": "2501.06862v1"
        }
      ],
      "20": [
        {
          "title": "SPGNet: Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional Space",
          "year": "2022-06",
          "abstract": "We propose a method SPGNet for 3D human pose estimation that mixes\nmulti-dimensional re-projection into supervised learning. In this method, the\n2D-to-3D-lifting network predicts the global position and coordinates of the 3D\nhuman pose. Then, we re-project the estimated 3D pose back to the 2D key points\nalong with spatial adjustments. The loss functions compare the estimated 3D\npose with the 3D pose ground truth, and re-projected 2D pose with the input 2D\npose. In addition, we propose a kinematic constraint to restrict the predicted\ntarget with constant human bone length. Based on the estimation results for the\ndataset Human3.6M, our approach outperforms many state-of-the-art methods both\nqualitatively and quantitatively.",
          "arxiv_id": "2206.01867v1"
        },
        {
          "title": "MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling",
          "year": "2023-06",
          "abstract": "Estimating 3D human poses only from a 2D human pose sequence is thoroughly\nexplored in recent years. Yet, prior to this, no such work has attempted to\nunify 2D and 3D pose representations in the shared feature space. In this\npaper, we propose \\mpm, a unified 2D-3D human pose representation framework via\nmasked pose modeling. We treat 2D and 3D poses as two different modalities like\nvision and language and build a single-stream transformer-based architecture.\nWe apply two pretext tasks, which are masked 2D pose modeling, and masked 3D\npose modeling to pre-train our network and use full-supervision to perform\nfurther fine-tuning. A high masking ratio of $71.8~\\%$ in total with a\nspatio-temporal mask sampling strategy leads to better relation modeling both\nin spatial and temporal domains. \\mpm~can handle multiple tasks including 3D\nhuman pose estimation, 3D pose estimation from occluded 2D pose, and 3D pose\ncompletion in a \\textbf{single} framework. We conduct extensive experiments and\nablation studies on several widely used human pose datasets and achieve\nstate-of-the-art performance on MPI-INF-3DHP.",
          "arxiv_id": "2306.17201v2"
        },
        {
          "title": "Lifting 2D Human Pose to 3D with Domain Adapted 3D Body Concept",
          "year": "2021-11",
          "abstract": "Lifting the 2D human pose to the 3D pose is an important yet challenging\ntask. Existing 3D pose estimation suffers from 1) the inherent ambiguity\nbetween the 2D and 3D data, and 2) the lack of well labeled 2D-3D pose pairs in\nthe wild. Human beings are able to imagine the human 3D pose from a 2D image or\na set of 2D body key-points with the least ambiguity, which should be\nattributed to the prior knowledge of the human body that we have acquired in\nour mind. Inspired by this, we propose a new framework that leverages the\nlabeled 3D human poses to learn a 3D concept of the human body to reduce the\nambiguity. To have consensus on the body concept from 2D pose, our key insight\nis to treat the 2D human pose and the 3D human pose as two different domains.\nBy adapting the two domains, the body knowledge learned from 3D poses is\napplied to 2D poses and guides the 2D pose encoder to generate informative 3D\n\"imagination\" as embedding in pose lifting. Benefiting from the domain\nadaptation perspective, the proposed framework unifies the supervised and\nsemi-supervised 3D pose estimation in a principled framework. Extensive\nexperiments demonstrate that the proposed approach can achieve state-of-the-art\nperformance on standard benchmarks. More importantly, it is validated that the\nexplicitly learned 3D body concept effectively alleviates the 2D-3D ambiguity\nin 2D pose lifting, improves the generalization, and enables the network to\nexploit the abundant unlabeled 2D data.",
          "arxiv_id": "2111.11969v1"
        }
      ],
      "21": [
        {
          "title": "Mechanisms of Generative Image-to-Image Translation Networks",
          "year": "2024-11",
          "abstract": "Generative Adversarial Networks (GANs) are a class of neural networks that\nhave been widely used in the field of image-to-image translation. In this\npaper, we propose a streamlined image-to-image translation network with a\nsimpler architecture compared to existing models. We investigate the\nrelationship between GANs and autoencoders and provide an explanation for the\nefficacy of employing only the GAN component for tasks involving image\ntranslation. We show that adversarial for GAN models yields results comparable\nto those of existing methods without additional complex loss penalties.\nSubsequently, we elucidate the rationale behind this phenomenon. We also\nincorporate experimental results to demonstrate the validity of our findings.",
          "arxiv_id": "2411.10368v1"
        },
        {
          "title": "Guiding GANs: How to control non-conditional pre-trained GANs for conditional image generation",
          "year": "2021-01",
          "abstract": "Generative Adversarial Networks (GANs) are an arrange of two neural networks\n-- the generator and the discriminator -- that are jointly trained to generate\nartificial data, such as images, from random inputs. The quality of these\ngenerated images has recently reached such levels that can often lead both\nmachines and humans into mistaking fake for real examples. However, the process\nperformed by the generator of the GAN has some limitations when we want to\ncondition the network to generate images from subcategories of a specific\nclass. Some recent approaches tackle this \\textit{conditional generation} by\nintroducing extra information prior to the training process, such as image\nsemantic segmentation or textual descriptions. While successful, these\ntechniques still require defining beforehand the desired subcategories and\ncollecting large labeled image datasets representing them to train the GAN from\nscratch. In this paper we present a novel and alternative method for guiding\ngeneric non-conditional GANs to behave as conditional GANs. Instead of\nre-training the GAN, our approach adds into the mix an encoder network to\ngenerate the high-dimensional random input vectors that are fed to the\ngenerator network of a non-conditional GAN to make it generate images from a\nspecific subcategory. In our experiments, when compared to training a\nconditional GAN from scratch, our guided GAN is able to generate artificial\nimages of perceived quality comparable to that of non-conditional GANs after\ntraining the encoder on just a few hundreds of images, which substantially\naccelerates the process and enables adding new subcategories seamlessly.",
          "arxiv_id": "2101.00990v1"
        },
        {
          "title": "Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey",
          "year": "2021-11",
          "abstract": "This is a tutorial and survey paper on Generative Adversarial Network (GAN),\nadversarial autoencoders, and their variants. We start with explaining\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\nand DCGAN. The mode collapse problem is introduced and various methods,\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\nintroduce some applications of GAN such as image-to-image translation\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\nGAN), text-to-image translation (including StackGAN), and mixing image\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\nautoencoders based on adversarial learning including adversarial autoencoder,\nPixelGAN, and implicit autoencoder.",
          "arxiv_id": "2111.13282v1"
        }
      ],
      "22": [
        {
          "title": "Domain Adaptation and Image Classification via Deep Conditional Adaptation Network",
          "year": "2020-06",
          "abstract": "Unsupervised domain adaptation aims to generalize the supervised model\ntrained on a source domain to an unlabeled target domain. Marginal distribution\nalignment of feature spaces is widely used to reduce the domain discrepancy\nbetween the source and target domains. However, it assumes that the source and\ntarget domains share the same label distribution, which limits their\napplication scope. In this paper, we consider a more general application\nscenario where the label distributions of the source and target domains are not\nthe same. In this scenario, marginal distribution alignment-based methods will\nbe vulnerable to negative transfer. To address this issue, we propose a novel\nunsupervised domain adaptation method, Deep Conditional Adaptation Network\n(DCAN), based on conditional distribution alignment of feature spaces. To be\nspecific, we reduce the domain discrepancy by minimizing the Conditional\nMaximum Mean Discrepancy between the conditional distributions of deep features\non the source and target domains, and extract the discriminant information from\ntarget domain by maximizing the mutual information between samples and the\nprediction labels. In addition, DCAN can be used to address a special scenario,\nPartial unsupervised domain adaptation, where the target domain category is a\nsubset of the source domain category. Experiments on both unsupervised domain\nadaptation and Partial unsupervised domain adaptation show that DCAN achieves\nsuperior classification performance over state-of-the-art methods.",
          "arxiv_id": "2006.07776v2"
        },
        {
          "title": "Domain-Augmented Domain Adaptation",
          "year": "2022-02",
          "abstract": "Unsupervised domain adaptation (UDA) enables knowledge transfer from the\nlabelled source domain to the unlabeled target domain by reducing the\ncross-domain discrepancy. However, most of the studies were based on direct\nadaptation from the source domain to the target domain and have suffered from\nlarge domain discrepancies. To overcome this challenge, in this paper, we\npropose the domain-augmented domain adaptation (DADA) to generate pseudo\ndomains that have smaller discrepancies with the target domain, to enhance the\nknowledge transfer process by minimizing the discrepancy between the target\ndomain and pseudo domains. Furthermore, we design a pseudo-labeling method for\nDADA by projecting representations from the target domain to multiple pseudo\ndomains and taking the averaged predictions on the classification from the\npseudo domains as the pseudo labels. We conduct extensive experiments with the\nstate-of-the-art domain adaptation methods on four benchmark datasets: Office\nHome, Office-31, VisDA2017, and Digital datasets. The results demonstrate the\nsuperiority of our model.",
          "arxiv_id": "2202.10000v1"
        },
        {
          "title": "Universal Multi-Source Domain Adaptation",
          "year": "2020-11",
          "abstract": "Unsupervised domain adaptation enables intelligent models to transfer\nknowledge from a labeled source domain to a similar but unlabeled target\ndomain. Recent study reveals that knowledge can be transferred from one source\ndomain to another unknown target domain, called Universal Domain Adaptation\n(UDA). However, in the real-world application, there are often more than one\nsource domain to be exploited for domain adaptation. In this paper, we formally\npropose a more general domain adaptation setting, universal multi-source domain\nadaptation (UMDA), where the label sets of multiple source domains can be\ndifferent and the label set of target domain is completely unknown. The main\nchallenges in UMDA are to identify the common label set between each source\ndomain and target domain, and to keep the model scalable as the number of\nsource domains increases. To address these challenges, we propose a universal\nmulti-source adaptation network (UMAN) to solve the domain adaptation problem\nwithout increasing the complexity of the model in various UMDA settings. In\nUMAN, we estimate the reliability of each known class in the common label set\nvia the prediction margin, which helps adversarial training to better align the\ndistributions of multiple source domains and target domain in the common label\nset. Moreover, the theoretical guarantee for UMAN is also provided. Massive\nexperimental results show that existing UDA and multi-source DA (MDA) methods\ncannot be directly applied to UMDA and the proposed UMAN achieves the\nstate-of-the-art performance in various UMDA settings.",
          "arxiv_id": "2011.02594v1"
        }
      ],
      "23": [
        {
          "title": "An EEG-Based Multi-Modal Emotion Database with Both Posed and Authentic Facial Actions for Emotion Analysis",
          "year": "2022-03",
          "abstract": "Emotion is an experience associated with a particular pattern of\nphysiological activity along with different physiological, behavioral and\ncognitive changes. One behavioral change is facial expression, which has been\nstudied extensively over the past few decades. Facial behavior varies with a\nperson's emotion according to differences in terms of culture, personality,\nage, context, and environment. In recent years, physiological activities have\nbeen used to study emotional responses. A typical signal is the\nelectroencephalogram (EEG), which measures brain activity. Most of existing\nEEG-based emotion analysis has overlooked the role of facial expression\nchanges. There exits little research on the relationship between facial\nbehavior and brain signals due to the lack of dataset measuring both EEG and\nfacial action signals simultaneously. To address this problem, we propose to\ndevelop a new database by collecting facial expressions, action units, and EEGs\nsimultaneously. We recorded the EEGs and face videos of both posed facial\nactions and spontaneous expressions from 29 participants with different ages,\ngenders, ethnic backgrounds. Differing from existing approaches, we designed a\nprotocol to capture the EEG signals by evoking participants' individual action\nunits explicitly. We also investigated the relation between the EEG signals and\nfacial action units. As a baseline, the database has been evaluated through the\nexperiments on both posed and spontaneous emotion recognition with images\nalone, EEG alone, and EEG fused with images, respectively. The database will be\nreleased to the research community to advance the state of the art for\nautomatic emotion recognition.",
          "arxiv_id": "2203.15829v1"
        },
        {
          "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition",
          "year": "2025-07",
          "abstract": "Facial expression recognition is crucial for human-computer interaction\napplications such as face animation, video surveillance, affective computing,\nmedical analysis, etc. Since the structure of facial attributes varies with\nfacial expressions, incorporating structural information into facial attributes\nis essential for facial expression recognition. In this paper, we propose\nExp-Graph, a novel framework designed to represent the structural relationships\namong facial attributes using graph-based modeling for facial expression\nrecognition. For facial attributes graph representation, facial landmarks are\nused as the graph's vertices. At the same time, the edges are determined based\non the proximity of the facial landmark and the similarity of the local\nappearance of the facial attributes encoded using the vision transformer.\nAdditionally, graph convolutional networks are utilized to capture and\nintegrate these structural dependencies into the encoding of facial attributes,\nthereby enhancing the accuracy of expression recognition. Thus, Exp-Graph\nlearns from the facial attribute graphs highly expressive semantic\nrepresentations. On the other hand, the vision transformer and graph\nconvolutional blocks help the framework exploit the local and global\ndependencies among the facial attributes that are essential for the recognition\nof facial expressions. We conducted comprehensive evaluations of the proposed\nExp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.\nThe model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%,\nrespectively. These results indicate that Exp-Graph maintains strong\ngeneralization capabilities across both controlled laboratory settings and\nreal-world, unconstrained environments, underscoring its effectiveness for\npractical facial expression recognition applications.",
          "arxiv_id": "2507.14608v1"
        },
        {
          "title": "AffectNet+: A Database for Enhancing Facial Expression Recognition with Soft-Labels",
          "year": "2024-10",
          "abstract": "Automated Facial Expression Recognition (FER) is challenging due to\nintra-class variations and inter-class similarities. FER can be especially\ndifficult when facial expressions reflect a mixture of various emotions (aka\ncompound expressions). Existing FER datasets, such as AffectNet, provide\ndiscrete emotion labels (hard-labels), where a single category of emotion is\nassigned to an expression. To alleviate inter- and intra-class challenges, as\nwell as provide a better facial expression descriptor, we propose a new\napproach to create FER datasets through a labeling method in which an image is\nlabeled with more than one emotion (called soft-labels), each with different\nconfidences. Specifically, we introduce the notion of soft-labels for facial\nexpression datasets, a new approach to affective computing for more realistic\nrecognition of facial expressions. To achieve this goal, we propose a novel\nmethodology to accurately calculate soft-labels: a vector representing the\nextent to which multiple categories of emotion are simultaneously present\nwithin a single facial expression. Finding smoother decision boundaries,\nenabling multi-labeling, and mitigating bias and imbalanced data are some of\nthe advantages of our proposed method. Building upon AffectNet, we introduce\nAffectNet+, the next-generation facial expression dataset. This dataset\ncontains soft-labels, three categories of data complexity subsets, and\nadditional metadata such as age, gender, ethnicity, head pose, facial\nlandmarks, valence, and arousal. AffectNet+ will be made publicly accessible to\nresearchers.",
          "arxiv_id": "2410.22506v1"
        }
      ],
      "24": [
        {
          "title": "Real-time Multi-Object Tracking Based on Bi-directional Matching",
          "year": "2023-03",
          "abstract": "In recent years, anchor-free object detection models combined with matching\nalgorithms are used to achieve real-time muti-object tracking and also ensure\nhigh tracking accuracy. However, there are still great challenges in\nmulti-object tracking. For example, when most part of a target is occluded or\nthe target just disappears from images temporarily, it often leads to tracking\ninterruptions for most of the existing tracking algorithms. Therefore, this\nstudy offers a bi-directional matching algorithm for multi-object tracking that\nmakes advantage of bi-directional motion prediction information to improve\nocclusion handling. A stranded area is used in the matching algorithm to\ntemporarily store the objects that fail to be tracked. When objects recover\nfrom occlusions, our method will first try to match them with objects in the\nstranded area to avoid erroneously generating new identities, thus forming a\nmore continuous trajectory. Experiments show that our approach can improve the\nmulti-object tracking performance in the presence of occlusions. In addition,\nthis study provides an attentional up-sampling module that not only assures\ntracking accuracy but also accelerates training speed. In the MOT17 challenge,\nthe proposed algorithm achieves 63.4% MOTA, 55.3% IDF1, and 20.1 FPS tracking\nspeed.",
          "arxiv_id": "2303.08444v1"
        },
        {
          "title": "VOVTrack: Exploring the Potentiality in Videos for Open-Vocabulary Object Tracking",
          "year": "2024-10",
          "abstract": "Open-vocabulary multi-object tracking (OVMOT) represents a critical new\nchallenge involving the detection and tracking of diverse object categories in\nvideos, encompassing both seen categories (base classes) and unseen categories\n(novel classes). This issue amalgamates the complexities of open-vocabulary\nobject detection (OVD) and multi-object tracking (MOT). Existing approaches to\nOVMOT often merge OVD and MOT methodologies as separate modules, predominantly\nfocusing on the problem through an image-centric lens. In this paper, we\npropose VOVTrack, a novel method that integrates object states relevant to MOT\nand video-centric training to address this challenge from a video object\ntracking standpoint. First, we consider the tracking-related state of the\nobjects during tracking and propose a new prompt-guided attention mechanism for\nmore accurate localization and classification (detection) of the time-varying\nobjects. Subsequently, we leverage raw video data without annotations for\ntraining by formulating a self-supervised object similarity learning technique\nto facilitate temporal object association (tracking). Experimental results\nunderscore that VOVTrack outperforms existing methods, establishing itself as a\nstate-of-the-art solution for open-vocabulary tracking task.",
          "arxiv_id": "2410.08529v1"
        },
        {
          "title": "You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object Tracking",
          "year": "2023-04",
          "abstract": "In the classical tracking-by-detection (TBD) paradigm, detection and tracking\nare separately and sequentially conducted, and data association must be\nproperly performed to achieve satisfactory tracking performance. In this paper,\na new end-to-end multi-object tracking framework is proposed, which integrates\nobject detection and multi-object tracking into a single model. The proposed\ntracking framework eliminates the complex data association process in the\nclassical TBD paradigm, and requires no additional training. Secondly, the\nregression confidence of historical trajectories is investigated, and the\npossible states of a trajectory (weak object or strong object) in the current\nframe are predicted. Then, a confidence fusion module is designed to guide\nnon-maximum suppression for trajectories and detections to achieve ordered and\nrobust tracking. Thirdly, by integrating historical trajectory features, the\nregression performance of the detector is enhanced, which better reflects the\nocclusion and disappearance patterns of objects in real world. Lastly,\nextensive experiments are conducted on the commonly used KITTI and Waymo\ndatasets. The results show that the proposed framework can achieve robust\ntracking by using only a 2D detector and a 3D detector, and it is proven more\naccurate than many of the state-of-the-art TBD-based multi-modal tracking\nmethods. The source codes of the proposed method are available at\nhttps://github.com/wangxiyang2022/YONTD-MOT.",
          "arxiv_id": "2304.08709v2"
        }
      ],
      "25": [
        {
          "title": "Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging",
          "year": "2025-07",
          "abstract": "Magnetic resonance imaging (MRI) is a crucial medical imaging modality.\nHowever, long acquisition times remain a significant challenge, leading to\nincreased costs, and reduced patient comfort. Recent studies have shown the\npotential of using deep learning models that incorporate information from prior\nsubject-specific MRI scans to improve reconstruction quality of present scans.\nIntegrating this prior information requires registration of the previous scan\nto the current image reconstruction, which can be time-consuming. We propose a\nnovel deep-learning-based MRI reconstruction framework which consists of an\ninitial reconstruction network, a deep registration model, and a\ntransformer-based enhancement network. We validated our method on a\nlongitudinal dataset of T1-weighted MRI scans with 2,808 images from 18\nsubjects at four acceleration factors (R5, R10, R15, R20). Quantitative metrics\nconfirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon\nsigned-rank test). Furthermore, we analyzed the impact of our MRI\nreconstruction method on the downstream task of brain segmentation and observed\nimproved accuracy and volumetric agreement with reference segmentations. Our\napproach also achieved a substantial reduction in total reconstruction time\ncompared to methods that use traditional registration algorithms, making it\nmore suitable for real-time clinical applications. The code associated with\nthis work is publicly available at\nhttps://github.com/amirshamaei/longitudinal-mri-deep-recon.",
          "arxiv_id": "2507.21349v1"
        },
        {
          "title": "Advancing MRI Reconstruction: A Systematic Review of Deep Learning and Compressed Sensing Integration",
          "year": "2025-01",
          "abstract": "Magnetic resonance imaging (MRI) is a non-invasive imaging modality and\nprovides comprehensive anatomical and functional insights into the human body.\nHowever, its long acquisition times can lead to patient discomfort, motion\nartifacts, and limiting real-time applications. To address these challenges,\nstrategies such as parallel imaging have been applied, which utilize multiple\nreceiver coils to speed up the data acquisition process. Additionally,\ncompressed sensing (CS) is a method that facilitates image reconstruction from\nsparse data, significantly reducing image acquisition time by minimizing the\namount of data collection needed. Recently, deep learning (DL) has emerged as a\npowerful tool for improving MRI reconstruction. It has been integrated with\nparallel imaging and CS principles to achieve faster and more accurate MRI\nreconstructions. This review comprehensively examines DL-based techniques for\nMRI reconstruction. We categorize and discuss various DL-based methods,\nincluding end-to-end approaches, unrolled optimization, and federated learning,\nhighlighting their potential benefits. Our systematic review highlights\nsignificant contributions and underscores the potential of DL in MRI\nreconstruction. Additionally, we summarize key results and trends in DL-based\nMRI reconstruction, including quantitative metrics, the dataset, acceleration\nfactors, and the progress of and research interest in DL techniques over time.\nFinally, we discuss potential future directions and the importance of DL-based\nMRI reconstruction in advancing medical imaging. To facilitate further research\nin this area, we provide a GitHub repository that includes up-to-date DL-based\nMRI reconstruction publications and public\ndatasets-https://github.com/mosaf/Awesome-DL-based-CS-MRI.",
          "arxiv_id": "2501.14158v2"
        },
        {
          "title": "Relaxometry Guided Quantitative Cardiac Magnetic Resonance Image Reconstruction",
          "year": "2024-03",
          "abstract": "Deep learning-based methods have achieved prestigious performance for\nmagnetic resonance imaging (MRI) reconstruction, enabling fast imaging for many\nclinical applications. Previous methods employ convolutional networks to learn\nthe image prior as the regularization term. In quantitative MRI, the physical\nmodel of nuclear magnetic resonance relaxometry is known, providing additional\nprior knowledge for image reconstruction. However, traditional reconstruction\nnetworks are limited to learning the spatial domain prior knowledge, ignoring\nthe relaxometry prior. Therefore, we propose a relaxometry-guided quantitative\nMRI reconstruction framework to learn the spatial prior from data and the\nrelaxometry prior from MRI physics. Additionally, we also evaluated the\nperformance of two popular reconstruction backbones, namely, recurrent\nvariational networks (RVN) and variational networks (VN) with U- Net.\nExperiments demonstrate that the proposed method achieves highly promising\nresults in quantitative MRI reconstruction.",
          "arxiv_id": "2403.00549v3"
        }
      ],
      "26": [
        {
          "title": "Multicam-SLAM: Non-overlapping Multi-camera SLAM for Indirect Visual Localization and Navigation",
          "year": "2024-06",
          "abstract": "This paper presents a novel approach to visual simultaneous localization and\nmapping (SLAM) using multiple RGB-D cameras. The proposed method,\nMulticam-SLAM, significantly enhances the robustness and accuracy of SLAM\nsystems by capturing more comprehensive spatial information from various\nperspectives. This method enables the accurate determination of pose\nrelationships among multiple cameras without the need for overlapping fields of\nview. The proposed Muticam-SLAM includes a unique multi-camera model, a\nmulti-keyframes structure, and several parallel SLAM threads. The multi-camera\nmodel allows for the integration of data from multiple cameras, while the\nmulti-keyframes and parallel SLAM threads ensure efficient and accurate pose\nestimation and mapping. Extensive experiments in various environments\ndemonstrate the superior accuracy and robustness of the proposed method\ncompared to conventional single-camera SLAM systems. The results highlight the\npotential of the proposed Multicam-SLAM for more complex and challenging\napplications. Code is available at\n\\url{https://github.com/AlterPang/Multi_ORB_SLAM}.",
          "arxiv_id": "2406.06374v2"
        },
        {
          "title": "3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments",
          "year": "2023-10",
          "abstract": "The existence of variable factors within the environment can cause a decline\nin camera localization accuracy, as it violates the fundamental assumption of a\nstatic environment in Simultaneous Localization and Mapping (SLAM) algorithms.\nRecent semantic SLAM systems towards dynamic environments either rely solely on\n2D semantic information, or solely on geometric information, or combine their\nresults in a loosely integrated manner. In this research paper, we introduce\n3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object\ndetection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic\nand geometric constraints sequentially. We designed a 3D part-aware hybrid\ntransformer for point cloud-based object detection to identify dynamic objects.\nSubsequently, we propose a dynamic feature filter based on HDBSCAN clustering\nto extract objects with significant absolute depth differences. When compared\nagainst ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across\nthe dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the\nperformance of the other four leading SLAM systems designed for dynamic\nenvironments.",
          "arxiv_id": "2310.06385v1"
        },
        {
          "title": "GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM",
          "year": "2024-03",
          "abstract": "Recent advancements in RGB-only dense Simultaneous Localization and Mapping\n(SLAM) have predominantly utilized grid-based neural implicit encodings and/or\nstruggle to efficiently realize global map and pose consistency. To this end,\nwe propose an efficient RGB-only dense SLAM system using a flexible neural\npoint cloud scene representation that adapts to keyframe poses and depth\nupdates, without needing costly backpropagation. Another critical challenge of\nRGB-only SLAM is the lack of geometric priors. To alleviate this issue, with\nthe aid of a monocular depth estimator, we introduce a novel DSPO layer for\nbundle adjustment which optimizes the pose and depth of keyframes along with\nthe scale of the monocular depth. Finally, our system benefits from loop\nclosure and online global bundle adjustment and performs either better or\ncompetitive to existing dense neural RGB SLAM methods in tracking, mapping and\nrendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source\ncode is available at https://github.com/zhangganlin/GlOIRE-SLAM",
          "arxiv_id": "2403.19549v3"
        }
      ],
      "27": [
        {
          "title": "VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models",
          "year": "2024-11",
          "abstract": "Video generation has witnessed significant advancements, yet evaluating these\nmodels remains a challenge. A comprehensive evaluation benchmark for video\ngeneration is indispensable for two reasons: 1) Existing metrics do not fully\nalign with human perceptions; 2) An ideal evaluation system should provide\ninsights to inform future developments of video generation. To this end, we\npresent VBench, a comprehensive benchmark suite that dissects \"video generation\nquality\" into specific, hierarchical, and disentangled dimensions, each with\ntailored prompts and evaluation methods. VBench has several appealing\nproperties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in\nvideo generation (e.g., subject identity inconsistency, motion smoothness,\ntemporal flickering, and spatial relationship, etc). The evaluation metrics\nwith fine-grained levels reveal individual models' strengths and weaknesses. 2)\nHuman Alignment: We also provide a dataset of human preference annotations to\nvalidate our benchmarks' alignment with human perception, for each evaluation\ndimension respectively. 3) Valuable Insights: We look into current models'\nability across various evaluation dimensions, and various content types. We\nalso investigate the gaps between video and image generation models. 4)\nVersatile Benchmarking: VBench++ supports evaluating text-to-video and\nimage-to-video. We introduce a high-quality Image Suite with an adaptive aspect\nratio to enable fair evaluations across different image-to-video generation\nsettings. Beyond assessing technical quality, VBench++ evaluates the\ntrustworthiness of video generative models, providing a more holistic view of\nmodel performance. 5) Full Open-Sourcing: We fully open-source VBench++ and\ncontinually add new video generation models to our leaderboard to drive forward\nthe field of video generation.",
          "arxiv_id": "2411.13503v1"
        },
        {
          "title": "VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation",
          "year": "2023-09",
          "abstract": "In this paper, we present VideoGen, a text-to-video generation approach,\nwhich can generate a high-definition video with high frame fidelity and strong\ntemporal consistency using reference-guided latent diffusion. We leverage an\noff-the-shelf text-to-image generation model, e.g., Stable Diffusion, to\ngenerate an image with high content quality from the text prompt, as a\nreference image to guide video generation. Then, we introduce an efficient\ncascaded latent diffusion module conditioned on both the reference image and\nthe text prompt, for generating latent video representations, followed by a\nflow-based temporal upsampling step to improve the temporal resolution.\nFinally, we map latent video representations into a high-definition video\nthrough an enhanced video decoder. During training, we use the first frame of a\nground-truth video as the reference image for training the cascaded latent\ndiffusion module. The main characterises of our approach include: the reference\nimage generated by the text-to-image model improves the visual fidelity; using\nit as the condition makes the diffusion model focus more on learning the video\ndynamics; and the video decoder is trained over unlabeled video data, thus\nbenefiting from high-quality easily-available videos. VideoGen sets a new\nstate-of-the-art in text-to-video generation in terms of both qualitative and\nquantitative evaluation. See \\url{https://videogen.github.io/VideoGen/} for\nmore samples.",
          "arxiv_id": "2309.00398v2"
        },
        {
          "title": "Control-A-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning",
          "year": "2023-05",
          "abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive image generation capabilities guided by text prompts. However,\nextending these techniques to video generation remains challenging, with\nexisting text-to-video (T2V) methods often struggling to produce high-quality\nand motion-consistent videos. In this work, we introduce Control-A-Video, a\ncontrollable T2V diffusion model that can generate videos conditioned on text\nprompts and reference control maps like edge and depth maps. To tackle video\nquality and motion consistency issues, we propose novel strategies to\nincorporate content prior and motion prior into the diffusion-based generation\nprocess. Specifically, we employ a first-frame condition scheme to transfer\nvideo generation from the image domain. Additionally, we introduce\nresidual-based and optical flow-based noise initialization to infuse motion\npriors from reference videos, promoting relevance among frame latents for\nreduced flickering. Furthermore, we present a Spatio-Temporal Reward Feedback\nLearning (ST-ReFL) algorithm that optimizes the video diffusion model using\nmultiple reward models for video quality and motion consistency, leading to\nsuperior outputs. Comprehensive experiments demonstrate that our framework\ngenerates higher-quality, more consistent videos compared to existing\nstate-of-the-art methods in controllable text-to-video generation",
          "arxiv_id": "2305.13840v3"
        }
      ],
      "28": [
        {
          "title": "Deep Learning for Reliable Classification of COVID-19, MERS, and SARS from Chest X-Ray Images",
          "year": "2020-05",
          "abstract": "Novel Coronavirus disease (COVID-19) is an extremely contagious and quickly\nspreading Coronavirus infestation. Severe Acute Respiratory Syndrome (SARS) and\nMiddle East Respiratory Syndrome (MERS), which outbreak in 2002 and 2011, and\nthe current COVID-19 pandemic are all from the same family of coronavirus. This\nwork aims to classify COVID-19, SARS, and MERS chest X-ray (CXR) images using\ndeep Convolutional Neural Networks (CNNs). A unique database was created,\nso-called QU-COVID-family, consisting of 423 COVID-19, 144 MERS, and 134 SARS\nCXR images. Besides, a robust COVID-19 recognition system was proposed to\nidentify lung regions using a CNN segmentation model (U-Net), and then classify\nthe segmented lung images as COVID-19, MERS, or SARS using a pre-trained CNN\nclassifier. Furthermore, the Score-CAM visualization method was utilized to\nvisualize classification output and understand the reasoning behind the\ndecision of deep CNNs. Several Deep Learning classifiers were trained and\ntested; four outperforming algorithms were reported. Original and preprocessed\nimages were used individually and all together as the input(s) to the networks.\nTwo recognition schemes were considered: plain CXR classification and segmented\nCXR classification. For plain CXRs, it was observed that InceptionV3\noutperforms other networks with a 3-channel scheme and achieves sensitivities\nof 99.5%, 93.1%, and 97% for classifying COVID-19, MERS, and SARS images,\nrespectively. In contrast, for segmented CXRs, InceptionV3 outperformed using\nthe original CXR dataset and achieved sensitivities of 96.94%, 79.68%, and\n90.26% for classifying COVID-19, MERS, and SARS images, respectively. All\nnetworks showed high COVID-19 detection sensitivity (>96%) with the segmented\nlung images. This indicates the unique radiographic signature of COVID-19 cases\nin the eyes of AI, which is often a challenging task for medical doctors.",
          "arxiv_id": "2005.11524v6"
        },
        {
          "title": "3D Tomographic Pattern Synthesis for Enhancing the Quantification of COVID-19",
          "year": "2020-05",
          "abstract": "The Coronavirus Disease (COVID-19) has affected 1.8 million people and\nresulted in more than 110,000 deaths as of April 12, 2020. Several studies have\nshown that tomographic patterns seen on chest Computed Tomography (CT), such as\nground-glass opacities, consolidations, and crazy paving pattern, are\ncorrelated with the disease severity and progression. CT imaging can thus\nemerge as an important modality for the management of COVID-19 patients.\nAI-based solutions can be used to support CT based quantitative reporting and\nmake reading efficient and reproducible if quantitative biomarkers, such as the\nPercentage of Opacity (PO), can be automatically computed. However, COVID-19\nhas posed unique challenges to the development of AI, specifically concerning\nthe availability of appropriate image data and annotations at scale. In this\npaper, we propose to use synthetic datasets to augment an existing COVID-19\ndatabase to tackle these challenges. We train a Generative Adversarial Network\n(GAN) to inpaint COVID-19 related tomographic patterns on chest CTs from\npatients without infectious diseases. Additionally, we leverage location priors\nderived from manually labeled COVID-19 chest CTs patients to generate\nappropriate abnormality distributions. Synthetic data are used to improve both\nlung segmentation and segmentation of COVID-19 patterns by adding 20% of\nsynthetic data to the real COVID-19 training data. We collected 2143 chest CTs,\ncontaining 327 COVID-19 positive cases, acquired from 12 sites across 7\ncountries. By testing on 100 COVID-19 positive and 100 control cases, we show\nthat synthetic data can help improve both lung segmentation (+6.02% lesion\ninclusion rate) and abnormality segmentation (+2.78% dice coefficient), leading\nto an overall more accurate PO computation (+2.82% Pearson coefficient).",
          "arxiv_id": "2005.01903v1"
        },
        {
          "title": "Auto-Lesion Segmentation with a Novel Intensity Dark Channel Prior for COVID-19 Detection",
          "year": "2023-09",
          "abstract": "During the COVID-19 pandemic, medical imaging techniques like computed\ntomography (CT) scans have demonstrated effectiveness in combating the rapid\nspread of the virus. Therefore, it is crucial to conduct research on\ncomputerized models for the detection of COVID-19 using CT imaging. A novel\nprocessing method has been developed, utilizing radiomic features, to assist in\nthe CT-based diagnosis of COVID-19. Given the lower specificity of traditional\nfeatures in distinguishing between different causes of pulmonary diseases, the\nobjective of this study is to develop a CT-based radiomics framework for the\ndifferentiation of COVID-19 from other lung diseases. The model is designed to\nfocus on outlining COVID-19 lesions, as traditional features often lack\nspecificity in this aspect. The model categorizes images into three classes:\nCOVID-19, non-COVID-19, or normal. It employs enhancement auto-segmentation\nprinciples using intensity dark channel prior (IDCP) and deep neural networks\n(ALS-IDCP-DNN) within a defined range of analysis thresholds. A publicly\navailable dataset comprising COVID-19, normal, and non-COVID-19 classes was\nutilized to validate the proposed model's effectiveness. The best performing\nclassification model, Residual Neural Network with 50 layers (Resnet-50),\nattained an average accuracy, precision, recall, and F1-score of 98.8%, 99%,\n98%, and 98% respectively. These results demonstrate the capability of our\nmodel to accurately classify COVID-19 images, which could aid radiologists in\ndiagnosing suspected COVID-19 patients. Furthermore, our model's performance\nsurpasses that of more than 10 current state-of-the-art studies conducted on\nthe same dataset.",
          "arxiv_id": "2309.12638v2"
        }
      ],
      "29": [
        {
          "title": "Deep Learning-based Occluded Person Re-identification: A Survey",
          "year": "2022-07",
          "abstract": "Occluded person re-identification (Re-ID) aims at addressing the occlusion\nproblem when retrieving the person of interest across multiple cameras. With\nthe promotion of deep learning technology and the increasing demand for\nintelligent video surveillance, the frequent occlusion in real-world\napplications has made occluded person Re-ID draw considerable interest from\nresearchers. A large number of occluded person Re-ID methods have been proposed\nwhile there are few surveys that focus on occlusion. To fill this gap and help\nboost future research, this paper provides a systematic survey of occluded\nperson Re-ID. Through an in-depth analysis of the occlusion in person Re-ID,\nmost existing methods are found to only consider part of the problems brought\nby occlusion. Therefore, we review occlusion-related person Re-ID methods from\nthe perspective of issues and solutions. We summarize four issues caused by\nocclusion in person Re-ID, i.e., position misalignment, scale misalignment,\nnoisy information, and missing information. The occlusion-related methods\naddressing different issues are then categorized and introduced accordingly.\nAfter that, we summarize and compare the performance of recent occluded person\nRe-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS,\nOccluded-ReID, and Occluded-DukeMTMC. Finally, we provide insights on promising\nfuture research directions.",
          "arxiv_id": "2207.14452v1"
        },
        {
          "title": "Unsupervised Person Re-Identification: A Systematic Survey of Challenges and Solutions",
          "year": "2021-09",
          "abstract": "Person re-identification (Re-ID) has been a significant research topic in the\npast decade due to its real-world applications and research significance. While\nsupervised person Re-ID methods achieve superior performance over unsupervised\ncounterparts, they can not scale to large unlabelled datasets and new domains\ndue to the prohibitive labelling cost. Therefore, unsupervised person Re-ID has\ndrawn increasing attention for its potential to address the scalability issue\nin person Re-ID. Unsupervised person Re-ID is challenging primarily due to\nlacking identity labels to supervise person feature learning. The corresponding\nsolutions are diverse and complex, with various merits and limitations.\nTherefore, comprehensive surveys on this topic are essential to summarise\nchallenges and solutions to foster future research. Existing person Re-ID\nsurveys have focused on supervised methods from classifications and\napplications but lack detailed discussion on how the person Re-ID solutions\naddress the underlying challenges. This survey review recent works on\nunsupervised person Re-ID from the perspective of challenges and solutions.\nSpecifically, we provide an in-depth analysis of highly influential methods\nconsidering the four significant challenges in unsupervised person Re-ID: 1)\nlacking ground-truth identity labels to supervise person feature learning; 2)\nlearning discriminative person features with pseudo-supervision; 3) learning\ncross-camera invariant person feature, and 4) the domain shift between\ndatasets. We summarise and analyse evaluation results and provide insights on\nthe effectiveness of the solutions. Finally, we discuss open issues and suggest\nsome promising future research directions.",
          "arxiv_id": "2109.06057v2"
        },
        {
          "title": "Making Person Search Enjoy the Merits of Person Re-identification",
          "year": "2021-08",
          "abstract": "Person search is an extended task of person re-identification (Re-ID).\nHowever, most existing one-step person search works have not studied how to\nemploy existing advanced Re-ID models to boost the one-step person search\nperformance due to the integration of person detection and Re-ID. To address\nthis issue, we propose a faster and stronger one-step person search framework,\nthe Teacher-guided Disentangling Networks (TDN), to make the one-step person\nsearch enjoy the merits of the existing Re-ID researches. The proposed TDN can\nsignificantly boost the person search performance by transferring the advanced\nperson Re-ID knowledge to the person search model. In the proposed TDN, for\nbetter knowledge transfer from the Re-ID teacher model to the one-step person\nsearch model, we design a strong one-step person search base framework by\npartially disentangling the two subtasks. Besides, we propose a Knowledge\nTransfer Bridge module to bridge the scale gap caused by different input\nformats between the Re-ID model and one-step person search model. During\ntesting, we further propose the Ranking with Context Persons strategy to\nexploit the context information in panoramic images for better retrieval.\nExperiments on two public person search datasets demonstrate the favorable\nperformance of the proposed method.",
          "arxiv_id": "2108.10536v2"
        }
      ],
      "30": [
        {
          "title": "Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation",
          "year": "2023-07",
          "abstract": "Text-to-3D generation has recently garnered significant attention, fueled by\n2D diffusion models trained on billions of image-text pairs. Existing methods\nprimarily rely on score distillation to leverage the 2D diffusion priors to\nsupervise the generation of 3D models, e.g., NeRF. However, score distillation\nis prone to suffer the view inconsistency problem, and implicit NeRF modeling\ncan also lead to an arbitrary shape, thus leading to less realistic and\nuncontrollable 3D generation. In this work, we propose a flexible framework of\nPoints-to-3D to bridge the gap between sparse yet freely available 3D points\nand realistic shape-controllable 3D generation by distilling the knowledge from\nboth 2D and 3D diffusion models. The core idea of Points-to-3D is to introduce\ncontrollable sparse 3D points to guide the text-to-3D generation. Specifically,\nwe use the sparse point cloud generated from the 3D diffusion model, Point-E,\nas the geometric prior, conditioned on a single reference image. To better\nutilize the sparse 3D points, we propose an efficient point cloud guidance loss\nto adaptively drive the NeRF's geometry to align with the shape of the sparse\n3D points. In addition to controlling the geometry, we propose to optimize the\nNeRF for a more view-consistent appearance. To be specific, we perform score\ndistillation to the publicly available 2D image diffusion model ControlNet,\nconditioned on text as well as depth map of the learned compact geometry.\nQualitative and quantitative comparisons demonstrate that Points-to-3D improves\nview consistency and achieves good shape controllability for text-to-3D\ngeneration. Points-to-3D provides users with a new way to improve and control\ntext-to-3D generation.",
          "arxiv_id": "2307.13908v1"
        },
        {
          "title": "Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation",
          "year": "2024-12",
          "abstract": "In this work, we introduce Prometheus, a 3D-aware latent diffusion model for\ntext-to-3D generation at both object and scene levels in seconds. We formulate\n3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian\ngeneration within the latent diffusion paradigm. To ensure generalizability, we\nbuild our model upon pre-trained text-to-image generation model with only\nminimal adjustments, and further train it using a large number of images from\nboth single-view and multi-view datasets. Furthermore, we introduce an RGB-D\nlatent space into 3D Gaussian generation to disentangle appearance and geometry\ninformation, enabling efficient feed-forward generation of 3D Gaussians with\nbetter fidelity and geometry. Extensive experimental results demonstrate the\neffectiveness of our method in both feed-forward 3D Gaussian reconstruction and\ntext-to-3D generation. Project page:\nhttps://freemty.github.io/project-prometheus/",
          "arxiv_id": "2412.21117v2"
        },
        {
          "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
          "year": "2025-03",
          "abstract": "Diffusion models have achieved great success in generating 2D images.\nHowever, the quality and generalizability of 3D content generation remain\nlimited. State-of-the-art methods often require large-scale 3D assets for\ntraining, which are challenging to collect. In this work, we introduce\nKiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient\nframework for generating, editing, and enhancing 3D objects by repurposing a\nwell-trained 2D image diffusion model for 3D generation. Specifically, we\nfine-tune a diffusion model to generate ''3D Bundle Image'', a tiled\nrepresentation composed of multi-view images and their corresponding normal\nmaps. The normal maps are then used to reconstruct a 3D mesh, and the\nmulti-view images provide texture mapping, resulting in a complete 3D model.\nThis simple method effectively transforms the 3D generation problem into a 2D\nimage generation task, maximizing the utilization of knowledge in pretrained\ndiffusion models. Furthermore, we demonstrate that our Kiss3DGen model is\ncompatible with various diffusion model techniques, enabling advanced features\nsuch as 3D editing, mesh and texture enhancement, etc. Through extensive\nexperiments, we demonstrate the effectiveness of our approach, showcasing its\nability to produce high-quality 3D models efficiently.",
          "arxiv_id": "2503.01370v2"
        }
      ],
      "31": [
        {
          "title": "MEGL: Multimodal Explanation-Guided Learning",
          "year": "2024-11",
          "abstract": "Explaining the decision-making processes of Artificial Intelligence (AI)\nmodels is crucial for addressing their \"black box\" nature, particularly in\ntasks like image classification. Traditional eXplainable AI (XAI) methods\ntypically rely on unimodal explanations, either visual or textual, each with\ninherent limitations. Visual explanations highlight key regions but often lack\nrationale, while textual explanations provide context without spatial\ngrounding. Further, both explanation types can be inconsistent or incomplete,\nlimiting their reliability. To address these challenges, we propose a novel\nMultimodal Explanation-Guided Learning (MEGL) framework that leverages both\nvisual and textual explanations to enhance model interpretability and improve\nclassification performance. Our Saliency-Driven Textual Grounding (SDTG)\napproach integrates spatial information from visual explanations into textual\nrationales, providing spatially grounded and contextually rich explanations.\nAdditionally, we introduce Textual Supervision on Visual Explanations to align\nvisual explanations with textual rationales, even in cases where ground truth\nvisual annotations are missing. A Visual Explanation Distribution Consistency\nloss further reinforces visual coherence by aligning the generated visual\nexplanations with dataset-level patterns, enabling the model to effectively\nlearn from incomplete multimodal supervision. We validate MEGL on two new\ndatasets, Object-ME and Action-ME, for image classification with multimodal\nexplanations. Experimental results demonstrate that MEGL outperforms previous\napproaches in prediction accuracy and explanation quality across both visual\nand textual domains. Our code will be made available upon the acceptance of the\npaper.",
          "arxiv_id": "2411.13053v1"
        },
        {
          "title": "Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning",
          "year": "2023-12",
          "abstract": "Explaining predictions of black-box neural networks is crucial when applied\nto decision-critical tasks. Thus, attribution maps are commonly used to\nidentify important image regions, despite prior work showing that humans prefer\nexplanations based on similar examples. To this end, ProtoPNet learns a set of\nclass-representative feature vectors (prototypes) for case-based reasoning.\nDuring inference, similarities of latent features to prototypes are linearly\nclassified to form predictions and attribution maps are provided to explain the\nsimilarity. In this work, we evaluate whether architectures for case-based\nreasoning fulfill established axioms required for faithful explanations using\nthe example of ProtoPNet. We show that such architectures allow the extraction\nof faithful explanations. However, we prove that the attribution maps used to\nexplain the similarities violate the axioms. We propose a new procedure to\nextract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,\nthese explanations are Shapley values, calculated on the similarity scores of\neach prototype. They allow to faithfully answer which prototypes are present in\nan unseen image and quantify each pixel's contribution to that presence,\nthereby complying with all axioms. The theoretical violations of ProtoPNet\nmanifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,\nRSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,\nResNeXt50). Our experiments show a qualitative difference between the\nexplanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the\nexplanations with the Area Over the Perturbation Curve, on which ProtoPFaith\noutperforms ProtoPNet on all experiments by a factor $>10^3$.",
          "arxiv_id": "2312.09783v3"
        },
        {
          "title": "Solving the enigma: Enhancing faithfulness and comprehensibility in explanations of deep networks",
          "year": "2024-05",
          "abstract": "The accelerated progress of artificial intelligence (AI) has popularized deep\nlearning models across various domains, yet their inherent opacity poses\nchallenges, particularly in critical fields like healthcare, medicine, and the\ngeosciences. Explainable AI (XAI) has emerged to shed light on these 'black\nbox' models, aiding in deciphering their decision-making processes. However,\ndifferent XAI methods often produce significantly different explanations,\nleading to high inter-method variability that increases uncertainty and\nundermines trust in deep networks' predictions. In this study, we address this\nchallenge by introducing a novel framework designed to enhance the\nexplainability of deep networks through a dual focus on maximizing both\naccuracy and comprehensibility in the explanations. Our framework integrates\noutputs from multiple established XAI methods and leverages a non-linear neural\nnetwork model, termed the 'explanation optimizer,' to construct a unified,\noptimal explanation. The optimizer evaluates explanations using two key\nmetrics: faithfulness (accuracy in reflecting the network's decisions) and\ncomplexity (comprehensibility). By balancing these, it provides accurate and\naccessible explanations, addressing a key XAI limitation. Experiments on\nmulti-class and binary classification in 2D object and 3D neuroscience imaging\nconfirm its efficacy. Our optimizer achieved faithfulness scores 155% and 63%\nhigher than the best XAI methods in 3D and 2D tasks, respectively, while also\nreducing complexity for better understanding. These results demonstrate that\noptimal explanations based on specific quality criteria are achievable,\noffering a solution to the issue of inter-method variability in the current XAI\nliterature and supporting more trustworthy deep network predictions",
          "arxiv_id": "2405.10008v3"
        }
      ],
      "32": [
        {
          "title": "Deep learning achieves perfect anomaly detection on 108,308 retinal images including unlearned diseases",
          "year": "2020-01",
          "abstract": "Optical coherence tomography (OCT) scanning is useful in detecting various\nretinal diseases. However, there are not enough ophthalmologists who can\ndiagnose retinal OCT images in much of the world. To provide OCT screening\ninexpensively and extensively, an automated diagnosis system is indispensable.\nAlthough many machine learning techniques have been presented for assisting\nophthalmologists in diagnosing retinal OCT images, there is no technique that\ncan diagnose independently without relying on an ophthalmologist, i.e., there\nis no technique that does not overlook any anomaly, including unlearned\ndiseases. As long as there is a risk of overlooking a disease with a technique,\nophthalmologists must double-check even those images that the technique\nclassifies as normal. Here, we show that our deep-learning-based binary\nclassifier (normal or abnormal) achieved a perfect classification on 108,308\ntwo-dimensional retinal OCT images, i.e., true positive rate = 1.000000 and\ntrue negative rate = 1.000000; hence, the area under the ROC curve = 1.0000000.\nAlthough the test set included three types of diseases, two of these were not\nused for training. However, all test images were correctly classified.\nFurthermore, we demonstrated that our scheme was able to cope with differences\nin patient race. No conventional approach has achieved the above performances.\nOur work has a sufficient possibility of raising automated diagnosis techniques\nfor retinal OCT images from \"assistant for ophthalmologists\" to \"independent\ndiagnosis system without ophthalmologists\".",
          "arxiv_id": "2001.05859v5"
        },
        {
          "title": "OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods",
          "year": "2023-12",
          "abstract": "Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.",
          "arxiv_id": "2312.08255v4"
        },
        {
          "title": "ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model",
          "year": "2020-07",
          "abstract": "Optical Coherence Tomography Angiography (OCT-A) is a non-invasive imaging\ntechnique, and has been increasingly used to image the retinal vasculature at\ncapillary level resolution. However, automated segmentation of retinal vessels\nin OCT-A has been under-studied due to various challenges such as low capillary\nvisibility and high vessel complexity, despite its significance in\nunderstanding many eye-related diseases. In addition, there is no publicly\navailable OCT-A dataset with manually graded vessels for training and\nvalidation. To address these issues, for the first time in the field of retinal\nimage analysis we construct a dedicated Retinal OCT-A SEgmentation dataset\n(ROSE), which consists of 229 OCT-A images with vessel annotations at either\ncenterline-level or pixel level. This dataset has been released for public\naccess to assist researchers in the community in undertaking research in\nrelated topics. Secondly, we propose a novel Split-based Coarse-to-Fine vessel\nsegmentation network (SCF-Net), with the ability to detect thick and thin\nvessels separately. In the SCF-Net, a split-based coarse segmentation (SCS)\nmodule is first introduced to produce a preliminary confidence map of vessels,\nand a split-based refinement (SRN) module is then used to optimize the\nshape/contour of the retinal microvasculature. Thirdly, we perform a thorough\nevaluation of the state-of-the-art vessel segmentation models and our SCF-Net\non the proposed ROSE dataset. The experimental results demonstrate that our\nSCF-Net yields better vessel segmentation performance in OCT-A than both\ntraditional methods and other deep learning methods.",
          "arxiv_id": "2007.05201v2"
        }
      ],
      "33": [
        {
          "title": "Dual-Attention Frequency Fusion at Multi-Scale for Joint Segmentation and Deformable Medical Image Registration",
          "year": "2024-09",
          "abstract": "Deformable medical image registration is a crucial aspect of medical image\nanalysis. In recent years, researchers have begun leveraging auxiliary tasks\n(such as supervised segmentation) to provide anatomical structure information\nfor the primary registration task, addressing complex deformation challenges in\nmedical image registration. In this work, we propose a multi-task learning\nframework based on multi-scale dual attention frequency fusion (DAFF-Net),\nwhich simultaneously achieves the segmentation masks and dense deformation\nfields in a single-step estimation. DAFF-Net consists of a global encoder, a\nsegmentation decoder, and a coarse-to-fine pyramid registration decoder. During\nthe registration decoding process, we design the dual attention frequency\nfeature fusion (DAFF) module to fuse registration and segmentation features at\ndifferent scales, fully leveraging the correlation between the two tasks. The\nDAFF module optimizes the features through global and local weighting\nmechanisms. During local weighting, it incorporates both high-frequency and\nlow-frequency information to further capture the features that are critical for\nthe registration task. With the aid of segmentation, the registration learns\nmore precise anatomical structure information, thereby enhancing the anatomical\nconsistency of the warped images after registration. Additionally, due to the\nDAFF module's outstanding ability to extract effective feature information, we\nextend its application to unsupervised registration. Extensive experiments on\nthree public 3D brain magnetic resonance imaging (MRI) datasets demonstrate\nthat the proposed DAFF-Net and its unsupervised variant outperform\nstate-of-the-art registration methods across several evaluation metrics,\ndemonstrating the effectiveness of our approach in deformable medical image\nregistration.",
          "arxiv_id": "2409.19658v1"
        },
        {
          "title": "Deep Learning for Medical Image Registration: A Comprehensive Review",
          "year": "2022-04",
          "abstract": "Image registration is a critical component in the applications of various\nmedical image analyses. In recent years, there has been a tremendous surge in\nthe development of deep learning (DL)-based medical image registration models.\nThis paper provides a comprehensive review of medical image registration.\nFirstly, a discussion is provided for supervised registration categories, for\nexample, fully supervised, dual supervised, and weakly supervised registration.\nNext, similarity-based as well as generative adversarial network (GAN)-based\nregistration are presented as part of unsupervised registration. Deep iterative\nregistration is then described with emphasis on deep similarity-based and\nreinforcement learning-based registration. Moreover, the application areas of\nmedical image registration are reviewed. This review focuses on monomodal and\nmultimodal registration and associated imaging, for instance, X-ray, CT scan,\nultrasound, and MRI. The existing challenges are highlighted in this review,\nwhere it is shown that a major challenge is the absence of a training dataset\nwith known transformations. Finally, a discussion is provided on the promising\nfuture research areas in the field of DL-based medical image registration.",
          "arxiv_id": "2204.11341v1"
        },
        {
          "title": "Joint segmentation and discontinuity-preserving deformable registration: Application to cardiac cine-MR images",
          "year": "2022-11",
          "abstract": "Medical image registration is a challenging task involving the estimation of\nspatial transformations to establish anatomical correspondence between pairs or\ngroups of images. Recently, deep learning-based image registration methods have\nbeen widely explored, and demonstrated to enable fast and accurate image\nregistration in a variety of applications. However, most deep learning-based\nregistration methods assume that the deformation fields are smooth and\ncontinuous everywhere in the image domain, which is not always true, especially\nwhen registering images whose fields of view contain discontinuities at\ntissue/organ boundaries. In such scenarios, enforcing smooth, globally\ncontinuous deformation fields leads to incorrect/implausible registration\nresults. We propose a novel discontinuity-preserving image registration method\nto tackle this challenge, which ensures globally discontinuous and locally\nsmooth deformation fields, leading to more accurate and realistic registration\nresults. The proposed method leverages the complementary nature of image\nsegmentation and registration and enables joint segmentation and pair-wise\nregistration of images. A co-attention block is proposed in the segmentation\ncomponent of the network to learn the structural correlations in the input\nimages, while a discontinuity-preserving registration strategy is employed in\nthe registration component of the network to ensure plausibility in the\nestimated deformation fields at tissue/organ interfaces. We evaluate our method\non the task of intra-subject spatio-temporal image registration using\nlarge-scale cinematic cardiac magnetic resonance image sequences, and\ndemonstrate that our method achieves significant improvements over the\nstate-of-the-art for medical image registration, and produces high-quality\nsegmentation masks for the regions of interest.",
          "arxiv_id": "2211.13828v1"
        }
      ],
      "34": [
        {
          "title": "A Representation Separation Perspective to Correspondences-free Unsupervised 3D Point Cloud Registration",
          "year": "2022-03",
          "abstract": "3D point cloud registration in remote sensing field has been greatly advanced\nby deep learning based methods, where the rigid transformation is either\ndirectly regressed from the two point clouds (correspondences-free approaches)\nor computed from the learned correspondences (correspondences-based\napproaches). Existing correspondences-free methods generally learn the holistic\nrepresentation of the entire point cloud, which is fragile for partial and\nnoisy point clouds. In this paper, we propose a correspondences-free\nunsupervised point cloud registration (UPCR) method from the representation\nseparation perspective. First, we model the input point cloud as a combination\nof pose-invariant representation and pose-related representation. Second, the\npose-related representation is used to learn the relative pose wrt a \"latent\ncanonical shape\" for the source and target point clouds respectively. Third,\nthe rigid transformation is obtained from the above two learned relative poses.\nOur method not only filters out the disturbance in pose-invariant\nrepresentation but also is robust to partial-to-partial point clouds or noise.\nExperiments on benchmark datasets demonstrate that our unsupervised method\nachieves comparable if not better performance than state-of-the-art supervised\nregistration methods.",
          "arxiv_id": "2203.13239v1"
        },
        {
          "title": "Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark",
          "year": "2025-09",
          "abstract": "Cross-source point cloud registration, which aims to align point cloud data\nfrom different sensors, is a fundamental task in 3D vision. However, compared\nto the same-source point cloud registration, cross-source registration faces\ntwo core challenges: the lack of publicly available large-scale real-world\ndatasets for training the deep registration models, and the inherent\ndifferences in point clouds captured by multiple sensors. The diverse patterns\ninduced by the sensors pose great challenges in robust and accurate point cloud\nfeature extraction and matching, which negatively influence the registration\naccuracy. To advance research in this field, we construct Cross3DReg, the\ncurrently largest and real-world multi-modal cross-source point cloud\nregistration dataset, which is collected by a rotating mechanical lidar and a\nhybrid semi-solid-state lidar, respectively. Moreover, we design an\noverlap-based cross-source registration framework, which utilizes unaligned\nimages to predict the overlapping region between source and target point\nclouds, effectively filtering out redundant points in the irrelevant regions\nand significantly mitigating the interference caused by noise in\nnon-overlapping areas. Then, a visual-geometric attention guided matching\nmodule is proposed to enhance the consistency of cross-source point cloud\nfeatures by fusing image and geometric information to establish reliable\ncorrespondences and ultimately achieve accurate and robust registration.\nExtensive experiments show that our method achieves state-of-the-art\nregistration performance. Our framework reduces the relative rotation error\n(RRE) and relative translation error (RTE) by $63.2\\%$ and $40.2\\%$,\nrespectively, and improves the registration recall (RR) by $5.4\\%$, which\nvalidates its effectiveness in achieving accurate cross-source registration.",
          "arxiv_id": "2509.06456v1"
        },
        {
          "title": "Robust Point Cloud Registration Framework Based on Deep Graph Matching",
          "year": "2021-03",
          "abstract": "3D point cloud registration is a fundamental problem in computer vision and\nrobotics. There has been extensive research in this area, but existing methods\nmeet great challenges in situations with a large proportion of outliers and\ntime constraints, but without good transformation initialization. Recently, a\nseries of learning-based algorithms have been introduced and show advantages in\nspeed. Many of them are based on correspondences between the two point clouds,\nso they do not rely on transformation initialization. However, these\nlearning-based methods are sensitive to outliers, which lead to more incorrect\ncorrespondences. In this paper, we propose a novel deep graph matchingbased\nframework for point cloud registration. Specifically, we first transform point\nclouds into graphs and extract deep features for each point. Then, we develop a\nmodule based on deep graph matching to calculate a soft correspondence matrix.\nBy using graph matching, not only the local geometry of each point but also its\nstructure and topology in a larger range are considered in establishing\ncorrespondences, so that more correct correspondences are found. We train the\nnetwork with a loss directly defined on the correspondences, and in the test\nstage the soft correspondences are transformed into hard one-to-one\ncorrespondences so that registration can be performed by singular value\ndecomposition. Furthermore, we introduce a transformer-based method to generate\nedges for graph construction, which further improves the quality of the\ncorrespondences. Extensive experiments on registering clean, noisy,\npartial-to-partial and unseen category point clouds show that the proposed\nmethod achieves state-of-the-art performance. The code will be made publicly\navailable at https://github.com/fukexue/RGM.",
          "arxiv_id": "2103.04256v1"
        }
      ],
      "35": [
        {
          "title": "Generating Representative Samples for Few-Shot Classification",
          "year": "2022-05",
          "abstract": "Few-shot learning (FSL) aims to learn new categories with a few visual\nsamples per class. Few-shot class representations are often biased due to data\nscarcity. To mitigate this issue, we propose to generate visual samples based\non semantic embeddings using a conditional variational autoencoder (CVAE)\nmodel. We train this CVAE model on base classes and use it to generate features\nfor novel classes. More importantly, we guide this VAE to strictly generate\nrepresentative samples by removing non-representative samples from the base\ntraining set when training the CVAE model. We show that this training scheme\nenhances the representativeness of the generated samples and therefore,\nimproves the few-shot classification results. Experimental results show that\nour method improves three FSL baseline methods by substantial margins,\nachieving state-of-the-art few-shot classification performance on miniImageNet\nand tieredImageNet datasets for both 1-shot and 5-shot settings. Code is\navailable at: https://github.com/cvlab-stonybrook/fsl-rsvae.",
          "arxiv_id": "2205.02918v1"
        },
        {
          "title": "Learning from Few Examples: A Summary of Approaches to Few-Shot Learning",
          "year": "2022-03",
          "abstract": "Few-Shot Learning refers to the problem of learning the underlying pattern in\nthe data just from a few training samples. Requiring a large number of data\nsamples, many deep learning solutions suffer from data hunger and extensively\nhigh computation time and resources. Furthermore, data is often not available\ndue to not only the nature of the problem or privacy concerns but also the cost\nof data preparation. Data collection, preprocessing, and labeling are strenuous\nhuman tasks. Therefore, few-shot learning that could drastically reduce the\nturnaround time of building machine learning applications emerges as a low-cost\nsolution. This survey paper comprises a representative list of recently\nproposed few-shot learning algorithms. Given the learning dynamics and\ncharacteristics, the approaches to few-shot learning problems are discussed in\nthe perspectives of meta-learning, transfer learning, and hybrid approaches\n(i.e., different variations of the few-shot learning problem).",
          "arxiv_id": "2203.04291v1"
        },
        {
          "title": "Meta-Learned Attribute Self-Gating for Continual Generalized Zero-Shot Learning",
          "year": "2021-02",
          "abstract": "Zero-shot learning (ZSL) has been shown to be a promising approach to\ngeneralizing a model to categories unseen during training by leveraging class\nattributes, but challenges still remain. Recently, methods using generative\nmodels to combat bias towards classes seen during training have pushed the\nstate of the art of ZSL, but these generative models can be slow or\ncomputationally expensive to train. Additionally, while many previous ZSL\nmethods assume a one-time adaptation to unseen classes, in reality, the world\nis always changing, necessitating a constant adjustment for deployed models.\nModels unprepared to handle a sequential stream of data are likely to\nexperience catastrophic forgetting. We propose a meta-continual zero-shot\nlearning (MCZSL) approach to address both these issues. In particular, by\npairing self-gating of attributes and scaled class normalization with\nmeta-learning based training, we are able to outperform state-of-the-art\nresults while being able to train our models substantially faster\n($>100\\times$) than expensive generative-based approaches. We demonstrate this\nby performing experiments on five standard ZSL datasets (CUB, aPY, AWA1, AWA2\nand SUN) in both generalized zero-shot learning and generalized continual\nzero-shot learning settings.",
          "arxiv_id": "2102.11856v1"
        }
      ],
      "36": [
        {
          "title": "VTPNet for 3D deep learning on point cloud",
          "year": "2023-05",
          "abstract": "Recently, Transformer-based methods for point cloud learning have achieved\ngood results on various point cloud learning benchmarks. However, since the\nattention mechanism needs to generate three feature vectors of query, key, and\nvalue to calculate attention features, most of the existing Transformer-based\npoint cloud learning methods usually consume a large amount of computational\ntime and memory resources when calculating global attention. To address this\nproblem, we propose a Voxel-Transformer-Point (VTP) Block for extracting local\nand global features of point clouds. VTP combines the advantages of\nvoxel-based, point-based and Transformer-based methods, which consists of\nVoxel-Based Branch (V branch), Point-Based Transformer Branch (PT branch) and\nPoint-Based Branch (P branch). The V branch extracts the coarse-grained\nfeatures of the point cloud through low voxel resolution; the PT branch obtains\nthe fine-grained features of the point cloud by calculating the self-attention\nin the local neighborhood and the inter-neighborhood cross-attention; the P\nbranch uses a simplified MLP network to generate the global location\ninformation of the point cloud. In addition, to enrich the local features of\npoint clouds at different scales, we set the voxel scale in the V branch and\nthe neighborhood sphere scale in the PT branch to one large and one small\n(large voxel scale \\& small neighborhood sphere scale or small voxel scale \\&\nlarge neighborhood sphere scale). Finally, we use VTP as the feature extraction\nnetwork to construct a VTPNet for point cloud learning, and performs shape\nclassification, part segmentation, and semantic segmentation tasks on the\nModelNet40, ShapeNet Part, and S3DIS datasets. The experimental results\nindicate that VTPNet has good performance in 3D point cloud learning.",
          "arxiv_id": "2305.06115v1"
        },
        {
          "title": "Global Context Aware Convolutions for 3D Point Cloud Understanding",
          "year": "2020-08",
          "abstract": "Recent advances in deep learning for 3D point clouds have shown great\npromises in scene understanding tasks thanks to the introduction of convolution\noperators to consume 3D point clouds directly in a neural network. Point cloud\ndata, however, could have arbitrary rotations, especially those acquired from\n3D scanning. Recent works show that it is possible to design point cloud\nconvolutions with rotation invariance property, but such methods generally do\nnot perform as well as translation-invariant only convolution. We found that a\nkey reason is that compared to point coordinates, rotation-invariant features\nconsumed by point cloud convolution are not as distinctive. To address this\nproblem, we propose a novel convolution operator that enhances feature\ndistinction by integrating global context information from the input point\ncloud to the convolution. To this end, a globally weighted local reference\nframe is constructed in each point neighborhood in which the local point set is\ndecomposed into bins. Anchor points are generated in each bin to represent\nglobal shape features. A convolution can then be performed to transform the\npoints and anchor features into final rotation-invariant features. We conduct\nseveral experiments on point cloud classification, part segmentation, shape\nretrieval, and normals estimation to evaluate our convolution, which achieves\nstate-of-the-art accuracy under challenging rotations.",
          "arxiv_id": "2008.02986v1"
        },
        {
          "title": "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling",
          "year": "2021-11",
          "abstract": "We present Point-BERT, a new paradigm for learning Transformers to generalize\nthe concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked\nPoint Modeling (MPM) task to pre-train point cloud Transformers. Specifically,\nwe first divide a point cloud into several local point patches, and a point\ncloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to\ngenerate discrete point tokens containing meaningful local information. Then,\nwe randomly mask out some patches of input point clouds and feed them into the\nbackbone Transformers. The pre-training objective is to recover the original\npoint tokens at the masked locations under the supervision of point tokens\nobtained by the Tokenizer. Extensive experiments demonstrate that the proposed\nBERT-style pre-training strategy significantly improves the performance of\nstandard point cloud Transformers. Equipped with our pre-training strategy, we\nshow that a pure Transformer architecture attains 93.8% accuracy on ModelNet40\nand 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully\ndesigned point cloud models with much fewer hand-made designs. We also\ndemonstrate that the representations learned by Point-BERT transfer well to new\ntasks and domains, where our models largely advance the state-of-the-art of\nfew-shot point cloud classification task. The code and pre-trained models are\navailable at https://github.com/lulutang0608/Point-BERT",
          "arxiv_id": "2111.14819v2"
        }
      ],
      "37": [
        {
          "title": "Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review",
          "year": "2024-03",
          "abstract": "Medical vision-language models (VLMs) combine computer vision (CV) and\nnatural language processing (NLP) to analyze visual and textual medical data.\nOur paper reviews recent advancements in developing VLMs specialized for\nhealthcare, focusing on models designed for medical report generation and\nvisual question answering (VQA). We provide background on NLP and CV,\nexplaining how techniques from both fields are integrated into VLMs to enable\nlearning from multimodal data. Key areas we address include the exploration of\nmedical vision-language datasets, in-depth analyses of architectures and\npre-training strategies employed in recent noteworthy medical VLMs, and\ncomprehensive discussion on evaluation metrics for assessing VLMs' performance\nin medical report generation and VQA. We also highlight current challenges and\npropose future directions, including enhancing clinical validity and addressing\npatient privacy concerns. Overall, our review summarizes recent progress in\ndeveloping VLMs to harness multimodal medical data for improved healthcare\napplications.",
          "arxiv_id": "2403.02469v2"
        },
        {
          "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning",
          "year": "2025-06",
          "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...",
          "arxiv_id": "2506.07044v4"
        },
        {
          "title": "A Survey of Medical Vision-and-Language Applications and Their Techniques",
          "year": "2024-11",
          "abstract": "Medical vision-and-language models (MVLMs) have attracted substantial\ninterest due to their capability to offer a natural language interface for\ninterpreting complex medical data. Their applications are versatile and have\nthe potential to improve diagnostic accuracy and decision-making for individual\npatients while also contributing to enhanced public health monitoring, disease\nsurveillance, and policy-making through more efficient analysis of large data\nsets. MVLMS integrate natural language processing with medical images to enable\na more comprehensive and contextual understanding of medical images alongside\ntheir corresponding textual information. Unlike general vision-and-language\nmodels trained on diverse, non-specialized datasets, MVLMs are purpose-built\nfor the medical domain, automatically extracting and interpreting critical\ninformation from medical images and textual reports to support clinical\ndecision-making. Popular clinical applications of MVLMs include automated\nmedical report generation, medical visual question answering, medical\nmultimodal segmentation, diagnosis and prognosis and medical image-text\nretrieval. Here, we provide a comprehensive overview of MVLMs and the various\nmedical tasks to which they have been applied. We conduct a detailed analysis\nof various vision-and-language model architectures, focusing on their distinct\nstrategies for cross-modal integration/exploitation of medical visual and\ntextual features. We also examine the datasets used for these tasks and compare\nthe performance of different models based on standardized evaluation metrics.\nFurthermore, we highlight potential challenges and summarize future research\ntrends and directions. The full collection of papers and codes is available at:\nhttps://github.com/YtongXie/Medical-Vision-and-Language-Tasks-and-Methodologies-A-Survey.",
          "arxiv_id": "2411.12195v1"
        }
      ],
      "38": [
        {
          "title": "Probing Representation Forgetting in Supervised and Unsupervised Continual Learning",
          "year": "2022-03",
          "abstract": "Continual Learning research typically focuses on tackling the phenomenon of\ncatastrophic forgetting in neural networks. Catastrophic forgetting is\nassociated with an abrupt loss of knowledge previously learned by a model when\nthe task, or more broadly the data distribution, being trained on changes. In\nsupervised learning problems this forgetting, resulting from a change in the\nmodel's representation, is typically measured or observed by evaluating the\ndecrease in old task performance. However, a model's representation can change\nwithout losing knowledge about prior tasks. In this work we consider the\nconcept of representation forgetting, observed by using the difference in\nperformance of an optimal linear classifier before and after a new task is\nintroduced. Using this tool we revisit a number of standard continual learning\nbenchmarks and observe that, through this lens, model representations trained\nwithout any explicit control for forgetting often experience small\nrepresentation forgetting and can sometimes be comparable to methods which\nexplicitly control for forgetting, especially in longer task sequences. We also\nshow that representation forgetting can lead to new insights on the effect of\nmodel capacity and loss function used in continual learning. Based on our\nresults, we show that a simple yet competitive approach is to learn\nrepresentations continually with standard supervised contrastive learning while\nconstructing prototypes of class samples when queried on old samples.",
          "arxiv_id": "2203.13381v2"
        },
        {
          "title": "Slowing Down Forgetting in Continual Learning",
          "year": "2024-11",
          "abstract": "A common challenge in continual learning (CL) is catastrophic forgetting,\nwhere the performance on old tasks drops after new, additional tasks are\nlearned. In this paper, we propose a novel framework called ReCL to slow down\nforgetting in CL. Our framework exploits an implicit bias of gradient-based\nneural networks due to which these converge to margin maximization points. Such\nconvergence points allow us to reconstruct old data from previous tasks, which\nwe then combine with the current training data. Our framework is flexible and\ncan be applied on top of existing, state-of-the-art CL methods. We further\ndemonstrate the performance gain from our framework across a large series of\nexperiments, including two challenging CL scenarios (class incremental and\ndomain incremental learning), different datasets (MNIST, CIFAR10,\nTinyImagenet), and different network architectures. Across all experiments, we\nfind large performance gains through ReCL. To the best of our knowledge, our\nframework is the first to address catastrophic forgetting by leveraging models\nin CL as their own memory buffers.",
          "arxiv_id": "2411.06916v2"
        },
        {
          "title": "Susceptibility of Continual Learning Against Adversarial Attacks",
          "year": "2022-07",
          "abstract": "Recent continual learning approaches have primarily focused on mitigating\ncatastrophic forgetting. Nevertheless, two critical areas have remained\nrelatively unexplored: 1) evaluating the robustness of proposed methods and 2)\nensuring the security of learned tasks. This paper investigates the\nsusceptibility of continually learned tasks, including current and previously\nacquired tasks, to adversarial attacks. Specifically, we have observed that any\nclass belonging to any task can be easily targeted and misclassified as the\ndesired target class of any other task. Such susceptibility or vulnerability of\nlearned tasks to adversarial attacks raises profound concerns regarding data\nintegrity and privacy. To assess the robustness of continual learning\napproaches, we consider continual learning approaches in all three scenarios,\ni.e., task-incremental learning, domain-incremental learning, and\nclass-incremental learning. In this regard, we explore the robustness of three\nregularization-based methods, three replay-based approaches, and one hybrid\ntechnique that combines replay and exemplar approaches. We empirically\ndemonstrated that in any setting of continual learning, any class, whether\nbelonging to the current or previously learned tasks, is susceptible to\nmisclassification. Our observations identify potential limitations of continual\nlearning approaches against adversarial attacks and highlight that current\ncontinual learning algorithms could not be suitable for deployment in\nreal-world settings.",
          "arxiv_id": "2207.05225v5"
        }
      ],
      "39": [
        {
          "title": "Cataract-1K: Cataract Surgery Dataset for Scene Segmentation, Phase Recognition, and Irregularity Detection",
          "year": "2023-12",
          "abstract": "In recent years, the landscape of computer-assisted interventions and\npost-operative surgical video analysis has been dramatically reshaped by\ndeep-learning techniques, resulting in significant advancements in surgeons'\nskills, operation room management, and overall surgical outcomes. However, the\nprogression of deep-learning-powered surgical technologies is profoundly\nreliant on large-scale datasets and annotations. Particularly, surgical scene\nunderstanding and phase recognition stand as pivotal pillars within the realm\nof computer-assisted surgery and post-operative assessment of cataract surgery\nvideos. In this context, we present the largest cataract surgery video dataset\nthat addresses diverse requisites for constructing computerized surgical\nworkflow analysis and detecting post-operative irregularities in cataract\nsurgery. We validate the quality of annotations by benchmarking the performance\nof several state-of-the-art neural network architectures for phase recognition\nand surgical scene segmentation. Besides, we initiate the research on domain\nadaptation for instrument segmentation in cataract surgery by evaluating\ncross-domain instrument segmentation performance in cataract surgery videos.\nThe dataset and annotations will be publicly available upon acceptance of the\npaper.",
          "arxiv_id": "2312.06295v1"
        },
        {
          "title": "SURGIVID: Annotation-Efficient Surgical Video Object Discovery",
          "year": "2024-09",
          "abstract": "Surgical scenes convey crucial information about the quality of surgery.\nPixel-wise localization of tools and anatomical structures is the first task\ntowards deeper surgical analysis for microscopic or endoscopic surgical views.\nThis is typically done via fully-supervised methods which are annotation greedy\nand in several cases, demanding medical expertise. Considering the profusion of\nsurgical videos obtained through standardized surgical workflows, we propose an\nannotation-efficient framework for the semantic segmentation of surgical\nscenes. We employ image-based self-supervised object discovery to identify the\nmost salient tools and anatomical structures in surgical videos. These\nproposals are further refined within a minimally supervised fine-tuning step.\nOur unsupervised setup reinforced with only 36 annotation labels indicates\ncomparable localization performance with fully-supervised segmentation models.\nFurther, leveraging surgical phase labels as weak labels can better guide model\nattention towards surgical tools, leading to $\\sim 2\\%$ improvement in tool\nlocalization. Extensive ablation studies on the CaDIS dataset validate the\neffectiveness of our proposed solution in discovering relevant surgical objects\nwith minimal or no supervision.",
          "arxiv_id": "2409.07801v1"
        },
        {
          "title": "OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding",
          "year": "2024-06",
          "abstract": "Surgical scene perception via videos is critical for advancing robotic\nsurgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.\nHowever, the scarcity of diverse and richly annotated video datasets has\nhindered the development of intelligent systems for surgical workflow analysis.\nExisting datasets face challenges such as small scale, lack of diversity in\nsurgery and phase categories, and absence of time-localized annotations. These\nlimitations impede action understanding and model generalization validation in\ncomplex and diverse real-world surgical scenarios. To address this gap, we\nintroduce OphNet, a large-scale, expert-annotated video benchmark for\nophthalmic surgical workflow understanding. OphNet features: 1) A diverse\ncollection of 2,278 surgical videos spanning 66 types of cataract, glaucoma,\nand corneal surgeries, with detailed annotations for 102 unique surgical phases\nand 150 fine-grained operations. 2) Sequential and hierarchical annotations for\neach surgery, phase, and operation, enabling comprehensive understanding and\nimproved interpretability. 3) Time-localized annotations, facilitating temporal\nlocalization and prediction tasks within surgical workflows. With approximately\n285 hours of surgical videos, OphNet is about 20 times larger than the largest\nexisting surgical workflow analysis benchmark. Code and dataset are available\nat: https://minghu0830.github.io/OphNet-benchmark/.",
          "arxiv_id": "2406.07471v4"
        }
      ],
      "40": [
        {
          "title": "Hybrid SNN-ANN: Energy-Efficient Classification and Object Detection for Event-Based Vision",
          "year": "2021-12",
          "abstract": "Event-based vision sensors encode local pixel-wise brightness changes in\nstreams of events rather than image frames and yield sparse, energy-efficient\nencodings of scenes, in addition to low latency, high dynamic range, and lack\nof motion blur. Recent progress in object recognition from event-based sensors\nhas come from conversions of deep neural networks, trained with\nbackpropagation. However, using these approaches for event streams requires a\ntransformation to a synchronous paradigm, which not only loses computational\nefficiency, but also misses opportunities to extract spatio-temporal features.\nIn this article we propose a hybrid architecture for end-to-end training of\ndeep neural networks for event-based pattern recognition and object detection,\ncombining a spiking neural network (SNN) backbone for efficient event-based\nfeature extraction, and a subsequent analog neural network (ANN) head to solve\nsynchronous classification and detection tasks. This is achieved by combining\nstandard backpropagation with surrogate gradient training to propagate\ngradients through the SNN. Hybrid SNN-ANNs can be trained without conversion,\nand result in highly accurate networks that are substantially more\ncomputationally efficient than their ANN counterparts. We demonstrate results\non event-based classification and object detection datasets, in which only the\narchitecture of the ANN heads need to be adapted to the tasks, and no\nconversion of the event-based input is necessary. Since ANNs and SNNs require\ndifferent hardware paradigms to maximize their efficiency, we envision that SNN\nbackbone and ANN head can be executed on different processing units, and thus\nanalyze the necessary bandwidth to communicate between the two parts. Hybrid\nnetworks are promising architectures to further advance machine learning\napproaches for event-based vision, without having to compromise on efficiency.",
          "arxiv_id": "2112.03423v1"
        },
        {
          "title": "SFOD: Spiking Fusion Object Detector",
          "year": "2024-03",
          "abstract": "Event cameras, characterized by high temporal resolution, high dynamic range,\nlow power consumption, and high pixel bandwidth, offer unique capabilities for\nobject detection in specialized contexts. Despite these advantages, the\ninherent sparsity and asynchrony of event data pose challenges to existing\nobject detection algorithms. Spiking Neural Networks (SNNs), inspired by the\nway the human brain codes and processes information, offer a potential solution\nto these difficulties. However, their performance in object detection using\nevent cameras is limited in current implementations. In this paper, we propose\nthe Spiking Fusion Object Detector (SFOD), a simple and efficient approach to\nSNN-based object detection. Specifically, we design a Spiking Fusion Module,\nachieving the first-time fusion of feature maps from different scales in SNNs\napplied to event cameras. Additionally, through integrating our analysis and\nexperiments conducted during the pretraining of the backbone network on the\nNCAR dataset, we delve deeply into the impact of spiking decoding strategies\nand loss functions on model performance. Thereby, we establish state-of-the-art\nclassification results based on SNNs, achieving 93.7\\% accuracy on the NCAR\ndataset. Experimental results on the GEN1 detection dataset demonstrate that\nthe SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing\nSNN-based approaches. Our research not only underscores the potential of SNNs\nin object detection with event cameras but also propels the advancement of\nSNNs. Code is available at https://github.com/yimeng-fan/SFOD.",
          "arxiv_id": "2403.15192v1"
        },
        {
          "title": "SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition",
          "year": "2023-10",
          "abstract": "Event cameras are bio-inspired sensors that respond to local changes in light\nintensity and feature low latency, high energy efficiency, and high dynamic\nrange. Meanwhile, Spiking Neural Networks (SNNs) have gained significant\nattention due to their remarkable efficiency and fault tolerance. By\nsynergistically harnessing the energy efficiency inherent in event cameras and\nthe spike-based processing capabilities of SNNs, their integration could enable\nultra-low-power application scenarios, such as action recognition tasks.\nHowever, existing approaches often entail converting asynchronous events into\nconventional frames, leading to additional data mapping efforts and a loss of\nsparsity, contradicting the design concept of SNNs and event cameras. To\naddress this challenge, we propose SpikePoint, a novel end-to-end point-based\nSNN architecture. SpikePoint excels at processing sparse event cloud data,\neffectively extracting both global and local features through a singular-stage\nstructure. Leveraging the surrogate training method, SpikePoint achieves high\naccuracy with few parameters and maintains low power consumption, specifically\nemploying the identity mapping feature extractor on diverse datasets.\nSpikePoint achieves state-of-the-art (SOTA) performance on four event-based\naction recognition datasets using only 16 timesteps, surpassing other SNN\nmethods. Moreover, it also achieves SOTA performance across all methods on\nthree datasets, utilizing approximately 0.3\\% of the parameters and 0.5\\% of\npower consumption employed by artificial neural networks (ANNs). These results\nemphasize the significance of Point Cloud and pave the way for many\nultra-low-power event-based data processing applications.",
          "arxiv_id": "2310.07189v2"
        }
      ],
      "41": [
        {
          "title": "S$^2$Mamba: A Spatial-spectral State Space Model for Hyperspectral Image Classification",
          "year": "2024-04",
          "abstract": "Land cover analysis using hyperspectral images (HSI) remains an open problem\ndue to their low spatial resolution and complex spectral information. Recent\nstudies are primarily dedicated to designing Transformer-based architectures\nfor spatial-spectral long-range dependencies modeling, which is computationally\nexpensive with quadratic complexity. Selective structured state space model\n(Mamba), which is efficient for modeling long-range dependencies with linear\ncomplexity, has recently shown promising progress. However, its potential in\nhyperspectral image processing that requires handling numerous spectral bands\nhas not yet been explored. In this paper, we innovatively propose S$^2$Mamba, a\nspatial-spectral state space model for hyperspectral image classification, to\nexcavate spatial-spectral contextual features, resulting in more efficient and\naccurate land cover analysis. In S$^2$Mamba, two selective structured state\nspace models through different dimensions are designed for feature extraction,\none for spatial, and the other for spectral, along with a spatial-spectral\nmixture gate for optimal fusion. More specifically, S$^2$Mamba first captures\nspatial contextual relations by interacting each pixel with its adjacent\nthrough a Patch Cross Scanning module and then explores semantic information\nfrom continuous spectral bands through a Bi-directional Spectral Scanning\nmodule. Considering the distinct expertise of the two attributes in homogenous\nand complicated texture scenes, we realize the Spatial-spectral Mixture Gate by\na group of learnable matrices, allowing for the adaptive incorporation of\nrepresentations learned across different dimensions. Extensive experiments\nconducted on HSI classification benchmarks demonstrate the superiority and\nprospect of S$^2$Mamba. The code will be made available at:\nhttps://github.com/PURE-melo/S2Mamba.",
          "arxiv_id": "2404.18213v2"
        },
        {
          "title": "Learning Spatial-Spectral Prior for Super-Resolution of Hyperspectral Imagery",
          "year": "2020-05",
          "abstract": "Recently, single gray/RGB image super-resolution reconstruction task has been\nextensively studied and made significant progress by leveraging the advanced\nmachine learning techniques based on deep convolutional neural networks\n(DCNNs). However, there has been limited technical development focusing on\nsingle hyperspectral image super-resolution due to the high-dimensional and\ncomplex spectral patterns in hyperspectral image. In this paper, we make a step\nforward by investigating how to adapt state-of-the-art residual learning based\nsingle gray/RGB image super-resolution approaches for computationally efficient\nsingle hyperspectral image super-resolution, referred as SSPSR. Specifically,\nwe introduce a spatial-spectral prior network (SSPN) to fully exploit the\nspatial information and the correlation between the spectra of the\nhyperspectral data. Considering that the hyperspectral training samples are\nscarce and the spectral dimension of hyperspectral image data is very high, it\nis nontrivial to train a stable and effective deep network. Therefore, a group\nconvolution (with shared network parameters) and progressive upsampling\nframework is proposed. This will not only alleviate the difficulty in feature\nextraction due to high-dimension of the hyperspectral data, but also make the\ntraining process more stable. To exploit the spatial and spectral prior, we\ndesign a spatial-spectral block (SSB), which consists of a spatial residual\nmodule and a spectral attention residual module. Experimental results on some\nhyperspectral images demonstrate that the proposed SSPSR method enhances the\ndetails of the recovered high-resolution hyperspectral images, and outperforms\nstate-of-the-arts. The source code is available at\n\\url{https://github.com/junjun-jiang/SSPSR",
          "arxiv_id": "2005.08752v2"
        },
        {
          "title": "High Spectral Spatial Resolution Synthetic HyperSpectral Dataset form multi-source fusion",
          "year": "2023-06",
          "abstract": "This research paper introduces a synthetic hyperspectral dataset that\ncombines high spectral and spatial resolution imaging to achieve a\ncomprehensive, accurate, and detailed representation of observed scenes or\nobjects. Obtaining such desirable qualities is challenging when relying on a\nsingle camera. The proposed dataset addresses this limitation by leveraging\nthree modalities: RGB, push-broom visible hyperspectral camera, and snapshot\ninfrared hyperspectral camera, each offering distinct spatial and spectral\nresolutions. Different camera systems exhibit varying photometric properties,\nresulting in a trade-off between spatial and spectral resolution. RGB cameras\ntypically offer high spatial resolution but limited spectral resolution, while\nhyperspectral cameras possess high spectral resolution at the expense of\nspatial resolution. Moreover, hyperspectral cameras themselves employ different\ncapturing techniques and spectral ranges, further complicating the acquisition\nof comprehensive data. By integrating the photometric properties of these\nmodalities, a single synthetic hyperspectral image can be generated,\nfacilitating the exploration of broader spectral-spatial relationships for\nimproved analysis, monitoring, and decision-making across various fields. This\npaper emphasizes the importance of multi-modal fusion in producing a\nhigh-quality synthetic hyperspectral dataset with consistent spectral intervals\nbetween bands.",
          "arxiv_id": "2309.00005v1"
        }
      ],
      "42": [
        {
          "title": "Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey",
          "year": "2022-06",
          "abstract": "Marine ecosystems and their fish habitats are becoming increasingly important\ndue to their integral role in providing a valuable food source and conservation\noutcomes. Due to their remote and difficult to access nature, marine\nenvironments and fish habitats are often monitored using underwater cameras.\nThese cameras generate a massive volume of digital data, which cannot be\nefficiently analysed by current manual processing methods, which involve a\nhuman observer. DL is a cutting-edge AI technology that has demonstrated\nunprecedented performance in analysing visual data. Despite its application to\na myriad of domains, its use in underwater fish habitat monitoring remains\nunder explored. In this paper, we provide a tutorial that covers the key\nconcepts of DL, which help the reader grasp a high-level understanding of how\nDL works. The tutorial also explains a step-by-step procedure on how DL\nalgorithms should be developed for challenging applications such as underwater\nfish monitoring. In addition, we provide a comprehensive survey of key deep\nlearning techniques for fish habitat monitoring including classification,\ncounting, localization, and segmentation. Furthermore, we survey publicly\navailable underwater fish datasets, and compare various DL techniques in the\nunderwater fish monitoring domains. We also discuss some challenges and\nopportunities in the emerging field of deep learning for fish habitat\nprocessing. This paper is written to serve as a tutorial for marine scientists\nwho would like to grasp a high-level understanding of DL, develop it for their\napplications by following our step-by-step tutorial, and see how it is evolving\nto facilitate their research efforts. At the same time, it is suitable for\ncomputer scientists who would like to survey state-of-the-art DL-based\nmethodologies for fish habitat monitoring.",
          "arxiv_id": "2206.05394v1"
        },
        {
          "title": "Automatic Coral Detection with YOLO: A Deep Learning Approach for Efficient and Accurate Coral Reef Monitoring",
          "year": "2024-04",
          "abstract": "Coral reefs are vital ecosystems that are under increasing threat due to\nlocal human impacts and climate change. Efficient and accurate monitoring of\ncoral reefs is crucial for their conservation and management. In this paper, we\npresent an automatic coral detection system utilizing the You Only Look Once\n(YOLO) deep learning model, which is specifically tailored for underwater\nimagery analysis. To train and evaluate our system, we employ a dataset\nconsisting of 400 original underwater images. We increased the number of\nannotated images to 580 through image manipulation using data augmentation\ntechniques, which can improve the model's performance by providing more diverse\nexamples for training. The dataset is carefully collected from underwater\nvideos that capture various coral reef environments, species, and lighting\nconditions. Our system leverages the YOLOv5 algorithm's real-time object\ndetection capabilities, enabling efficient and accurate coral detection. We\nused YOLOv5 to extract discriminating features from the annotated dataset,\nenabling the system to generalize, including previously unseen underwater\nimages. The successful implementation of the automatic coral detection system\nwith YOLOv5 on our original image dataset highlights the potential of advanced\ncomputer vision techniques for coral reef research and conservation. Further\nresearch will focus on refining the algorithm to handle challenging underwater\nimage conditions, and expanding the dataset to incorporate a wider range of\ncoral species and spatio-temporal variations.",
          "arxiv_id": "2405.14879v1"
        },
        {
          "title": "Is Underwater Image Enhancement All Object Detectors Need?",
          "year": "2023-11",
          "abstract": "Underwater object detection is a crucial and challenging problem in marine\nengineering and aquatic robot. The difficulty is partly because of the\ndegradation of underwater images caused by light selective absorption and\nscattering. Intuitively, enhancing underwater images can benefit high-level\napplications like underwater object detection. However, it is still unclear\nwhether all object detectors need underwater image enhancement as\npre-processing. We therefore pose the questions \"Does underwater image\nenhancement really improve underwater object detection?\" and \"How does\nunderwater image enhancement contribute to underwater object detection?\". With\nthese two questions, we conduct extensive studies. Specifically, we use 18\nstate-of-the-art underwater image enhancement algorithms, covering traditional,\nCNN-based, and GAN-based algorithms, to pre-process underwater object detection\ndata. Then, we retrain 7 popular deep learning-based object detectors using the\ncorresponding results enhanced by different algorithms, obtaining 126\nunderwater object detection models. Coupled with 7 object detection models\nretrained using raw underwater images, we employ these 133 models to\ncomprehensively analyze the effect of underwater image enhancement on\nunderwater object detection. We expect this study can provide sufficient\nexploration to answer the aforementioned questions and draw more attention of\nthe community to the joint problem of underwater image enhancement and\nunderwater object detection. The pre-trained models and results are publicly\navailable and will be regularly updated. Project page:\nhttps://github.com/BIGWangYuDong/lqit/tree/main/configs/detection/uw_enhancement_affect_detection.",
          "arxiv_id": "2311.18814v1"
        }
      ],
      "43": [
        {
          "title": "Eight Years of Face Recognition Research: Reproducibility, Achievements and Open Issues",
          "year": "2022-08",
          "abstract": "Automatic face recognition is a research area with high popularity. Many\ndifferent face recognition algorithms have been proposed in the last thirty\nyears of intensive research in the field. With the popularity of deep learning\nand its capability to solve a huge variety of different problems, face\nrecognition researchers have concentrated effort on creating better models\nunder this paradigm. From the year 2015, state-of-the-art face recognition has\nbeen rooted in deep learning models. Despite the availability of large-scale\nand diverse datasets for evaluating the performance of face recognition\nalgorithms, many of the modern datasets just combine different factors that\ninfluence face recognition, such as face pose, occlusion, illumination, facial\nexpression and image quality. When algorithms produce errors on these datasets,\nit is not clear which of the factors has caused this error and, hence, there is\nno guidance in which direction more research is required. This work is a\nfollowup from our previous works developed in 2014 and eventually published in\n2016, showing the impact of various facial aspects on face recognition\nalgorithms. By comparing the current state-of-the-art with the best systems\nfrom the past, we demonstrate that faces under strong occlusions, some types of\nillumination, and strong expressions are problems mastered by deep learning\nalgorithms, whereas recognition with low-resolution images, extreme pose\nvariations, and open-set recognition is still an open problem. To show this, we\nrun a sequence of experiments using six different datasets and five different\nface recognition algorithms in an open-source and reproducible manner. We\nprovide the source code to run all of our experiments, which is easily\nextensible so that utilizing your own deep network in our evaluation is just a\nfew minutes away.",
          "arxiv_id": "2208.04040v2"
        },
        {
          "title": "MixFairFace: Towards Ultimate Fairness via MixFair Adapter in Face Recognition",
          "year": "2022-11",
          "abstract": "Although significant progress has been made in face recognition, demographic\nbias still exists in face recognition systems. For instance, it usually happens\nthat the face recognition performance for a certain demographic group is lower\nthan the others. In this paper, we propose MixFairFace framework to improve the\nfairness in face recognition models. First of all, we argue that the commonly\nused attribute-based fairness metric is not appropriate for face recognition. A\nface recognition system can only be considered fair while every person has a\nclose performance. Hence, we propose a new evaluation protocol to fairly\nevaluate the fairness performance of different approaches. Different from\nprevious approaches that require sensitive attribute labels such as race and\ngender for reducing the demographic bias, we aim at addressing the identity\nbias in face representation, i.e., the performance inconsistency between\ndifferent identities, without the need for sensitive attribute labels. To this\nend, we propose MixFair Adapter to determine and reduce the identity bias of\ntraining samples. Our extensive experiments demonstrate that our MixFairFace\napproach achieves state-of-the-art fairness performance on all benchmark\ndatasets.",
          "arxiv_id": "2211.15181v1"
        },
        {
          "title": "Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification",
          "year": "2020-01",
          "abstract": "Modern face recognition systems leverage datasets containing images of\nhundreds of thousands of specific individuals' faces to train deep\nconvolutional neural networks to learn an embedding space that maps an\narbitrary individual's face to a vector representation of their identity. The\nperformance of a face recognition system in face verification (1:1) and face\nidentification (1:N) tasks is directly related to the ability of an embedding\nspace to discriminate between identities. Recently, there has been significant\npublic scrutiny into the source and privacy implications of large-scale face\nrecognition training datasets such as MS-Celeb-1M and MegaFace, as many people\nare uncomfortable with their face being used to train dual-use technologies\nthat can enable mass surveillance. However, the impact of an individual's\ninclusion in training data on a derived system's ability to recognize them has\nnot previously been studied. In this work, we audit ArcFace, a\nstate-of-the-art, open source face recognition system, in a large-scale face\nidentification experiment with more than one million distractor images. We find\na Rank-1 face identification accuracy of 79.71% for individuals present in the\nmodel's training data and an accuracy of 75.73% for those not present. This\nmodest difference in accuracy demonstrates that face recognition systems using\ndeep learning work better for individuals they are trained on, which has\nserious privacy implications when one considers all major open source face\nrecognition training datasets do not obtain informed consent from individuals\nduring their collection.",
          "arxiv_id": "2001.03071v2"
        }
      ],
      "44": [
        {
          "title": "Towards Consistent and Controllable Image Synthesis for Face Editing",
          "year": "2025-02",
          "abstract": "Face editing methods, essential for tasks like virtual avatars, digital human\nsynthesis and identity preservation, have traditionally been built upon\nGAN-based techniques, while recent focus has shifted to diffusion-based models\ndue to their success in image reconstruction. However, diffusion models still\nface challenges in controlling specific attributes and preserving the\nconsistency of other unchanged attributes especially the identity\ncharacteristics. To address these issues and facilitate more convenient editing\nof face images, we propose a novel approach that leverages the power of\nStable-Diffusion (SD) models and crude 3D face models to control the lighting,\nfacial expression and head pose of a portrait photo. We observe that this task\nessentially involves the combinations of target background, identity and face\nattributes aimed to edit. We strive to sufficiently disentangle the control of\nthese factors to enable consistency of face editing. Specifically, our method,\ncoined as RigFace, contains: 1) A Spatial Attribute Encoder that provides\npresise and decoupled conditions of background, pose, expression and lighting;\n2) A high-consistency FaceFusion method that transfers identity features from\nthe Identity Encoder to the denoising UNet of a pre-trained SD model; 3) An\nAttribute Rigger that injects those conditions into the denoising UNet. Our\nmodel achieves comparable or even superior performance in both identity\npreservation and photorealism compared to existing face editing models. Code is\npublicly available at https://github.com/weimengting/RigFace.",
          "arxiv_id": "2502.02465v2"
        },
        {
          "title": "AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars",
          "year": "2022-10",
          "abstract": "Although 2D generative models have made great progress in face image\ngeneration and animation, they often suffer from undesirable artifacts such as\n3D inconsistency when rendering images from different camera viewpoints. This\nprevents them from synthesizing video animations indistinguishable from real\nones. Recently, 3D-aware GANs extend 2D GANs for explicit disentanglement of\ncamera pose by leveraging 3D scene representations. These methods can well\npreserve the 3D consistency of the generated images across different views, yet\nthey cannot achieve fine-grained control over other attributes, among which\nfacial expression control is arguably the most useful and desirable for face\nanimation. In this paper, we propose an animatable 3D-aware GAN for multiview\nconsistent face animation generation. The key idea is to decompose the 3D\nrepresentation of the 3D-aware GAN into a template field and a deformation\nfield, where the former represents different identities with a canonical\nexpression, and the latter characterizes expression variations of each\nidentity. To achieve meaningful control over facial expressions via\ndeformation, we propose a 3D-level imitative learning scheme between the\ngenerator and a parametric 3D face model during adversarial training of the\n3D-aware GAN. This helps our method achieve high-quality animatable face image\ngeneration with strong visual 3D consistency, even though trained with only\nunstructured 2D images. Extensive experiments demonstrate our superior\nperformance over prior works. Project page:\nhttps://yuewuhkust.github.io/AniFaceGAN",
          "arxiv_id": "2210.06465v1"
        },
        {
          "title": "Causal Representation Learning for Context-Aware Face Transfer",
          "year": "2021-10",
          "abstract": "Human face synthesis involves transferring knowledge about the identity and\nidentity-dependent face shape (IDFS) of a human face to target face images\nwhere the context (e.g., facial expressions, head poses, and other background\nfactors) may change dramatically. Human faces are non-rigid, so facial\nexpression leads to deformation of face shape, and head pose also affects the\nface observed in 2D images. A key challenge in face transfer is to match the\nface with unobserved new contexts, adapting the face appearance to different\nposes and expressions accordingly. In this work, we find a way to provide prior\nknowledge for generative models to reason about the appropriate appearance of a\nhuman face in response to various expressions and poses. We propose a novel\ncontext-aware face transfer method, called CarTrans, that incorporates causal\neffects of contextual factors into face representation, and thus is able to be\naware of the uncertainty of new contexts. We estimate the effect of facial\nexpression and head pose in terms of counterfactual inference by designing a\ncontrolled intervention trial, thus avoiding the requirement of a large number\nof observations to cover the pose-expression space well. Moreover, we propose a\nkernel regression-based encoder that eliminates the identity specificity of\ntarget faces when encoding contextual information from target images. The\nresulting method shows impressive performance, allowing fine-grained control\nover face shape and appearance under various contextual conditions.",
          "arxiv_id": "2110.01571v4"
        }
      ],
      "45": [
        {
          "title": "Topology-aware Human Avatars with Semantically-guided Gaussian Splatting",
          "year": "2024-08",
          "abstract": "Reconstructing photo-realistic and topology-aware animatable human avatars\nfrom monocular videos remains challenging in computer vision and graphics.\nRecently, methods using 3D Gaussians to represent the human body have emerged,\noffering faster optimization and real-time rendering. However, due to ignoring\nthe crucial role of human body semantic information which represents the\nexplicit topological and intrinsic structure within human body, they fail to\nachieve fine-detail reconstruction of human avatars. To address this issue, we\npropose SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven\nrigid deformation, and non-rigid cloth dynamics deformation to create\nphoto-realistic human avatars. We then design a Semantic Human-Body Annotator\n(SHA) which utilizes SMPL's semantic prior for efficient body part semantic\nlabeling. The generated labels are used to guide the optimization of semantic\nattributes of Gaussian. To capture the explicit topological structure of the\nhuman body, we employ a 3D network that integrates both topological and\ngeometric associations for human avatar deformation. We further implement three\nkey strategies to enhance the semantic accuracy of 3D Gaussians and rendering\nquality: semantic projection with 2D regularization, semantic-guided density\nregularization and semantic-aware regularization with neighborhood consistency.\nExtensive experiments demonstrate that SG-GS achieves state-of-the-art geometry\nand appearance reconstruction performance.",
          "arxiv_id": "2408.09665v2"
        },
        {
          "title": "AvatarGen: a 3D Generative Model for Animatable Human Avatars",
          "year": "2022-08",
          "abstract": "Unsupervised generation of clothed virtual humans with various appearance and\nanimatable poses is important for creating 3D human avatars and other AR/VR\napplications. Existing methods are either limited to rigid object modeling, or\nnot generative and thus unable to synthesize high-quality virtual humans and\nanimate them. In this work, we propose AvatarGen, the first method that enables\nnot only non-rigid human generation with diverse appearance but also full\ncontrol over poses and viewpoints, while only requiring 2D images for training.\nSpecifically, it extends the recent 3D GANs to clothed human generation by\nutilizing a coarse human body model as a proxy to warp the observation space\ninto a standard avatar under a canonical space. To model non-rigid dynamics, it\nintroduces a deformation network to learn pose-dependent deformations in the\ncanonical space. To improve geometry quality of the generated human avatars, it\nleverages signed distance field as geometric representation, which allows more\ndirect regularization from the body model on the geometry learning. Benefiting\nfrom these designs, our method can generate animatable human avatars with\nhigh-quality appearance and geometry modeling, significantly outperforming\nprevious 3D GANs. Furthermore, it is competent for many applications, e.g.,\nsingle-view reconstruction, reanimation, and text-guided synthesis. Code and\npre-trained model will be available.",
          "arxiv_id": "2208.00561v1"
        },
        {
          "title": "AvatarGen: A 3D Generative Model for Animatable Human Avatars",
          "year": "2022-11",
          "abstract": "Unsupervised generation of 3D-aware clothed humans with various appearances\nand controllable geometries is important for creating virtual human avatars and\nother AR/VR applications. Existing methods are either limited to rigid object\nmodeling, or not generative and thus unable to generate high-quality virtual\nhumans and animate them. In this work, we propose AvatarGen, the first method\nthat enables not only geometry-aware clothed human synthesis with high-fidelity\nappearances but also disentangled human animation controllability, while only\nrequiring 2D images for training. Specifically, we decompose the generative 3D\nhuman synthesis into pose-guided mapping and canonical representation with\npredefined human pose and shape, such that the canonical representation can be\nexplicitly driven to different poses and shapes with the guidance of a 3D\nparametric human model SMPL. AvatarGen further introduces a deformation network\nto learn non-rigid deformations for modeling fine-grained geometric details and\npose-dependent dynamics. To improve the geometry quality of the generated human\navatars, it leverages the signed distance field as geometric proxy, which\nallows more direct regularization from the 3D geometric priors of SMPL.\nBenefiting from these designs, our method can generate animatable 3D human\navatars with high-quality appearance and geometry modeling, significantly\noutperforming previous 3D GANs. Furthermore, it is competent for many\napplications, e.g., single-view reconstruction, re-animation, and text-guided\nsynthesis/editing. Code and pre-trained model will be available at\nhttp://jeff95.me/projects/avatargen.html.",
          "arxiv_id": "2211.14589v1"
        }
      ],
      "46": [
        {
          "title": "Distribution-Aware Prompt Tuning for Vision-Language Models",
          "year": "2023-09",
          "abstract": "Pre-trained vision-language models (VLMs) have shown impressive performance\non various downstream tasks by utilizing knowledge learned from large data. In\ngeneral, the performance of VLMs on target tasks can be further improved by\nprompt tuning, which adds context to the input image or text. By leveraging\ndata from target tasks, various prompt-tuning methods have been studied in the\nliterature. A key to prompt tuning is the feature space alignment between two\nmodalities via learnable vectors with model parameters fixed. We observed that\nthe alignment becomes more effective when embeddings of each modality are\n`well-arranged' in the latent space. Inspired by this observation, we proposed\ndistribution-aware prompt tuning (DAPT) for vision-language models, which is\nsimple yet effective. Specifically, the prompts are learned by maximizing\ninter-dispersion, the distance between classes, as well as minimizing the\nintra-dispersion measured by the distance between embeddings from the same\nclass. Our extensive experiments on 11 benchmark datasets demonstrate that our\nmethod significantly improves generalizability. The code is available at\nhttps://github.com/mlvlab/DAPT.",
          "arxiv_id": "2309.03406v1"
        },
        {
          "title": "Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models",
          "year": "2022-09",
          "abstract": "Pre-trained vision-language models (e.g., CLIP) have shown promising\nzero-shot generalization in many downstream tasks with properly designed text\nprompts. Instead of relying on hand-engineered prompts, recent works learn\nprompts using the training data from downstream tasks. While effective,\ntraining on domain-specific data reduces a model's generalization capability to\nunseen new domains. In this work, we propose test-time prompt tuning (TPT), a\nmethod that can learn adaptive prompts on the fly with a single test sample.\nFor image classification, TPT optimizes the prompt by minimizing the entropy\nwith confidence selection so that the model has consistent predictions across\ndifferent augmented views of each test sample. In evaluating generalization to\nnatural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP\nby 3.6% on average, surpassing previous prompt tuning approaches that require\nadditional task-specific training data. In evaluating cross-dataset\ngeneralization with unseen categories, TPT performs on par with the\nstate-of-the-art approaches that use additional training data. Project page:\nhttps://azshue.github.io/TPT.",
          "arxiv_id": "2209.07511v1"
        },
        {
          "title": "Prompt-aligned Gradient for Prompt Tuning",
          "year": "2022-05",
          "abstract": "Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we\ncan craft a zero-shot classifier by \"prompt\", e.g., the confidence score of an\nimage being \"[CLASS]\" can be obtained by using the VLM provided similarity\nmeasure between the image and the prompt sentence \"a photo of a [CLASS]\".\nTherefore, prompt shows a great potential for fast adaptation of VLMs to\ndownstream tasks if we fine-tune the prompt-based similarity measure. However,\nwe find a common failure that improper fine-tuning may not only undermine the\nprompt's inherent prediction for the task-related classes, but also for other\nclasses in the VLM vocabulary. Existing methods still address this problem by\nusing traditional anti-overfitting techniques such as early stopping and data\naugmentation, which lack a principled solution specific to prompt. We present\nPrompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from\nforgetting the the general knowledge learned from VLMs. In particular, ProGrad\nonly updates the prompt whose gradient is aligned (or non-conflicting) to the\n\"general direction\", which is represented as the gradient of the KL loss of the\npre-defined prompt prediction. Extensive experiments demonstrate the stronger\nfew-shot generalization ability of ProGrad over state-of-the-art prompt tuning\nmethods. Codes are available at https://github.com/BeierZhu/Prompt-align.",
          "arxiv_id": "2205.14865v4"
        }
      ],
      "47": [
        {
          "title": "Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation",
          "year": "2025-08",
          "abstract": "Category-level object pose estimation aims to predict the 6D pose and 3D size\nof objects within given categories. Existing approaches for this task rely\nsolely on 6D poses as supervisory signals without explicitly capturing the\nintrinsic continuity of poses, leading to inconsistencies in predictions and\nreduced generalization to unseen poses. To address this limitation, we propose\nHRC-Pose, a novel depth-only framework for category-level object pose\nestimation, which leverages contrastive learning to learn point cloud\nrepresentations that preserve the continuity of 6D poses. HRC-Pose decouples\nobject pose into rotation and translation components, which are separately\nencoded and leveraged throughout the network. Specifically, we introduce a\ncontrastive learning strategy for multi-task, multi-category scenarios based on\nour 6D pose-aware hierarchical ranking scheme, which contrasts point clouds\nfrom multiple categories by considering rotational and translational\ndifferences as well as categorical information. We further design pose\nestimation modules that separately process the learned rotation-aware and\ntranslation-aware embeddings. Our experiments demonstrate that HRC-Pose\nsuccessfully learns continuous feature spaces. Results on REAL275 and CAMERA25\nbenchmarks show that our method consistently outperforms existing depth-only\nstate-of-the-art methods and runs in real-time, demonstrating its effectiveness\nand potential for real-world applications. Our code is at\nhttps://github.com/zhujunli1993/HRC-Pose.",
          "arxiv_id": "2508.14358v1"
        },
        {
          "title": "EfficientPose: An efficient, accurate and scalable end-to-end 6D multi object pose estimation approach",
          "year": "2020-11",
          "abstract": "In this paper we introduce EfficientPose, a new approach for 6D object pose\nestimation. Our method is highly accurate, efficient and scalable over a wide\nrange of computational resources. Moreover, it can detect the 2D bounding box\nof multiple objects and instances as well as estimate their full 6D poses in a\nsingle shot. This eliminates the significant increase in runtime when dealing\nwith multiple objects other approaches suffer from. These approaches aim to\nfirst detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point\nproblem for their 6D pose for each object afterwards. We also propose a novel\naugmentation method for direct 6D pose estimation approaches to improve\nperformance and generalization, called 6D augmentation. Our approach achieves a\nnew state-of-the-art accuracy of 97.35% in terms of the ADD(-S) metric on the\nwidely-used 6D pose estimation benchmark dataset Linemod using RGB input, while\nstill running end-to-end at over 27 FPS. Through the inherent handling of\nmultiple objects and instances and the fused single shot 2D object detection as\nwell as 6D pose estimation, our approach runs even with multiple objects\n(eight) end-to-end at over 26 FPS, making it highly attractive to many real\nworld scenarios. Code will be made publicly available at\nhttps://github.com/ybkscht/EfficientPose.",
          "arxiv_id": "2011.04307v2"
        },
        {
          "title": "Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset",
          "year": "2022-06",
          "abstract": "6D object pose estimation is one of the fundamental problems in computer\nvision and robotics research. While a lot of recent efforts have been made on\ngeneralizing pose estimation to novel object instances within the same\ncategory, namely category-level 6D pose estimation, it is still restricted in\nconstrained environments given the limited number of annotated data. In this\npaper, we collect Wild6D, a new unlabeled RGBD object video dataset with\ndiverse instances and backgrounds. We utilize this data to generalize\ncategory-level 6D object pose estimation in the wild with semi-supervised\nlearning. We propose a new model, called Rendering for Pose estimation network\nRePoNet, that is jointly trained using the free ground-truths with the\nsynthetic data, and a silhouette matching objective function on the real-world\ndata. Without using any 3D annotations on real data, our method outperforms\nstate-of-the-art methods on the previous dataset and our Wild6D test set (with\nmanual annotations for evaluation) by a large margin. Project page with Wild6D\ndata: https://oasisyang.github.io/semi-pose .",
          "arxiv_id": "2206.15436v1"
        }
      ],
      "48": [
        {
          "title": "Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation",
          "year": "2022-09",
          "abstract": "Vessel segmentation in medical images is one of the important tasks in the\ndiagnosis of vascular diseases and therapy planning. Although learning-based\nsegmentation approaches have been extensively studied, a large amount of\nground-truth labels are required in supervised methods and confusing background\nstructures make neural networks hard to segment vessels in an unsupervised\nmanner. To address this, here we introduce a novel diffusion adversarial\nrepresentation learning (DARL) model that leverages a denoising diffusion\nprobabilistic model with adversarial learning, and apply it to vessel\nsegmentation. In particular, for self-supervised vessel segmentation, DARL\nlearns the background signal using a diffusion module, which lets a generation\nmodule effectively provide vessel representations. Also, by adversarial\nlearning based on the proposed switchable spatially-adaptive denormalization,\nour model estimates synthetic fake vessel images as well as vessel segmentation\nmasks, which further makes the model capture vessel-relevant semantic\ninformation. Once the proposed model is trained, the model generates\nsegmentation masks in a single step and can be applied to general vascular\nstructure segmentation of coronary angiography and retinal images. Experimental\nresults on various datasets show that our method significantly outperforms\nexisting unsupervised and self-supervised vessel segmentation methods.",
          "arxiv_id": "2209.14566v2"
        },
        {
          "title": "Deep vessel segmentation with joint multi-prior encoding",
          "year": "2024-09",
          "abstract": "The precise delineation of blood vessels in medical images is critical for\nmany clinical applications, including pathology detection and surgical\nplanning. However, fully-automated vascular segmentation is challenging because\nof the variability in shape, size, and topology. Manual segmentation remains\nthe gold standard but is time-consuming, subjective, and impractical for\nlarge-scale studies. Hence, there is a need for automatic and reliable\nsegmentation methods that can accurately detect blood vessels from medical\nimages. The integration of shape and topological priors into vessel\nsegmentation models has been shown to improve segmentation accuracy by offering\ncontextual information about the shape of the blood vessels and their spatial\nrelationships within the vascular tree. To further improve anatomical\nconsistency, we propose a new joint prior encoding mechanism which incorporates\nboth shape and topology in a single latent space. The effectiveness of our\nmethod is demonstrated on the publicly available 3D-IRCADb dataset. More\nglobally, the proposed approach holds promise in overcoming the challenges\nassociated with automatic vessel delineation and has the potential to advance\nthe field of deep priors encoding.",
          "arxiv_id": "2409.12334v1"
        },
        {
          "title": "Automatic diagnosis of cardiac magnetic resonance images based on semi-supervised learning",
          "year": "2024-05",
          "abstract": "Cardiac magnetic resonance imaging (MRI) is a pivotal tool for assessing\ncardiac function. Precise segmentation of cardiac structures is imperative for\naccurate cardiac functional evaluation. This paper introduces a semi-supervised\nmodel for automatic segmentation of cardiac images and auxiliary diagnosis. By\nharnessing cardiac MRI images and necessitating only a small portion of\nannotated image data, the model achieves fully automated, high-precision\nsegmentation of cardiac images, extraction of features, calculation of clinical\nindices, and prediction of diseases. The provided segmentation results,\nclinical indices, and prediction outcomes can aid physicians in diagnosis,\nthereby serving as auxiliary diagnostic tools. Experimental results showcase\nthat this semi-supervised model for automatic segmentation of cardiac images\nand auxiliary diagnosis attains high accuracy in segmentation and correctness\nin prediction, demonstrating substantial practical guidance and application\nvalue.",
          "arxiv_id": "2405.14300v1"
        }
      ],
      "49": [
        {
          "title": "Look Before You Match: Instance Understanding Matters in Video Object Segmentation",
          "year": "2022-12",
          "abstract": "Exploring dense matching between the current frame and past frames for\nlong-range context modeling, memory-based methods have demonstrated impressive\nresults in video object segmentation (VOS) recently. Nevertheless, due to the\nlack of instance understanding ability, the above approaches are oftentimes\nbrittle to large appearance variations or viewpoint changes resulted from the\nmovement of objects and cameras. In this paper, we argue that instance\nunderstanding matters in VOS, and integrating it with memory-based matching can\nenjoy the synergy, which is intuitively sensible from the definition of VOS\ntask, \\ie, identifying and segmenting object instances within the video.\nTowards this goal, we present a two-branch network for VOS, where the\nquery-based instance segmentation (IS) branch delves into the instance details\nof the current frame and the VOS branch performs spatial-temporal matching with\nthe memory bank. We employ the well-learned object queries from IS branch to\ninject instance-specific information into the query key, with which the\ninstance-augmented matching is further performed. In addition, we introduce a\nmulti-path fusion block to effectively combine the memory readout with\nmulti-scale features from the instance segmentation decoder, which incorporates\nhigh-resolution instance-aware features to produce final segmentation results.\nOur method achieves state-of-the-art performance on DAVIS 2016/2017 val (92.6%\nand 87.1%), DAVIS 2017 test-dev (82.8%), and YouTube-VOS 2018/2019 val (86.3%\nand 86.3%), outperforming alternative methods by clear margins.",
          "arxiv_id": "2212.06826v1"
        },
        {
          "title": "Region Aware Video Object Segmentation with Deep Motion Modeling",
          "year": "2022-07",
          "abstract": "Current semi-supervised video object segmentation (VOS) methods usually\nleverage the entire features of one frame to predict object masks and update\nmemory. This introduces significant redundant computations. To reduce\nredundancy, we present a Region Aware Video Object Segmentation (RAVOS)\napproach that predicts regions of interest (ROIs) for efficient object\nsegmentation and memory storage. RAVOS includes a fast object motion tracker to\npredict their ROIs in the next frame. For efficient segmentation, object\nfeatures are extracted according to the ROIs, and an object decoder is designed\nfor object-level segmentation. For efficient memory storage, we propose motion\npath memory to filter out redundant context by memorizing the features within\nthe motion path of objects between two frames. Besides RAVOS, we also propose a\nlarge-scale dataset, dubbed OVOS, to benchmark the performance of VOS models\nunder occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new\nOVOS dataset show that our method achieves state-of-the-art performance with\nsignificantly faster inference time, e.g., 86.1 J&F at 42 FPS on DAVIS and 84.4\nJ&F at 23 FPS on YouTube-VOS.",
          "arxiv_id": "2207.10258v1"
        },
        {
          "title": "Temporally Consistent Referring Video Object Segmentation with Hybrid Memory",
          "year": "2024-03",
          "abstract": "Referring Video Object Segmentation (R-VOS) methods face challenges in\nmaintaining consistent object segmentation due to temporal context variability\nand the presence of other visually similar objects. We propose an end-to-end\nR-VOS paradigm that explicitly models temporal instance consistency alongside\nthe referring segmentation. Specifically, we introduce a novel hybrid memory\nthat facilitates inter-frame collaboration for robust spatio-temporal matching\nand propagation. Features of frames with automatically generated high-quality\nreference masks are propagated to segment the remaining frames based on\nmulti-granularity association to achieve temporally consistent R-VOS.\nFurthermore, we propose a new Mask Consistency Score (MCS) metric to evaluate\nthe temporal consistency of video segmentation. Extensive experiments\ndemonstrate that our approach enhances temporal consistency by a significant\nmargin, leading to top-ranked performance on popular R-VOS benchmarks, i.e.,\nRef-YouTube-VOS (67.1%) and Ref-DAVIS17 (65.6%). The code is available at\nhttps://github.com/bo-miao/HTR.",
          "arxiv_id": "2403.19407v2"
        }
      ],
      "50": [
        {
          "title": "Brain Tumor Segmentation from MRI Images using Deep Learning Techniques",
          "year": "2023-04",
          "abstract": "A brain tumor, whether benign or malignant, can potentially be life\nthreatening and requires painstaking efforts in order to identify the type,\norigin and location, let alone cure one. Manual segmentation by medical\nspecialists can be time-consuming, which calls out for the involvement of\ntechnology to hasten the process with high accuracy. For the purpose of medical\nimage segmentation, we inspected and identified the capable deep learning\nmodel, which shows consistent results in the dataset used for brain tumor\nsegmentation. In this study, a public MRI imaging dataset contains 3064\nTI-weighted images from 233 patients with three variants of brain tumor, viz.\nmeningioma, glioma, and pituitary tumor. The dataset files were converted and\npreprocessed before indulging into the methodology which employs implementation\nand training of some well-known image segmentation deep learning models like\nU-Net & Attention U-Net with various backbones, Deep Residual U-Net, ResUnet++\nand Recurrent Residual U-Net. with varying parameters, acquired from our review\nof the literature related to human brain tumor classification and segmentation.\nThe experimental findings showed that among all the applied approaches, the\nrecurrent residual U-Net which uses Adam optimizer reaches a Mean Intersection\nOver Union of 0.8665 and outperforms other compared state-of-the-art deep\nlearning models. The visual findings also show the remarkable results of the\nbrain tumor segmentation from MRI scans and demonstrates how useful the\nalgorithm will be for physicians to extract the brain cancers automatically\nfrom MRI scans and serve humanity.",
          "arxiv_id": "2305.00257v1"
        },
        {
          "title": "Brain Tumor Classification From MRI Images Using Machine Learning",
          "year": "2024-07",
          "abstract": "Brain tumor is a life-threatening problem and hampers the normal functioning\nof the human body. The average five-year relative survival rate for malignant\nbrain tumors is 35.6 percent. For proper diagnosis and efficient treatment\nplanning, it is necessary to detect the brain tumor in early stages. Due to\nadvancement in medical imaging technology, the brain images are taken in\ndifferent modalities. The ability to extract relevant characteristics from\nmagnetic resonance imaging (MRI) scans is a crucial step for brain tumor\nclassifiers. Several studies have proposed various strategies to extract\nrelevant features from different modalities of MRI to predict the growth of\nabnormal tumors. Most techniques used conventional methods of image processing\nfor feature extraction and machine learning for classification. More recently,\nthe use of deep learning algorithms in medical imaging has resulted in\nsignificant improvements in the classification and diagnosis of brain tumors.\nSince tumors are located at different regions of the brain, localizing the\ntumor and classifying it to a particular category is a challenging task. The\nobjective of this project is to develop a predictive system for brain tumor\ndetection using machine learning(ensembling).",
          "arxiv_id": "2407.10630v1"
        },
        {
          "title": "QuickTumorNet: Fast Automatic Multi-Class Segmentation of Brain Tumors",
          "year": "2020-12",
          "abstract": "Non-invasive techniques such as magnetic resonance imaging (MRI) are widely\nemployed in brain tumor diagnostics. However, manual segmentation of brain\ntumors from 3D MRI volumes is a time-consuming task that requires trained\nexpert radiologists. Due to the subjectivity of manual segmentation, there is\nlow inter-rater reliability which can result in diagnostic discrepancies. As\nthe success of many brain tumor treatments depends on early intervention, early\ndetection is paramount. In this context, a fully automated segmentation method\nfor brain tumor segmentation is necessary as an efficient and reliable method\nfor brain tumor detection and quantification. In this study, we propose an\nend-to-end approach for brain tumor segmentation, capitalizing on a modified\nversion of QuickNAT, a brain tissue type segmentation deep convolutional neural\nnetwork (CNN). Our method was evaluated on a data set of 233 patient's T1\nweighted images containing three tumor type classes annotated (meningioma,\nglioma, and pituitary). Our model, QuickTumorNet, demonstrated fast, reliable,\nand accurate brain tumor segmentation that can be utilized to assist clinicians\nin diagnosis and treatment.",
          "arxiv_id": "2012.12410v1"
        }
      ],
      "51": [
        {
          "title": "Strong and Controllable 3D Motion Generation",
          "year": "2025-01",
          "abstract": "Human motion generation is a significant pursuit in generative computer\nvision with widespread applications in film-making, video games, AR/VR, and\nhuman-robot interaction. Current methods mainly utilize either diffusion-based\ngenerative models or autoregressive models for text-to-motion generation.\nHowever, they face two significant challenges: (1) The generation process is\ntime-consuming, posing a major obstacle for real-time applications such as\ngaming, robot manipulation, and other online settings. (2) These methods\ntypically learn a relative motion representation guided by text, making it\ndifficult to generate motion sequences with precise joint-level control. These\nchallenges significantly hinder progress and limit the real-world application\nof human motion generation techniques. To address this gap, we propose a simple\nyet effective architecture consisting of two key components. Firstly, we aim to\nimprove hardware efficiency and computational complexity in transformer-based\ndiffusion models for human motion generation. By customizing flash linear\nattention, we can optimize these models specifically for generating human\nmotion efficiently. Furthermore, we will customize the consistency model in the\nmotion latent space to further accelerate motion generation. Secondly, we\nintroduce Motion ControlNet, which enables more precise joint-level control of\nhuman motion compared to previous text-to-motion generation methods. These\ncontributions represent a significant advancement for text-to-motion\ngeneration, bringing it closer to real-world applications.",
          "arxiv_id": "2501.18726v1"
        },
        {
          "title": "MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model",
          "year": "2022-08",
          "abstract": "Human motion modeling is important for many modern graphics applications,\nwhich typically require professional skills. In order to remove the skill\nbarriers for laymen, recent motion generation methods can directly generate\nhuman motions conditioned on natural languages. However, it remains challenging\nto achieve diverse and fine-grained motion generation with various text inputs.\nTo address this problem, we propose MotionDiffuse, the first diffusion\nmodel-based text-driven motion generation framework, which demonstrates several\ndesired properties over existing methods. 1) Probabilistic Mapping. Instead of\na deterministic language-motion mapping, MotionDiffuse generates motions\nthrough a series of denoising steps in which variations are injected. 2)\nRealistic Synthesis. MotionDiffuse excels at modeling complicated data\ndistribution and generating vivid motion sequences. 3) Multi-Level\nManipulation. MotionDiffuse responds to fine-grained instructions on body\nparts, and arbitrary-length motion synthesis with time-varied text prompts. Our\nexperiments show MotionDiffuse outperforms existing SoTA methods by convincing\nmargins on text-driven motion generation and action-conditioned motion\ngeneration. A qualitative analysis further demonstrates MotionDiffuse's\ncontrollability for comprehensive motion generation. Homepage:\nhttps://mingyuan-zhang.github.io/projects/MotionDiffuse.html",
          "arxiv_id": "2208.15001v1"
        },
        {
          "title": "MotionGPT: Human Motion as a Foreign Language",
          "year": "2023-06",
          "abstract": "Though the advancement of pre-trained large language models unfolds, the\nexploration of building a unified model for language and other multi-modal\ndata, such as motion, remains challenging and untouched so far. Fortunately,\nhuman motion displays a semantic coupling akin to human language, often\nperceived as a form of body language. By fusing language data with large-scale\nmotion models, motion-language pre-training that can enhance the performance of\nmotion-related tasks becomes feasible. Driven by this insight, we propose\nMotionGPT, a unified, versatile, and user-friendly motion-language model to\nhandle multiple motion-relevant tasks. Specifically, we employ the discrete\nvector quantization for human motion and transfer 3D motion into motion tokens,\nsimilar to the generation process of word tokens. Building upon this \"motion\nvocabulary\", we perform language modeling on both motion and text in a unified\nmanner, treating human motion as a specific language. Moreover, inspired by\nprompt learning, we pre-train MotionGPT with a mixture of motion-language data\nand fine-tune it on prompt-based question-and-answer tasks. Extensive\nexperiments demonstrate that MotionGPT achieves state-of-the-art performances\non multiple motion tasks including text-driven motion generation, motion\ncaptioning, motion prediction, and motion in-between.",
          "arxiv_id": "2306.14795v2"
        }
      ],
      "52": [
        {
          "title": "PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation",
          "year": "2024-09",
          "abstract": "While previous audio-driven talking head generation (THG) methods generate\nhead poses from driving audio, the generated poses or lips cannot match the\naudio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a\nTHG system that can freely generate lip-synchronized talking head videos with\nfree head poses conditioned on text prompts and audio. The core insight of our\nmethod is using head pose to connect visual, linguistic, and audio signals.\nFirst, we propose to generate poses from both audio and text prompts, where the\naudio offers short-term variations and rhythm correspondence of the head\nmovements and the text prompts describe the long-term semantics of head\nmotions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to\ngenerate motion latent from text prompts and audio cues in a pose latent space.\nSecond, we observe a loss-imbalance problem: the loss for the lip region\ncontributes less than 4\\% of the total reconstruction loss caused by both pose\nand lip, making optimization lean towards head movements rather than lip\nshapes. To address this issue, we propose a refinement-based learning strategy\nto synthesize natural talking videos using two cascaded networks, i.e.,\nCoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce\nanimated images in novel poses and the RefineNet focuses on learning finer lip\nmotions by progressively estimating lip motions from low-to-high resolutions,\nyielding improved lip-synchronization performance. Experiments demonstrate our\npose prediction strategy achieves better pose diversity and realness compared\nto text-only or audio-only, and our video generator model outperforms\nstate-of-the-art methods in synthesizing talking videos with natural head\nmotions. Project: https://junleen.github.io/projects/posetalk.",
          "arxiv_id": "2409.02657v1"
        },
        {
          "title": "Learn2Talk: 3D Talking Face Learns from 2D Talking Face",
          "year": "2024-04",
          "abstract": "Speech-driven facial animation methods usually contain two main classes, 3D\nand 2D talking face, both of which attract considerable research attention in\nrecent years. However, to the best of our knowledge, the research on 3D talking\nface does not go deeper as 2D talking face, in the aspect of\nlip-synchronization (lip-sync) and speech perception. To mind the gap between\nthe two sub-fields, we propose a learning framework named Learn2Talk, which can\nconstruct a better 3D talking face network by exploiting two expertise points\nfrom the field of 2D talking face. Firstly, inspired by the audio-video sync\nnetwork, a 3D sync-lip expert model is devised for the pursuit of lip-sync\nbetween audio and 3D facial motion. Secondly, a teacher model selected from 2D\ntalking face methods is used to guide the training of the audio-to-3D motions\nregression network to yield more 3D vertex accuracy. Extensive experiments show\nthe advantages of the proposed framework in terms of lip-sync, vertex accuracy\nand speech perception, compared with state-of-the-arts. Finally, we show two\napplications of the proposed framework: audio-visual speech recognition and\nspeech-driven 3D Gaussian Splatting based avatar animation.",
          "arxiv_id": "2404.12888v1"
        },
        {
          "title": "StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation",
          "year": "2022-08",
          "abstract": "We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.",
          "arxiv_id": "2208.10922v2"
        }
      ],
      "53": [
        {
          "title": "Does pre-training on brain-related tasks results in better deep-learning-based brain age biomarkers?",
          "year": "2023-07",
          "abstract": "Brain age prediction using neuroimaging data has shown great potential as an\nindicator of overall brain health and successful aging, as well as a disease\nbiomarker. Deep learning models have been established as reliable and efficient\nbrain age estimators, being trained to predict the chronological age of healthy\nsubjects. In this paper, we investigate the impact of a pre-training step on\ndeep learning models for brain age prediction. More precisely, instead of the\ncommon approach of pre-training on natural imaging classification, we propose\npre-training the models on brain-related tasks, which led to state-of-the-art\nresults in our experiments on ADNI data. Furthermore, we validate the resulting\nbrain age biomarker on images of patients with mild cognitive impairment and\nAlzheimer's disease. Interestingly, our results indicate that better-performing\ndeep learning models in terms of brain age prediction on healthy patients do\nnot result in more reliable biomarkers.",
          "arxiv_id": "2307.05241v1"
        },
        {
          "title": "Altered Topological Properties of Functional Brain Network Associated with Alzheimer's Disease",
          "year": "2023-05",
          "abstract": "Functional Magnetic Resonance Imaging (fMRI) is commonly utilized to study\nhuman brain activity, including abnormal functional properties related to\nneurodegenerative diseases. This study aims to investigate the differences in\nthe topological properties of functional brain networks between individuals\nwith Alzheimer's Disease (AD) and normal controls. A total of 590 subjects,\nconsisting of 175 with AD dementia and 415 age-, gender-, and\nhandedness-matched controls, were included. The topological properties of the\nbrain network were quantified using graph-theory-based analyses. The results\nindicate abnormal network integration and segregation in the AD group. These\nfindings enhance our understanding of AD pathophysiology from a functional\nbrain network structure perspective and may aid in identifying AD biomarkers.\nSupplementary data to aid in the validation of this research are available at\nhttps://github.com/YongchengYAO/AD-FunctionalBrainNetwork.",
          "arxiv_id": "2305.08159v2"
        },
        {
          "title": "Brain-Aware Readout Layers in GNNs: Advancing Alzheimer's early Detection and Neuroimaging",
          "year": "2024-10",
          "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder characterized by\nprogressive memory and cognitive decline, affecting millions worldwide.\nDiagnosing AD is challenging due to its heterogeneous nature and variable\nprogression. This study introduces a novel brain-aware readout layer (BA\nreadout layer) for Graph Neural Networks (GNNs), designed to improve\ninterpretability and predictive accuracy in neuroimaging for early AD\ndiagnosis. By clustering brain regions based on functional connectivity and\nnode embedding, this layer improves the GNN's capability to capture complex\nbrain network characteristics. We analyzed neuroimaging data from 383\nparticipants, including both cognitively normal and preclinical AD individuals,\nusing T1-weighted MRI, resting-state fMRI, and FBB-PET to construct brain\ngraphs. Our results show that GNNs with the BA readout layer significantly\noutperform traditional models in predicting the Preclinical Alzheimer's\nCognitive Composite (PACC) score, demonstrating higher robustness and\nstability. The adaptive BA readout layer also offers enhanced interpretability\nby highlighting task-specific brain regions critical to cognitive functions\nimpacted by AD. These findings suggest that our approach provides a valuable\ntool for the early diagnosis and analysis of Alzheimer's disease.",
          "arxiv_id": "2410.14683v1"
        }
      ],
      "54": [
        {
          "title": "OffsetOPT: Explicit Surface Reconstruction without Normals",
          "year": "2025-03",
          "abstract": "Neural surface reconstruction has been dominated by implicit representations\nwith marching cubes for explicit surface extraction. However, those methods\ntypically require high-quality normals for accurate reconstruction. We propose\nOffsetOPT, a method that reconstructs explicit surfaces directly from 3D point\nclouds and eliminates the need for point normals. The approach comprises two\nstages: first, we train a neural network to predict surface triangles based on\nlocal point geometry, given uniformly distributed training point clouds. Next,\nwe apply the frozen network to reconstruct surfaces from unseen point clouds by\noptimizing a per-point offset to maximize the accuracy of triangle predictions.\nCompared to state-of-the-art methods, OffsetOPT not only excels at\nreconstructing overall surfaces but also significantly preserves sharp surface\nfeatures. We demonstrate its accuracy on popular benchmarks, including\nsmall-scale shapes and large-scale open surfaces.",
          "arxiv_id": "2503.15763v1"
        },
        {
          "title": "Shape As Points: A Differentiable Poisson Solver",
          "year": "2021-06",
          "abstract": "In recent years, neural implicit representations gained popularity in 3D\nreconstruction due to their expressiveness and flexibility. However, the\nimplicit nature of neural implicit representations results in slow inference\ntime and requires careful initialization. In this paper, we revisit the classic\nyet ubiquitous point cloud representation and introduce a differentiable\npoint-to-mesh layer using a differentiable formulation of Poisson Surface\nReconstruction (PSR) that allows for a GPU-accelerated fast solution of the\nindicator function given an oriented point cloud. The differentiable PSR layer\nallows us to efficiently and differentiably bridge the explicit 3D point\nrepresentation with the 3D mesh via the implicit indicator field, enabling\nend-to-end optimization of surface reconstruction metrics such as Chamfer\ndistance. This duality between points and meshes hence allows us to represent\nshapes as oriented point clouds, which are explicit, lightweight and\nexpressive. Compared to neural implicit representations, our Shape-As-Points\n(SAP) model is more interpretable, lightweight, and accelerates inference time\nby one order of magnitude. Compared to other explicit representations such as\npoints, patches, and meshes, SAP produces topology-agnostic, watertight\nmanifold surfaces. We demonstrate the effectiveness of SAP on the task of\nsurface reconstruction from unoriented point clouds and learning-based\nreconstruction.",
          "arxiv_id": "2106.03452v2"
        },
        {
          "title": "Learning Occupancy Function from Point Clouds for Surface Reconstruction",
          "year": "2020-10",
          "abstract": "Implicit function based surface reconstruction has been studied for a long\ntime to recover 3D shapes from point clouds sampled from surfaces. Recently,\nSigned Distance Functions (SDFs) and Occupany Functions are adopted in\nlearning-based shape reconstruction methods as implicit 3D shape\nrepresentation. This paper proposes a novel method for learning occupancy\nfunctions from sparse point clouds and achieves better performance on\nchallenging surface reconstruction tasks. Unlike the previous methods, which\npredict point occupancy with fully-connected multi-layer networks, we adapt the\npoint cloud deep learning architecture, Point Convolution Neural Network\n(PCNN), to build our learning model. Specifically, we create a sampling\noperator and insert it into PCNN to continuously sample the feature space at\nthe points where occupancy states need to be predicted. This method natively\nobtains point cloud data's geometric nature, and it's invariant to point\npermutation. Our occupancy function learning can be easily fit into procedures\nof point cloud up-sampling and surface reconstruction. Our experiments show\nstate-of-the-art performance for reconstructing With ShapeNet dataset and\ndemonstrate this method's well-generalization by testing it with McGill 3D\ndataset \\cite{siddiqi2008retrieving}. Moreover, we find the learned occupancy\nfunction is relatively more rotation invariant than previous shape learning\nmethods.",
          "arxiv_id": "2010.11378v1"
        }
      ],
      "55": [
        {
          "title": "Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need",
          "year": "2023-02",
          "abstract": "The core of out-of-distribution (OOD) detection is to learn the\nin-distribution (ID) representation, which is distinguishable from OOD samples.\nPrevious work applied recognition-based methods to learn the ID features, which\ntend to learn shortcuts instead of comprehensive representations. In this work,\nwe find surprisingly that simply using reconstruction-based methods could boost\nthe performance of OOD detection significantly. We deeply explore the main\ncontributors of OOD detection and find that reconstruction-based pretext tasks\nhave the potential to provide a generally applicable and efficacious prior,\nwhich benefits the model in learning intrinsic data distributions of the ID\ndataset. Specifically, we take Masked Image Modeling as a pretext task for our\nOOD detection framework (MOOD). Without bells and whistles, MOOD outperforms\nprevious SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by\n3.0%, and near-distribution OOD detection by 2.1%. It even defeats the\n10-shot-per-class outlier exposure OOD detection, although we do not include\nany OOD samples for our detection",
          "arxiv_id": "2302.02615v2"
        },
        {
          "title": "Unsupervised Evaluation of Out-of-distribution Detection: A Data-centric Perspective",
          "year": "2023-02",
          "abstract": "Out-of-distribution (OOD) detection methods assume that they have test ground\ntruths, i.e., whether individual test samples are in-distribution (IND) or OOD.\nHowever, in the real world, we do not always have such ground truths, and thus\ndo not know which sample is correctly detected and cannot compute the metric\nlike AUROC to evaluate the performance of different OOD detection methods. In\nthis paper, we are the first to introduce the unsupervised evaluation problem\nin OOD detection, which aims to evaluate OOD detection methods in real-world\nchanging environments without OOD labels. We propose three methods to compute\nGscore as an unsupervised indicator of OOD detection performance. We further\nintroduce a new benchmark Gbench, which has 200 real-world OOD datasets of\nvarious label spaces to train and evaluate our method. Through experiments, we\nfind a strong quantitative correlation betwwen Gscore and the OOD detection\nperformance. Extensive experiments demonstrate that our Gscore achieves\nstate-of-the-art performance. Gscore also generalizes well with different\nIND/OOD datasets, OOD detection methods, backbones and dataset sizes. We\nfurther provide interesting analyses of the effects of backbones and IND/OOD\ndatasets on OOD detection performance. The data and code will be available.",
          "arxiv_id": "2302.08287v2"
        },
        {
          "title": "In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation",
          "year": "2023-06",
          "abstract": "Out-of-distribution (OOD) detection is the problem of identifying inputs\nwhich are unrelated to the in-distribution task. The OOD detection performance\nwhen the in-distribution (ID) is ImageNet-1K is commonly being tested on a\nsmall range of test OOD datasets. We find that most of the currently used test\nOOD datasets, including datasets from the open set recognition (OSR)\nliterature, have severe issues: In some cases more than 50$\\%$ of the dataset\ncontains objects belonging to one of the ID classes. These erroneous samples\nheavily distort the evaluation of OOD detectors. As a solution, we introduce\nwith NINCO a novel test OOD dataset, each sample checked to be ID free, which\nwith its fine-grained range of OOD classes allows for a detailed analysis of an\nOOD detector's strengths and failure modes, particularly when paired with a\nnumber of synthetic \"OOD unit-tests\". We provide detailed evaluations across a\nlarge set of architectures and OOD detection methods on NINCO and the\nunit-tests, revealing new insights about model weaknesses and the effects of\npretraining on OOD detection performance. We provide code and data at\nhttps://github.com/j-cb/NINCO.",
          "arxiv_id": "2306.00826v1"
        }
      ],
      "56": [
        {
          "title": "LAVSS: Location-Guided Audio-Visual Spatial Audio Separation",
          "year": "2023-10",
          "abstract": "Existing machine learning research has achieved promising results in monaural\naudio-visual separation (MAVS). However, most MAVS methods purely consider what\nthe sound source is, not where it is located. This can be a problem in VR/AR\nscenarios, where listeners need to be able to distinguish between similar audio\nsources located in different directions. To address this limitation, we have\ngeneralized MAVS to spatial audio separation and proposed LAVSS: a\nlocation-guided audio-visual spatial audio separator. LAVSS is inspired by the\ncorrelation between spatial audio and visual location. We introduce the phase\ndifference carried by binaural audio as spatial cues, and we utilize positional\nrepresentations of sounding objects as additional modality guidance. We also\nleverage multi-level cross-modal attention to perform visual-positional\ncollaboration with audio features. In addition, we adopt a pre-trained monaural\nseparator to transfer knowledge from rich mono sounds to boost spatial audio\nseparation. This exploits the correlation between monaural and binaural\nchannels. Experiments on the FAIR-Play dataset demonstrate the superiority of\nthe proposed LAVSS over existing benchmarks of audio-visual separation. Our\nproject page: https://yyx666660.github.io/LAVSS/.",
          "arxiv_id": "2310.20446v1"
        },
        {
          "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation",
          "year": "2024-09",
          "abstract": "Video encompasses both visual and auditory data, creating a perceptually rich\nexperience where these two modalities complement each other. As such, videos\nare a valuable type of media for the investigation of the interplay between\naudio and visual elements. Previous studies of audio-visual modalities\nprimarily focused on either audio-visual representation learning or generative\nmodeling of a modality conditioned on the other, creating a disconnect between\nthese two branches. A unified framework that learns representation and\ngenerates modalities has not been developed yet. In this work, we introduce a\nnovel framework called Vision to Audio and Beyond (VAB) to bridge the gap\nbetween audio-visual representation learning and vision-to-audio generation.\nThe key approach of VAB is that rather than working with raw video frames and\naudio data, VAB performs representation learning and generative modeling within\nlatent spaces. In particular, VAB uses a pre-trained audio tokenizer and an\nimage encoder to obtain audio tokens and visual features, respectively. It then\nperforms the pre-training task of visual-conditioned masked audio token\nprediction. This training strategy enables the model to engage in contextual\nlearning and simultaneous video-to-audio generation. After the pre-training\nphase, VAB employs the iterative-decoding approach to rapidly generate audio\ntokens conditioned on visual features. Since VAB is a unified model, its\nbackbone can be fine-tuned for various audio-visual downstream tasks. Our\nexperiments showcase the efficiency of VAB in producing high-quality audio from\nvideo, and its capability to acquire semantic audio-visual features, leading to\ncompetitive results in audio-visual retrieval and classification.",
          "arxiv_id": "2409.19132v1"
        },
        {
          "title": "BAVS: Bootstrapping Audio-Visual Segmentation by Integrating Foundation Knowledge",
          "year": "2023-08",
          "abstract": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate\nsounding sources by predicting pixel-wise maps. Previous methods assume that\neach sound component in an audio signal always has a visual counterpart in the\nimage. However, this assumption overlooks that off-screen sounds and background\nnoise often contaminate the audio recordings in real-world scenarios. They\nimpose significant challenges on building a consistent semantic mapping between\naudio and visual signals for AVS models and thus impede precise sound\nlocalization. In this work, we propose a two-stage bootstrapping audio-visual\nsegmentation framework by incorporating multi-modal foundation knowledge. In a\nnutshell, our BAVS is designed to eliminate the interference of background\nnoise or off-screen sounds in segmentation by establishing the audio-visual\ncorrespondences in an explicit manner. In the first stage, we employ a\nsegmentation model to localize potential sounding objects from visual data\nwithout being affected by contaminated audio signals. Meanwhile, we also\nutilize a foundation audio classification model to discern audio semantics.\nConsidering the audio tags provided by the audio foundation model are noisy,\nassociating object masks with audio tags is not trivial. Thus, in the second\nstage, we develop an audio-visual semantic integration strategy (AVIS) to\nlocalize the authentic-sounding objects. Here, we construct an audio-visual\ntree based on the hierarchical correspondence between sounds and object\ncategories. We then examine the label concurrency between the localized objects\nand classified audio tags by tracing the audio-visual tree. With AVIS, we can\neffectively segment real-sounding objects. Extensive experiments demonstrate\nthe superiority of our method on AVS datasets, particularly in scenarios\ninvolving background noise. Our project website is\nhttps://yenanliu.github.io/AVSS.github.io/.",
          "arxiv_id": "2308.10175v1"
        }
      ],
      "57": [
        {
          "title": "Evaluating Deep Neural Networks Trained on Clinical Images in Dermatology with the Fitzpatrick 17k Dataset",
          "year": "2021-04",
          "abstract": "How does the accuracy of deep neural network models trained to classify\nclinical images of skin conditions vary across skin color? While recent studies\ndemonstrate computer vision models can serve as a useful decision support tool\nin healthcare and provide dermatologist-level classification on a number of\nspecific tasks, darker skin is underrepresented in the data. Most publicly\navailable data sets do not include Fitzpatrick skin type labels. We annotate\n16,577 clinical images sourced from two dermatology atlases with Fitzpatrick\nskin type labels and open-source these annotations. Based on these labels, we\nfind that there are significantly more images of light skin types than dark\nskin types in this dataset. We train a deep neural network model to classify\n114 skin conditions and find that the model is most accurate on skin types\nsimilar to those it was trained on. In addition, we evaluate how an algorithmic\napproach to identifying skin tones, individual typology angle, compares with\nFitzpatrick skin type labels annotated by a team of human labelers.",
          "arxiv_id": "2104.09957v1"
        },
        {
          "title": "Multi-class Skin Cancer Classification Architecture Based on Deep Convolutional Neural Network",
          "year": "2023-03",
          "abstract": "Skin cancer detection is challenging since different types of skin lesions\nshare high similarities. This paper proposes a computer-based deep learning\napproach that will accurately identify different kinds of skin lesions. Deep\nlearning approaches can detect skin cancer very accurately since the models\nlearn each pixel of an image. Sometimes humans can get confused by the\nsimilarities of the skin lesions, which we can minimize by involving the\nmachine. However, not all deep learning approaches can give better predictions.\nSome deep learning models have limitations, leading the model to a\nfalse-positive result. We have introduced several deep learning models to\nclassify skin lesions to distinguish skin cancer from different types of skin\nlesions. Before classifying the skin lesions, data preprocessing and data\naugmentation methods are used. Finally, a Convolutional Neural Network (CNN)\nmodel and six transfer learning models such as Resnet-50, VGG-16, Densenet,\nMobilenet, Inceptionv3, and Xception are applied to the publically available\nbenchmark HAM10000 dataset to classify seven classes of skin lesions and to\nconduct a comparative analysis. The models will detect skin cancer by\ndifferentiating the cancerous cell from the non-cancerous ones. The models\nperformance is measured using performance metrics such as precision, recall, f1\nscore, and accuracy. We receive accuracy of 90, 88, 88, 87, 82, and 77 percent\nfor inceptionv3, Xception, Densenet, Mobilenet, Resnet, CNN, and VGG16,\nrespectively. Furthermore, we develop five different stacking models such as\ninceptionv3-inceptionv3, Densenet-mobilenet, inceptionv3-Xception,\nResnet50-Vgg16, and stack-six for classifying the skin lesions and found that\nthe stacking models perform poorly. We achieve the highest accuracy of 78\npercent among all the stacking models.",
          "arxiv_id": "2303.07520v1"
        },
        {
          "title": "Assessing the Generalizability of Deep Neural Networks-Based Models for Black Skin Lesions",
          "year": "2023-09",
          "abstract": "Melanoma is the most severe type of skin cancer due to its ability to cause\nmetastasis. It is more common in black people, often affecting acral regions:\npalms, soles, and nails. Deep neural networks have shown tremendous potential\nfor improving clinical care and skin cancer diagnosis. Nevertheless, prevailing\nstudies predominantly rely on datasets of white skin tones, neglecting to\nreport diagnostic outcomes for diverse patient skin tones. In this work, we\nevaluate supervised and self-supervised models in skin lesion images extracted\nfrom acral regions commonly observed in black individuals. Also, we carefully\ncurate a dataset containing skin lesions in acral regions and assess the\ndatasets concerning the Fitzpatrick scale to verify performance on black skin.\nOur results expose the poor generalizability of these models, revealing their\nfavorable performance for lesions on white skin. Neglecting to create diverse\ndatasets, which necessitates the development of specialized models, is\nunacceptable. Deep neural networks have great potential to improve diagnosis,\nparticularly for populations with limited access to dermatology. However,\nincluding black skin lesions is necessary to ensure these populations can\naccess the benefits of inclusive technology.",
          "arxiv_id": "2310.00517v2"
        }
      ],
      "58": [
        {
          "title": "GANs for Medical Image Synthesis: An Empirical Study",
          "year": "2021-05",
          "abstract": "Generative Adversarial Networks (GANs) have become increasingly powerful,\ngenerating mind-blowing photorealistic images that mimic the content of\ndatasets they were trained to replicate. One recurrent theme in medical imaging\nis whether GANs can also be effective at generating workable medical data as\nthey are for generating realistic RGB images. In this paper, we perform a\nmulti-GAN and multi-application study to gauge the benefits of GANs in medical\nimaging. We tested various GAN architectures from basic DCGAN to more\nsophisticated style-based GANs on three medical imaging modalities and organs\nnamely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on\nwell-known and widely utilized datasets from which their FID score were\ncomputed to measure the visual acuity of their generated images. We further\ntested their usefulness by measuring the segmentation accuracy of a U-Net\ntrained on these generated images.\n  Results reveal that GANs are far from being equal as some are ill-suited for\nmedical imaging applications while others are much better off. The\ntop-performing GANs are capable of generating realistic-looking medical images\nby FID standards that can fool trained experts in a visual Turing test and\ncomply to some metrics. However, segmentation results suggests that no GAN is\ncapable of reproducing the full richness of a medical datasets.",
          "arxiv_id": "2105.05318v2"
        },
        {
          "title": "seg2med: a bridge from artificial anatomy to multimodal medical images",
          "year": "2025-04",
          "abstract": "We present seg2med, a modular framework for anatomy-driven multimodal medical\nimage synthesis. The system integrates three components to enable\nhigh-fidelity, cross-modality generation of CT and MR images based on\nstructured anatomical priors. First, anatomical maps are independently derived\nfrom three sources: real patient data, XCAT digital phantoms, and synthetic\nanatomies created by combining organs from multiple patients. Second, we\nintroduce PhysioSynth, a modality-specific simulator that converts anatomical\nmasks into prior volumes using tissue-dependent parameters (e.g., HU, T1, T2,\nproton density) and modality-specific signal models. It supports simulation of\nCT and multiple MR sequences including GRE, SPACE, and VIBE. Third, the\nsynthesized anatomical priors are used to train 2-channel conditional denoising\ndiffusion models, which take the anatomical prior as structural condition\nalongside the noisy image, enabling generation of high-quality, structurally\naligned images. The framework achieves SSIM of 0.94 for CT and 0.89 for MR\ncompared to real data, and FSIM of 0.78 for simulated CT. The generative\nquality is further supported by a Frechet Inception Distance (FID) of 3.62 for\nCT synthesis. In modality conversion, seg2med achieves SSIM of 0.91 for MR to\nCT and 0.77 for CT to MR. Anatomical fidelity evaluation shows synthetic CT\nachieves mean Dice scores above 0.90 for 11 key abdominal organs, and above\n0.80 for 34 of 59 total organs. These results underscore seg2med's utility in\ncross-modality synthesis, data augmentation, and anatomy-aware medical AI.",
          "arxiv_id": "2504.09182v2"
        },
        {
          "title": "Structurally aware bidirectional unpaired image to image translation between CT and MR",
          "year": "2020-06",
          "abstract": "Magnetic Resonance (MR) Imaging and Computed Tomography (CT) are the primary\ndiagnostic imaging modalities quite frequently used for surgical planning and\nanalysis. A general problem with medical imaging is that the acquisition\nprocess is quite expensive and time-consuming. Deep learning techniques like\ngenerative adversarial networks (GANs) can help us to leverage the possibility\nof an image to image translation between multiple imaging modalities, which in\nturn helps in saving time and cost. These techniques will help to conduct\nsurgical planning under CT with the feedback of MRI information. While previous\nstudies have shown paired and unpaired image synthesis from MR to CT, image\nsynthesis from CT to MR still remains a challenge, since it involves the\naddition of extra tissue information. In this manuscript, we have implemented\ntwo different variations of Generative Adversarial Networks exploiting the\ncycling consistency and structural similarity between both CT and MR image\nmodalities on a pelvis dataset, thus facilitating a bidirectional exchange of\ncontent and style between these image modalities. The proposed GANs translate\nthe input medical images by different mechanisms, and hence generated images\nnot only appears realistic but also performs well across various comparison\nmetrics, and these images have also been cross verified with a radiologist. The\nradiologist verification has shown that slight variations in generated MR and\nCT images may not be exactly the same as their true counterpart but it can be\nused for medical purposes.",
          "arxiv_id": "2006.03374v1"
        }
      ],
      "59": [
        {
          "title": "Dynamic Distinction Learning: Adaptive Pseudo Anomalies for Video Anomaly Detection",
          "year": "2024-04",
          "abstract": "We introduce Dynamic Distinction Learning (DDL) for Video Anomaly Detection,\na novel video anomaly detection methodology that combines pseudo-anomalies,\ndynamic anomaly weighting, and a distinction loss function to improve detection\naccuracy. By training on pseudo-anomalies, our approach adapts to the\nvariability of normal and anomalous behaviors without fixed anomaly thresholds.\nOur model showcases superior performance on the Ped2, Avenue and ShanghaiTech\ndatasets, where individual models are tailored for each scene. These\nachievements highlight DDL's effectiveness in advancing anomaly detection,\noffering a scalable and adaptable solution for video surveillance challenges.",
          "arxiv_id": "2404.04986v1"
        },
        {
          "title": "Weakly Supervised Video Anomaly Detection via Center-guided Discriminative Learning",
          "year": "2021-04",
          "abstract": "Anomaly detection in surveillance videos is a challenging task due to the\ndiversity of anomalous video content and duration. In this paper, we consider\nvideo anomaly detection as a regression problem with respect to anomaly scores\nof video clips under weak supervision. Hence, we propose an anomaly detection\nframework, called Anomaly Regression Net (AR-Net), which only requires\nvideo-level labels in training stage. Further, to learn discriminative features\nfor anomaly detection, we design a dynamic multiple-instance learning loss and\na center loss for the proposed AR-Net. The former is used to enlarge the\ninter-class distance between anomalous and normal instances, while the latter\nis proposed to reduce the intra-class distance of normal instances.\nComprehensive experiments are performed on a challenging benchmark:\nShanghaiTech. Our method yields a new state-of-the-art result for video anomaly\ndetection on ShanghaiTech dataset",
          "arxiv_id": "2104.07268v1"
        },
        {
          "title": "Anomaly Detection in Video Sequences: A Benchmark and Computational Model",
          "year": "2021-06",
          "abstract": "Anomaly detection has attracted considerable search attention. However,\nexisting anomaly detection databases encounter two major problems. Firstly,\nthey are limited in scale. Secondly, training sets contain only video-level\nlabels indicating the existence of an abnormal event during the full video\nwhile lacking annotations of precise time durations. To tackle these problems,\nwe contribute a new Large-scale Anomaly Detection (LAD) database as the\nbenchmark for anomaly detection in video sequences, which is featured in two\naspects. 1) It contains 2000 video sequences including normal and abnormal\nvideo clips with 14 anomaly categories including crash, fire, violence, etc.\nwith large scene varieties, making it the largest anomaly analysis database to\ndate. 2) It provides the annotation data, including video-level labels\n(abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal\nvideo frame) to facilitate anomaly detection. Leveraging the above benefits\nfrom the LAD database, we further formulate anomaly detection as a\nfully-supervised learning problem and propose a multi-task deep neural network\nto solve it. We first obtain the local spatiotemporal contextual feature by\nusing an Inflated 3D convolutional (I3D) network. Then we construct a recurrent\nconvolutional neural network fed the local spatiotemporal contextual feature to\nextract the spatiotemporal contextual feature. With the global spatiotemporal\ncontextual feature, the anomaly type and score can be computed simultaneously\nby a multi-task neural network. Experimental results show that the proposed\nmethod outperforms the state-of-the-art anomaly detection methods on our\ndatabase and other public databases of anomaly detection. Codes are available\nat https://github.com/wanboyang/anomaly_detection_LAD2000.",
          "arxiv_id": "2106.08570v1"
        }
      ],
      "60": [
        {
          "title": "WUDA: Unsupervised Domain Adaptation Based on Weak Source Domain Labels",
          "year": "2022-10",
          "abstract": "Unsupervised domain adaptation (UDA) for semantic segmentation addresses the\ncross-domain problem with fine source domain labels. However, the acquisition\nof semantic labels has always been a difficult step, many scenarios only have\nweak labels (e.g. bounding boxes). For scenarios where weak supervision and\ncross-domain problems coexist, this paper defines a new task: unsupervised\ndomain adaptation based on weak source domain labels (WUDA). To explore\nsolutions for this task, this paper proposes two intuitive frameworks: 1)\nPerform weakly supervised semantic segmentation in the source domain, and then\nimplement unsupervised domain adaptation; 2) Train an object detection model\nusing source domain data, then detect objects in the target domain and\nimplement weakly supervised semantic segmentation. We observe that the two\nframeworks behave differently when the datasets change. Therefore, we construct\ndataset pairs with a wide range of domain shifts and conduct extended\nexperiments to analyze the impact of different domain shifts on the two\nframeworks. In addition, to measure domain shift, we apply the metric\nrepresentation shift to urban landscape image segmentation for the first time.\nThe source code and constructed datasets are available at\n\\url{https://github.com/bupt-ai-cz/WUDA}.",
          "arxiv_id": "2210.02088v1"
        },
        {
          "title": "Unsupervised BatchNorm Adaptation (UBNA): A Domain Adaptation Method for Semantic Segmentation Without Using Source Domain Representations",
          "year": "2020-11",
          "abstract": "In this paper we present a solution to the task of \"unsupervised domain\nadaptation (UDA) of a given pre-trained semantic segmentation model without\nrelying on any source domain representations\". Previous UDA approaches for\nsemantic segmentation either employed simultaneous training of the model in the\nsource and target domains, or they relied on an additional network, replaying\nsource domain knowledge to the model during adaptation. In contrast, we present\nour novel Unsupervised BatchNorm Adaptation (UBNA) method, which adapts a given\npre-trained model to an unseen target domain without using -- beyond the\nexisting model parameters from pre-training -- any source domain\nrepresentations (neither data, nor networks) and which can also be applied in\nan online setting or using just a few unlabeled images from the target domain\nin a few-shot manner. Specifically, we partially adapt the normalization layer\nstatistics to the target domain using an exponentially decaying momentum\nfactor, thereby mixing the statistics from both domains. By evaluation on\nstandard UDA benchmarks for semantic segmentation we show that this is superior\nto a model without adaptation and to baseline approaches using statistics from\nthe target domain only. Compared to standard UDA approaches we report a\ntrade-off between performance and usage of source domain representations.",
          "arxiv_id": "2011.08502v2"
        },
        {
          "title": "Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation",
          "year": "2021-03",
          "abstract": "Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models\ntrained on multiple labeled source domains to an unlabeled target domain. In\nthis paper, we propose a novel multi-source domain adaptation framework based\non collaborative learning for semantic segmentation. Firstly, a simple image\ntranslation method is introduced to align the pixel value distribution to\nreduce the gap between source domains and target domain to some extent. Then,\nto fully exploit the essential semantic information across source domains, we\npropose a collaborative learning method for domain adaptation without seeing\nany data from target domain. In addition, similar to the setting of\nunsupervised domain adaptation, unlabeled target domain data is leveraged to\nfurther improve the performance of domain adaptation. This is achieved by\nadditionally constraining the outputs of multiple adaptation models with pseudo\nlabels online generated by an ensembled model. Extensive experiments and\nablation studies are conducted on the widely-used domain adaptation benchmark\ndatasets in semantic segmentation. Our proposed method achieves 59.0\\% mIoU on\nthe validation set of Cityscapes by training on the labeled Synscapes and GTA5\ndatasets and unlabeled training set of Cityscapes. It significantly outperforms\nall previous state-of-the-arts single-source and multi-source unsupervised\ndomain adaptation methods.",
          "arxiv_id": "2103.04717v3"
        }
      ],
      "61": [
        {
          "title": "Inpainting-Driven Mask Optimization for Object Removal",
          "year": "2024-03",
          "abstract": "This paper proposes a mask optimization method for improving the quality of\nobject removal using image inpainting. While many inpainting methods are\ntrained with a set of random masks, a target for inpainting may be an object,\nsuch as a person, in many realistic scenarios. This domain gap between masks in\ntraining and inference images increases the difficulty of the inpainting task.\nIn our method, this domain gap is resolved by training the inpainting network\nwith object masks extracted by segmentation, and such object masks are also\nused in the inference step. Furthermore, to optimize the object masks for\ninpainting, the segmentation network is connected to the inpainting network and\nend-to-end trained to improve the inpainting performance. The effect of this\nend-to-end training is further enhanced by our mask expansion loss for\nachieving the trade-off between large and small masks. Experimental results\ndemonstrate the effectiveness of our method for better object removal using\nimage inpainting.",
          "arxiv_id": "2403.15849v1"
        },
        {
          "title": "Line Drawing Guided Progressive Inpainting of Mural Damage",
          "year": "2022-11",
          "abstract": "Mural image inpainting is far less explored compared to its natural image\ncounterpart and remains largely unsolved. Most existing image-inpainting\nmethods tend to take the target image as the only input and directly repair the\ndamage to generate a visually plausible result. These methods obtain high\nperformance in restoration or completion of some pre-defined objects, e.g.,\nhuman face, fabric texture, and printed texts, etc., however, are not suitable\nfor repairing murals with varying subjects and large damaged areas. Moreover,\ndue to discrete colors in paints, mural inpainting may suffer from apparent\ncolor bias. To this end, in this paper, we propose a line drawing guided\nprogressive mural inpainting method. It divides the inpainting process into two\nsteps: structure reconstruction and color correction, implemented by a\nstructure reconstruction network (SRN) and a color correction network (CCN),\nrespectively. In structure reconstruction, SRN utilizes the line drawing as an\nassistant to achieve large-scale content authenticity and structural stability.\nIn color correction, CCN operates a local color adjustment for missing pixels\nwhich reduces the negative effects of color bias and edge jumping. The proposed\napproach is evaluated against the current state-of-the-art image inpainting\nmethods. Qualitative and quantitative results demonstrate the superiority of\nthe proposed method in mural image inpainting. The codes and data are available\nat https://github.com/qinnzou/mural-image-inpainting.",
          "arxiv_id": "2211.06649v2"
        },
        {
          "title": "Shape-guided Object Inpainting",
          "year": "2022-04",
          "abstract": "Previous works on image inpainting mainly focus on inpainting background or\npartially missing objects, while the problem of inpainting an entire missing\nobject remains unexplored. This work studies a new image inpainting task, i.e.\nshape-guided object inpainting. Given an incomplete input image, the goal is to\nfill in the hole by generating an object based on the context and implicit\nguidance given by the hole shape. Since previous methods for image inpainting\nare mainly designed for background inpainting, they are not suitable for this\ntask. Therefore, we propose a new data preparation method and a novel\nContextual Object Generator (CogNet) for the object inpainting task. On the\ndata side, we incorporate object priors into training data by using object\ninstances as holes. The CogNet has a two-stream architecture that combines the\nstandard bottom-up image completion process with a top-down object generation\nprocess. A predictive class embedding module bridges the two streams by\npredicting the class of the missing object from the bottom-up features, from\nwhich a semantic object map is derived as the input of the top-down stream.\nExperiments demonstrate that the proposed method can generate realistic objects\nthat fit the context in terms of both visual appearance and semantic meanings.\nCode can be found at the project page\n\\url{https://zengxianyu.github.io/objpaint}",
          "arxiv_id": "2204.07845v1"
        }
      ],
      "62": [
        {
          "title": "Locally Linear Region Knowledge Distillation",
          "year": "2020-10",
          "abstract": "Knowledge distillation (KD) is an effective technique to transfer knowledge\nfrom one neural network (teacher) to another (student), thus improving the\nperformance of the student. To make the student better mimic the behavior of\nthe teacher, the existing work focuses on designing different criteria to align\ntheir logits or representations. Different from these efforts, we address\nknowledge distillation from a novel data perspective. We argue that\ntransferring knowledge at sparse training data points cannot enable the student\nto well capture the local shape of the teacher function. To address this issue,\nwe propose locally linear region knowledge distillation ($\\rm L^2$RKD) which\ntransfers the knowledge in local, linear regions from a teacher to a student.\nThis is achieved by enforcing the student to mimic the outputs of the teacher\nfunction in local, linear regions. To the end, the student is able to better\ncapture the local shape of the teacher function and thus achieves a better\nperformance. Despite its simplicity, extensive experiments demonstrate that\n$\\rm L^2$RKD is superior to the original KD in many aspects as it outperforms\nKD and the other state-of-the-art approaches by a large margin, shows\nrobustness and superiority under few-shot settings, and is more compatible with\nthe existing distillation approaches to further improve their performances\nsignificantly.",
          "arxiv_id": "2010.04812v2"
        },
        {
          "title": "Semi-Online Knowledge Distillation",
          "year": "2021-11",
          "abstract": "Knowledge distillation is an effective and stable method for model\ncompression via knowledge transfer. Conventional knowledge distillation (KD) is\nto transfer knowledge from a large and well pre-trained teacher network to a\nsmall student network, which is a one-way process. Recently, deep mutual\nlearning (DML) has been proposed to help student networks learn collaboratively\nand simultaneously. However, to the best of our knowledge, KD and DML have\nnever been jointly explored in a unified framework to solve the knowledge\ndistillation problem. In this paper, we investigate that the teacher model\nsupports more trustworthy supervision signals in KD, while the student captures\nmore similar behaviors from the teacher in DML. Based on these observations, we\nfirst propose to combine KD with DML in a unified framework. Furthermore, we\npropose a Semi-Online Knowledge Distillation (SOKD) method that effectively\nimproves the performance of the student and the teacher. In this method, we\nintroduce the peer-teaching training fashion in DML in order to alleviate the\nstudent's imitation difficulty, and also leverage the supervision signals\nprovided by the well-trained teacher in KD. Besides, we also show our framework\ncan be easily extended to feature-based distillation methods. Extensive\nexperiments on CIFAR-100 and ImageNet datasets demonstrate the proposed method\nachieves state-of-the-art performance.",
          "arxiv_id": "2111.11747v1"
        },
        {
          "title": "Student-Oriented Teacher Knowledge Refinement for Knowledge Distillation",
          "year": "2024-09",
          "abstract": "Knowledge distillation has become widely recognized for its ability to\ntransfer knowledge from a large teacher network to a compact and more\nstreamlined student network. Traditional knowledge distillation methods\nprimarily follow a teacher-oriented paradigm that imposes the task of learning\nthe teacher's complex knowledge onto the student network. However, significant\ndisparities in model capacity and architectural design hinder the student's\ncomprehension of the complex knowledge imparted by the teacher, resulting in\nsub-optimal performance. This paper introduces a novel perspective emphasizing\nstudent-oriented and refining the teacher's knowledge to better align with the\nstudent's needs, thereby improving knowledge transfer effectiveness.\nSpecifically, we present the Student-Oriented Knowledge Distillation (SoKD),\nwhich incorporates a learnable feature augmentation strategy during training to\nrefine the teacher's knowledge of the student dynamically. Furthermore, we\ndeploy the Distinctive Area Detection Module (DAM) to identify areas of mutual\ninterest between the teacher and student, concentrating knowledge transfer\nwithin these critical areas to avoid transferring irrelevant information. This\ncustomized module ensures a more focused and effective knowledge distillation\nprocess. Our approach, functioning as a plug-in, could be integrated with\nvarious knowledge distillation methods. Extensive experimental results\ndemonstrate the efficacy and generalizability of our method.",
          "arxiv_id": "2409.18785v1"
        }
      ],
      "63": [
        {
          "title": "3D-LEX v1.0: 3D Lexicons for American Sign Language and Sign Language of the Netherlands",
          "year": "2024-09",
          "abstract": "In this work, we present an efficient approach for capturing sign language in\n3D, introduce the 3D-LEX v1.0 dataset, and detail a method for semi-automatic\nannotation of phonetic properties. Our procedure integrates three motion\ncapture techniques encompassing high-resolution 3D poses, 3D handshapes, and\ndepth-aware facial features, and attains an average sampling rate of one sign\nevery 10 seconds. This includes the time for presenting a sign example,\nperforming and recording the sign, and archiving the capture. The 3D-LEX\ndataset includes 1,000 signs from American Sign Language and an additional\n1,000 signs from the Sign Language of the Netherlands. We showcase the dataset\nutility by presenting a simple method for generating handshape annotations\ndirectly from 3D-LEX. We produce handshape labels for 1,000 signs from American\nSign Language and evaluate the labels in a sign recognition task. The labels\nenhance gloss recognition accuracy by 5% over using no handshape annotations,\nand by 1% over expert annotations. Our motion capture data supports in-depth\nanalysis of sign features and facilitates the generation of 2D projections from\nany viewpoint. The 3D-LEX collection has been aligned with existing sign\nlanguage benchmarks and linguistic resources, to support studies in 3D-aware\nsign language processing.",
          "arxiv_id": "2409.01901v1"
        },
        {
          "title": "Indian Sign Language Recognition Using Mediapipe Holistic",
          "year": "2023-04",
          "abstract": "Deaf individuals confront significant communication obstacles on a daily\nbasis. Their inability to hear makes it difficult for them to communicate with\nthose who do not understand sign language. Moreover, it presents difficulties\nin educational, occupational, and social contexts. By providing alternative\ncommunication channels, technology can play a crucial role in overcoming these\nobstacles. One such technology that can facilitate communication between deaf\nand hearing individuals is sign language recognition. We will create a robust\nsystem for sign language recognition in order to convert Indian Sign Language\nto text or speech. We will evaluate the proposed system and compare CNN and\nLSTM models. Since there are both static and gesture sign languages, a robust\nmodel is required to distinguish between them. In this study, we discovered\nthat a CNN model captures letters and characters for recognition of static sign\nlanguage better than an LSTM model, but it outperforms CNN by monitoring hands,\nfaces, and pose in gesture sign language phrases and sentences. The creation of\na text-to-sign language paradigm is essential since it will enhance the sign\nlanguage-dependent deaf and hard-of-hearing population's communication skills.\nEven though the sign-to-text translation is just one side of communication, not\nall deaf or hard-of-hearing people are proficient in reading or writing text.\nSome may have difficulty comprehending written language due to educational or\nliteracy issues. Therefore, a text-to-sign language paradigm would allow them\nto comprehend text-based information and participate in a variety of social,\neducational, and professional settings.\n  Keywords: deaf and hard-of-hearing, DHH, Indian sign language, CNN, LSTM,\nstatic and gesture sign languages, text-to-sign language model, MediaPipe\nHolistic, sign language recognition, SLR, SLT",
          "arxiv_id": "2304.10256v1"
        },
        {
          "title": "Continuous Sign Language Recognition System using Deep Learning with MediaPipe Holistic",
          "year": "2024-11",
          "abstract": "Sign languages are the language of hearing-impaired people who use visuals\nlike the hand, facial, and body movements for communication. There are\ndifferent signs and gestures representing alphabets, words, and phrases.\nNowadays approximately 300 sign languages are being practiced worldwide such as\nAmerican Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language\n(ISL), and many more. Sign languages are dependent on the vocal language of a\nplace. Unlike vocal or spoken languages, there are no helping words in sign\nlanguage like is, am, are, was, were, will, be, etc. As only a limited\npopulation is well-versed in sign language, this lack of familiarity of sign\nlanguage hinders hearing-impaired people from communicating freely and easily\nwith everyone. This issue can be addressed by a sign language recognition (SLR)\nsystem which has the capability to translate the sign language into vocal\nlanguage. In this paper, a continuous SLR system is proposed using a deep\nlearning model employing Long Short-Term Memory (LSTM), trained and tested on\nan ISL primary dataset. This dataset is created using MediaPipe Holistic\npipeline for tracking face, hand, and body movements and collecting landmarks.\nThe system recognizes the signs and gestures in real-time with 88.23% accuracy.",
          "arxiv_id": "2411.04517v1"
        }
      ],
      "64": [
        {
          "title": "Towards Instance-adaptive Inference for Federated Learning",
          "year": "2023-08",
          "abstract": "Federated learning (FL) is a distributed learning paradigm that enables\nmultiple clients to learn a powerful global model by aggregating local\ntraining. However, the performance of the global model is often hampered by\nnon-i.i.d. distribution among the clients, requiring extensive efforts to\nmitigate inter-client data heterogeneity. Going beyond inter-client data\nheterogeneity, we note that intra-client heterogeneity can also be observed on\ncomplex real-world data and seriously deteriorate FL performance. In this\npaper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client\ndata heterogeneity by enabling instance-adaptive inference in the FL framework.\nInstead of huge instance-adaptive models, we resort to a parameter-efficient\nfine-tuning method, i.e., scale and shift deep features (SSF), upon a\npre-trained model. Specifically, we first train an SSF pool for each client,\nand aggregate these SSF pools on the server side, thus still maintaining a low\ncommunication cost. To enable instance-adaptive inference, for a given\ninstance, we dynamically find the best-matched SSF subsets from the pool and\naggregate them to generate an adaptive SSF specified for the instance, thereby\nreducing the intra-client as well as the inter-client heterogeneity. Extensive\nexperiments show that our FedIns outperforms state-of-the-art FL algorithms,\ne.g., a 6.64\\% improvement against the top-performing method with less than\n15\\% communication cost on Tiny-ImageNet. Our code and models will be publicly\nreleased.",
          "arxiv_id": "2308.06051v2"
        },
        {
          "title": "Federated Learning Client Pruning for Noisy Labels",
          "year": "2024-11",
          "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized edge devices while preserving data privacy. However, existing FL\nmethods often assume clean annotated datasets, impractical for\nresource-constrained edge devices. In reality, noisy labels are prevalent,\nposing significant challenges to FL performance. Prior approaches attempt label\ncorrection and robust training techniques but exhibit limited efficacy,\nparticularly under high noise levels. This paper introduces ClipFL (Federated\nLearning Client Pruning), a novel framework addressing noisy labels from a\nfresh perspective. ClipFL identifies and excludes noisy clients based on their\nperformance on a clean validation dataset, tracked using a Noise Candidacy\nScore (NCS). The framework comprises three phases: pre-client pruning to\nidentify potential noisy clients and calculate their NCS, client pruning to\nexclude a percentage of clients with the highest NCS, and post-client pruning\nfor fine-tuning the global model with standard FL on clean clients. Empirical\nevaluation demonstrates ClipFL's efficacy across diverse datasets and noise\nlevels, achieving accurate noisy client identification, superior performance,\nfaster convergence, and reduced communication costs compared to\nstate-of-the-art FL methods. Our code is available at\nhttps://github.com/MMorafah/ClipFL.",
          "arxiv_id": "2411.07391v1"
        },
        {
          "title": "An Aggregation-Free Federated Learning for Tackling Data Heterogeneity",
          "year": "2024-04",
          "abstract": "The performance of Federated Learning (FL) hinges on the effectiveness of\nutilizing knowledge from distributed datasets. Traditional FL methods adopt an\naggregate-then-adapt framework, where clients update local models based on a\nglobal model aggregated by the server from the previous training round. This\nprocess can cause client drift, especially with significant cross-client data\nheterogeneity, impacting model performance and convergence of the FL algorithm.\nTo address these challenges, we introduce FedAF, a novel aggregation-free FL\nalgorithm. In this framework, clients collaboratively learn condensed data by\nleveraging peer knowledge, the server subsequently trains the global model\nusing the condensed data and soft labels received from the clients. FedAF\ninherently avoids the issue of client drift, enhances the quality of condensed\ndata amid notable data heterogeneity, and improves the global model\nperformance. Extensive numerical studies on several popular benchmark datasets\nshow FedAF surpasses various state-of-the-art FL algorithms in handling\nlabel-skew and feature-skew data heterogeneity, leading to superior global\nmodel accuracy and faster convergence.",
          "arxiv_id": "2404.18962v1"
        }
      ],
      "65": [
        {
          "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage",
          "year": "2024-11",
          "abstract": "Text-to-image diffusion models, such as Stable Diffusion, have shown\nexceptional potential in generating high-quality images. However, recent\nstudies highlight concerns over the use of unauthorized data in training these\nmodels, which may lead to intellectual property infringement or privacy\nviolations. A promising approach to mitigate these issues is to apply a\nwatermark to images and subsequently check if generative models reproduce\nsimilar watermark features. In this paper, we examine the robustness of various\nwatermark-based protection methods applied to text-to-image models. We observe\nthat common image transformations are ineffective at removing the watermark\neffect. Therefore, we propose RATTAN, that leverages the diffusion process to\nconduct controlled image generation on the protected input, preserving the\nhigh-level features of the input while ignoring the low-level details utilized\nby watermarks. A small number of generated images are then used to fine-tune\nprotected models. Our experiments on three datasets and 140 text-to-image\ndiffusion models reveal that existing state-of-the-art protections are not\nrobust against RATTAN.",
          "arxiv_id": "2411.15367v2"
        },
        {
          "title": "Dynamic watermarks in images generated by diffusion models",
          "year": "2025-02",
          "abstract": "High-fidelity text-to-image diffusion models have revolutionized visual\ncontent generation, but their widespread use raises significant ethical\nconcerns, including intellectual property protection and the misuse of\nsynthetic media. To address these challenges, we propose a novel multi-stage\nwatermarking framework for diffusion models, designed to establish copyright\nand trace generated images back to their source. Our multi-stage watermarking\ntechnique involves embedding: (i) a fixed watermark that is localized in the\ndiffusion model's learned noise distribution and, (ii) a human-imperceptible,\ndynamic watermark in generates images, leveraging a fine-tuned decoder. By\nleveraging the Structural Similarity Index Measure (SSIM) and cosine\nsimilarity, we adapt the watermark's shape and color to the generated content\nwhile maintaining robustness. We demonstrate that our method enables reliable\nsource verification through watermark classification, even when the dynamic\nwatermark is adjusted for content-specific variations. Source model\nverification is enabled through watermark classification. o support further\nresearch, we generate a dataset of watermarked images and introduce a\nmethodology to evaluate the statistical impact of watermarking on generated\ncontent.Additionally, we rigorously test our framework against various attack\nscenarios, demonstrating its robustness and minimal impact on image quality.\nOur work advances the field of AI-generated content security by providing a\nscalable solution for model ownership verification and misuse prevention.",
          "arxiv_id": "2502.08927v2"
        },
        {
          "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
          "year": "2025-05",
          "abstract": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance.",
          "arxiv_id": "2505.02824v1"
        }
      ],
      "66": [
        {
          "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction",
          "year": "2022-01",
          "abstract": "Video recordings of speech contain correlated audio and visual information,\nproviding a strong signal for speech representation learning from the speaker's\nlip movements and the produced sound. We introduce Audio-Visual Hidden Unit\nBERT (AV-HuBERT), a self-supervised representation learning framework for\naudio-visual speech, which masks multi-stream video input and predicts\nautomatically discovered and iteratively refined multimodal hidden units.\nAV-HuBERT learns powerful audio-visual speech representation benefiting both\nlip-reading and automatic speech recognition. On the largest public lip-reading\nbenchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of\nlabeled data, outperforming the former state-of-the-art approach (33.6%)\ntrained with a thousand times more transcribed video data (31K hours). The\nlip-reading WER is further reduced to 26.9% when using all 433 hours of labeled\ndata from LRS3 and combined with self-training. Using our audio-visual\nrepresentation on the same benchmark for audio-only speech recognition leads to\na 40% relative WER reduction over the state-of-the-art performance (1.3% vs\n2.3%). Our code and models are available at\nhttps://github.com/facebookresearch/av_hubert",
          "arxiv_id": "2201.02184v2"
        },
        {
          "title": "AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation",
          "year": "2023-12",
          "abstract": "This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.",
          "arxiv_id": "2312.02512v2"
        },
        {
          "title": "Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement by Re-Synthesis",
          "year": "2022-03",
          "abstract": "Since facial actions such as lip movements contain significant information\nabout speech content, it is not surprising that audio-visual speech enhancement\nmethods are more accurate than their audio-only counterparts. Yet,\nstate-of-the-art approaches still struggle to generate clean, realistic speech\nwithout noise artifacts and unnatural distortions in challenging acoustic\nenvironments. In this paper, we propose a novel audio-visual speech enhancement\nframework for high-fidelity telecommunications in AR/VR. Our approach leverages\naudio-visual speech cues to generate the codes of a neural speech codec,\nenabling efficient synthesis of clean, realistic speech from noisy signals.\nGiven the importance of speaker-specific cues in speech, we focus on developing\npersonalized models that work well for individual speakers. We demonstrate the\nefficacy of our approach on a new audio-visual speech dataset collected in an\nunconstrained, large vocabulary setting, as well as existing audio-visual\ndatasets, outperforming speech enhancement baselines on both quantitative\nmetrics and human evaluation studies. Please see the supplemental video for\nqualitative results at\nhttps://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.",
          "arxiv_id": "2203.17263v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:39:27Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}