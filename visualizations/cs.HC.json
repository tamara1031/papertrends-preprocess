{
  "topics": {
    "data": {
      "0": {
        "name": "0_data_visualization_visualizations_visual",
        "keywords": [
          [
            "data",
            0.03146545071385499
          ],
          [
            "visualization",
            0.03043946114861387
          ],
          [
            "visualizations",
            0.0161824531020438
          ],
          [
            "visual",
            0.01442136198051645
          ],
          [
            "analysis",
            0.012845491786241909
          ],
          [
            "design",
            0.011139609857952851
          ],
          [
            "Visualization",
            0.00936992114162275
          ],
          [
            "Data",
            0.009150732844110446
          ],
          [
            "users",
            0.007824139582912577
          ],
          [
            "user",
            0.00751602435026764
          ]
        ],
        "count": 1757
      },
      "1": {
        "name": "1_robot_robots_human_Robot",
        "keywords": [
          [
            "robot",
            0.04211639237703686
          ],
          [
            "robots",
            0.02591882193352932
          ],
          [
            "human",
            0.025446860657514867
          ],
          [
            "Robot",
            0.013161164067500973
          ],
          [
            "human robot",
            0.012877804963509841
          ],
          [
            "Human",
            0.010911343370126081
          ],
          [
            "task",
            0.01068596507116554
          ],
          [
            "social",
            0.010619536042304683
          ],
          [
            "learning",
            0.010545719546896764
          ],
          [
            "interaction",
            0.010447559847348886
          ]
        ],
        "count": 1162
      },
      "2": {
        "name": "2_students_learning_AI_code",
        "keywords": [
          [
            "students",
            0.029713659269735092
          ],
          [
            "learning",
            0.021578881700382257
          ],
          [
            "AI",
            0.020269355953323975
          ],
          [
            "code",
            0.01706084910791157
          ],
          [
            "programming",
            0.014885348156862722
          ],
          [
            "student",
            0.01322050407752739
          ],
          [
            "education",
            0.011978397846945347
          ],
          [
            "LLMs",
            0.010367435782659671
          ],
          [
            "ChatGPT",
            0.0101316230236588
          ],
          [
            "study",
            0.009988375846228852
          ]
        ],
        "count": 1046
      },
      "3": {
        "name": "3_VR_virtual_Reality_AR",
        "keywords": [
          [
            "VR",
            0.03545744065511932
          ],
          [
            "virtual",
            0.022030977100713462
          ],
          [
            "Reality",
            0.02062722616681863
          ],
          [
            "AR",
            0.019101107220861703
          ],
          [
            "reality",
            0.01589577917475091
          ],
          [
            "Virtual",
            0.014623217369540059
          ],
          [
            "user",
            0.012120911251115741
          ],
          [
            "Virtual Reality",
            0.011154749961491768
          ],
          [
            "experience",
            0.010675130189709298
          ],
          [
            "immersive",
            0.009939915106559942
          ]
        ],
        "count": 976
      },
      "4": {
        "name": "4_privacy_users_online_social",
        "keywords": [
          [
            "privacy",
            0.02267494757000204
          ],
          [
            "users",
            0.018237199149481364
          ],
          [
            "online",
            0.018192052004786376
          ],
          [
            "social",
            0.017168507029244266
          ],
          [
            "media",
            0.016474886458042064
          ],
          [
            "content",
            0.014038576721539449
          ],
          [
            "social media",
            0.013918573407585035
          ],
          [
            "platforms",
            0.011206342847702866
          ],
          [
            "security",
            0.00981925768154737
          ],
          [
            "user",
            0.009525006052783195
          ]
        ],
        "count": 894
      },
      "5": {
        "name": "5_AI_explanations_XAI_decision",
        "keywords": [
          [
            "AI",
            0.028709024506589503
          ],
          [
            "explanations",
            0.025570704464398852
          ],
          [
            "XAI",
            0.023158547043067003
          ],
          [
            "decision",
            0.021075590057788966
          ],
          [
            "human",
            0.01850793049476928
          ],
          [
            "model",
            0.014883321190261438
          ],
          [
            "explanation",
            0.013650921268417996
          ],
          [
            "making",
            0.012653775146671287
          ],
          [
            "decision making",
            0.011610116335886538
          ],
          [
            "models",
            0.011196114535794388
          ]
        ],
        "count": 744
      },
      "6": {
        "name": "6_LLMs_LLM_writing_language",
        "keywords": [
          [
            "LLMs",
            0.02642587853581307
          ],
          [
            "LLM",
            0.017127814467247097
          ],
          [
            "writing",
            0.016644610860752737
          ],
          [
            "language",
            0.015935988527034744
          ],
          [
            "AI",
            0.015819710775374758
          ],
          [
            "models",
            0.014954567645282395
          ],
          [
            "human",
            0.013997370533910798
          ],
          [
            "Large",
            0.012863672701619104
          ],
          [
            "Language",
            0.012468300876119979
          ],
          [
            "Large Language",
            0.010957723490109764
          ]
        ],
        "count": 625
      },
      "7": {
        "name": "7_AI_design_image_creative",
        "keywords": [
          [
            "AI",
            0.028388641454956603
          ],
          [
            "design",
            0.024667027786311486
          ],
          [
            "image",
            0.019689346682941918
          ],
          [
            "creative",
            0.019004039846027627
          ],
          [
            "generative",
            0.018982861053933025
          ],
          [
            "images",
            0.014100955554953894
          ],
          [
            "Generative",
            0.013321713298233225
          ],
          [
            "creativity",
            0.01245151394281719
          ],
          [
            "models",
            0.012045045834654738
          ],
          [
            "generation",
            0.01194961798623897
          ]
        ],
        "count": 560
      },
      "8": {
        "name": "8_driving_driver_vehicle_drivers",
        "keywords": [
          [
            "driving",
            0.04902432293596808
          ],
          [
            "driver",
            0.030121294911064397
          ],
          [
            "vehicle",
            0.02821123421400705
          ],
          [
            "drivers",
            0.022687718187597496
          ],
          [
            "vehicles",
            0.02199630916386748
          ],
          [
            "AV",
            0.017105840600099096
          ],
          [
            "safety",
            0.01641331223032774
          ],
          [
            "road",
            0.015710659714944713
          ],
          [
            "autonomous",
            0.015624649481945429
          ],
          [
            "traffic",
            0.014127644087845798
          ]
        ],
        "count": 506
      },
      "9": {
        "name": "9_EEG_BCI_brain_signals",
        "keywords": [
          [
            "EEG",
            0.05302753096793595
          ],
          [
            "BCI",
            0.034407583802602226
          ],
          [
            "brain",
            0.032555204465779954
          ],
          [
            "signals",
            0.021236748415827754
          ],
          [
            "decoding",
            0.018660434537273222
          ],
          [
            "Brain",
            0.01659321568431287
          ],
          [
            "classification",
            0.01559430636418716
          ],
          [
            "BCIs",
            0.01430077802151183
          ],
          [
            "EEG signals",
            0.013792327078842676
          ],
          [
            "brain computer",
            0.013553394284057682
          ]
        ],
        "count": 456
      },
      "10": {
        "name": "10_AI_systems_human_fairness",
        "keywords": [
          [
            "AI",
            0.06375618525381149
          ],
          [
            "systems",
            0.020805543613734708
          ],
          [
            "human",
            0.018220842539761584
          ],
          [
            "fairness",
            0.011193862749540932
          ],
          [
            "ethical",
            0.010516391913895692
          ],
          [
            "intelligence",
            0.009104872178316554
          ],
          [
            "decision",
            0.009026514457339265
          ],
          [
            "development",
            0.008843192344660553
          ],
          [
            "design",
            0.00878011184036437
          ],
          [
            "research",
            0.00858704252701609
          ]
        ],
        "count": 449
      },
      "11": {
        "name": "11_speech_emotion_recognition_facial",
        "keywords": [
          [
            "speech",
            0.035185845187835525
          ],
          [
            "emotion",
            0.02540282037916435
          ],
          [
            "recognition",
            0.018927025492012375
          ],
          [
            "facial",
            0.015700070655301365
          ],
          [
            "models",
            0.012834783531868284
          ],
          [
            "Emotion",
            0.012280201265102172
          ],
          [
            "emotions",
            0.01207203329872929
          ],
          [
            "model",
            0.01196853172528856
          ],
          [
            "Speech",
            0.011751726322281471
          ],
          [
            "speaker",
            0.010853999578650205
          ]
        ],
        "count": 402
      },
      "12": {
        "name": "12_HAR_activity_data_sensor",
        "keywords": [
          [
            "HAR",
            0.020510615916819408
          ],
          [
            "activity",
            0.01878908622642584
          ],
          [
            "data",
            0.018155672380307582
          ],
          [
            "sensor",
            0.015244546798456063
          ],
          [
            "Activity",
            0.013618137921495126
          ],
          [
            "activities",
            0.013235226214627062
          ],
          [
            "sensors",
            0.01232514469988704
          ],
          [
            "sensing",
            0.011690867621295491
          ],
          [
            "wearable",
            0.011219766271442584
          ],
          [
            "monitoring",
            0.01108082773080551
          ]
        ],
        "count": 396
      },
      "13": {
        "name": "13_hand_gesture_recognition_gestures",
        "keywords": [
          [
            "hand",
            0.02347502581796258
          ],
          [
            "gesture",
            0.0195533909943999
          ],
          [
            "recognition",
            0.015698773889262024
          ],
          [
            "gestures",
            0.015337644975524542
          ],
          [
            "pose",
            0.01341394183825607
          ],
          [
            "gesture recognition",
            0.012141893771657355
          ],
          [
            "data",
            0.012127929138170306
          ],
          [
            "motion",
            0.011994214034290402
          ],
          [
            "body",
            0.010476928270112915
          ],
          [
            "accuracy",
            0.00941712501977547
          ]
        ],
        "count": 371
      },
      "14": {
        "name": "14_eye_gaze_tracking_eye tracking",
        "keywords": [
          [
            "eye",
            0.049580020574163344
          ],
          [
            "gaze",
            0.04533741715795055
          ],
          [
            "tracking",
            0.026814677547884337
          ],
          [
            "eye tracking",
            0.02300055101124338
          ],
          [
            "Eye",
            0.016065840345063552
          ],
          [
            "Gaze",
            0.01435392177432342
          ],
          [
            "visual",
            0.013236725565734967
          ],
          [
            "user",
            0.01172989150039529
          ],
          [
            "data",
            0.010203919044510222
          ],
          [
            "attention",
            0.009932465072795498
          ]
        ],
        "count": 365
      },
      "15": {
        "name": "15_haptic_tactile_feedback_force",
        "keywords": [
          [
            "haptic",
            0.0501550482791241
          ],
          [
            "tactile",
            0.03258719992993094
          ],
          [
            "feedback",
            0.024578371582008932
          ],
          [
            "force",
            0.018245478284954812
          ],
          [
            "vibrotactile",
            0.015150137125574045
          ],
          [
            "Haptic",
            0.015021979925364493
          ],
          [
            "virtual",
            0.01497528605230867
          ],
          [
            "haptic feedback",
            0.014314604519418794
          ],
          [
            "device",
            0.014232275863841213
          ],
          [
            "perception",
            0.014197678540321553
          ]
        ],
        "count": 316
      },
      "16": {
        "name": "16_GUI_UI_agents_agent",
        "keywords": [
          [
            "GUI",
            0.03000685824051792
          ],
          [
            "UI",
            0.02503174419270307
          ],
          [
            "agents",
            0.021104815416350366
          ],
          [
            "agent",
            0.016651708336505
          ],
          [
            "user",
            0.015570281646424306
          ],
          [
            "mobile",
            0.015150932297890463
          ],
          [
            "tasks",
            0.01481146518770101
          ],
          [
            "LLM",
            0.013793513159738058
          ],
          [
            "language",
            0.013002226472507596
          ],
          [
            "task",
            0.011626370356833022
          ]
        ],
        "count": 313
      },
      "17": {
        "name": "17_health_mental health_mental_LLMs",
        "keywords": [
          [
            "health",
            0.02950955296199875
          ],
          [
            "mental health",
            0.02493881989141894
          ],
          [
            "mental",
            0.024747827508539627
          ],
          [
            "LLMs",
            0.019437268166724538
          ],
          [
            "clinical",
            0.014635148429607848
          ],
          [
            "support",
            0.014196535479266702
          ],
          [
            "LLM",
            0.013331054600885687
          ],
          [
            "patient",
            0.012943318706931656
          ],
          [
            "AI",
            0.012526550182113518
          ],
          [
            "medical",
            0.011131306385696503
          ]
        ],
        "count": 272
      },
      "18": {
        "name": "18_game_games_player_players",
        "keywords": [
          [
            "game",
            0.06791102156375031
          ],
          [
            "games",
            0.04772919510373866
          ],
          [
            "player",
            0.028817799305802378
          ],
          [
            "players",
            0.02853278524496608
          ],
          [
            "gamification",
            0.018064135867658638
          ],
          [
            "Game",
            0.0141277876250025
          ],
          [
            "Games",
            0.011806291168768659
          ],
          [
            "gaming",
            0.010923088480377192
          ],
          [
            "learning",
            0.010570841041061782
          ],
          [
            "design",
            0.010378635302576381
          ]
        ],
        "count": 205
      },
      "19": {
        "name": "19_recommender_recommendation_recommender systems_user",
        "keywords": [
          [
            "recommender",
            0.039204095090526345
          ],
          [
            "recommendation",
            0.03483827603057043
          ],
          [
            "recommender systems",
            0.031168279352689125
          ],
          [
            "user",
            0.026928363636695502
          ],
          [
            "systems",
            0.026572365330209066
          ],
          [
            "users",
            0.02297483723981065
          ],
          [
            "recommendations",
            0.02231132218080799
          ],
          [
            "Recommender",
            0.020583340300891647
          ],
          [
            "Recommendation",
            0.011049276767766888
          ],
          [
            "fairness",
            0.01045185271450026
          ]
        ],
        "count": 199
      },
      "20": {
        "name": "20_music_musical_Music_sound",
        "keywords": [
          [
            "music",
            0.07019449958809841
          ],
          [
            "musical",
            0.03820967946363936
          ],
          [
            "Music",
            0.021178272692747153
          ],
          [
            "sound",
            0.015003346752128449
          ],
          [
            "sonification",
            0.014168555163931314
          ],
          [
            "audio",
            0.014011251550653652
          ],
          [
            "AI",
            0.013537948462262086
          ],
          [
            "generation",
            0.010606323633514594
          ],
          [
            "musicians",
            0.010241279502922161
          ],
          [
            "creative",
            0.010196287812370114
          ]
        ],
        "count": 187
      },
      "21": {
        "name": "21_AI_clinical_medical_clinicians",
        "keywords": [
          [
            "AI",
            0.037397986615433566
          ],
          [
            "clinical",
            0.027370423184011762
          ],
          [
            "medical",
            0.019048354185587642
          ],
          [
            "clinicians",
            0.018711219983749486
          ],
          [
            "decision",
            0.01866045979440489
          ],
          [
            "healthcare",
            0.016767118520697087
          ],
          [
            "patients",
            0.012942105503746507
          ],
          [
            "patient",
            0.012446122848812377
          ],
          [
            "support",
            0.01100970175903174
          ],
          [
            "care",
            0.010830859479433224
          ]
        ],
        "count": 174
      },
      "22": {
        "name": "22_crowdsourcing_workers_crowd_Crowdsourcing",
        "keywords": [
          [
            "crowdsourcing",
            0.04876228787881317
          ],
          [
            "workers",
            0.035011307128292715
          ],
          [
            "crowd",
            0.0278754020209235
          ],
          [
            "Crowdsourcing",
            0.023757633874414475
          ],
          [
            "labels",
            0.02181318660237855
          ],
          [
            "tasks",
            0.018106204699367044
          ],
          [
            "worker",
            0.016408736507661788
          ],
          [
            "task",
            0.016284625504962992
          ],
          [
            "data",
            0.01431819307435791
          ],
          [
            "annotation",
            0.013119181404640917
          ]
        ],
        "count": 158
      }
    },
    "correlations": [
      [
        1.0,
        -0.750982449642037,
        -0.7230606418264257,
        -0.73593055818975,
        -0.6998415040084249,
        -0.714284157138406,
        -0.7176937172570832,
        -0.71387338680256,
        -0.750061189335802,
        -0.7464921781889031,
        -0.7329177823097727,
        -0.7497804733596783,
        -0.40480769537279626,
        -0.7438985853704267,
        -0.7458684246046547,
        -0.7517071092898739,
        -0.7449079449863094,
        -0.7309907409698062,
        -0.7529038804551926,
        -0.6962642392862659,
        -0.7576353253297532,
        -0.7221113528882339,
        -0.7476267533555159
      ],
      [
        -0.750982449642037,
        1.0,
        -0.7394580489352607,
        -0.737837724035848,
        -0.7403421931643762,
        -0.6418725844836499,
        -0.7183256171593615,
        -0.7336207952770606,
        -0.7477480517553308,
        -0.7536036805038517,
        -0.6783617174738794,
        -0.7295568490292845,
        -0.7488725199249391,
        -0.7355221232555019,
        -0.7317305868506878,
        -0.7368785944899698,
        -0.7295572365777385,
        -0.7329330571724693,
        -0.7448961411576968,
        -0.7303618439380573,
        -0.7611643022178147,
        -0.7374994260368148,
        -0.7553756288679635
      ],
      [
        -0.7230606418264257,
        -0.7394580489352607,
        1.0,
        -0.7431231904530342,
        -0.7254379228280337,
        -0.6713595593964896,
        -0.5885873468930916,
        -0.6524923946018102,
        -0.7588333281736761,
        -0.7542658764017287,
        -0.6196672795078029,
        -0.7490895257566611,
        -0.7379903136624579,
        -0.7521024136837755,
        -0.7533568698609512,
        -0.7480241594637169,
        -0.6657424031988561,
        -0.6428762151004888,
        -0.7418141980856743,
        -0.7375318008789671,
        -0.7552379943654864,
        -0.658460292202263,
        -0.7554323010646197
      ],
      [
        -0.73593055818975,
        -0.737837724035848,
        -0.7431231904530342,
        1.0,
        -0.7339382992248087,
        -0.7526863961834854,
        -0.7482216799433771,
        -0.7324702239914098,
        -0.7427754296336267,
        -0.7478132133910578,
        -0.7444285394027512,
        -0.7482652493770506,
        -0.7487726486671794,
        -0.7236008224108683,
        -0.6984624352224679,
        -0.7041334509820747,
        -0.7501847015853096,
        -0.7444363565390475,
        -0.7084377720073008,
        -0.7177622545954647,
        -0.7566326521858868,
        -0.7499119954473406,
        -0.7565276362496848
      ],
      [
        -0.6998415040084249,
        -0.7403421931643762,
        -0.7254379228280337,
        -0.7339382992248087,
        1.0,
        -0.7124151986143581,
        -0.7079144603900506,
        -0.6993265420244182,
        -0.7487175545966973,
        -0.7539298313933365,
        -0.7052427946678237,
        -0.7308079293848766,
        -0.7178796504279243,
        -0.7408406768341533,
        -0.7441778058177086,
        -0.7492267234458714,
        -0.7344091484072325,
        -0.7141494257867296,
        -0.7437221602758888,
        -0.4805992575172003,
        -0.7589243272396751,
        -0.7309682574663382,
        -0.7301118138068399
      ],
      [
        -0.714284157138406,
        -0.6418725844836499,
        -0.6713595593964896,
        -0.7526863961834854,
        -0.7124151986143581,
        1.0,
        -0.6566170406970413,
        -0.6405194981510246,
        -0.7355492531367235,
        -0.7561750539981495,
        -0.3906634793458915,
        -0.748286865181025,
        -0.7358739517056851,
        -0.7558272596406628,
        -0.751637290983241,
        -0.7536044415855623,
        -0.7208347576778837,
        -0.7163272512569905,
        -0.7358885880570696,
        -0.6867496270037288,
        -0.7585472578878146,
        -0.27198815550006544,
        -0.7429102030296575
      ],
      [
        -0.7176937172570832,
        -0.7183256171593615,
        -0.5885873468930916,
        -0.7482216799433771,
        -0.7079144603900506,
        -0.6566170406970413,
        1.0,
        -0.6544361878753586,
        -0.7491860233055829,
        -0.7533101357888595,
        -0.6093149994803758,
        -0.7262974122525301,
        -0.7367725388072386,
        -0.7505230317931957,
        -0.7499577861145152,
        -0.7531501236668999,
        -0.4965719446726121,
        -0.13374807503062341,
        -0.7411776745205612,
        -0.7075956390078835,
        -0.7588742024372512,
        -0.6622423861315896,
        -0.7462579261673745
      ],
      [
        -0.71387338680256,
        -0.7336207952770606,
        -0.6524923946018102,
        -0.7324702239914098,
        -0.6993265420244182,
        -0.6405194981510246,
        -0.6544361878753586,
        1.0,
        -0.7450466602000774,
        -0.755009397339464,
        -0.48536527085072895,
        -0.7467147203792017,
        -0.7416869493187737,
        -0.7460331034072241,
        -0.754024136446858,
        -0.7433553666552561,
        -0.7212062608971932,
        -0.7145832463470678,
        -0.7404622256830108,
        -0.7024231364600401,
        -0.7464563712628693,
        -0.6217796021271847,
        -0.7433175176264213
      ],
      [
        -0.750061189335802,
        -0.7477480517553308,
        -0.7588333281736761,
        -0.7427754296336267,
        -0.7487175545966973,
        -0.7355492531367235,
        -0.7491860233055829,
        -0.7450466602000774,
        1.0,
        -0.7430324932718146,
        -0.7375365220680801,
        -0.7585551128485538,
        -0.7525793745330737,
        -0.7559767192628277,
        -0.7298794858594557,
        -0.7449973701988488,
        -0.7469913383507822,
        -0.756829599400673,
        -0.7537055456996385,
        -0.7454263897462512,
        -0.7631610863774653,
        -0.7483260074934266,
        -0.7537684955437269
      ],
      [
        -0.7464921781889031,
        -0.7536036805038517,
        -0.7542658764017287,
        -0.7478132133910578,
        -0.7539298313933365,
        -0.7561750539981495,
        -0.7533101357888595,
        -0.755009397339464,
        -0.7430324932718146,
        1.0,
        -0.7565619946384261,
        -0.6958899756199475,
        -0.7381453249539688,
        -0.7576087078883424,
        -0.7328686035157702,
        -0.7523872173202415,
        -0.7594391358184154,
        -0.7526532526358762,
        -0.7544537005287224,
        -0.7516390383418686,
        -0.7481987568762467,
        -0.7542270016795958,
        -0.7614747110063955
      ],
      [
        -0.7329177823097727,
        -0.6783617174738794,
        -0.6196672795078029,
        -0.7444285394027512,
        -0.7052427946678237,
        -0.3906634793458915,
        -0.6093149994803758,
        -0.48536527085072895,
        -0.7375365220680801,
        -0.7565619946384261,
        1.0,
        -0.7388546027350472,
        -0.7439379682758739,
        -0.7521489777686147,
        -0.7526200445556299,
        -0.7509230298696474,
        -0.7032246227618248,
        -0.6851324088677486,
        -0.7324233695687001,
        -0.6207817981864072,
        -0.748532123674037,
        -0.41109593828570007,
        -0.7408847202916932
      ],
      [
        -0.7497804733596783,
        -0.7295568490292845,
        -0.7490895257566611,
        -0.7482652493770506,
        -0.7308079293848766,
        -0.748286865181025,
        -0.7262974122525301,
        -0.7467147203792017,
        -0.7585551128485538,
        -0.6958899756199475,
        -0.7388546027350472,
        1.0,
        -0.7432194350005461,
        -0.6825039063591539,
        -0.7456311969609959,
        -0.7599660755749824,
        -0.7464501879953718,
        -0.7399540844343484,
        -0.7544080427345017,
        -0.742975794052918,
        -0.7551788979823197,
        -0.7498232097682309,
        -0.7594449852239797
      ],
      [
        -0.40480769537279626,
        -0.7488725199249391,
        -0.7379903136624579,
        -0.7487726486671794,
        -0.7178796504279243,
        -0.7358739517056851,
        -0.7367725388072386,
        -0.7416869493187737,
        -0.7525793745330737,
        -0.7381453249539688,
        -0.7439379682758739,
        -0.7432194350005461,
        1.0,
        -0.7315303909254631,
        -0.7471483344008982,
        -0.7595619187049009,
        -0.7502686691223787,
        -0.7297925231733868,
        -0.7542110336867882,
        -0.7320534993165222,
        -0.7613436434331023,
        -0.7366650446175345,
        -0.74727233650341
      ],
      [
        -0.7438985853704267,
        -0.7355221232555019,
        -0.7521024136837755,
        -0.7236008224108683,
        -0.7408406768341533,
        -0.7558272596406628,
        -0.7505230317931957,
        -0.7460331034072241,
        -0.7559767192628277,
        -0.7576087078883424,
        -0.7521489777686147,
        -0.6825039063591539,
        -0.7315303909254631,
        1.0,
        -0.746251898877188,
        -0.7381644746262258,
        -0.7588417055792107,
        -0.7540455266790695,
        -0.750140512400043,
        -0.7347327698331672,
        -0.7567556292521158,
        -0.7547400765835615,
        -0.7622399957164766
      ],
      [
        -0.7458684246046547,
        -0.7317305868506878,
        -0.7533568698609512,
        -0.6984624352224679,
        -0.7441778058177086,
        -0.751637290983241,
        -0.7499577861145152,
        -0.754024136446858,
        -0.7298794858594557,
        -0.7328686035157702,
        -0.7526200445556299,
        -0.7456311969609959,
        -0.7471483344008982,
        -0.746251898877188,
        1.0,
        -0.7545869231443376,
        -0.7539262397614795,
        -0.7536438563388215,
        -0.7507509468353977,
        -0.740378574811218,
        -0.761216924580316,
        -0.7546259103172053,
        -0.7617778317415045
      ],
      [
        -0.7517071092898739,
        -0.7368785944899698,
        -0.7480241594637169,
        -0.7041334509820747,
        -0.7492267234458714,
        -0.7536044415855623,
        -0.7531501236668999,
        -0.7433553666552561,
        -0.7449973701988488,
        -0.7523872173202415,
        -0.7509230298696474,
        -0.7599660755749824,
        -0.7595619187049009,
        -0.7381644746262258,
        -0.7545869231443376,
        1.0,
        -0.7579098190163389,
        -0.7531359930468104,
        -0.7530247822088045,
        -0.7430456401084009,
        -0.7406695472338078,
        -0.7512299593636462,
        -0.7633369356587154
      ],
      [
        -0.7449079449863094,
        -0.7295572365777385,
        -0.6657424031988561,
        -0.7501847015853096,
        -0.7344091484072325,
        -0.7208347576778837,
        -0.4965719446726121,
        -0.7212062608971932,
        -0.7469913383507822,
        -0.7594391358184154,
        -0.7032246227618248,
        -0.7464501879953718,
        -0.7502686691223787,
        -0.7588417055792107,
        -0.7539262397614795,
        -0.7579098190163389,
        1.0,
        -0.5456341093900219,
        -0.7396325636053677,
        -0.7295400053342362,
        -0.7603822870665113,
        -0.735771962199465,
        -0.7535356521270905
      ],
      [
        -0.7309907409698062,
        -0.7329330571724693,
        -0.6428762151004888,
        -0.7444363565390475,
        -0.7141494257867296,
        -0.7163272512569905,
        -0.13374807503062341,
        -0.7145832463470678,
        -0.756829599400673,
        -0.7526532526358762,
        -0.6851324088677486,
        -0.7399540844343484,
        -0.7297925231733868,
        -0.7540455266790695,
        -0.7536438563388215,
        -0.7531359930468104,
        -0.5456341093900219,
        1.0,
        -0.7430942615715121,
        -0.7242793094867717,
        -0.7605835184280452,
        -0.7030824883681301,
        -0.7536972465922657
      ],
      [
        -0.7529038804551926,
        -0.7448961411576968,
        -0.7418141980856743,
        -0.7084377720073008,
        -0.7437221602758888,
        -0.7358885880570696,
        -0.7411776745205612,
        -0.7404622256830108,
        -0.7537055456996385,
        -0.7544537005287224,
        -0.7324233695687001,
        -0.7544080427345017,
        -0.7542110336867882,
        -0.750140512400043,
        -0.7507509468353977,
        -0.7530247822088045,
        -0.7396325636053677,
        -0.7430942615715121,
        1.0,
        -0.7434132774670134,
        -0.7531793536785427,
        -0.7462983008574638,
        -0.7593776697117797
      ],
      [
        -0.6962642392862659,
        -0.7303618439380573,
        -0.7375318008789671,
        -0.7177622545954647,
        -0.4805992575172003,
        -0.6867496270037288,
        -0.7075956390078835,
        -0.7024231364600401,
        -0.7454263897462512,
        -0.7516390383418686,
        -0.6207817981864072,
        -0.742975794052918,
        -0.7320534993165222,
        -0.7347327698331672,
        -0.740378574811218,
        -0.7430456401084009,
        -0.7295400053342362,
        -0.7242793094867717,
        -0.7434132774670134,
        1.0,
        -0.7435715515695722,
        -0.7168861252788153,
        -0.7525992119779756
      ],
      [
        -0.7576353253297532,
        -0.7611643022178147,
        -0.7552379943654864,
        -0.7566326521858868,
        -0.7589243272396751,
        -0.7585472578878146,
        -0.7588742024372512,
        -0.7464563712628693,
        -0.7631610863774653,
        -0.7481987568762467,
        -0.748532123674037,
        -0.7551788979823197,
        -0.7613436434331023,
        -0.7567556292521158,
        -0.761216924580316,
        -0.7406695472338078,
        -0.7603822870665113,
        -0.7605835184280452,
        -0.7531793536785427,
        -0.7435715515695722,
        1.0,
        -0.7558998943414335,
        -0.762925640515769
      ],
      [
        -0.7221113528882339,
        -0.7374994260368148,
        -0.658460292202263,
        -0.7499119954473406,
        -0.7309682574663382,
        -0.27198815550006544,
        -0.6622423861315896,
        -0.6217796021271847,
        -0.7483260074934266,
        -0.7542270016795958,
        -0.41109593828570007,
        -0.7498232097682309,
        -0.7366650446175345,
        -0.7547400765835615,
        -0.7546259103172053,
        -0.7512299593636462,
        -0.735771962199465,
        -0.7030824883681301,
        -0.7462983008574638,
        -0.7168861252788153,
        -0.7558998943414335,
        1.0,
        -0.7462635853281117
      ],
      [
        -0.7476267533555159,
        -0.7553756288679635,
        -0.7554323010646197,
        -0.7565276362496848,
        -0.7301118138068399,
        -0.7429102030296575,
        -0.7462579261673745,
        -0.7433175176264213,
        -0.7537684955437269,
        -0.7614747110063955,
        -0.7408847202916932,
        -0.7594449852239797,
        -0.74727233650341,
        -0.7622399957164766,
        -0.7617778317415045,
        -0.7633369356587154,
        -0.7535356521270905,
        -0.7536972465922657,
        -0.7593776697117797,
        -0.7525992119779756,
        -0.762925640515769,
        -0.7462635853281117,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        44,
        11,
        5,
        11,
        10,
        6,
        3,
        10,
        4,
        3,
        6,
        4,
        1,
        2,
        2,
        5,
        4,
        1,
        5,
        9,
        2,
        2,
        3
      ],
      "2020-02": [
        39,
        20,
        5,
        6,
        7,
        10,
        2,
        7,
        3,
        16,
        4,
        3,
        1,
        3,
        3,
        5,
        1,
        0,
        2,
        4,
        1,
        1,
        4
      ],
      "2020-03": [
        36,
        12,
        1,
        5,
        6,
        4,
        1,
        10,
        8,
        4,
        6,
        6,
        2,
        5,
        6,
        4,
        2,
        3,
        6,
        5,
        2,
        1,
        5
      ],
      "2020-04": [
        41,
        6,
        2,
        8,
        6,
        2,
        2,
        3,
        0,
        6,
        8,
        7,
        2,
        0,
        6,
        4,
        1,
        4,
        4,
        8,
        3,
        3,
        12
      ],
      "2020-05": [
        46,
        9,
        5,
        8,
        13,
        6,
        1,
        3,
        7,
        10,
        5,
        4,
        1,
        2,
        8,
        9,
        1,
        2,
        4,
        11,
        0,
        2,
        5
      ],
      "2020-06": [
        35,
        13,
        3,
        3,
        13,
        3,
        2,
        4,
        7,
        1,
        5,
        5,
        4,
        4,
        5,
        6,
        3,
        4,
        13,
        10,
        2,
        1,
        3
      ],
      "2020-07": [
        63,
        13,
        6,
        6,
        11,
        6,
        2,
        5,
        9,
        2,
        5,
        3,
        5,
        5,
        7,
        3,
        2,
        7,
        4,
        6,
        1,
        1,
        2
      ],
      "2020-08": [
        70,
        14,
        7,
        9,
        15,
        4,
        2,
        7,
        8,
        2,
        1,
        7,
        4,
        1,
        3,
        1,
        4,
        7,
        10,
        15,
        0,
        5,
        3
      ],
      "2020-09": [
        89,
        10,
        4,
        4,
        13,
        8,
        4,
        8,
        3,
        3,
        6,
        9,
        0,
        3,
        10,
        0,
        2,
        2,
        7,
        9,
        2,
        2,
        7
      ],
      "2020-10": [
        43,
        8,
        8,
        17,
        10,
        6,
        2,
        6,
        5,
        7,
        9,
        7,
        1,
        6,
        8,
        3,
        2,
        2,
        10,
        8,
        20,
        0,
        4
      ],
      "2020-11": [
        44,
        8,
        1,
        3,
        7,
        6,
        2,
        7,
        3,
        2,
        3,
        6,
        0,
        5,
        2,
        1,
        4,
        2,
        8,
        5,
        2,
        2,
        5
      ],
      "2020-12": [
        36,
        10,
        2,
        3,
        11,
        7,
        1,
        1,
        6,
        7,
        6,
        5,
        2,
        2,
        6,
        4,
        1,
        3,
        6,
        8,
        8,
        1,
        7
      ],
      "2021-01": [
        57,
        7,
        8,
        18,
        18,
        6,
        1,
        4,
        4,
        9,
        10,
        5,
        3,
        2,
        9,
        3,
        8,
        3,
        7,
        14,
        3,
        2,
        8
      ],
      "2021-02": [
        42,
        10,
        3,
        13,
        15,
        7,
        1,
        7,
        3,
        8,
        11,
        3,
        0,
        3,
        4,
        3,
        1,
        2,
        5,
        9,
        5,
        3,
        5
      ],
      "2021-03": [
        56,
        10,
        5,
        10,
        9,
        4,
        5,
        8,
        7,
        7,
        5,
        9,
        2,
        3,
        7,
        6,
        3,
        3,
        9,
        11,
        0,
        1,
        7
      ],
      "2021-04": [
        51,
        13,
        9,
        13,
        15,
        4,
        3,
        8,
        8,
        3,
        6,
        1,
        3,
        5,
        6,
        3,
        1,
        2,
        10,
        10,
        2,
        0,
        2
      ],
      "2021-05": [
        60,
        11,
        5,
        9,
        10,
        7,
        0,
        7,
        1,
        4,
        9,
        5,
        2,
        4,
        8,
        4,
        4,
        1,
        8,
        15,
        2,
        1,
        5
      ],
      "2021-06": [
        47,
        14,
        6,
        6,
        15,
        3,
        6,
        5,
        8,
        5,
        13,
        2,
        2,
        5,
        9,
        1,
        1,
        5,
        13,
        10,
        1,
        2,
        4
      ],
      "2021-07": [
        57,
        9,
        2,
        10,
        10,
        9,
        2,
        7,
        7,
        4,
        8,
        5,
        4,
        4,
        1,
        9,
        4,
        5,
        18,
        13,
        2,
        0,
        6
      ],
      "2021-08": [
        64,
        11,
        2,
        13,
        18,
        5,
        2,
        8,
        10,
        4,
        7,
        6,
        3,
        5,
        4,
        2,
        1,
        1,
        5,
        5,
        1,
        2,
        4
      ],
      "2021-09": [
        60,
        10,
        0,
        11,
        11,
        12,
        6,
        9,
        3,
        6,
        12,
        4,
        2,
        4,
        5,
        4,
        3,
        1,
        9,
        9,
        2,
        0,
        7
      ],
      "2021-10": [
        58,
        16,
        2,
        15,
        12,
        8,
        4,
        6,
        3,
        7,
        12,
        4,
        3,
        2,
        2,
        2,
        6,
        0,
        12,
        8,
        1,
        4,
        6
      ],
      "2021-11": [
        47,
        10,
        3,
        8,
        8,
        6,
        4,
        1,
        6,
        5,
        9,
        9,
        2,
        4,
        6,
        3,
        1,
        1,
        4,
        1,
        2,
        2,
        10
      ],
      "2021-12": [
        40,
        3,
        5,
        8,
        7,
        3,
        2,
        2,
        2,
        16,
        13,
        5,
        3,
        1,
        4,
        4,
        3,
        1,
        4,
        9,
        4,
        0,
        6
      ],
      "2022-01": [
        59,
        14,
        4,
        10,
        12,
        6,
        2,
        8,
        4,
        8,
        9,
        3,
        6,
        6,
        9,
        2,
        3,
        3,
        10,
        9,
        1,
        0,
        3
      ],
      "2022-02": [
        45,
        11,
        6,
        8,
        19,
        9,
        2,
        8,
        6,
        7,
        13,
        13,
        3,
        3,
        9,
        4,
        4,
        2,
        4,
        11,
        2,
        3,
        6
      ],
      "2022-03": [
        59,
        27,
        6,
        14,
        15,
        5,
        3,
        4,
        5,
        8,
        14,
        4,
        3,
        2,
        4,
        5,
        2,
        2,
        5,
        12,
        3,
        0,
        2
      ],
      "2022-04": [
        72,
        8,
        11,
        10,
        16,
        12,
        2,
        15,
        3,
        4,
        22,
        7,
        5,
        7,
        8,
        1,
        5,
        5,
        4,
        15,
        2,
        1,
        10
      ],
      "2022-05": [
        47,
        15,
        5,
        3,
        18,
        8,
        8,
        13,
        6,
        5,
        14,
        8,
        1,
        2,
        9,
        1,
        1,
        3,
        12,
        9,
        3,
        4,
        6
      ],
      "2022-06": [
        60,
        13,
        0,
        10,
        8,
        7,
        5,
        5,
        7,
        3,
        23,
        7,
        6,
        4,
        3,
        2,
        4,
        5,
        2,
        6,
        2,
        3,
        4
      ],
      "2022-07": [
        64,
        9,
        7,
        16,
        11,
        2,
        3,
        7,
        7,
        6,
        11,
        3,
        0,
        5,
        7,
        6,
        1,
        6,
        7,
        12,
        0,
        3,
        8
      ],
      "2022-08": [
        79,
        16,
        5,
        12,
        16,
        4,
        11,
        8,
        14,
        5,
        14,
        5,
        4,
        1,
        6,
        4,
        2,
        5,
        15,
        13,
        4,
        3,
        2
      ],
      "2022-09": [
        86,
        17,
        1,
        16,
        13,
        6,
        6,
        10,
        6,
        5,
        8,
        4,
        0,
        5,
        8,
        5,
        5,
        3,
        9,
        16,
        2,
        1,
        6
      ],
      "2022-10": [
        62,
        25,
        5,
        23,
        13,
        8,
        8,
        8,
        6,
        8,
        11,
        7,
        3,
        4,
        8,
        8,
        5,
        0,
        14,
        8,
        1,
        1,
        4
      ],
      "2022-11": [
        42,
        6,
        6,
        10,
        22,
        6,
        2,
        13,
        2,
        7,
        15,
        11,
        3,
        2,
        12,
        5,
        1,
        4,
        5,
        7,
        2,
        2,
        4
      ],
      "2022-12": [
        43,
        8,
        0,
        11,
        14,
        6,
        4,
        5,
        11,
        10,
        10,
        6,
        2,
        1,
        4,
        5,
        1,
        0,
        4,
        10,
        3,
        1,
        4
      ],
      "2023-01": [
        55,
        15,
        2,
        15,
        20,
        9,
        6,
        4,
        8,
        9,
        29,
        6,
        2,
        2,
        5,
        6,
        3,
        4,
        7,
        8,
        4,
        2,
        5
      ],
      "2023-02": [
        58,
        14,
        8,
        27,
        30,
        11,
        16,
        11,
        10,
        8,
        22,
        4,
        3,
        1,
        12,
        2,
        2,
        1,
        9,
        11,
        6,
        0,
        7
      ],
      "2023-03": [
        107,
        32,
        8,
        17,
        13,
        11,
        17,
        20,
        9,
        10,
        26,
        7,
        3,
        7,
        9,
        7,
        1,
        4,
        6,
        11,
        2,
        3,
        9
      ],
      "2023-04": [
        68,
        23,
        15,
        15,
        22,
        11,
        35,
        22,
        5,
        5,
        23,
        9,
        6,
        4,
        7,
        5,
        2,
        5,
        10,
        14,
        6,
        2,
        1
      ],
      "2023-05": [
        62,
        11,
        6,
        10,
        22,
        16,
        29,
        14,
        10,
        7,
        38,
        15,
        7,
        2,
        3,
        8,
        3,
        10,
        18,
        10,
        1,
        1,
        6
      ],
      "2023-06": [
        62,
        15,
        13,
        20,
        24,
        4,
        35,
        7,
        16,
        5,
        26,
        9,
        6,
        6,
        6,
        5,
        2,
        2,
        7,
        10,
        4,
        3,
        14
      ],
      "2023-07": [
        68,
        17,
        5,
        14,
        10,
        8,
        21,
        19,
        4,
        15,
        18,
        8,
        6,
        3,
        9,
        4,
        2,
        10,
        15,
        15,
        4,
        1,
        3
      ],
      "2023-08": [
        103,
        21,
        9,
        29,
        27,
        17,
        22,
        7,
        8,
        12,
        26,
        9,
        6,
        4,
        11,
        7,
        5,
        4,
        11,
        17,
        11,
        1,
        10
      ],
      "2023-09": [
        89,
        20,
        7,
        22,
        22,
        10,
        48,
        6,
        12,
        11,
        26,
        9,
        3,
        9,
        9,
        5,
        5,
        3,
        14,
        9,
        4,
        2,
        2
      ],
      "2023-10": [
        79,
        16,
        3,
        19,
        24,
        12,
        59,
        13,
        8,
        7,
        30,
        8,
        2,
        4,
        8,
        5,
        9,
        11,
        10,
        8,
        2,
        2,
        1
      ],
      "2023-11": [
        53,
        14,
        10,
        21,
        17,
        9,
        41,
        16,
        9,
        20,
        27,
        8,
        3,
        6,
        11,
        5,
        0,
        10,
        8,
        8,
        4,
        3,
        3
      ],
      "2023-12": [
        52,
        13,
        10,
        13,
        11,
        16,
        33,
        19,
        9,
        7,
        24,
        15,
        4,
        8,
        11,
        3,
        4,
        6,
        9,
        7,
        3,
        5,
        4
      ],
      "2024-01": [
        82,
        23,
        13,
        17,
        22,
        15,
        48,
        18,
        6,
        11,
        35,
        9,
        4,
        9,
        7,
        6,
        3,
        13,
        13,
        8,
        2,
        2,
        10
      ],
      "2024-02": [
        87,
        20,
        8,
        25,
        20,
        20,
        69,
        25,
        11,
        5,
        53,
        8,
        4,
        4,
        20,
        7,
        5,
        16,
        12,
        7,
        7,
        8,
        5
      ],
      "2024-03": [
        91,
        39,
        14,
        34,
        27,
        7,
        59,
        25,
        18,
        10,
        54,
        9,
        7,
        5,
        13,
        8,
        9,
        10,
        18,
        13,
        2,
        5,
        10
      ],
      "2024-04": [
        93,
        19,
        9,
        43,
        14,
        15,
        70,
        35,
        11,
        14,
        32,
        15,
        7,
        5,
        23,
        7,
        12,
        7,
        14,
        8,
        2,
        4,
        5
      ],
      "2024-05": [
        96,
        22,
        17,
        17,
        21,
        16,
        74,
        24,
        10,
        11,
        52,
        10,
        2,
        10,
        12,
        5,
        8,
        10,
        15,
        20,
        2,
        1,
        6
      ],
      "2024-06": [
        74,
        19,
        12,
        18,
        15,
        6,
        63,
        16,
        12,
        11,
        31,
        11,
        4,
        3,
        12,
        6,
        5,
        12,
        18,
        12,
        3,
        8,
        4
      ],
      "2024-07": [
        107,
        11,
        16,
        21,
        17,
        10,
        64,
        20,
        9,
        13,
        39,
        9,
        7,
        5,
        9,
        9,
        3,
        13,
        22,
        13,
        6,
        3,
        4
      ],
      "2024-08": [
        121,
        14,
        7,
        24,
        18,
        13,
        60,
        29,
        11,
        19,
        31,
        12,
        3,
        3,
        3,
        11,
        5,
        7,
        16,
        7,
        4,
        1,
        4
      ],
      "2024-09": [
        109,
        29,
        15,
        24,
        28,
        14,
        86,
        19,
        10,
        10,
        50,
        13,
        2,
        8,
        17,
        17,
        4,
        16,
        11,
        16,
        6,
        6,
        9
      ],
      "2024-10": [
        93,
        25,
        9,
        34,
        25,
        19,
        80,
        25,
        13,
        18,
        48,
        10,
        4,
        6,
        11,
        8,
        10,
        17,
        16,
        11,
        4,
        4,
        10
      ],
      "2024-11": [
        85,
        22,
        9,
        33,
        17,
        12,
        64,
        23,
        5,
        14,
        46,
        10,
        1,
        7,
        13,
        44,
        9,
        8,
        16,
        9,
        3,
        6,
        6
      ],
      "2024-12": [
        85,
        21,
        12,
        16,
        18,
        7,
        52,
        16,
        8,
        22,
        59,
        6,
        3,
        3,
        10,
        5,
        7,
        12,
        9,
        8,
        7,
        6,
        4
      ],
      "2025-01": [
        97,
        35,
        12,
        28,
        19,
        7,
        59,
        20,
        6,
        20,
        55,
        12,
        3,
        6,
        14,
        6,
        9,
        10,
        11,
        5,
        7,
        3,
        2
      ],
      "2025-02": [
        83,
        39,
        11,
        35,
        28,
        27,
        102,
        31,
        14,
        16,
        93,
        12,
        2,
        3,
        17,
        13,
        12,
        16,
        23,
        11,
        9,
        1,
        5
      ],
      "2025-03": [
        105,
        39,
        11,
        28,
        30,
        18,
        102,
        36,
        13,
        6,
        76,
        11,
        3,
        4,
        25,
        14,
        11,
        16,
        24,
        11,
        2,
        5,
        3
      ],
      "2025-04": [
        108,
        30,
        10,
        45,
        27,
        13,
        107,
        37,
        15,
        13,
        102,
        10,
        1,
        3,
        13,
        8,
        17,
        17,
        11,
        15,
        12,
        5,
        1
      ],
      "2025-05": [
        103,
        27,
        15,
        24,
        24,
        15,
        95,
        28,
        10,
        15,
        82,
        12,
        9,
        2,
        24,
        8,
        10,
        12,
        15,
        14,
        7,
        5,
        8
      ],
      "2025-06": [
        98,
        33,
        14,
        17,
        21,
        14,
        81,
        30,
        13,
        15,
        76,
        13,
        3,
        6,
        21,
        9,
        4,
        16,
        10,
        14,
        8,
        4,
        3
      ],
      "2025-07": [
        152,
        28,
        13,
        25,
        20,
        12,
        90,
        26,
        14,
        18,
        55,
        20,
        3,
        3,
        11,
        12,
        8,
        17,
        13,
        9,
        10,
        5,
        1
      ],
      "2025-08": [
        120,
        22,
        11,
        32,
        18,
        13,
        91,
        32,
        16,
        22,
        74,
        18,
        6,
        2,
        24,
        11,
        8,
        10,
        14,
        16,
        3,
        4,
        6
      ],
      "2025-09": [
        57,
        16,
        6,
        18,
        13,
        5,
        50,
        16,
        3,
        3,
        44,
        4,
        0,
        1,
        13,
        3,
        6,
        5,
        9,
        5,
        2,
        3,
        3
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Design-Specific Transformations in Visualization",
          "year": "2024-07",
          "abstract": "In visualization, the process of transforming raw data into visually\ncomprehensible representations is pivotal. While existing models like the\nInformation Visualization Reference Model describe the data-to-visual mapping\nprocess, they often overlook a crucial intermediary step: design-specific\ntransformations. This process, occurring after data transformation but before\nvisual-data mapping, further derives data, such as groupings, layout, and\nstatistics, that are essential to properly render the visualization. In this\npaper, we advocate for a deeper exploration of design-specific transformations,\nhighlighting their importance in understanding visualization properties,\nparticularly in relation to user tasks. We incorporate design-specific\ntransformations into the Information Visualization Reference Model and propose\na new formalism that encompasses the user task as a function over data. The\nresulting formalism offers three key benefits over existing visualization\nmodels: (1) describing task as compositions of functions, (2) enabling analysis\nof data transformations for visual-data mapping, and (3) empowering reasoning\nabout visualization correctness and effectiveness. We further discuss the\npotential implications of this model on visualization theory and visualization\nexperiment design.",
          "arxiv_id": "2407.06404v2"
        },
        {
          "title": "The Landscape of College-level Data Visualization Courses, and the Benefits of Incorporating Statistical Thinking",
          "year": "2024-12",
          "abstract": "Data visualization is a core part of statistical practice and is ubiquitous\nin many fields. Although there are numerous books on data visualization,\ninstructors in statistics and data science may be unsure how to teach data\nvisualization, because it is such a broad discipline. To give guidance on\nteaching data visualization from a statistical perspective, we make two\ncontributions. First, we conduct a survey of data visualization courses at top\ncolleges and universities in the United States, in order to understand the\nlandscape of data visualization courses. We find that most courses are not\ntaught by statistics and data science departments and do not focus on\nstatistical topics, especially those related to inference. Instead, most\ncourses focus on visual storytelling, aesthetic design, dashboard design, and\nother topics specialized for other disciplines. Second, we outline three\nteaching principles for incorporating statistical inference in data\nvisualization courses, and provide several examples that demonstrate how to\nfollow these principles. The dataset from our survey allows others to explore\nthe diversity of data visualization courses, and our teaching principles give\nguidance for encouraging statistical thinking when teaching data visualization.",
          "arxiv_id": "2412.16402v2"
        },
        {
          "title": "A Formalism and Library for Database Visualization",
          "year": "2025-04",
          "abstract": "Existing data visualization formalisms are restricted to single-table inputs,\nwhich makes existing visualization grammars like Vega-lite or ggplot2 tedious\nto use, have overly complex APIs, and unsound when visualization multi-table\ndata. This paper presents the first visualization formalism to support\ndatabases as input -- in other words, *database visualization*. A visualization\nspecification is defined as a mapping from database constraints (e.g., schemas,\ntypes, foreign keys) to visual representations of those constraints, and we\nstate that a visualization is *faithful* if it visually preserves the\nunderlying database constraints. This formalism explains how visualization\ndesigns are the result of implicit data modeling decisions. We further develop\na javascript library called dvl and use a series of case studies to show its\nexpressiveness over specialized visualization systems and existing\ngrammar-based languages.",
          "arxiv_id": "2504.08979v1"
        }
      ],
      "1": [
        {
          "title": "A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training",
          "year": "2023-10",
          "abstract": "This paper presents a novel human-robot interaction setup for robot and human\nlearning of symbolic language for identifying robot homeostatic needs. The\nrobot and human learn to use and respond to the same language symbols that\nconvey homeostatic needs and the stimuli that satisfy the homeostatic needs,\nrespectively. We adopted a differential outcomes training (DOT) protocol\nwhereby the robot provides feedback specific (differential) to its internal\nneeds (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We\nfound evidence that DOT can enhance the human's learning efficiency, which in\nturn enables more efficient robot language acquisition. The robot used in the\nstudy has a vocabulary similar to that of a human infant in the linguistic\n``babbling'' phase. The robot software architecture is built upon a model for\naffect-grounded language acquisition where the robot associates vocabulary with\ninternal needs (hunger, thirst, curiosity) through interactions with the human.\nThe paper presents the results of an initial pilot study conducted with the\ninteractive setup, which reveal that the robot's language acquisition achieves\nhigher convergence rate in the DOT condition compared to the non-DOT control\ncondition. Additionally, participants reported positive affective experiences,\nfeeling of being in control, and an empathetic connection with the robot. This\nmutual learning (teacher-student learning) approach offers a potential\ncontribution of facilitating cognitive interventions with DOT (e.g. for people\nwith dementia) through increased therapy adherence as a result of engaging\nhumans more in training tasks by taking an active teaching-learning role. The\nhomeostatic motivational grounding of the robot's language acquisition has\npotential to contribute to more ecologically valid and social\n(collaborative/nurturing) interactions with robots.",
          "arxiv_id": "2310.13377v1"
        },
        {
          "title": "A Review on Trust in Human-Robot Interaction",
          "year": "2021-05",
          "abstract": "Due to agile developments in the field of robotics and human-robot\ninteraction, prospective robotic agents are intended to play the role of\nteammates and partner with humans to perform operations, rather than tools that\nare replacing humans helping humans in a specific task. this notion of\npartnering with robots raises new challenges for human-robot interaction (HRI),\nwhich gives rise to a new field of research in HRI, namely human-robot trust.\nWhere humans and robots are working as partners, the performance of the work\ncan be diminished if humans do not trust robots appropriately. Considering the\nimpact of human-robot trust observed in different HRI fields, many researchers\nhave investigated the field of human-robot trust and examined various concerns\nrelated to human-robot trust. In this work, we review the past works on\nhuman-robot trust based on the research topics and discuss selected trends in\nthis field. Based on these reviews, we finally propose some ideas and areas of\npotential future research at the end of this paper.",
          "arxiv_id": "2105.10045v1"
        },
        {
          "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes",
          "year": "2025-08",
          "abstract": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human\nand robot interactions, especially for close-proximity collaboration tasks such\nas human-robot handovers. Learning robot manipulation policies from raw,\nreal-world image data requires a large number of robot-action trials in the\nphysical environment. Although simulation training offers a cost-effective\nalternative, the visual domain gap between simulation and robot workspace\nremains a major limitation. We introduce a method for training HRT policies,\nfocusing on human-to-robot handovers, solely from RGB images without the need\nfor real-robot training or real-robot data collection. The goal is to enable\nthe robot to reliably receive objects from a human with stable grasping while\navoiding collisions with the human hand. The proposed policy learner leverages\nsparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes\nto generate robot demonstrations containing image-action pairs captured with a\ncamera mounted on the robot gripper. As a result, the simulated camera pose\nchanges in the reconstructed scene can be directly translated into gripper pose\nchanges. Experiments in both Gaussian Splatting reconstructed scene and\nreal-world human-to-robot handover experiments demonstrate that our method\nserves as a new and effective representation for the human-to-robot handover\ntask, contributing to more seamless and robust HRT.",
          "arxiv_id": "2508.09855v1"
        }
      ],
      "2": [
        {
          "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing Essays",
          "year": "2025-01",
          "abstract": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom.",
          "arxiv_id": "2501.10551v4"
        },
        {
          "title": "An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project",
          "year": "2024-03",
          "abstract": "Background: Large Language Models (LLMs) such as ChatGPT and CoPilot are\ninfluencing software engineering practice. Software engineering educators must\nteach future software engineers how to use such tools well. As of yet, there\nhave been few studies that report on the use of LLMs in the classroom. It is,\ntherefore, important to evaluate students' perception of LLMs and possible ways\nof adapting the computing curriculum to these shifting paradigms.\n  Purpose: The purpose of this study is to explore computing students'\nexperiences and approaches to using LLMs during a semester-long software\nengineering project.\n  Design/Method: We collected data from a senior-level software engineering\ncourse at Purdue University. This course uses a project-based learning (PBL)\ndesign. The students used LLMs such as ChatGPT and Copilot in their projects. A\nsample of these student teams were interviewed to understand (1) how they used\nLLMs in their projects; and (2) whether and how their perspectives on LLMs\nchanged over the course of the semester. We analyzed the data to identify\nthemes related to students' usage patterns and learning outcomes.\n  Results/Discussion: When computing students utilize LLMs within a project,\ntheir use cases cover both technical and professional applications. In\naddition, these students perceive LLMs to be efficient tools in obtaining\ninformation and completion of tasks. However, there were concerns about the\nresponsible use of LLMs without being detrimental to their own learning\noutcomes. Based on our findings, we recommend future research to investigate\nthe usage of LLM's in lower-level computer engineering courses to understand\nwhether and how LLMs can be integrated as a learning aid without hurting the\nlearning outcomes.",
          "arxiv_id": "2403.18679v2"
        },
        {
          "title": "A \"watch your replay videos\" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection",
          "year": "2025-07",
          "abstract": "Generative AI is disrupting computing education. Most interventions focus on\nteaching GenAI use rather than helping students understand how AI changes their\nprogramming process. We designed and deployed a novel comparative video\nreflection assignment adapting the Describe, Examine, then Articulate Learning\n(DEAL) framework. In an introductory software engineering course, students\nrecorded themselves programming during their team project two times: first\nwithout, then with using generative AI. Students then analyzed their own videos\nusing a scaffolded set of reflection questions, including on their programming\nprocess and human, internet, and AI help-seeking. We conducted a qualitative\nthematic analysis of the reflections, finding students developed insights about\nplanning, debugging, and help-seeking behaviors that transcended AI use.\nStudents reported learning to slow down and understand before writing or\ngenerating code, recognized patterns in their problem-solving approaches, and\narticulated specific process improvements. Students also learned and reflected\non AI limits and downsides, and strategies to use AI more critically, including\nbetter prompting but also to benefit their learning instead of just completing\ntasks. Unexpectedly, the comparative reflection also scaffolded reflection on\nprogramming not involving AI use, and even led to students spontaneously\nsetting future goals to adopt video and other regular reflection. This work\ndemonstrates structured reflection on programming session videos can develop\nmetacognitive skills essential for programming with and without generative AI\nand also lifelong learning in our evolving field.",
          "arxiv_id": "2507.17226v1"
        }
      ],
      "3": [
        {
          "title": "Facing the Illusion and Reality of Safety in Social VR",
          "year": "2022-04",
          "abstract": "The ethical design of social Virtual Reality (VR) is not a new topic, but\n\"safety\" concerns of using social VR are escalated to a different level given\nthe heat of the Metaverse. For example, it was reported that nearly half of the\nfemale-identifying VR participants have had at least one instance of virtual\nsexual harassment. Feeling safe is a basic human right - in any place,\nregardless in real or virtual spaces. In this paper, we are seeking to\nunderstand the discrepancy between user concerns and designs in protecting user\nsafety in social VR applications. We study safety concerns on social VR\nexperience first by analyzing Twitter posts and then synthesize practices on\nsafety protection adopted by four mainstream social VR platforms. We argue that\nfuture research and platforms should explore the design of social VR with\nboundary-awareness.",
          "arxiv_id": "2204.07121v2"
        },
        {
          "title": "How to Improve Your Virtual Experience -- Exploring the Obstacles of Mainstream VR",
          "year": "2020-09",
          "abstract": "What is Virtual Reality? A professional tool, made to facilitate our everyday\ntasks? A conceptual mistake, accompanied by cybersickness and unsolved\nlocomotion issues since the very beginning? Or just another source of\nentertainment that helps us escape from our deteriorating world? The public and\nscientific opinions in this respect are diverse. Furthermore, as researchers,\nwe sometimes ask ourselves whether our work in this area is really \"worth it\",\ngiven the ambiguous prognosis regarding the future of VR. To tackle this\nquestion, we explore three different areas of VR research in this dissertation,\nnamely locomotion, interaction, and perception. We begin our journey by\nstructuring VR locomotion and by introducing a novel locomotion concept for\nlarge distance traveling via virtual body resizing. In the second part, we\nfocus on our interaction possibilities in VR. We learn how to represent virtual\nobjects via self-transforming controllers and how to store our items in VR\ninventories. We design comprehensive 3D gestures for the audience and provide\nan I/O abstraction layer to facilitate the realization and usage of such\ndiverse interaction modalities. The third part is dedicated to the exploration\nof perceptual phenomena in VR. In contrast to locomotion and interaction, our\ncontributions in the field of perception emphasize the strong points of\nimmersive setups. We utilize VR to transfer the illusion of virtual body\nownership to nonhumanoid avatars and exploit this phenomenon for novel gaming\nexperiences with animals in the leading role. As one of our contributions, we\ndemonstrate how to repurpose the dichoptic presentation capability of immersive\nsetups for preattentive zero-overhead highlighting in visualizations. We round\noff the dissertation by coming back to VR research in general, providing a\ncritical assessment of our contributions and sharing our lessons learned along\nthe way.",
          "arxiv_id": "2009.04272v1"
        },
        {
          "title": "VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People",
          "year": "2025-08",
          "abstract": "Virtual Reality (VR) is inaccessible to blind people. While research has\ninvestigated many techniques to enhance VR accessibility, they require\nadditional developer effort to integrate. As such, most mainstream VR apps\nremain inaccessible as the industry de-prioritizes accessibility. We present\nVRSight, an end-to-end system that recognizes VR scenes post hoc through a set\nof AI models (e.g., object detection, depth estimation, LLM-based atmosphere\ninterpretation) and generates tone-based, spatial audio feedback, empowering\nblind users to interact in VR without developer intervention. To enable virtual\nelement detection, we further contribute DISCOVR, a VR dataset consisting of 30\nvirtual object classes from 17 social VR apps, substituting real-world datasets\nthat remain not applicable to VR contexts. Nine participants used VRSight to\nexplore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in\nfacilitating social tasks like avatar awareness and available seat\nidentification.",
          "arxiv_id": "2508.02958v1"
        }
      ],
      "4": [
        {
          "title": "Personalizing Content Moderation on Social Media: User Perspectives on Moderation Choices, Interface Design, and Labor",
          "year": "2023-05",
          "abstract": "Social media platforms moderate content for each user by incorporating the\noutputs of both platform-wide content moderation systems and, in some cases,\nuser-configured personal moderation preferences. However, it is unclear (1) how\nend users perceive the choices and affordances of different kinds of personal\ncontent moderation tools, and (2) how the introduction of personalization\nimpacts user perceptions of platforms' content moderation responsibilities.\nThis paper investigates end users' perspectives on personal content moderation\ntools by conducting an interview study with a diverse sample of 24 active\nsocial media users. We probe interviewees' preferences using simulated personal\nmoderation interfaces, including word filters, sliders for toxicity levels, and\nboolean toxicity toggles. We also examine the labor involved for users in\nchoosing moderation settings and present users' attitudes about the roles and\nresponsibilities of social media platforms and other stakeholders towards\nmoderation. We discuss how our findings can inform design solutions to improve\ntransparency and controllability in personal content moderation tools.",
          "arxiv_id": "2305.10374v1"
        },
        {
          "title": "Trust and Friction: Negotiating How Information Flows Through Decentralized Social Media",
          "year": "2025-03",
          "abstract": "Decentralized social media protocols enable users in independent, user-hosted\nservers (i.e., instances) to interact with each other while they self-govern.\nThis community-based model of social media governance opens up new\nopportunities for tailored decision-making about information flows -- i.e.,\nwhat user data is shared to whom and when -- and in turn, for protecting user\nprivacy. To better understand how community governance shapes privacy\nexpectations on decentralized social media, we conducted a semi-structured\ninterview with 23 users of the Fediverse, a decentralized social media network.\nOur findings illustrate important factors that shape a community's\nunderstandings of information flows, such as rules and proactive efforts from\nadmins who are perceived as trustworthy. We also highlight ''governance\nfrictions'' between communities that raise new privacy risks due to\nincompatibilities in values, security practices, and software. Our findings\nhighlight the unique challenges of decentralized social media, suggest design\nopportunities to address frictions, and outline the role of participatory\ndecision-making to realize the full potential of decentralization.",
          "arxiv_id": "2503.02150v1"
        },
        {
          "title": "Teen Talk: The Good, the Bad, and the Neutral of Adolescent Social Media Use",
          "year": "2024-09",
          "abstract": "The debate on whether social media has a net positive or negative effect on\nyouth is ongoing. Therefore, we conducted a thematic analysis on 2,061 posts\nmade by 1,038 adolescents aged 15-17 on an online peer-support platform to\ninvestigate the ways in which these teens discussed popular social media\nplatforms in their posts and to identify differences in their experiences\nacross platforms. Our findings revealed four main emergent themes for the ways\nin which social media was discussed: 1) Sharing negative experiences or\noutcomes of social media use (58%, n = 1,095), 2) Attempts to connect with\nothers (45%, n = 922), 3) Highlighting the positive side of social media use\n(20%, n = 409), and 4) Seeking information (20%, n = 491). Overall, while\nsharing about negative experiences was more prominent, teens also discussed\nbalanced perspectives of connection-seeking, positive experiences, and\ninformation support on social media that should not be discounted. Moreover, we\nfound statistical significance for how these experiences differed across social\nmedia platforms. For instance, teens were most likely to seek romantic\nrelationships on Snapchat and self-promote on YouTube. Meanwhile, Instagram was\nmentioned most frequently for body shaming, and Facebook was the most commonly\ndiscussed platform for privacy violations (mostly from parents). The key\ntakeaway from our study is that the benefits and drawbacks of teens' social\nmedia usage can co-exist and net effects (positive or negative) can vary across\ndifferent teens across various contexts. As such, we advocate for mitigating\nthe negative experiences and outcomes of social media use as voiced by teens,\nto improve, rather than limit or restrict, their overall social media\nexperience. We do this by taking an affordance perspective that aims to promote\nthe digital well-being and online safety of youth \"by design.\"",
          "arxiv_id": "2409.02358v2"
        }
      ],
      "5": [
        {
          "title": "Explainable AI-Based Interface System for Weather Forecasting Model",
          "year": "2025-04",
          "abstract": "Machine learning (ML) is becoming increasingly popular in meteorological\ndecision-making. Although the literature on explainable artificial intelligence\n(XAI) is growing steadily, user-centered XAI studies have not extend to this\ndomain yet. This study defines three requirements for explanations of black-box\nmodels in meteorology through user studies: statistical model performance for\ndifferent rainfall scenarios to identify model bias, model reasoning, and the\nconfidence of model outputs. Appropriate XAI methods are mapped to each\nrequirement, and the generated explanations are tested quantitatively and\nqualitatively. An XAI interface system is designed based on user feedback. The\nresults indicate that the explanations increase decision utility and user\ntrust. Users prefer intuitive explanations over those based on XAI algorithms\neven for potentially easy-to-recognize examples. These findings can provide\nevidence for future research on user-centered XAI algorithms, as well as a\nbasis to improve the usability of AI systems in practice.",
          "arxiv_id": "2504.00795v1"
        },
        {
          "title": "Engaging with AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision-Making",
          "year": "2025-01",
          "abstract": "As reliance on AI systems for decision-making grows, it becomes critical to\nensure that human users can appropriately balance trust in AI suggestions with\ntheir own judgment, especially in high-stakes domains like healthcare. However,\nhuman + AI teams have been shown to perform worse than AI alone, with evidence\nindicating automation bias as the reason for poorer performance, particularly\nbecause humans tend to follow AI's recommendations even when they are\nincorrect. In many existing human + AI systems, decision-making support is\ntypically provided in the form of text explanations (XAI) to help users\nunderstand the AI's reasoning. Since human decision-making often relies on\nSystem 1 thinking, users may ignore or insufficiently engage with the\nexplanations, leading to poor decision-making. Previous research suggests that\nthere is a need for new approaches that encourage users to engage with the\nexplanations and one proposed method is the use of cognitive forcing functions\n(CFFs). In this work, we examine how various decision-support mechanisms impact\nuser engagement, trust, and human-AI collaborative task performance in a\ndiabetes management decision-making scenario. In a controlled experiment with\n108 participants, we evaluated the effects of six decision-support mechanisms\nsplit into two categories of explanations (text, visual) and four CFFs. Our\nfindings reveal that mechanisms like AI confidence levels, text explanations,\nand performance visualizations enhanced human-AI collaborative task\nperformance, and improved trust when AI reasoning clues were provided.\nMechanisms like human feedback and AI-driven questions encouraged deeper\nreflection but often reduced task performance by increasing cognitive effort,\nwhich in turn affected trust. Simple mechanisms like visual explanations had\nlittle effect on trust, highlighting the importance of striking a balance in\nCFF and XAI design.",
          "arxiv_id": "2501.16627v1"
        },
        {
          "title": "The Impact of Imperfect XAI on Human-AI Decision-Making",
          "year": "2023-07",
          "abstract": "Explainability techniques are rapidly being developed to improve human-AI\ndecision-making across various cooperative work settings. Consequently,\nprevious research has evaluated how decision-makers collaborate with imperfect\nAI by investigating appropriate reliance and task performance with the aim of\ndesigning more human-centered computer-supported collaborative tools. Several\nhuman-centered explainable AI (XAI) techniques have been proposed in hopes of\nimproving decision-makers' collaboration with AI; however, these techniques are\ngrounded in findings from previous studies that primarily focus on the impact\nof incorrect AI advice. Few studies acknowledge the possibility of the\nexplanations being incorrect even if the AI advice is correct. Thus, it is\ncrucial to understand how imperfect XAI affects human-AI decision-making. In\nthis work, we contribute a robust, mixed-methods user study with 136\nparticipants to evaluate how incorrect explanations influence humans'\ndecision-making behavior in a bird species identification task, taking into\naccount their level of expertise and an explanation's level of assertiveness.\nOur findings reveal the influence of imperfect XAI and humans' level of\nexpertise on their reliance on AI and human-AI team performance. We also\ndiscuss how explanations can deceive decision-makers during human-AI\ncollaboration. Hence, we shed light on the impacts of imperfect XAI in the\nfield of computer-supported cooperative work and provide guidelines for\ndesigners of human-AI collaboration systems.",
          "arxiv_id": "2307.13566v4"
        }
      ],
      "6": [
        {
          "title": "Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages",
          "year": "2025-02",
          "abstract": "Recent advances in generative AI have precipitated a proliferation of novel\nwriting assistants. These systems typically rely on multilingual large language\nmodels (LLMs), providing globalized workers the ability to revise or create\ndiverse forms of content in different languages. However, there is substantial\nevidence indicating that the performance of multilingual LLMs varies between\nlanguages. Users who employ writing assistance for multiple languages are\ntherefore susceptible to disparate output quality. Importantly, recent research\nhas shown that people tend to generalize algorithmic errors across independent\ntasks, violating the behavioral axiom of choice independence. In this paper, we\nanalyze whether user utilization of novel writing assistants in a charity\nadvertisement writing task is affected by the AI's performance in a second\nlanguage. Furthermore, we quantify the extent to which these patterns translate\ninto the persuasiveness of generated charity advertisements, as well as the\nrole of peoples' beliefs about LLM utilization in their donation choices. Our\nresults provide evidence that writers who engage with an LLM-based writing\nassistant violate choice independence, as prior exposure to a Spanish LLM\nreduces subsequent utilization of an English LLM. While these patterns do not\naffect the aggregate persuasiveness of the generated advertisements, people's\nbeliefs about the source of an advertisement (human versus AI) do. In\nparticular, Spanish-speaking female participants who believed that they read an\nAI-generated advertisement strongly adjusted their donation behavior downwards.\nFurthermore, people are generally not able to adequately differentiate between\nhuman-generated and LLM-generated ads. Our work has important implications for\nthe design, development, integration, and adoption of multilingual LLMs as\nassistive agents -- particularly in writing tasks.",
          "arxiv_id": "2502.09532v1"
        },
        {
          "title": "Navigating the Path of Writing: Outline-guided Text Generation with Large Language Models",
          "year": "2024-04",
          "abstract": "Large Language Models (LLMs) have impacted the writing process, enhancing\nproductivity by collaborating with humans in content creation platforms.\nHowever, generating high-quality, user-aligned text to satisfy real-world\ncontent creation needs remains challenging. We propose WritingPath, a framework\nthat uses explicit outlines to guide LLMs in generating goal-oriented,\nhigh-quality text. Our approach draws inspiration from structured writing\nplanning and reasoning paths, focusing on reflecting user intentions throughout\nthe writing process. To validate our approach in real-world scenarios, we\nconstruct a diverse dataset from unstructured blog posts to benchmark writing\nperformance and introduce a comprehensive evaluation framework assessing the\nquality of outlines and generated texts. Our evaluations with various LLMs\ndemonstrate that the WritingPath approach significantly enhances text quality\naccording to evaluations by both LLMs and professional writers.",
          "arxiv_id": "2404.13919v2"
        },
        {
          "title": "Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers",
          "year": "2023-09",
          "abstract": "The development of large language models (LLMs) capable of following\ninstructions and engaging in conversational interactions sparked increased\ninterest in their utilization across various support tools. We investigate the\nutility of modern LLMs in assisting professional writers via an empirical user\nstudy (n=30). The design of our collaborative writing interface is grounded in\nthe cognitive process model of writing that views writing as a goal-oriented\nthinking process encompassing non-linear cognitive activities: planning,\ntranslating, and reviewing. Participants are asked to submit a post-completion\nsurvey to provide feedback on the potential and pitfalls of LLMs as writing\ncollaborators. Upon analyzing the writer-LLM interactions, we find that while\nwriters seek LLM's help across all three types of cognitive activities, they\nfind LLMs more helpful in translation and reviewing. Our findings from\nanalyzing both the interactions and the survey responses highlight future\nresearch directions in creative writing assistance using LLMs.",
          "arxiv_id": "2309.12570v3"
        }
      ],
      "7": [
        {
          "title": "Constructing Dreams using Generative AI",
          "year": "2023-05",
          "abstract": "Generative AI tools introduce new and accessible forms of media creation for\nyouth. They also raise ethical concerns about the generation of fake media,\ndata protection, privacy and ownership of AI-generated art. Since generative AI\nis already being used in products used by youth, it is critical that they\nunderstand how these tools work and how they can be used or misused. In this\nwork, we facilitated students' generative AI learning through expression of\ntheir imagined future identities. We designed a learning workshop - Dreaming\nwith AI - where students learned about the inner workings of generative AI\ntools, used text-to-image generation algorithms to create their imaged future\ndreams, reflected on the potential benefits and harms of generative AI tools\nand voiced their opinions about policies for the use of these tools in\nclassrooms. In this paper, we present the learning activities and experiences\nof 34 high school students who engaged in our workshops. Students reached\ncreative learning objectives by using prompt engineering to create their future\ndreams, gained technical knowledge by learning the abilities, limitations,\ntext-visual mappings and applications of generative AI, and identified most\npotential societal benefits and harms of generative AI.",
          "arxiv_id": "2305.12013v1"
        },
        {
          "title": "3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows",
          "year": "2022-10",
          "abstract": "Text-to-image AI are capable of generating novel images for inspiration, but\ntheir applications for 3D design workflows and how designers can build 3D\nmodels using AI-provided inspiration have not yet been explored. To investigate\nthis, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a\nplugin that generates 2D image inspiration for 3D design. 3DALL-E allows users\nto construct text and image prompts based on what they are modeling. In a study\nwith 13 designers, we found that designers saw great potential in 3DALL-E\nwithin their workflows and could use text-to-image AI to produce reference\nimages, prevent design fixation, and inspire design considerations. We\nelaborate on prompting patterns observed across 3D modeling tasks and provide\nmeasures of prompt complexity observed across participants. From our findings,\nwe discuss how 3DALL-E can merge with existing generative design workflows and\npropose prompt bibliographies as a form of human-AI design history.",
          "arxiv_id": "2210.11603v2"
        },
        {
          "title": "The Creativity of Text-to-Image Generation",
          "year": "2022-05",
          "abstract": "Text-guided synthesis of images has made a giant leap towards becoming a\nmainstream phenomenon. With text-to-image generation systems, anybody can\ncreate digital images and artworks. This provokes the question of whether\ntext-to-image generation is creative. This paper expounds on the nature of\nhuman creativity involved in text-to-image art (so-called \"AI art\") with a\nspecific focus on the practice of prompt engineering. The paper argues that the\ncurrent product-centered view of creativity falls short in the context of\ntext-to-image generation. A case exemplifying this shortcoming is provided and\nthe importance of online communities for the creative ecosystem of\ntext-to-image art is highlighted. The paper provides a high-level summary of\nthis online ecosystem drawing on Rhodes' conceptual four P model of creativity.\nChallenges for evaluating the creativity of text-to-image generation and\nopportunities for research on text-to-image generation in the field of\nHuman-Computer Interaction (HCI) are discussed.",
          "arxiv_id": "2206.02904v4"
        }
      ],
      "8": [
        {
          "title": "How Do Drivers Self-Regulate their Secondary Task Engagements? The Effect of Driving Automation on Touchscreen Interactions and Glance Behavior",
          "year": "2022-07",
          "abstract": "With ever-improving driver assistance systems and large touchscreens becoming\nthe main in-vehicle interface, drivers are more tempted than ever to engage in\ndistracting non-driving-related tasks. However, little research exists on how\ndriving automation affects drivers' self-regulation when interacting with\ncenter stack touchscreens. To investigate this, we employ multilevel models on\na real-world driving dataset consisting of 10,139 sequences. Our results show\nsignificant differences in drivers' interaction and glance behavior in response\nto varying levels of driving automation, vehicle speed, and road curvature.\nDuring partially automated driving, drivers are not only more likely to engage\nin secondary touchscreen tasks, but their mean glance duration toward the\ntouchscreen also increases by 12% (Level 1) and 20% (Level 2) compared to\nmanual driving. We further show that the effect of driving automation on\ndrivers' self-regulation is larger than that of vehicle speed and road\ncurvature. The derived knowledge can facilitate the safety evaluation of\ninfotainment systems and the development of context-aware driver monitoring\nsystems.",
          "arxiv_id": "2207.04284v2"
        },
        {
          "title": "Importance of Instruction for Pedestrian-Automated Driving Vehicle Interaction with an External Human Machine Interface: Effects on Pedestrians' Situation Awareness, Trust, Perceived Risks and Decision Making",
          "year": "2021-02",
          "abstract": "Compared to a manual driving vehicle (MV), an automated driving vehicle lacks\na way to communicate with the pedestrian through the driver when it interacts\nwith the pedestrian because the driver usually does not participate in driving\ntasks. Thus, an external human machine interface (eHMI) can be viewed as a\nnovel explicit communication method for providing driving intentions of an\nautomated driving vehicle (AV) to pedestrians when they need to negotiate in an\ninteraction, e.g., an encountering scene. However, the eHMI may not guarantee\nthat the pedestrians will fully recognize the intention of the AV. In this\npaper, we propose that the instruction of the eHMI's rationale can help\npedestrians correctly understand the driving intentions and predict the\nbehavior of the AV, and thus their subjective feelings (i.e., dangerous\nfeeling, trust in the AV, and feeling of relief) and decision-making are also\nimproved. The results of an interaction experiment in a road-crossing scene\nindicate that the participants were more difficult to be aware of the situation\nwhen they encountered an AV w/o eHMI compared to when they encountered an MV;\nfurther, the participants' subjective feelings and hesitation in\ndecision-making also deteriorated significantly. When the eHMI was used in the\nAV, the situational awareness, subjective feelings and decision-making of the\nparticipants regarding the AV w/ eHMI were improved. After the instruction, it\nwas easier for the participants to understand the driving intention and predict\ndriving behavior of the AV w/ eHMI. Further, the subjective feelings and the\nhesitation related to decision-making were improved and reached the same\nstandards as that for the MV.",
          "arxiv_id": "2102.07958v4"
        },
        {
          "title": "Vehicle Automation Field Test: Impact on Driver Behavior and Trust",
          "year": "2020-06",
          "abstract": "With the growing technological advances in autonomous driving, the transport\nindustry and research community seek to determine the impact that autonomous\nvehicles (AV) will have on consumers, as well as identify the different factors\nthat will influence their use. Most of the research performed so far relies on\nlaboratory-controlled conditions using driving simulators, as they offer a safe\nenvironment for testing advanced driving assistance systems (ADAS). In this\nstudy we analyze the behavior of drivers that are placed in control of an\nautomated vehicle in a real life driving environment. The vehicle is equipped\nwith advanced autonomy, making driver control of the vehicle unnecessary in\nmany scenarios, although a driver take over is possible and sometimes required.\nIn doing so, we aim to determine the impact of such a system on the driver and\ntheir driving performance. To this end road users' behavior from naturalistic\ndriving data is analyzed focusing on awareness and diagnosis of the road\nsituation. Results showed that the road features determined the level of visual\nattention and trust in the automation. They also showed that the activities\nperformed during the automation affected the reaction time to take over the\ncontrol of the vehicle.",
          "arxiv_id": "2006.02737v2"
        }
      ],
      "9": [
        {
          "title": "Motor Imagery Classification of Single-Arm Tasks Using Convolutional Neural Network based on Feature Refining",
          "year": "2020-02",
          "abstract": "Brain-computer interface (BCI) decodes brain signals to understand user\nintention and status. Because of its simple and safe data acquisition process,\nelectroencephalogram (EEG) is commonly used in non-invasive BCI. One of EEG\nparadigms, motor imagery (MI) is commonly used for recovery or rehabilitation\nof motor functions due to its signal origin. However, the EEG signals are an\noscillatory and non-stationary signal that makes it difficult to collect and\nclassify MI accurately. In this study, we proposed a band-power feature\nrefining convolutional neural network (BFR-CNN) which is composed of two\nconvolution blocks to achieve high classification accuracy. We collected EEG\nsignals to create MI dataset contained the movement imagination of a\nsingle-arm. The proposed model outperforms conventional approaches in 4-class\nMI tasks classification. Hence, we demonstrate that the decoding of user\nintention is possible by using only EEG signals with robust performance using\nBFR-CNN.",
          "arxiv_id": "2002.01122v1"
        },
        {
          "title": "Generating Ten BCI Commands Using Four Simple Motor Imageries",
          "year": "2021-05",
          "abstract": "The brain computer interface (BCI) systems are utilized for transferring\ninformation among humans and computers by analyzing electroencephalogram (EEG)\nrecordings.The process of mentally previewing a motor movement without\ngenerating the corporal output can be described as motor imagery (MI).In this\nemerging research field, the number of commands is also limited in relation to\nthe number of MI tasks; in the current literature, mostly two or four commands\n(classes) are studied. As a solution to this problem, it is recommended to use\nmental tasks as well as MI tasks. Unfortunately, the use of this approach\nreduces the classification performance of MI EEG signals. The fMRI analyses\nshow that the resources in the brain associated with the motor imagery can be\nactivated independently. It is assumed that the brain activity induced by the\nMI of the combination of body parts corresponds to the superposition of the\nactivities generated during each body parts's simple MI. In this study, in\norder to create more than four BCI commands, we suggest to generate combined MI\nEEG signals artificially by using left hand, right hand, tongue, and feet motor\nimageries in pairs. A maximum of ten different BCI commands can be generated by\nusing four motor imageries in pairs.This study aims to achieve high\nclassification performances for BCI commands produced from four motor imageries\nby implementing a small-sized deep neural network (DNN).The presented method is\nevaluated on the four-class datasets of BCI Competitions III and IV, and an\naverage classification performance of 81.8% is achieved for ten classes. The\nabove assumption is also validated on a different dataset which consists of\nsimple and combined MI EEG signals acquired in real time. Trained with the\nartificially generated combined MI EEG signals, DivFE resulted in an average of\n76.5% success rate for the combined MI EEG signals acquired in real-time.",
          "arxiv_id": "2105.14493v1"
        },
        {
          "title": "EEG-Inception: An Accurate and Robust End-to-End Neural Network for EEG-based Motor Imagery Classification",
          "year": "2021-01",
          "abstract": "Classification of EEG-based motor imagery (MI) is a crucial non-invasive\napplication in brain-computer interface (BCI) research. This paper proposes a\nnovel convolutional neural network (CNN) architecture for accurate and robust\nEEG-based MI classification that outperforms the state-of-the-art methods. The\nproposed CNN model, namely EEG-Inception, is built on the backbone of the\nInception-Time network, which showed to be highly efficient and accurate for\ntime-series classification. Also, the proposed network is an end-to-end\nclassification, as it takes the raw EEG signals as the input and does not\nrequire complex EEG signal-preprocessing. Furthermore, this paper proposes a\nnovel data augmentation method for EEG signals to enhance the accuracy, at\nleast by 3%, and reduce overfitting with limited BCI datasets. The proposed\nmodel outperforms all the state-of-the-art methods by achieving the average\naccuracy of 88.4% and 88.6% on the 2008 BCI Competition IV 2a (four-classes)\nand 2b datasets (binary-classes), respectively. Furthermore, it takes less than\n0.025 seconds to test a sample suitable for real-time processing. Moreover, the\nclassification standard deviation for nine different subjects achieves the\nlowest value of 5.5 for the 2b dataset and 7.1 for the 2a dataset, which\nvalidates that the proposed method is highly robust. From the experiment\nresults, it can be inferred that the EEG-Inception network exhibits a strong\npotential as a subject-independent classifier for EEG-based MI tasks.",
          "arxiv_id": "2101.10932v3"
        }
      ],
      "10": [
        {
          "title": "The Model Mastery Lifecycle: A Framework for Designing Human-AI Interaction",
          "year": "2024-08",
          "abstract": "The utilization of AI in an increasing number of fields is the latest\niteration of a long process, where machines and systems have been replacing\nhumans, or changing the roles that they play, in various tasks. Although humans\nare often resistant to technological innovation, especially in workplaces,\nthere is a general trend towards increasing automation, and more recently, AI.\nAI is now capable of carrying out, or assisting with, many tasks that used to\nbe regarded as exclusively requiring human expertise. In this paper we consider\nthe case of tasks that could be performed either by human experts or by AI and\nlocate them on a continuum running from exclusively human task performance at\none end to AI autonomy on the other, with a variety of forms of human-AI\ninteraction between those extremes. Implementation of AI is constrained by the\ncontext of the systems and workflows that it will be embedded within. There is\nan urgent need for methods to determine how AI should be used in different\nsituations and to develop appropriate methods of human-AI interaction so that\nhumans and AI can work together effectively to perform tasks. In response to\nthe evolving landscape of AI progress and increasing mastery, we introduce an\nAI Mastery Lifecycle framework and discuss its implications for human-AI\ninteraction. The framework provides guidance on human-AI task allocation and\nhow human-AI interfaces need to adapt to improvements in AI task performance\nover time. Within the framework we identify a zone of uncertainty where the\nissues of human-AI task allocation and user interface design are likely to be\nmost challenging.",
          "arxiv_id": "2408.12781v1"
        },
        {
          "title": "The European Commitment to Human-Centered Technology: The Integral Role of HCI in the EU AI Act's Success",
          "year": "2024-02",
          "abstract": "The evolution of AI is set to profoundly reshape the future. The European\nUnion, recognizing this impending prominence, has enacted the AI Act,\nregulating market access for AI-based systems. A salient feature of the Act is\nto guard democratic and humanistic values by focusing regulation on\ntransparency, explainability, and the human ability to understand and control\nAI systems. Hereby, the EU AI Act does not merely specify technological\nrequirements for AI systems. The EU issues a democratic call for human-centered\nAI systems and, in turn, an interdisciplinary research agenda for\nhuman-centered innovation in AI development. Without robust methods to assess\nAI systems and their effect on individuals and society, the EU AI Act may lead\nto repeating the mistakes of the General Data Protection Regulation of the EU\nand to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more\nconfusion than lending guidance. Moreover, determined research activities in\nHuman-AI interaction will be pivotal for both regulatory compliance and the\nadvancement of AI in a manner that is both ethical and effective. Such an\napproach will ensure that AI development aligns with human values and needs,\nfostering a technology landscape that is innovative, responsible, and an\nintegral part of our society.",
          "arxiv_id": "2402.14728v2"
        },
        {
          "title": "Enabling human-centered AI: A new junction and shared journey between AI and HCI communities",
          "year": "2021-11",
          "abstract": "Artificial intelligence (AI) has brought benefits, but it may also cause harm\nif it is not appropriately developed. Current development is mainly driven by a\n\"technology-centered\" approach, causing many failures. For example, the AI\nIncident Database has documented over a thousand AI-related accidents. To\naddress these challenges, a human-centered AI (HCAI) approach has been promoted\nand has received a growing level of acceptance over the last few years. HCAI\ncalls for combining AI with user experience (UX) design will enable the\ndevelopment of AI systems (e.g., autonomous vehicles, intelligent user\ninterfaces, or intelligent decision-making systems) to achieve its design goals\nsuch as usable/explainable AI, human-controlled AI, and ethical AI. While HCAI\npromotion continues, it has not specifically addressed the collaboration\nbetween AI and human-computer interaction (HCI) communities, resulting in\nuncertainty about what action should be taken by both sides to apply HCAI in\ndeveloping AI systems. This Viewpoint focuses on the collaboration between the\nAI and HCI communities, which leads to nine recommendations for effective\ncollaboration to enable HCAI in developing AI systems.",
          "arxiv_id": "2111.08460v4"
        }
      ],
      "11": [
        {
          "title": "Multimodal Emotion Recognition using Transfer Learning from Speaker Recognition and BERT-based models",
          "year": "2022-02",
          "abstract": "Automatic emotion recognition plays a key role in computer-human interaction\nas it has the potential to enrich the next-generation artificial intelligence\nwith emotional intelligence. It finds applications in customer and/or\nrepresentative behavior analysis in call centers, gaming, personal assistants,\nand social robots, to mention a few. Therefore, there has been an increasing\ndemand to develop robust automatic methods to analyze and recognize the various\nemotions. In this paper, we propose a neural network-based emotion recognition\nframework that uses a late fusion of transfer-learned and fine-tuned models\nfrom speech and text modalities. More specifically, we i) adapt a residual\nnetwork (ResNet) based model trained on a large-scale speaker recognition task\nusing transfer learning along with a spectrogram augmentation approach to\nrecognize emotions from speech, and ii) use a fine-tuned bidirectional encoder\nrepresentations from transformers (BERT) based model to represent and recognize\nemotions from the text. The proposed system then combines the ResNet and\nBERT-based model scores using a late fusion strategy to further improve the\nemotion recognition performance. The proposed multimodal solution addresses the\ndata scarcity limitation in emotion recognition using transfer learning, data\naugmentation, and fine-tuning, thereby improving the generalization performance\nof the emotion recognition models. We evaluate the effectiveness of our\nproposed multimodal approach on the interactive emotional dyadic motion capture\n(IEMOCAP) dataset. Experimental results indicate that both audio and text-based\nmodels improve the emotion recognition performance and that the proposed\nmultimodal solution achieves state-of-the-art results on the IEMOCAP benchmark.",
          "arxiv_id": "2202.08974v1"
        },
        {
          "title": "A Transfer Learning Method for Speech Emotion Recognition from Automatic Speech Recognition",
          "year": "2020-08",
          "abstract": "This paper presents a transfer learning method in speech emotion recognition\nbased on a Time-Delay Neural Network (TDNN) architecture. A major challenge in\nthe current speech-based emotion detection research is data scarcity. The\nproposed method resolves this problem by applying transfer learning techniques\nin order to leverage data from the automatic speech recognition (ASR) task for\nwhich ample data is available. Our experiments also show the advantage of\nspeaker-class adaptation modeling techniques by adopting identity-vector\n(i-vector) based features in addition to standard Mel-Frequency Cepstral\nCoefficient (MFCC) features.[1] We show the transfer learning models\nsignificantly outperform the other methods without pretraining on ASR. The\nexperiments performed on the publicly available IEMOCAP dataset which provides\n12 hours of motional speech data. The transfer learning was initialized by\nusing the Ted-Lium v.2 speech dataset providing 207 hours of audio with the\ncorresponding transcripts. We achieve the highest significantly higher accuracy\nwhen compared to state-of-the-art, using five-fold cross validation. Using only\nspeech, we obtain an accuracy 71.7% for anger, excitement, sadness, and\nneutrality emotion content.",
          "arxiv_id": "2008.02863v2"
        },
        {
          "title": "iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis based on Disentanglement between Prosody and Timbre",
          "year": "2022-06",
          "abstract": "The capability of generating speech with specific type of emotion is desired\nfor many applications of human-computer interaction. Cross-speaker emotion\ntransfer is a common approach to generating emotional speech when speech with\nemotion labels from target speakers is not available for model training. This\npaper presents a novel cross-speaker emotion transfer system, named iEmoTTS.\nThe system is composed of an emotion encoder, a prosody predictor, and a timbre\nencoder. The emotion encoder extracts the identity of emotion type as well as\nthe respective emotion intensity from the mel-spectrogram of input speech. The\nemotion intensity is measured by the posterior probability that the input\nutterance carries that emotion. The prosody predictor is used to provide\nprosodic features for emotion transfer. The timber encoder provides\ntimbre-related information for the system. Unlike many other studies which\nfocus on disentangling speaker and style factors of speech, the iEmoTTS is\ndesigned to achieve cross-speaker emotion transfer via disentanglement between\nprosody and timbre. Prosody is considered as the main carrier of\nemotion-related speech characteristics and timbre accounts for the essential\ncharacteristics for speaker identification. Zero-shot emotion transfer, meaning\nthat speech of target speakers are not seen in model training, is also realized\nwith iEmoTTS. Extensive experiments of subjective evaluation have been carried\nout. The results demonstrate the effectiveness of iEmoTTS as compared with\nother recently proposed systems of cross-speaker emotion transfer. It is shown\nthat iEmoTTS can produce speech with designated emotion type and controllable\nemotion intensity. With appropriate information bottleneck capacity, iEmoTTS is\nable to effectively transfer emotion information to a new speaker. Audio\nsamples are publicly available https://patrick-g-zhang.github.io/iemotts/",
          "arxiv_id": "2206.14866v2"
        }
      ],
      "12": [
        {
          "title": "Contactless Human Activity Recognition using Deep Learning with Flexible and Scalable Software Define Radio",
          "year": "2023-04",
          "abstract": "Ambient computing is gaining popularity as a major technological advancement\nfor the future. The modern era has witnessed a surge in the advancement in\nhealthcare systems, with viable radio frequency solutions proposed for remote\nand unobtrusive human activity recognition (HAR). Specifically, this study\ninvestigates the use of Wi-Fi channel state information (CSI) as a novel method\nof ambient sensing that can be employed as a contactless means of recognizing\nhuman activity in indoor environments. These methods avoid additional costly\nhardware required for vision-based systems, which are privacy-intrusive, by\n(re)using Wi-Fi CSI for various safety and security applications. During an\nexperiment utilizing universal software-defined radio (USRP) to collect CSI\nsamples, it was observed that a subject engaged in six distinct activities,\nwhich included no activity, standing, sitting, and leaning forward, across\ndifferent areas of the room. Additionally, more CSI samples were collected when\nthe subject walked in two different directions. This study presents a Wi-Fi\nCSI-based HAR system that assesses and contrasts deep learning approaches,\nnamely convolutional neural network (CNN), long short-term memory (LSTM), and\nhybrid (LSTM+CNN), employed for accurate activity recognition. The experimental\nresults indicate that LSTM surpasses current models and achieves an average\naccuracy of 95.3% in multi-activity classification when compared to CNN and\nhybrid techniques. In the future, research needs to study the significance of\nresilience in diverse and dynamic environments to identify the activity of\nmultiple users.",
          "arxiv_id": "2304.09756v1"
        },
        {
          "title": "Deep Learning in Human Activity Recognition with Wearable Sensors: A Review on Advances",
          "year": "2021-10",
          "abstract": "Mobile and wearable devices have enabled numerous applications, including\nactivity tracking, wellness monitoring, and human--computer interaction, that\nmeasure and improve our daily lives. Many of these applications are made\npossible by leveraging the rich collection of low-power sensors found in many\nmobile and wearable devices to perform human activity recognition (HAR).\nRecently, deep learning has greatly pushed the boundaries of HAR on mobile and\nwearable devices. This paper systematically categorizes and summarizes existing\nwork that introduces deep learning methods for wearables-based HAR and provides\na comprehensive analysis of the current advancements, developing trends, and\nmajor challenges. We also present cutting-edge frontiers and future directions\nfor deep learning-based HAR.",
          "arxiv_id": "2111.00418v5"
        },
        {
          "title": "On-Device Training Empowered Transfer Learning For Human Activity Recognition",
          "year": "2024-07",
          "abstract": "Human Activity Recognition (HAR) is an attractive topic to perceive human\nbehavior and supplying assistive services. Besides the classical inertial unit\nand vision-based HAR methods, new sensing technologies, such as ultrasound and\nbody-area electric fields, have emerged in HAR to enhance user experience and\naccommodate new application scenarios. As those sensors are often paired with\nAI for HAR, they frequently encounter challenges due to limited training data\ncompared to the more widely IMU or vision-based HAR solutions. Additionally,\nuser-induced concept drift (UICD) is common in such HAR scenarios. UICD is\ncharacterized by deviations in the sample distribution of new users from that\nof the training participants, leading to deteriorated recognition performance.\nThis paper proposes an on-device transfer learning (ODTL) scheme tailored for\nenergy- and resource-constrained IoT edge devices. Optimized on-device training\nengines are developed for two representative MCU-level edge computing\nplatforms: STM32F756ZG and GAP9. Based on this, we evaluated the ODTL benefits\nin three HAR scenarios: body capacitance-based gym activity recognition, QVAR-\nand ultrasonic-based hand gesture recognition. We demonstrated an improvement\nof 3.73%, 17.38%, and 3.70% in the activity recognition accuracy, respectively.\nBesides this, we observed that the RISC-V-based GAP9 achieves 20x and 280x less\nlatency and power consumption than STM32F7 MCU during the ODTL deployment,\ndemonstrating the advantages of employing the latest low-power parallel\ncomputing devices for edge tasks.",
          "arxiv_id": "2407.03644v1"
        }
      ],
      "13": [
        {
          "title": "Lightweight assistive technology: A wearable, optical-fiber gesture recognition system",
          "year": "2020-09",
          "abstract": "The goal of this project is to create an inexpensive, lightweight, wearable\nassistive device that can measure hand or finger movements accurately enough to\nidentify a range of hand gestures. One eventual application is to provide\nassistive technology and sign language detection for the hearing impaired. My\nsystem, called LiTe (Light-based Technology), uses optical fibers embedded into\na wristband. The wrist is an optimal place for the band since the light\npropagation in the optical fibers is impacted even by the slight movements of\nthe tendons in the wrist when gestures are performed. The prototype\nincorporates light dependent resistors to measure these light propagation\nchanges. When creating LiTe, I considered a variety of fiber materials, light\nfrequencies, and physical shapes to optimize the tendon movement detection so\nthat it can be accurately correlated with different gestures. I implemented and\nevaluated two approaches for gesture recognition. The first uses an algorithm\nthat combines moving averages of sensor readings with gesture sensor reading\nsignatures to determine the current gesture. The second uses a neural network\ntrained on a labelled set of gesture readings to recognize gestures. Using the\nsignature-based approach, I was able to achieve a 99.8% accuracy at recognizing\ndistinct gestures. Using the neural network the recognition accuracy was 98.8%.\nThis shows that high accuracy is feasible using both approaches. The results\nindicate that this novel method of using fiber optics-based sensors is a\npromising first step to creating a gesture recognition system.",
          "arxiv_id": "2009.13322v1"
        },
        {
          "title": "Simultaneous Estimation of Hand Configurations and Finger Joint Angles using Forearm Ultrasound",
          "year": "2022-11",
          "abstract": "With the advancement in computing and robotics, it is necessary to develop\nfluent and intuitive methods for interacting with digital systems,\naugmented/virtual reality (AR/VR) interfaces, and physical robotic systems.\nHand motion recognition is widely used to enable these interactions. Hand\nconfiguration classification and MCP joint angle detection is important for a\ncomprehensive reconstruction of hand motion. sEMG and other technologies have\nbeen used for the detection of hand motions. Forearm ultrasound images provide\na musculoskeletal visualization that can be used to understand hand motion.\nRecent work has shown that these ultrasound images can be classified using\nmachine learning to estimate discrete hand configurations. Estimating both hand\nconfiguration and MCP joint angles based on forearm ultrasound has not been\naddressed in the literature. In this paper, we propose a CNN based deep\nlearning pipeline for predicting the MCP joint angles. The results for the hand\nconfiguration classification were compared by using different machine learning\nalgorithms. SVC with different kernels, MLP, and the proposed CNN have been\nused to classify the ultrasound images into 11 hand configurations based on\nactivities of daily living. Forearm ultrasound images were acquired from 6\nsubjects instructed to move their hands according to predefined hand\nconfigurations. Motion capture data was acquired to get the finger angles\ncorresponding to the hand movements at different speeds. Average classification\naccuracy of 82.7% for the proposed CNN and over 80% for SVC for different\nkernels was observed on a subset of the dataset. An average RMSE of 7.35\ndegrees was obtained between the predicted and the true MCP joint angles. A low\nlatency (6.25 - 9.1 Hz) pipeline has been proposed for estimating both MCP\njoint angles and hand configuration aimed at real-time control of human-machine\ninterfaces.",
          "arxiv_id": "2211.15871v1"
        },
        {
          "title": "GesturePrint: Enabling User Identification for mmWave-based Gesture Recognition Systems",
          "year": "2024-07",
          "abstract": "The millimeter-wave (mmWave) radar has been exploited for gesture\nrecognition. However, existing mmWave-based gesture recognition methods cannot\nidentify different users, which is important for ubiquitous gesture interaction\nin many applications. In this paper, we propose GesturePrint, which is the\nfirst to achieve gesture recognition and gesture-based user identification\nusing a commodity mmWave radar sensor. GesturePrint features an effective\npipeline that enables the gesture recognition system to identify users at a\nminor additional cost. By introducing an efficient signal preprocessing stage\nand a network architecture GesIDNet, which employs an attention-based\nmultilevel feature fusion mechanism, GesturePrint effectively extracts unique\ngesture features for gesture recognition and personalized motion pattern\nfeatures for user identification. We implement GesturePrint and collect data\nfrom 17 participants performing 15 gestures in a meeting room and an office,\nrespectively. GesturePrint achieves a gesture recognition accuracy (GRA) of\n98.87% with a user identification accuracy (UIA) of 99.78% in the meeting room,\nand 98.22% GRA with 99.26% UIA in the office. Extensive experiments on three\npublic datasets and a new gesture dataset show GesturePrint's superior\nperformance in enabling effective user identification for gesture recognition\nsystems.",
          "arxiv_id": "2408.05358v1"
        }
      ],
      "14": [
        {
          "title": "Eye Gaze Controlled Robotic Arm for Persons with SSMI",
          "year": "2020-05",
          "abstract": "Background: People with severe speech and motor impairment (SSMI) often uses\na technique called eye pointing to communicate with outside world. One of their\nparents, caretakers or teachers hold a printed board in front of them and by\nanalyzing their eye gaze manually, their intentions are interpreted. This\ntechnique is often error prone and time consuming and depends on a single\ncaretaker.\n  Objective: We aimed to automate the eye tracking process electronically by\nusing commercially available tablet, computer or laptop and without requiring\nany dedicated hardware for eye gaze tracking. The eye gaze tracker is used to\ndevelop a video see through based AR (augmented reality) display that controls\na robotic device with eye gaze and deployed for a fabric printing task.\n  Methodology: We undertook a user centred design process and separately\nevaluated the web cam based gaze tracker and the video see through based human\nrobot interaction involving users with SSMI. We also reported a user study on\nmanipulating a robotic arm with webcam based eye gaze tracker.\n  Results: Using our bespoke eye gaze controlled interface, able bodied users\ncan select one of nine regions of screen at a median of less than 2 secs and\nusers with SSMI can do so at a median of 4 secs. Using the eye gaze controlled\nhuman-robot AR display, users with SSMI could undertake representative pick and\ndrop task at an average duration less than 15 secs and reach a randomly\ndesignated target within 60 secs using a COTS eye tracker and at an average\ntime of 2 mins using the webcam based eye gaze tracker.",
          "arxiv_id": "2005.11994v1"
        },
        {
          "title": "Advanced Gaze Analytics Dashboard",
          "year": "2024-09",
          "abstract": "Eye movements can provide informative cues to understand human visual\nscan/search behavior and cognitive load during varying tasks. Visualizations of\nreal-time gaze measures during tasks, provide an understanding of human\nbehavior as the experiment is being conducted. Even though existing eye\ntracking analysis tools provide calculation and visualization of eye-tracking\ndata, none of them support real-time visualizations of advanced gaze measures,\nsuch as ambient or focal processing, or eye-tracked measures of cognitive load.\nIn this paper, we present an eye movements analytics dashboard that enables\nvisualizations of various gaze measures, fixations, saccades, cognitive load,\nambient-focal attention, and gaze transitions analysis by extracting eye\nmovements from participants utilizing common off-the-shelf eye trackers. We\nvalidate the proposed eye movement visualizations by using two publicly\navailable eye-tracking datasets. We showcase that, the proposed dashboard could\nbe utilized to visualize advanced eye movement measures generated using\nmultiple data sources.",
          "arxiv_id": "2409.06628v1"
        },
        {
          "title": "The Detection of Saccadic Eye Movements and Per-Eye Comparisons using Virtual Reality Eye Tracking Devices",
          "year": "2025-03",
          "abstract": "Eye tracking has been found to be useful in various tasks including\ndiagnostic and screening tools. However, traditional eye trackers had a\ncomplicated setup and operated at a higher frequency to measure eye movements.\nThe use of more commonly available eye trackers such as those in head-mounted\nvirtual reality (VR) headsets greatly expands the utility of these eye trackers\nfor research and analytical purposes. In this study, the research question is\nfocused on detecting saccades, which is a common task when analyzing eye\ntracking data, but it is not well-established for VR headset-mounted eye\ntrackers. The aim is to determine how accurately saccadic eye movements can be\ndetected using an eye tracker that operates at 60 or 90Hz. The study involves\nVR eye tracking technology and neuroscience with respect to saccadic eye\nmovements. The goal is to build prototype software implemented using VR eye\ntracking technology to detect saccadic eye movements, and per-eye differences\nin an individual. It is anticipated that the software will be able to\naccurately detect when saccades occur and analyze the differences in saccadic\neye movements per-eye. The field of research surrounding VR eye tracking\nsoftware is still developing rapidly, specifically its applications to\nneuroscience. Since previous methods of eye tracking involved specialized\nequipment, using commercially and consumer available VR eye tracking technology\nto assist in the detection of saccades and per-eye differences would be novel.\nThis project will impact the field of neuroscience by providing a tool that can\nbe used to detect saccadic eye movements and neurological and neurodegenerative\ndisorders. However, this project is limited by the short time frame and that\nthe eye tracker used in this study operates at a maximum frequency of 90Hz.",
          "arxiv_id": "2503.08926v1"
        }
      ],
      "15": [
        {
          "title": "ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering Perceived Texture Roughness in Mixed Reality",
          "year": "2024-03",
          "abstract": "Extensive research has been done in haptic feedback for texture simulation in\nvirtual reality (VR). However, it is challenging to modify the perceived\ntactile texture of existing physical objects which usually serve as anchors for\nvirtual objects in mixed reality (MR). In this paper, we present ViboPneumo, a\nfinger-worn haptic device that uses vibratory-pneumatic feedback to modulate\n(i.e., increase and decrease) the perceived roughness of the material surface\ncontacted by the user's fingerpad while supporting the perceived sensation of\nother haptic properties (e.g., temperature or stickiness) in MR. Our device\nincludes a silicone-based pneumatic actuator that can lift the user's fingerpad\non the physical surface to reduce the contact area for roughness decreasing,\nand an on-finger vibrator for roughness increasing. Our user-perception\nexperimental results showed that the participants could perceive changes in\nroughness, both increasing and decreasing, compared to the original material\nsurface. We also observed the overlapping roughness ratings among certain\nhaptic stimuli (i.e., vibrotactile and pneumatic) and the originally perceived\nroughness of some materials without any haptic feedback. This suggests the\npotential to alter the perceived texture of one type of material to another in\nterms of roughness (e.g., modifying the perceived texture of ceramics as\nglass). Lastly, a user study of MR experience showed that ViboPneumo could\nsignificantly improve the MR user experience, particularly for visual-haptic\nmatching, compared to the condition of a bare finger. We also demonstrated a\nfew application scenarios for ViboPneumo.",
          "arxiv_id": "2403.05182v1"
        },
        {
          "title": "Haptic Tracing: A new paradigm for spatialized Haptic rendering",
          "year": "2025-08",
          "abstract": "Haptic technology enhances interactive experiences by providing force and\ntactile feedback, improving user performance and immersion. However, despite\nadvancements, creating tactile experiences still remains challenging due to\ndevice diversity and complexity. Most available haptic frameworks rely on\ntrigger-based or event-based systems, and disregard the information of the 3D\nscene to render haptic information. This paper introduces Haptic Tracing, a\nnovel method for spatial haptic rendering that simplifies the creation of\ninteractive haptic experiences without relying on physical simulations. It uses\nconcepts from visual and audio rendering to model and propagate haptic\ninformation through a 3D scene. The paper also describes how our proposed\nhaptic rendering method can be used to create a vibrotactile rendering system,\nenabling the creation of perceptually coherent and dynamic haptic interactions.\nFinally, the paper discusses a user study that explores the role of the haptic\npropagation and multi-actuator rendering on the users' haptic experience. The\nresults show that our approach significantly enhances the realism and the\nexpressivity of the haptic feedback, showcasing its potential for developing\nmore complex and realistic haptic experiences.",
          "arxiv_id": "2508.19703v1"
        },
        {
          "title": "Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation",
          "year": "2025-06",
          "abstract": "This study presents a lightweight, wearable fingertip haptic device that\nprovides physics-based haptic feedback for dexterous manipulation in virtual\nenvironments without hindering real-world interactions. The device, designed\nwith thin strings and actuators attached to the fingernails, ensures minimal\nweight (1.55 g per finger) and preserves finger flexibility. Integrating the\nsoftware with a physics engine renders multiple types of haptic feedback (grip\nforce, collision, and sliding vibration feedback). We evaluated the device's\nperformance in pressure perception, slip feedback, typical dexterous\nmanipulation tasks, and daily operations, and we gathered user experience\nthrough subjective assessments. Our results show that participants could\nperceive and respond to pressure and vibration feedback. Through dexterous\nmanipulation experiments, we further demonstrated that these minimal haptic\ncues significantly improved virtual task efficiency, showcasing how lightweight\nhaptic feedback can enhance manipulation performance without complex\nmechanisms. The device's ability to preserve tactile sensations and minimize\nhindrance to real-world operations is a key advantage over glove-type haptic\ndevices. This research offers a potential solution for designing haptic\ninterfaces that balance lightweight construction, haptic feedback for dexterous\nmanipulation, and daily wearability.",
          "arxiv_id": "2506.21417v1"
        }
      ],
      "16": [
        {
          "title": "SpiritSight Agent: Advanced GUI Agent with One Look",
          "year": "2025-03",
          "abstract": "Graphical User Interface (GUI) agents show amazing abilities in assisting\nhuman-computer interaction, automating human user's navigation on digital\ndevices. An ideal GUI agent is expected to achieve high accuracy, low latency,\nand compatibility for different GUI platforms. Recent vision-based approaches\nhave shown promise by leveraging advanced Vision Language Models (VLMs). While\nthey generally meet the requirements of compatibility and low latency, these\nvision-based GUI agents tend to have low accuracy due to their limitations in\nelement grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a\nvision-based, end-to-end GUI agent that excels in GUI navigation tasks across\nvarious GUI platforms. First, we create a multi-level, large-scale,\nhigh-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods,\nempowering SpiritSight with robust GUI understanding and grounding\ncapabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$\nmethod to resolve the ambiguity problem in dynamic high-resolution of visual\ninputs, further enhancing SpiritSight's ability to ground GUI objects. Through\nthese efforts, SpiritSight agent outperforms other advanced methods on diverse\nGUI benchmarks, demonstrating its superior capability and compatibility in GUI\nnavigation tasks. Models and datasets are available at\nhttps://hzhiyuan.github.io/SpiritSight-Agent.",
          "arxiv_id": "2503.03196v2"
        },
        {
          "title": "SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents",
          "year": "2024-01",
          "abstract": "Graphical User Interface (GUI) agents are designed to automate complex tasks\non digital devices, such as smartphones and desktops. Most existing GUI agents\ninteract with the environment through extracted structured data, which can be\nnotably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops).\nTo alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which\nonly relies on screenshots for task automation. In our preliminary study, we\nhave discovered a key challenge in developing visual GUI agents: GUI grounding\n-- the capacity to accurately locate screen elements based on instructions. To\ntackle this challenge, we propose to enhance SeeClick with GUI grounding\npre-training and devise a method to automate the curation of GUI grounding\ndata. Along with the efforts above, we have also created ScreenSpot, the first\nrealistic GUI grounding benchmark that encompasses mobile, desktop, and web\nenvironments. After pre-training, SeeClick demonstrates significant improvement\nin ScreenSpot over various baselines. Moreover, comprehensive evaluations on\nthree widely used benchmarks consistently support our finding that advancements\nin GUI grounding directly correlate with enhanced performance in downstream GUI\nagent tasks. The model, data and code are available at\nhttps://github.com/njucckevin/SeeClick.",
          "arxiv_id": "2401.10935v2"
        },
        {
          "title": "WinClick: GUI Grounding with Multimodal Large Language Models",
          "year": "2025-01",
          "abstract": "Graphical User Interface (GUI) tasks are vital for automating workflows such\nas software testing, user interface navigation. For users, the GUI is the most\nintuitive platform for interacting with a computer. Previous work identified a\nkey challenge in developing visual GUI agents: GUI grounding - the ability to\naccurately locate screen elements based on instructions. However, most existing\nGUI agents rely on structured data formats like DOM or HTML files in training\nor inferencing, which are inaccessible across all applications, particular in a\ngeneral desktop environments such as Windows OS. To address this, we introduce\nWinClick, a novel visual GUI agent developed in Windows platform. WinClick\nleverages screenshots to detect actionable regions. To overcome the challenge\nof GUI grounding, we enhance WinClick with GUI grounding pre-training and\npropose an LLM-based method for aligning GUI grounding data. Additionally, we\nintroduce WinSpot, the first comprehensive benchmark for GUI grounding on\nWindows. Our experiments demonstrate that WinClick, combined with GUI grounding\npre-training, significantly outperforms existing baselines, offering a scalable\nsolution for GUI automation in desktop environments. WinSpot is publicly\navailable at https://github.com/zackhuiiiii/WinSpot.",
          "arxiv_id": "2503.04730v1"
        }
      ],
      "17": [
        {
          "title": "WundtGPT: Shaping Large Language Models To Be An Empathetic, Proactive Psychologist",
          "year": "2024-06",
          "abstract": "Large language models (LLMs) are raging over the medical domain, and their\nmomentum has carried over into the mental health domain, leading to the\nemergence of few mental health LLMs. Although such mental health LLMs could\nprovide reasonable suggestions for psychological counseling, how to develop an\nauthentic and effective doctor-patient relationship (DPR) through LLMs is still\nan important problem. To fill this gap, we dissect DPR into two key attributes,\ni.e., the psychologist's empathy and proactive guidance. We thus present\nWundtGPT, an empathetic and proactive mental health large language model that\nis acquired by fine-tuning it with instruction and real conversation between\npsychologists and patients. It is designed to assist psychologists in diagnosis\nand help patients who are reluctant to communicate face-to-face understand\ntheir psychological conditions. Its uniqueness lies in that it could not only\npose purposeful questions to guide patients in detailing their symptoms but\nalso offer warm emotional reassurance. In particular, WundtGPT incorporates\nCollection of Questions, Chain of Psychodiagnosis, and Empathy Constraints into\na comprehensive prompt for eliciting LLMs' questions and diagnoses.\nAdditionally, WundtGPT proposes a reward model to promote alignment with\nempathetic mental health professionals, which encompasses two key factors:\ncognitive empathy and emotional empathy. We offer a comprehensive evaluation of\nour proposed model. Based on these outcomes, we further conduct the manual\nevaluation based on proactivity, effectiveness, professionalism and coherence.\nWe notice that WundtGPT can offer professional and effective consultation. The\nmodel is available at huggingface.",
          "arxiv_id": "2406.15474v1"
        },
        {
          "title": "\"I've talked to ChatGPT about my issues last night.\": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",
          "year": "2025-04",
          "abstract": "We investigate the role of large language models (LLMs) in supporting mental\nhealth by analyzing Reddit posts and comments about mental health conversations\nwith ChatGPT. Our findings reveal that users value ChatGPT as a safe,\nnon-judgmental space, often favoring it over human support due to its\naccessibility, availability, and knowledgeable responses. ChatGPT provides a\nrange of support, including actionable advice, emotional support, and\nvalidation, while helping users better understand their mental states.\nAdditionally, we found that ChatGPT offers innovative support for individuals\nfacing mental health challenges, such as assistance in navigating difficult\nconversations, preparing for therapy sessions, and exploring therapeutic\ninterventions. However, users also voiced potential risks, including the spread\nof incorrect health advice, ChatGPT's overly validating nature, and privacy\nconcerns. We discuss the implications of LLMs as tools for mental health\nsupport in both everyday health and clinical therapy settings and suggest\nstrategies to mitigate risks in LLM-powered interactions.",
          "arxiv_id": "2504.20320v1"
        },
        {
          "title": "The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support",
          "year": "2024-01",
          "abstract": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.",
          "arxiv_id": "2401.14362v3"
        }
      ],
      "18": [
        {
          "title": "Leveraging Cluster Analysis to Understand Educational Game Player Experiences and Support Design",
          "year": "2022-10",
          "abstract": "The ability for an educational game designer to understand their audience's\nplay styles and resulting experience is an essential tool for improving their\ngame's design. As a game is subjected to large-scale player testing, the\ndesigners require inexpensive, automated methods for categorizing patterns of\nplayer-game interactions. In this paper we present a simple, reusable process\nusing best practices for data clustering, feasible for use within a small\neducational game studio. We utilize the method to analyze a real-time strategy\ngame, processing game telemetry data to determine categories of players based\non their in-game actions, the feedback they received, and their progress\nthrough the game. An interpretive analysis of these clusters results in\nactionable insights for the game's designers.",
          "arxiv_id": "2210.09911v1"
        },
        {
          "title": "Heterogeneous Effects of Software Patches in a Multiplayer Online Battle Arena Game",
          "year": "2021-10",
          "abstract": "The popularity of online gaming has grown dramatically, driven in part by\nstreaming and the billion-dollar e-sports industry. Online games regularly\nupdate their software to fix bugs, add functionality that improve the game's\nlook and feel, and change the game mechanics to keep the games fun and\nchallenging. An open question, however, is the impact of these changes on\nplayer performance and game balance, as well as how players adapt to these\nsudden changes. To address these questions, we use causal inference to measure\nthe impact of software patches to League of Legends, a popular team-based\nmultiplayer online game. We show that game patches have substantially different\nimpacts on players depending on their skill level and whether they take breaks\nbetween games. We find that the gap between good and bad players increases\nafter a patch, despite efforts to make gameplay more equal. Moreover, longer\nbetween-game breaks tend to improve player performance after patches. Overall,\nour results highlight the utility of causal inference, and specifically\nheterogeneous treatment effect estimation, as a tool to quantify the complex\nmechanisms of game balance and its interplay with players' performance.",
          "arxiv_id": "2110.14632v1"
        },
        {
          "title": "One Pixel, One Interaction, One Game: An Experiment in Minimalist Game Design",
          "year": "2022-07",
          "abstract": "Minimalist game design was introduced a decade ago as a general design\nprinciple with a list of key properties for minimalist games: basic controls,\nsimple but aesthetically pleasing visuals, interesting player choices with vast\npossibility spaces, and sounds that resonate with the design. In this paper, we\npresent an experiment we did to explore minimalism in games using a bottom-up\napproach. We invited a small group of professional game designers and a larger\ngroup of game design students to participate in a seminal experiment on\nminimalism in game design. We started from the most basic game elements: one\npixel and one key which provide the least amount of information we can display\nand reasonably the most elementary action players can perform. We designed a\ngame that starts with a black pixel and asks players to press a key when the\npixel turns white. This minimal game, almost a Skinner box, captures the\nessential elements of the mechanics of games like \"The Impossible Game,\" which\nasks players to do nothing more than press a key at the right moment. We\npresented this game concept to the professional game designers and challenged\nthem to create other games with the least amount of player interaction and\ndisplayed information. We did not specify any constraints (as usually done in\nother contexts) and left them free to express their view of minimalistic game\ndesign. We repeated the experiment with 100+ students attending a master-level\ncourse on video game design and development at our institution. We then\nanalyzed the creations of the two groups, discussing the idea of minimalistic\ndesign that emerges from the submitted game concepts.",
          "arxiv_id": "2207.03827v1"
        }
      ],
      "19": [
        {
          "title": "Fairness and Transparency in Recommendation: The Users' Perspective",
          "year": "2021-03",
          "abstract": "Though recommender systems are defined by personalization, recent work has\nshown the importance of additional, beyond-accuracy objectives, such as\nfairness. Because users often expect their recommendations to be purely\npersonalized, these new algorithmic objectives must be communicated\ntransparently in a fairness-aware recommender system. While explanation has a\nlong history in recommender systems research, there has been little work that\nattempts to explain systems that use a fairness objective. Even though the\nprevious work in other branches of AI has explored the use of explanations as a\ntool to increase fairness, this work has not been focused on recommendation.\nHere, we consider user perspectives of fairness-aware recommender systems and\ntechniques for enhancing their transparency. We describe the results of an\nexploratory interview study that investigates user perceptions of fairness,\nrecommender systems, and fairness-aware objectives. We propose three features\n-- informed by the needs of our participants -- that could improve user\nunderstanding of and trust in fairness-aware recommender systems.",
          "arxiv_id": "2103.08786v1"
        },
        {
          "title": "Dynamic Adaptation of User Preferences and Results in a Destination Recommender System",
          "year": "2023-02",
          "abstract": "Studying human factors has gained a lot of interest in recommender systems\nresearch recently. User experience plays a vital role in tourism recommender\nsystems since user satisfaction is the main factor that guarantees the success\nof such recommender systems. In this work, we have designed and implemented a\ndestination recommender system in which the recommendations adapt instantly\nbased on the user preferences. The recommendations can be explored on a world\nmap with additional information. This interface addresses common visualization\nchallenges in recommender systems, such as transparency, justification,\ncontrollability, explorability, the cold-start problem, and context awareness.\nWe have conducted a user study to evaluate different aspects of this\nrecommender system from the users' perspective.",
          "arxiv_id": "2302.09803v1"
        },
        {
          "title": "Visualization for Recommendation Explainability: A Survey and New Perspectives",
          "year": "2023-05",
          "abstract": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
          "arxiv_id": "2305.11755v3"
        }
      ],
      "20": [
        {
          "title": "MusicJam: Visualizing Music Insights via Generated Narrative Illustrations",
          "year": "2023-08",
          "abstract": "Visualizing the insights of the invisible music is able to bring listeners an\nenjoyable and immersive listening experience, and therefore has attracted much\nattention in the field of information visualization. Over the past decades,\nvarious music visualization techniques have been introduced. However, most of\nthem are manually designed by following the visual encoding rules, thus shown\nin form of a graphical visual representation whose visual encoding schema is\nusually taking effort to understand. Recently, some researchers use figures or\nillustrations to represent music moods, lyrics, and musical features, which are\nmore intuitive and attractive. However, in these techniques, the figures are\nusually pre-selected or statically generated, so they cannot precisely convey\ninsights of different pieces of music. To address this issue, in this paper, we\nintroduce MusicJam, a music visualization system that is able to generate\nnarrative illustrations to represent the insight of the input music. The system\nleverages a novel generation model designed based on GPT-2 to generate\nmeaningful lyrics given the input music and then employs the stable diffusion\nmodel to transform the lyrics into coherent illustrations. Finally, the\ngenerated results are synchronized and rendered as an MP4 video accompanied by\nthe input music. We evaluated the proposed lyric generation model by comparing\nit to the baseline models and conducted a user study to estimate the quality of\nthe generated illustrations and the final music videos. The results showed the\npower of our technique.",
          "arxiv_id": "2308.11329v2"
        },
        {
          "title": "Live Music Models",
          "year": "2025-08",
          "abstract": "We introduce a new class of generative models for music called live music\nmodels that produce a continuous stream of music in real-time with synchronized\nuser control. We release Magenta RealTime, an open-weights live music model\nthat can be steered using text or audio prompts to control acoustic style. On\nautomatic metrics of music quality, Magenta RealTime outperforms other\nopen-weights music generation models, despite using fewer parameters and\noffering first-of-its-kind live generation capabilities. We also release Lyria\nRealTime, an API-based model with extended controls, offering access to our\nmost powerful model with wide prompt coverage. These models demonstrate a new\nparadigm for AI-assisted music creation that emphasizes human-in-the-loop\ninteraction for live music performance.",
          "arxiv_id": "2508.04651v2"
        },
        {
          "title": "Understanding Human-AI Collaboration in Music Therapy Through Co-Design with Therapists",
          "year": "2024-02",
          "abstract": "The rapid development of musical AI technologies has expanded the creative\npotential of various musical activities, ranging from music style\ntransformation to music generation. However, little research has investigated\nhow musical AIs can support music therapists, who urgently need new technology\nsupport. This study used a mixed method, including semi-structured interviews\nand a participatory design approach. By collaborating with music therapists, we\nexplored design opportunities for musical AIs in music therapy. We presented\nthe co-design outcomes involving the integration of musical AIs into a music\ntherapy process, which was developed from a theoretical framework rooted in\nemotion-focused therapy. After that, we concluded the benefits and concerns\nsurrounding music AIs from the perspective of music therapists. Based on our\nfindings, we discussed the opportunities and design implications for applying\nmusical AIs to music therapy. Our work offers valuable insights for developing\nhuman-AI collaborative music systems in therapy involving complex procedures\nand specific requirements.",
          "arxiv_id": "2402.14503v3"
        }
      ],
      "21": [
        {
          "title": "Over-Relying on Reliance: Towards Realistic Evaluations of AI-Based Clinical Decision Support",
          "year": "2025-04",
          "abstract": "As AI-based clinical decision support (AI-CDS) is introduced in more and more\naspects of healthcare services, HCI research plays an increasingly important\nrole in designing for complementarity between AI and clinicians. However,\ncurrent evaluations of AI-CDS often fail to capture when AI is and is not\nuseful to clinicians. This position paper reflects on our work and influential\nAI-CDS literature to advocate for moving beyond evaluation metrics like Trust,\nReliance, Acceptance, and Performance on the AI's task (what we term the \"trap\"\nof human-AI collaboration). Although these metrics can be meaningful in some\nsimple scenarios, we argue that optimizing for them ignores important ways that\nAI falls short of clinical benefit, as well as ways that clinicians\nsuccessfully use AI. As the fields of HCI and AI in healthcare develop new ways\nto design and evaluate CDS tools, we call on the community to prioritize\necologically valid, domain-appropriate study setups that measure the emergent\nforms of value that AI can bring to healthcare professionals.",
          "arxiv_id": "2504.07423v1"
        },
        {
          "title": "Exploring the Requirements of Clinicians for Explainable AI Decision Support Systems in Intensive Care",
          "year": "2024-11",
          "abstract": "There is a growing need to understand how digital systems can support\nclinical decision-making, particularly as artificial intelligence (AI) models\nbecome increasingly complex and less human-interpretable. This complexity\nraises concerns about trustworthiness, impacting safe and effective adoption of\nsuch technologies. Improved understanding of decision-making processes and\nrequirements for explanations coming from decision support tools is a vital\ncomponent in providing effective explainable solutions. This is particularly\nrelevant in the data-intensive, fast-paced environments of intensive care units\n(ICUs). To explore these issues, group interviews were conducted with seven ICU\nclinicians, representing various roles and experience levels. Thematic analysis\nrevealed three core themes: (T1) ICU decision-making relies on a wide range of\nfactors, (T2) the complexity of patient state is challenging for shared\ndecision-making, and (T3) requirements and capabilities of AI decision support\nsystems. We include design recommendations from clinical input, providing\ninsights to inform future AI systems for intensive care.",
          "arxiv_id": "2411.11774v1"
        },
        {
          "title": "An Iterative, User-Centered Design of a Clinical Decision Support System for Critical Care Assessments: Co-Design Sessions with ICU Clinical Providers",
          "year": "2025-03",
          "abstract": "This study reports the findings of qualitative interview sessions conducted\nwith ICU clinicians for the co-design of a system user interface of an\nartificial intelligence (AI)-driven clinical decision support (CDS) system.\nThis system integrates medical record data with wearable sensor, video, and\nenvironmental data into a real-time dynamic model that quantifies patients'\nrisk of clinical decompensation and risk of developing delirium, providing\nactionable alerts to augment clinical decision-making in the ICU setting.\nCo-design sessions were conducted as semi-structured focus groups and\ninterviews with ICU clinicians, including physicians, mid-level practitioners,\nand nurses. Study participants were asked about their perceptions on AI-CDS\nsystems, their system preferences, and were asked to provide feedback on the\ncurrent user interface prototype. Session transcripts were qualitatively\nanalyzed to identify key themes related to system utility, interface design\nfeatures, alert preferences, and implementation considerations. Ten clinicians\nparticipated in eight sessions. The analysis identified five themes: (1) AI's\ncomputational utility, (2) workflow optimization, (3) effects on patient care,\n(4) technical considerations, and (5) implementation considerations. Clinicians\nvalued the CDS system's multi-modal continuous monitoring and AI's capacity to\nprocess large volumes of data in real-time to identify patient risk factors and\nsuggest action items. Participants underscored the system's unique value in\ndetecting delirium and promoting non-pharmacological delirium prevention\nmeasures. The actionability and intuitive interpretation of the presented\ninformation was emphasized. ICU clinicians recognize the potential of an\nAI-driven CDS system for ICU delirium and acuity to improve patient outcomes\nand clinical workflows.",
          "arxiv_id": "2503.08814v1"
        }
      ],
      "22": [
        {
          "title": "From Zero to The Hero: A Collaborative Market Aware Recommendation System for Crowd Workers",
          "year": "2021-07",
          "abstract": "The success of software crowdsourcing depends on active and trustworthy pool\nof worker supply. The uncertainty of crowd workers' behaviors makes it\nchallenging to predict workers' success and plan accordingly. In a competitive\ncrowdsourcing marketplace, competition for success over shared tasks adds\nanother layer of uncertainty in crowd workers' decision-making process.\nPreliminary analysis on software worker behaviors reveals an alarming task\ndropping rate of 82.9%. These factors lead to the need for an automated\nrecommendation system for CSD workers to improve the visibility and\npredictability of their success in the competition. To that end, this paper\nproposes a collaborative recommendation system for crowd workers. The proposed\nrecommendation system method uses five input metrics based on workers'\ncollaboration history in the pool, workers' preferences in taking tasks in\nterms of monetary prize and duration, workers' specialty, and workers'\nproficiency. The proposed method then recommends the most suitable tasks for a\nworker to compete on based on workers' probability of success in the task.\nExperimental results on 260 active crowd workers demonstrate that just\nfollowing the top three success probabilities of task recommendations, workers\ncan achieve success up to 86%",
          "arxiv_id": "2107.02890v1"
        },
        {
          "title": "What do crowd workers think about creative work?",
          "year": "2020-02",
          "abstract": "Crowdsourcing platforms are a powerful and convenient means for recruiting\nparticipants in online studies and collecting data from the crowd. As\ninformation work is being more and more automated by Machine Learning\nalgorithms, creativity $-$ that is, a human's ability for divergent and\nconvergent thinking $-$ will play an increasingly important role on online\ncrowdsourcing platforms. However, we lack insights into what crowd workers\nthink about creative work. In studies in Human-Computer Interaction (HCI), the\nability and willingness of the crowd to participate in creative work seems to\nbe largely unquestioned. Insights into the workers' perspective are rare, but\nimportant, as they may inform the design of studies with higher validity. Given\nthat creativity will play an increasingly important role in crowdsourcing, it\nis imperative to develop an understanding of how workers perceive creative\nwork. In this paper, we summarize our recent worker-centered study of creative\nwork on two general-purpose crowdsourcing platforms (Amazon Mechanical Turk and\nProlific). Our study illuminates what creative work is like for crowd workers\non these two crowdsourcing platforms. The work identifies several archetypal\ntypes of workers with different attitudes towards creative work, and discusses\ncommon pitfalls with creative work on crowdsourcing platforms.",
          "arxiv_id": "2002.10887v1"
        },
        {
          "title": "A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation",
          "year": "2024-01",
          "abstract": "Whether Large Language Models (LLMs) can outperform crowdsourcing on the data\nannotation task is attracting interest recently. Some works verified this issue\nwith the average performance of individual crowd workers and LLM workers on\nsome specific NLP tasks by collecting new datasets. However, on the one hand,\nexisting datasets for the studies of annotation quality in crowdsourcing are\nnot yet utilized in such evaluations, which potentially provide reliable\nevaluations from a different viewpoint. On the other hand, the quality of these\naggregated labels is crucial because, when utilizing crowdsourcing, the\nestimated labels aggregated from multiple crowd labels to the same instances\nare the eventually collected labels. Therefore, in this paper, we first\ninvestigate which existing crowdsourcing datasets can be used for a comparative\nstudy and create a benchmark. We then compare the quality between individual\ncrowd labels and LLM labels and make the evaluations on the aggregated labels.\nIn addition, we propose a Crowd-LLM hybrid label aggregation method and verify\nthe performance. We find that adding LLM labels from good LLMs to existing\ncrowdsourcing datasets can enhance the quality of the aggregated labels of the\ndatasets, which is also higher than the quality of LLM labels themselves.",
          "arxiv_id": "2401.09760v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:42:24Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}