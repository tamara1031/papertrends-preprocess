{
  "topics": {
    "data": {
      "0": {
        "name": "0_performance_element_finite_GPU",
        "keywords": [
          [
            "performance",
            0.020821418966037435
          ],
          [
            "element",
            0.019408178768404668
          ],
          [
            "finite",
            0.019316835304817367
          ],
          [
            "GPU",
            0.017233721649723286
          ],
          [
            "methods",
            0.017214642060263886
          ],
          [
            "high",
            0.016518341418240785
          ],
          [
            "finite element",
            0.016366233707864705
          ],
          [
            "method",
            0.015641848967992103
          ],
          [
            "mesh",
            0.01538637925288405
          ],
          [
            "problems",
            0.015253584097360974
          ]
        ],
        "count": 237
      },
      "1": {
        "name": "1_precision_rounding_arithmetic_library",
        "keywords": [
          [
            "precision",
            0.02453068107708479
          ],
          [
            "rounding",
            0.023529346511525044
          ],
          [
            "arithmetic",
            0.021342360531927774
          ],
          [
            "library",
            0.02088910011590755
          ],
          [
            "functions",
            0.020212304718161028
          ],
          [
            "mathematical",
            0.02002252703156514
          ],
          [
            "rounded",
            0.019261712517148685
          ],
          [
            "point",
            0.01898891129242119
          ],
          [
            "algorithm",
            0.018800457389943445
          ],
          [
            "systems",
            0.016547673889813964
          ]
        ],
        "count": 97
      },
      "2": {
        "name": "2_optimization_problems_problem_objective",
        "keywords": [
          [
            "optimization",
            0.06037392214499005
          ],
          [
            "problems",
            0.03521617052073882
          ],
          [
            "problem",
            0.022766094452813607
          ],
          [
            "objective",
            0.021613155260252325
          ],
          [
            "algorithm",
            0.019884763180364275
          ],
          [
            "optimization problems",
            0.01736227978918273
          ],
          [
            "algorithms",
            0.017001696504500112
          ],
          [
            "Optimization",
            0.0165767231577395
          ],
          [
            "solvers",
            0.016263386460168523
          ],
          [
            "constraints",
            0.015936151272062556
          ]
        ],
        "count": 87
      },
      "3": {
        "name": "3_package_data_Python_causal",
        "keywords": [
          [
            "package",
            0.049736900790724146
          ],
          [
            "data",
            0.04329207014977127
          ],
          [
            "Python",
            0.038049237484431946
          ],
          [
            "causal",
            0.03360412766926975
          ],
          [
            "models",
            0.0296986194905424
          ],
          [
            "model",
            0.02511564011804178
          ],
          [
            "packages",
            0.021449822907304187
          ],
          [
            "analysis",
            0.01970643832517919
          ],
          [
            "statistical",
            0.019295155787190404
          ],
          [
            "estimation",
            0.018746969287873807
          ]
        ],
        "count": 52
      },
      "4": {
        "name": "4_matrix_rank_matrices_algorithm",
        "keywords": [
          [
            "matrix",
            0.03601801107103876
          ],
          [
            "rank",
            0.03477214161618198
          ],
          [
            "matrices",
            0.0314975536507499
          ],
          [
            "algorithm",
            0.028746012275822845
          ],
          [
            "SVD",
            0.028258519788191983
          ],
          [
            "factorization",
            0.02681017876440613
          ],
          [
            "QR",
            0.025519770191614714
          ],
          [
            "low rank",
            0.024176886207939886
          ],
          [
            "singular",
            0.02355671094610589
          ],
          [
            "memory",
            0.022450964125806164
          ]
        ],
        "count": 42
      },
      "5": {
        "name": "5_algebra_sparse_performance_tensor",
        "keywords": [
          [
            "algebra",
            0.04291019570764711
          ],
          [
            "sparse",
            0.04045016738701544
          ],
          [
            "performance",
            0.036656646177852956
          ],
          [
            "tensor",
            0.034254652532225684
          ],
          [
            "linear algebra",
            0.03267977269859863
          ],
          [
            "linear",
            0.027254449950120038
          ],
          [
            "BLAS",
            0.025672738381723802
          ],
          [
            "matrix",
            0.02328013723062547
          ],
          [
            "tensors",
            0.021040195716264472
          ],
          [
            "data",
            0.021025553482194273
          ]
        ],
        "count": 40
      },
      "6": {
        "name": "6_Bayesian_Carlo_Monte_sampling",
        "keywords": [
          [
            "Bayesian",
            0.043816796824450846
          ],
          [
            "Carlo",
            0.03430480619471747
          ],
          [
            "Monte",
            0.03430480619471747
          ],
          [
            "sampling",
            0.02773083552704052
          ],
          [
            "UQ",
            0.02473674102368511
          ],
          [
            "uncertainty",
            0.023925537877619828
          ],
          [
            "inference",
            0.021550311383603898
          ],
          [
            "models",
            0.02154639098855397
          ],
          [
            "Python",
            0.020274694462589834
          ],
          [
            "model",
            0.019739885628592718
          ]
        ],
        "count": 36
      },
      "7": {
        "name": "7_graph_data_graphs_algorithms",
        "keywords": [
          [
            "graph",
            0.06004360164437843
          ],
          [
            "data",
            0.03357525144331023
          ],
          [
            "graphs",
            0.033563810142574466
          ],
          [
            "algorithms",
            0.029373531812777402
          ],
          [
            "analysis",
            0.0280897756046001
          ],
          [
            "library",
            0.021032542583575526
          ],
          [
            "package",
            0.019768945766388473
          ],
          [
            "complex",
            0.01872856401527437
          ],
          [
            "topological",
            0.01824084287337258
          ],
          [
            "theory",
            0.01788327683844324
          ]
        ],
        "count": 34
      }
    },
    "correlations": [
      [
        1.0,
        -0.5880766988095874,
        -0.6241874310906299,
        -0.6442176232313163,
        -0.6505517942426002,
        -0.4295312344065684,
        -0.7282778291198203,
        -0.65756920915458
      ],
      [
        -0.5880766988095874,
        1.0,
        -0.6613982862002796,
        -0.5981490873718436,
        -0.6362576972845471,
        -0.6233255581629786,
        -0.7256544795333848,
        -0.62336573205212
      ],
      [
        -0.6241874310906299,
        -0.6613982862002796,
        1.0,
        -0.6688485980689538,
        -0.6941095233962323,
        -0.6539356051056633,
        -0.704551712333911,
        -0.6714164973036738
      ],
      [
        -0.6442176232313163,
        -0.5981490873718436,
        -0.6688485980689538,
        1.0,
        -0.6722095686640217,
        -0.6626364715178126,
        -0.6703828891386017,
        -0.4553144373029006
      ],
      [
        -0.6505517942426002,
        -0.6362576972845471,
        -0.6941095233962323,
        -0.6722095686640217,
        1.0,
        -0.5903807474234322,
        -0.7401473930090604,
        -0.6216529935221271
      ],
      [
        -0.4295312344065684,
        -0.6233255581629786,
        -0.6539356051056633,
        -0.6626364715178126,
        -0.5903807474234322,
        1.0,
        -0.755323587994593,
        -0.6368191516943353
      ],
      [
        -0.7282778291198203,
        -0.7256544795333848,
        -0.704551712333911,
        -0.6703828891386017,
        -0.7401473930090604,
        -0.755323587994593,
        1.0,
        -0.7383534632769275
      ],
      [
        -0.65756920915458,
        -0.62336573205212,
        -0.6714164973036738,
        -0.4553144373029006,
        -0.6216529935221271,
        -0.6368191516943353,
        -0.7383534632769275,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        4,
        2,
        1,
        2,
        0,
        2,
        1,
        1
      ],
      "2020-02": [
        3,
        0,
        2,
        2,
        3,
        1,
        0,
        0
      ],
      "2020-03": [
        1,
        9,
        2,
        4,
        2,
        0,
        0,
        1
      ],
      "2020-04": [
        2,
        3,
        1,
        3,
        0,
        1,
        0,
        0
      ],
      "2020-05": [
        2,
        1,
        1,
        3,
        2,
        2,
        1,
        1
      ],
      "2020-06": [
        5,
        0,
        1,
        2,
        1,
        5,
        0,
        0
      ],
      "2020-07": [
        3,
        4,
        2,
        4,
        4,
        2,
        0,
        3
      ],
      "2020-08": [
        1,
        0,
        1,
        4,
        4,
        2,
        0,
        0
      ],
      "2020-09": [
        3,
        2,
        3,
        4,
        2,
        3,
        2,
        2
      ],
      "2020-10": [
        1,
        7,
        3,
        3,
        1,
        2,
        1,
        1
      ],
      "2020-11": [
        3,
        6,
        2,
        5,
        2,
        2,
        1,
        2
      ],
      "2020-12": [
        2,
        1,
        3,
        4,
        0,
        3,
        0,
        2
      ],
      "2021-01": [
        2,
        5,
        1,
        5,
        1,
        2,
        0,
        0
      ],
      "2021-02": [
        5,
        2,
        1,
        6,
        2,
        1,
        1,
        0
      ],
      "2021-03": [
        7,
        1,
        3,
        3,
        1,
        5,
        0,
        4
      ],
      "2021-04": [
        5,
        2,
        3,
        4,
        3,
        5,
        1,
        1
      ],
      "2021-05": [
        4,
        2,
        2,
        2,
        2,
        1,
        4,
        2
      ],
      "2021-06": [
        1,
        1,
        3,
        3,
        3,
        1,
        0,
        0
      ],
      "2021-07": [
        6,
        4,
        2,
        5,
        3,
        1,
        1,
        0
      ],
      "2021-08": [
        4,
        5,
        2,
        0,
        5,
        1,
        0,
        0
      ],
      "2021-09": [
        5,
        0,
        0,
        5,
        2,
        6,
        0,
        0
      ],
      "2021-10": [
        4,
        3,
        0,
        1,
        0,
        5,
        0,
        0
      ],
      "2021-11": [
        7,
        2,
        4,
        4,
        1,
        2,
        2,
        2
      ],
      "2021-12": [
        6,
        0,
        2,
        6,
        1,
        5,
        1,
        0
      ],
      "2022-01": [
        4,
        2,
        1,
        6,
        3,
        1,
        0,
        0
      ],
      "2022-02": [
        4,
        3,
        1,
        4,
        2,
        3,
        1,
        0
      ],
      "2022-03": [
        2,
        1,
        3,
        3,
        1,
        4,
        1,
        0
      ],
      "2022-04": [
        2,
        2,
        4,
        2,
        1,
        0,
        0,
        1
      ],
      "2022-05": [
        5,
        1,
        4,
        4,
        2,
        1,
        0,
        0
      ],
      "2022-06": [
        4,
        4,
        2,
        2,
        0,
        2,
        0,
        0
      ],
      "2022-07": [
        6,
        3,
        2,
        1,
        3,
        2,
        1,
        1
      ],
      "2022-08": [
        2,
        0,
        5,
        3,
        1,
        2,
        0,
        0
      ],
      "2022-09": [
        1,
        5,
        1,
        1,
        0,
        1,
        0,
        2
      ],
      "2022-10": [
        5,
        1,
        4,
        2,
        0,
        0,
        0,
        0
      ],
      "2022-11": [
        1,
        1,
        3,
        2,
        1,
        1,
        0,
        0
      ],
      "2022-12": [
        3,
        0,
        3,
        1,
        0,
        1,
        1,
        1
      ],
      "2023-01": [
        2,
        1,
        3,
        3,
        0,
        1,
        2,
        0
      ],
      "2023-02": [
        3,
        2,
        2,
        5,
        1,
        2,
        0,
        0
      ],
      "2023-03": [
        2,
        0,
        3,
        7,
        1,
        4,
        1,
        1
      ],
      "2023-04": [
        4,
        7,
        1,
        5,
        1,
        2,
        0,
        1
      ],
      "2023-05": [
        4,
        1,
        0,
        1,
        0,
        1,
        3,
        0
      ],
      "2023-06": [
        2,
        4,
        3,
        1,
        0,
        0,
        1,
        1
      ],
      "2023-07": [
        0,
        3,
        5,
        5,
        0,
        2,
        0,
        0
      ],
      "2023-08": [
        3,
        4,
        3,
        3,
        0,
        1,
        0,
        2
      ],
      "2023-09": [
        6,
        4,
        0,
        5,
        1,
        0,
        0,
        0
      ],
      "2023-10": [
        0,
        2,
        1,
        4,
        1,
        1,
        1,
        1
      ],
      "2023-11": [
        4,
        1,
        3,
        1,
        2,
        3,
        0,
        0
      ],
      "2023-12": [
        3,
        1,
        5,
        1,
        1,
        2,
        0,
        0
      ],
      "2024-01": [
        5,
        2,
        0,
        3,
        0,
        1,
        0,
        3
      ],
      "2024-02": [
        4,
        4,
        2,
        1,
        0,
        0,
        2,
        1
      ],
      "2024-03": [
        1,
        6,
        2,
        3,
        2,
        1,
        0,
        1
      ],
      "2024-04": [
        4,
        0,
        0,
        7,
        2,
        2,
        0,
        0
      ],
      "2024-05": [
        5,
        4,
        3,
        3,
        3,
        4,
        0,
        1
      ],
      "2024-06": [
        4,
        3,
        4,
        2,
        0,
        2,
        0,
        2
      ],
      "2024-07": [
        4,
        0,
        3,
        2,
        0,
        1,
        0,
        0
      ],
      "2024-08": [
        4,
        2,
        2,
        2,
        0,
        2,
        0,
        1
      ],
      "2024-09": [
        3,
        3,
        5,
        2,
        2,
        3,
        0,
        1
      ],
      "2024-10": [
        7,
        3,
        4,
        4,
        0,
        1,
        1,
        1
      ],
      "2024-11": [
        2,
        0,
        3,
        0,
        0,
        2,
        1,
        2
      ],
      "2024-12": [
        2,
        2,
        3,
        2,
        0,
        3,
        1,
        0
      ],
      "2025-01": [
        2,
        0,
        2,
        3,
        0,
        1,
        0,
        0
      ],
      "2025-02": [
        5,
        2,
        3,
        0,
        1,
        1,
        1,
        0
      ],
      "2025-03": [
        2,
        0,
        1,
        4,
        2,
        0,
        0,
        3
      ],
      "2025-04": [
        5,
        5,
        3,
        2,
        2,
        1,
        0,
        0
      ],
      "2025-05": [
        1,
        3,
        7,
        5,
        0,
        1,
        0,
        1
      ],
      "2025-06": [
        3,
        5,
        3,
        2,
        2,
        5,
        1,
        0
      ],
      "2025-07": [
        3,
        0,
        5,
        0,
        0,
        3,
        0,
        0
      ],
      "2025-08": [
        8,
        1,
        1,
        1,
        1,
        3,
        2,
        2
      ],
      "2025-09": [
        1,
        1,
        1,
        2,
        3,
        2,
        1,
        0
      ]
    },
    "papers": {
      "0": [
        {
          "title": "End-to-end GPU acceleration of low-order-refined preconditioning for high-order finite element discretizations",
          "year": "2022-10",
          "abstract": "In this paper, we present algorithms and implementations for the end-to-end\nGPU acceleration of matrix-free low-order-refined preconditioning of high-order\nfinite element problems. The methods described here allow for the construction\nof effective preconditioners for high-order problems with optimal memory usage\nand computational complexity. The preconditioners are based on the construction\nof a spectrally equivalent low-order discretization on a refined mesh, which is\nthen amenable to, for example, algebraic multigrid preconditioning. The\nconstants of equivalence are independent of mesh size and polynomial degree.\nFor vector finite element problems in $H({\\rm curl})$ and $H({\\rm div})$ (e.g.\nfor electromagnetic or radiation diffusion problems) a specially constructed\ninterpolation-histopolation basis is used to ensure fast convergence. Detailed\nperformance studies are carried out to analyze the efficiency of the GPU\nalgorithms. The kernel throughput of each of the main algorithmic components is\nmeasured, and the strong and weak parallel scalability of the methods is\ndemonstrated. The different relative weighting and significance of the\nalgorithmic components on GPUs and CPUs is discussed. Results on problems\ninvolving adaptively refined nonconforming meshes are shown, and the use of the\npreconditioners on a large-scale magnetic diffusion problem using all spaces of\nthe finite element de Rham complex is illustrated.",
          "arxiv_id": "2210.12253v1"
        },
        {
          "title": "Robust and scalable h-adaptive aggregated unfitted finite elements for interface elliptic problems",
          "year": "2020-06",
          "abstract": "This work introduces a novel, fully robust and highly-scalable, $h$-adaptive\naggregated unfitted finite element method for large-scale interface elliptic\nproblems. The new method is based on a recent distributed-memory implementation\nof the aggregated finite element method atop a highly-scalable Cartesian\nforest-of-trees mesh engine. It follows the classical approach of weakly\ncoupling nonmatching discretisations at the interface to model internal\ndiscontinuities at the interface. We propose a natural extension of a\nsingle-domain parallel cell aggregation scheme to problems with a finite number\nof interfaces; it straightforwardly leads to aggregated finite element spaces\nthat have the structure of a Cartesian product. We demonstrate, through\nstandard numerical analysis and exhaustive numerical experimentation on several\ncomplex Poisson and linear elasticity benchmarks, that the new technique enjoys\nthe following properties: well-posedness, robustness with respect to cut\nlocation and material contrast, optimal ($h$-adaptive) approximation\nproperties, high scalability and easy implementation in large-scale finite\nelement codes. As a result, the method offers great potential as a useful\nfinite element solver for large-scale interface problems modelled by partial\ndifferential equations.",
          "arxiv_id": "2006.11042v2"
        },
        {
          "title": "Accelerating High-Order Mesh Optimization Using Finite Element Partial Assembly on GPUs",
          "year": "2022-05",
          "abstract": "In this paper we present a new GPU-oriented mesh optimization method based on\nhigh-order finite elements. Our approach relies on node movement with fixed\ntopology, through the Target-Matrix Optimization Paradigm (TMOP) and uses a\nglobal nonlinear solve over the whole computational mesh, i.e., all mesh nodes\nare moved together. A key property of the method is that the mesh optimization\nprocess is recast in terms of finite element operations, which allows us to\nutilize recent advances in the field of GPU-accelerated high-order finite\nelement algorithms. For example, we reduce data motion by using tensor\nfactorization and matrix-free methods, which have superior performance\ncharacteristics compared to traditional full finite element matrix assembly and\noffer advantages for GPU-based HPC hardware. We describe the major mathematical\ncomponents of the method along with their efficient GPU-oriented\nimplementation. In addition, we propose an easily reproducible mesh\noptimization test that can serve as a performance benchmark for the mesh\noptimization community.",
          "arxiv_id": "2205.12721v2"
        }
      ],
      "1": [
        {
          "title": "A Novel Approach to Generate Correctly Rounded Math Libraries for New Floating Point Representations",
          "year": "2020-07",
          "abstract": "Given the importance of floating-point~(FP) performance in numerous domains,\nseveral new variants of FP and its alternatives have been proposed (e.g.,\nBfloat16, TensorFloat32, and Posits). These representations do not have\ncorrectly rounded math libraries. Further, the use of existing FP libraries for\nthese new representations can produce incorrect results. This paper proposes a\nnovel approach for generating polynomial approximations that can be used to\nimplement correctly rounded math libraries. Existing methods generate\npolynomials that approximate the real value of an elementary function $f(x)$\nand produce wrong results due to approximation errors and rounding errors in\nthe implementation. In contrast, our approach generates polynomials that\napproximate the correctly rounded value of $f(x)$ (i.e., the value of $f(x)$\nrounded to the target representation). It provides more margin to identify\nefficient polynomials that produce correctly rounded results for all inputs. We\nframe the problem of generating efficient polynomials that produce correctly\nrounded results as a linear programming problem. Our approach guarantees that\nwe produce the correct result even with range reduction techniques. Using our\napproach, we have developed correctly rounded, yet faster, implementations of\nelementary functions for multiple target representations.",
          "arxiv_id": "2007.05344v3"
        },
        {
          "title": "Accuracy of Mathematical Functions in Julia",
          "year": "2025-09",
          "abstract": "Basic computer arithmetic operations, such as $+$, $\\times$, or $\\div$ are\ncorrectly rounded, whilst mathematical functions such as $e^x$, $\\ln(x)$, or\n$\\sin(x)$ in general are not, meaning that separate implementations may provide\ndifferent results when presented with an exact same input, and that their\naccuracy may differ. We present a methodology and a software tool that is\nsuited for exhaustive and non-exhaustive testing of mathematical functions of\nJulia in various floating-point formats. The software tool is useful to the\nusers of Julia, to quantise the level of accuracy of the mathematical functions\nand interpret possible effects of errors on their scientific computation codes\nthat depend on these functions. It is also useful to the developers and\nmaintainers of the functions in Julia Base, to test the modifications to\nexisting functions and to test the accuracy of new functions. The software (a\ntest bench) is designed to be easy to set up for running the accuracy tests in\nautomatic regression testing. Our focus is to provide software that is user\nfriendly and allows to avoid the need for specialised knowledge of\nfloating-point arithmetic or the workings of mathematical functions; users only\nneed to supply a list of formats, choose the rounding modes, and specify the\ninput space search strategies based on how long they can afford the testing to\nrun. We have utilized the test bench to determine the errors of a subset of\nmathematical functions in the latest version of Julia, for binary16, binary32,\nand binary64 IEEE 754 floating-point formats, and found $0.49$ to $0.51$ULPs in\nbinary16, and $0.5$ to $2.4$ULPs of error in binary32 and binary64. The\nfunctions that may be correctly rounded (error of $0.5$ULP) in all the three\nformats are sqrt and cbrt. The following functions may be correctly rounded\nonly for binary16: sinh, asin, cospi, sinpi, atanh, log2, tanh.",
          "arxiv_id": "2509.05666v1"
        },
        {
          "title": "RLIBM-ALL: A Novel Polynomial Approximation Method to Produce Correctly Rounded Results for Multiple Representations and Rounding Modes",
          "year": "2021-08",
          "abstract": "Mainstream math libraries for floating point (FP) do not produce correctly\nrounded results for all inputs. In contrast, CR-LIBM and RLIBM provide\ncorrectly rounded implementations for a specific FP representation with one\nrounding mode. Using such libraries for a representation with a new rounding\nmode or with different precision will result in wrong results due to double\nrounding. This paper proposes a novel method to generate a single polynomial\napproximation that produces correctly rounded results for all inputs for\nmultiple rounding modes and multiple precision configurations. To generate a\ncorrectly rounded library for $n$-bits, our key idea is to generate such a\npolynomial approximation for a representation with $n+2$-bits using the\n\\emph{round-to-odd} mode. We prove that the resulting polynomial approximation\nwill produce correctly rounded results for all five rounding modes in the\nstandard and for multiple representations with $k$-bits such that $|E| +1 < k\n\\leq n$, where $|E|$ is the number of exponent bits in the representation.\nBuilding on our prior work in the RLIBM project, we also approximate the\ncorrectly rounded result when we generate the library with $n+2$-bits using the\nround-to-odd mode. We also generate polynomial approximations by structuring it\nas a linear programming problem but propose enhancements to polynomial\ngeneration to handle the round-to-odd mode. Our prototype is the first 32-bit\nfloat library that produces correctly rounded results with all rounding modes\nin the IEEE standard for all inputs with a single polynomial approximation. It\nalso produces correctly rounded results for any FP configuration ranging from\n10-bits to 32-bits while also being faster than mainstream libraries.",
          "arxiv_id": "2108.06756v2"
        }
      ],
      "2": [
        {
          "title": "A New Challenging Curve Fitting Benchmark Test Set for Global Optimization",
          "year": "2023-12",
          "abstract": "Benchmark sets are extremely important for evaluating and developing global\noptimization algorithms and related solvers. A new test set named PCC benchmark\nis proposed especially for optimization problems of nonlinear curve fitting for\nthe first time, with the aspiration of helping developers to investigate and\ncompare the performance of different global optimization solvers, as well as\nmore effective optimization algorithms could be developed. Compared with the\nwell-known classical nonlinear curve fitting benchmark set given by the\nNational Institute of Standards and Technology (NIST) of USA, the most\ndistinguishable features of the PCC benchmark are small problem dimensions,\nunconstrained with free search domain and high level of difficulty for\nobtaining global optimization solutions, which make the PCC benchmark be not\nonly suitable for validating the effectiveness of different global optimization\nalgorithms, but also more ideal for verifying and comparing various related\nsolvers. Seven of the world's leading global optimization solvers, including\nBaron, Antigone, Couenne, Lingo, Scip, Matlab-GA and 1stOpt, are employed to\ntest NIST and PCC benchmark thoroughly in terms of both effectiveness and\nefficiency. The results showed that the NIST benchmark is relatively simple and\nnot suitable for global optimization testing, meanwhile the PCC benchmark is a\nunique, challenging and effective test dataset for global optimization.",
          "arxiv_id": "2312.01709v4"
        },
        {
          "title": "GPSAF: A Generalized Probabilistic Surrogate-Assisted Framework for Constrained Single- and Multi-objective Optimization",
          "year": "2022-04",
          "abstract": "Significant effort has been made to solve computationally expensive\noptimization problems in the past two decades, and various optimization methods\nincorporating surrogates into optimization have been proposed. Most research\nfocuses on either exploiting the surrogate by defining a utility optimization\nproblem or customizing an existing optimization method to use one or multiple\napproximation models. However, only a little attention has been paid to generic\nconcepts applicable to different types of algorithms and optimization problems\nsimultaneously. Thus this paper proposes a generalized probabilistic\nsurrogate-assisted framework (GPSAF), applicable to a broad category of\nunconstrained and constrained, single- and multi-objective optimization\nalgorithms. The idea is based on a surrogate assisting an existing optimization\nmethod. The assistance is based on two distinct phases, one facilitating\nexploration and another exploiting the surrogates. The exploration and\nexploitation of surrogates are automatically balanced by performing a\nprobabilistic knockout tournament among different clusters of solutions. A\nstudy of multiple well-known population-based optimization algorithms is\nconducted with and without the proposed surrogate assistance on single- and\nmulti-objective optimization problems with a maximum solution evaluation budget\nof 300 or less. The results indicate the effectiveness of applying GPSAF to an\noptimization algorithm and the competitiveness with other surrogate-assisted\nalgorithms.",
          "arxiv_id": "2204.04054v1"
        },
        {
          "title": "An efficient optimization model and tabu search-based global optimization approach for continuous p-dispersion problem",
          "year": "2024-05",
          "abstract": "Continuous p-dispersion problems with and without boundary constraints are\nNP-hard optimization problems with numerous real-world applications, notably in\nfacility location and circle packing, which are widely studied in mathematics\nand operations research. In this work, we concentrate on general cases with a\nnon-convex multiply-connected region that are rarely studied in the literature\ndue to their intractability and the absence of an efficient optimization model.\nUsing the penalty function approach, we design a unified and almost everywhere\ndifferentiable optimization model for these complex problems and propose a tabu\nsearch-based global optimization (TSGO) algorithm for solving them.\nComputational results over a variety of benchmark instances show that the\nproposed model works very well, allowing popular local optimization methods\n(e.g., the quasi-Newton methods and the conjugate gradient methods) to reach\nhigh-precision solutions due to the differentiability of the model. These\nresults further demonstrate that the proposed TSGO algorithm is very efficient\nand significantly outperforms several popular global optimization algorithms in\nthe literature, improving the best-known solutions for several existing\ninstances in a short computational time. Experimental analyses are conducted to\nshow the influence of several key ingredients of the algorithm on computational\nperformance.",
          "arxiv_id": "2405.16618v1"
        }
      ],
      "3": [
        {
          "title": "RIFLE: Imputation and Robust Inference from Low Order Marginals",
          "year": "2021-09",
          "abstract": "The ubiquity of missing values in real-world datasets poses a challenge for\nstatistical inference and can prevent similar datasets from being analyzed in\nthe same study, precluding many existing datasets from being used for new\nanalyses. While an extensive collection of packages and algorithms have been\ndeveloped for data imputation, the overwhelming majority perform poorly if\nthere are many missing values and low sample sizes, which are unfortunately\ncommon characteristics in empirical data. Such low-accuracy estimations\nadversely affect the performance of downstream statistical models. We develop a\nstatistical inference framework for regression and classification in the\npresence of missing data without imputation. Our framework, RIFLE (Robust\nInFerence via Low-order moment Estimations), estimates low-order moments of the\nunderlying data distribution with corresponding confidence intervals to learn a\ndistributionally robust model. We specialize our framework to linear regression\nand normal discriminant analysis, and we provide convergence and performance\nguarantees. This framework can also be adapted to impute missing data. In\nnumerical experiments, we compare RIFLE to several state-of-the-art approaches\n(including MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer) for\nimputation and inference in the presence of missing values. Our experiments\ndemonstrate that RIFLE outperforms other benchmark algorithms when the\npercentage of missing values is high and/or when the number of data points is\nrelatively small. RIFLE is publicly available at\nhttps://github.com/optimization-for-data-driven-science/RIFLE.",
          "arxiv_id": "2109.00644v3"
        },
        {
          "title": "CausalGPS: An R Package for Causal Inference With Continuous Exposures",
          "year": "2023-10",
          "abstract": "Quantifying the causal effects of continuous exposures on outcomes of\ninterest is critical for social, economic, health, and medical research.\nHowever, most existing software packages focus on binary exposures. We develop\nthe CausalGPS R package that implements a collection of algorithms to provide\nalgorithmic solutions for causal inference with continuous exposures. CausalGPS\nimplements a causal inference workflow, with algorithms based on generalized\npropensity scores (GPS) as the core, extending propensity scores (the\nprobability of a unit being exposed given pre-exposure covariates) from binary\nto continuous exposures. As the first step, the package implements efficient\nand flexible estimations of the GPS, allowing multiple user-specified modeling\noptions. As the second step, the package provides two ways to adjust for\nconfounding: weighting and matching, generating weighted and matched data sets,\nrespectively. Lastly, the package provides built-in functions to fit flexible\nparametric, semi-parametric, or non-parametric regression models on the\nweighted or matched data to estimate the exposure-response function relating\nthe outcome with the exposures. The computationally intensive tasks are\nimplemented in C++, and efficient shared-memory parallelization is achieved by\nOpenMP API. This paper outlines the main components of the CausalGPS R package\nand demonstrates its application to assess the effect of long-term exposure to\nPM2.5 on educational attainment using zip code-level data from the contiguous\nUnited States from 2000-2016.",
          "arxiv_id": "2310.00561v1"
        },
        {
          "title": "An R package for parametric estimation of causal effects",
          "year": "2023-07",
          "abstract": "This article explains the usage of R package CausalModels, which is publicly\navailable on the Comprehensive R Archive Network. While packages are available\nfor sufficiently estimating causal effects, there lacks a package that provides\na collection of structural models using the conventional statistical approach\ndeveloped by Hernan and Robins (2020). CausalModels addresses this deficiency\nof software in R concerning causal inference by offering tools for methods that\naccount for biases in observational data without requiring extensive\nstatistical knowledge. These methods should not be ignored and may be more\nappropriate or efficient in solving particular problems. While implementations\nof these statistical models are distributed among a number of causal packages,\nCausalModels introduces a simple and accessible framework for a consistent\nmodeling pipeline among a variety of statistical methods for estimating causal\neffects in a single R package. It consists of common methods including\nstandardization, IP weighting, G-estimation, outcome regression, instrumental\nvariables and propensity matching.",
          "arxiv_id": "2307.08686v2"
        }
      ],
      "4": [
        {
          "title": "Randomized Projection for Rank-Revealing Matrix Factorizations and Low-Rank Approximations",
          "year": "2020-08",
          "abstract": "Rank-revealing matrix decompositions provide an essential tool in spectral\nanalysis of matrices, including the Singular Value Decomposition (SVD) and\nrelated low-rank approximation techniques. QR with Column Pivoting (QRCP) is\nusually suitable for these purposes, but it can be much slower than the\nunpivoted QR algorithm. For large matrices, the difference in performance is\ndue to increased communication between the processor and slow memory, which\nQRCP needs in order to choose pivots during decomposition. Our main algorithm,\nRandomized QR with Column Pivoting (RQRCP), uses randomized projection to make\npivot decisions from a much smaller sample matrix, which we can construct to\nreside in a faster level of memory than the original matrix. This technique may\nbe understood as trading vastly reduced communication for a controlled increase\nin uncertainty during the decision process. For rank-revealing purposes, the\nselection mechanism in RQRCP produces results that are the same quality as the\nstandard algorithm, but with performance near that of unpivoted QR (often an\norder of magnitude faster for large matrices). We also propose two formulas\nthat facilitate further performance improvements. The first efficiently updates\nsample matrices to avoid computing new randomized projections. The second\navoids large trailing updates during the decomposition in truncated low-rank\napproximations. Our truncated version of RQRCP also provides a key initial step\nin our truncated SVD approximation, TUXV. These advances open up a new\nperformance domain for large matrix factorizations that will support efficient\nproblem-solving techniques for challenging applications in science,\nengineering, and data analysis.",
          "arxiv_id": "2008.04447v1"
        },
        {
          "title": "Efficient algorithms for computing a rank-revealing UTV factorization on parallel computing architectures",
          "year": "2021-04",
          "abstract": "The randomized singular value decomposition (RSVD) is by now a well\nestablished technique for efficiently computing an approximate singular value\ndecomposition of a matrix. Building on the ideas that underpin the RSVD, the\nrecently proposed algorithm \"randUTV\" computes a FULL factorization of a given\nmatrix that provides low-rank approximations with near-optimal error. Because\nthe bulk of randUTV is cast in terms of communication-efficient operations like\nmatrix-matrix multiplication and unpivoted QR factorizations, it is faster than\ncompeting rank-revealing factorization methods like column pivoted QR in most\nhigh performance computational settings. In this article, optimized randUTV\nimplementations are presented for both shared memory and distributed memory\ncomputing environments. For shared memory, randUTV is redesigned in terms of an\n\"algorithm-by-blocks\" that, together with a runtime task scheduler, eliminates\nbottlenecks from data synchronization points to achieve acceleration over the\nstandard \"blocked algorithm\", based on a purely fork-join approach. The\ndistributed memory implementation is based on the ScaLAPACK library. The\nperformances of our new codes compare favorably with competing factorizations\navailable on both shared memory and distributed memory architectures.",
          "arxiv_id": "2104.05782v1"
        },
        {
          "title": "Computing rank-revealing factorizations of matrices stored out-of-core",
          "year": "2020-02",
          "abstract": "This paper describes efficient algorithms for computing rank-revealing\nfactorizations of matrices that are too large to fit in RAM, and must instead\nbe stored on slow external memory devices such as solid-state or spinning disk\nhard drives (out-of-core or out-of-memory). Traditional algorithms for\ncomputing rank revealing factorizations, such as the column pivoted QR\nfactorization, or techniques for computing a full singular value decomposition\nof a matrix, are very communication intensive. They are naturally expressed as\na sequence of matrix-vector operations, which become prohibitively expensive\nwhen data is not available in main memory. Randomization allows these methods\nto be reformulated so that large contiguous blocks of the matrix can be\nprocessed in bulk. The paper describes two distinct methods. The first is a\nblocked version of column pivoted Householder QR, organized as a \"left-looking\"\nmethod to minimize the number of write operations (which are more expensive\nthan read operations on a spinning disk drive). The second method results in a\nso called UTV factorization which expresses a matrix $A$ as $A = U T V^*$ where\n$U$ and $V$ are unitary, and $T$ is triangular. This method is organized as an\nalgorithm-by-blocks, in which floating point operations overlap read and write\noperations. The second method incorporates power iterations, and is\nexceptionally good at revealing the numerical rank; it can often be used as a\nsubstitute for a full singular value decomposition. Numerical experiments\ndemonstrate that the new algorithms are almost as fast when processing data\nstored on a hard drive as traditional algorithms are for data stored in main\nmemory. To be precise, the computational time for fully factorizing an $n\\times\nn$ matrix scales as $cn^{3}$, with a scaling constant $c$ that is only\nmarginally larger when the matrix is stored out of core.",
          "arxiv_id": "2002.06960v2"
        }
      ],
      "5": [
        {
          "title": "Dynamic Sparse Tensor Algebra Compilation",
          "year": "2021-12",
          "abstract": "This paper shows how to generate efficient tensor algebra code that compute\non dynamic sparse tensors, which have sparsity structures that evolve over\ntime. We propose a language for precisely specifying recursive, pointer-based\ndata structures, and we show how this language can express a wide range of\ndynamic data structures that support efficient modification, such as linked\nlists, binary search trees, and B-trees. We then describe how, given high-level\nspecifications of such data structures, a compiler can generate code to\nefficiently iterate over and compute with dynamic sparse tensors that are\nstored in the aforementioned data structures. Furthermore, we define an\nabstract interface that captures how nonzeros can be inserted into dynamic data\nstructures, and we show how this abstraction guides a compiler to emit\nefficient code that store the results of sparse tensor algebra computations in\ndynamic data structures.\n  We evaluate our technique and find that it generates efficient dynamic sparse\ntensor algebra kernels. Code that our technique emits to compute the main\nkernel of the PageRank algorithm is 1.05$\\times$ as fast as Aspen, a\nstate-of-the-art dynamic graph processing framework. Furthermore, our technique\noutperforms PAM, a parallel ordered (key-value) maps library, by 7.40$\\times$\nwhen used to implement element-wise addition of a dynamic sparse matrix to a\nstatic sparse matrix.",
          "arxiv_id": "2112.01394v1"
        },
        {
          "title": "Automatic Generation of Efficient Sparse Tensor Format Conversion Routines",
          "year": "2020-01",
          "abstract": "This paper shows how to generate code that efficiently converts sparse\ntensors between disparate storage formats (data layouts) such as CSR, DIA, ELL,\nand many others. We decompose sparse tensor conversion into three logical\nphases: coordinate remapping, analysis, and assembly. We then develop a\nlanguage that precisely describes how different formats group together and\norder a tensor's nonzeros in memory. This lets a compiler emit code that\nperforms complex remappings of nonzeros when converting between formats. We\nalso develop a query language that can extract statistics about sparse tensors,\nand we show how to emit efficient analysis code that computes such queries.\nFinally, we define an abstract interface that captures how data structures for\nstoring a tensor can be efficiently assembled given specific statistics about\nthe tensor. Disparate formats can implement this common interface, thus letting\na compiler emit optimized sparse tensor conversion code for arbitrary\ncombinations of many formats without hard-coding for any specific combination.\n  Our evaluation shows that the technique generates sparse tensor conversion\nroutines with performance between 1.00 and 2.01$\\times$ that of hand-optimized\nversions in SPARSKIT and Intel MKL, two popular sparse linear algebra\nlibraries. And by emitting code that avoids materializing temporaries, which\nboth libraries need for many combinations of source and target formats, our\ntechnique outperforms those libraries by 1.78 to 4.01$\\times$ for CSC/COO to\nDIA/ELL conversion.",
          "arxiv_id": "2001.02609v3"
        },
        {
          "title": "Interface for Sparse Linear Algebra Operations",
          "year": "2024-11",
          "abstract": "The standardization of an interface for dense linear algebra operations in\nthe BLAS standard has enabled interoperability between different linear algebra\nlibraries, thereby boosting the success of scientific computing, in particular\nin scientific HPC. Despite numerous efforts in the past, the community has not\nyet agreed on a standardization for sparse linear algebra operations due to\nnumerous reasons. One is the fact that sparse linear algebra objects allow for\nmany different storage formats, and different hardware may favor different\nstorage formats. This makes the definition of a FORTRAN-style all-circumventing\ninterface extremely challenging. Another reason is that opposed to dense linear\nalgebra functionality, in sparse linear algebra, the size of the sparse data\nstructure for the operation result is not always known prior to the\ninformation. Furthermore, as opposed to the standardization effort for dense\nlinear algebra, we are late in the technology readiness cycle, and many\nproduction-ready software libraries using sparse linear algebra routines have\nimplemented and committed to their own sparse BLAS interface. At the same time,\nthere exists a demand for standardization that would improve interoperability,\nand sustainability, and allow for easier integration of building blocks. In an\ninclusive, cross-institutional effort involving numerous academic institutions,\nUS National Labs, and industry, we spent two years designing a\nhardware-portable interface for basic sparse linear algebra functionality that\nserves the user needs and is compatible with the different interfaces currently\nused by different vendors. In this paper, we present a C++ API for sparse\nlinear algebra functionality, discuss the design choices, and detail how\nsoftware developers preserve a lot of freedom in terms of how to implement\nfunctionality behind this API.",
          "arxiv_id": "2411.13259v1"
        }
      ],
      "6": [
        {
          "title": "High Performance Uncertainty Quantification with Parallelized Multilevel Markov Chain Monte Carlo",
          "year": "2021-07",
          "abstract": "Numerical models of complex real-world phenomena often necessitate High\nPerformance Computing (HPC). Uncertainties increase problem dimensionality\nfurther and pose even greater challenges.\n  We present a parallelization strategy for multilevel Markov chain Monte\nCarlo, a state-of-the-art, algorithmically scalable Uncertainty Quantification\n(UQ) algorithm for Bayesian inverse problems, and a new software framework\nallowing for large-scale parallelism across forward model evaluations and the\nUQ algorithms themselves. The main scalability challenge presents itself in the\nform of strong data dependencies introduced by the MLMCMC method, prohibiting\ntrivial parallelization.\n  Our software is released as part of the modular and open-source MIT UQ\nLibrary (MUQ), and can easily be coupled with arbitrary user codes. We\ndemonstrate it using the DUNE and the ExaHyPE Engine. The latter provides a\nrealistic, large-scale tsunami model in which identify the source of a tsunami\nfrom buoy-elevation data.",
          "arxiv_id": "2107.14552v1"
        },
        {
          "title": "Fast fully-reproducible serial/parallel Monte Carlo and MCMC simulations and visualizations via ParaMonte::Python library",
          "year": "2020-10",
          "abstract": "ParaMonte::Python (standing for Parallel Monte Carlo in Python) is a serial\nand MPI-parallelized library of (Markov Chain) Monte Carlo (MCMC) routines for\nsampling mathematical objective functions, in particular, the posterior\ndistributions of parameters in Bayesian modeling and analysis in data science,\nMachine Learning, and scientific inference in general. In addition to providing\naccess to fast high-performance serial/parallel Monte Carlo and MCMC sampling\nroutines, the ParaMonte::Python library provides extensive post-processing and\nvisualization tools that aim to automate and streamline the process of model\ncalibration and uncertainty quantification in Bayesian data analysis.\nFurthermore, the automatically-enabled restart functionality of\nParaMonte::Python samplers ensure seamless fully-deterministic into-the-future\nrestart of Monte Carlo simulations, should any interruptions happen. The\nParaMonte::Python library is MIT-licensed and is permanently maintained on\nGitHub at\nhttps://github.com/cdslaborg/paramonte/tree/master/src/interface/Python.",
          "arxiv_id": "2010.00724v1"
        },
        {
          "title": "hIPPYlib-MUQ: A Bayesian Inference Software Framework for Integration of Data with Complex Predictive Models under Uncertainty",
          "year": "2021-12",
          "abstract": "Bayesian inference provides a systematic framework for integration of data\nwith mathematical models to quantify the uncertainty in the solution of the\ninverse problem. However, the solution of Bayesian inverse problems governed by\ncomplex forward models described by partial differential equations (PDEs)\nremains prohibitive with black-box Markov chain Monte Carlo (MCMC) methods. We\npresent hIPPYlib-MUQ, an extensible and scalable software framework that\ncontains implementations of state-of-the art algorithms aimed to overcome the\nchallenges of high-dimensional, PDE-constrained Bayesian inverse problems.\nThese algorithms accelerate MCMC sampling by exploiting the geometry and\nintrinsic low-dimensionality of parameter space via derivative information and\nlow rank approximation. The software integrates two complementary open-source\nsoftware packages, hIPPYlib and MUQ. hIPPYlib solves PDE-constrained inverse\nproblems using automatically-generated adjoint-based derivatives, but it lacks\nfull Bayesian capabilities. MUQ provides a spectrum of powerful Bayesian\ninversion models and algorithms, but expects forward models to come equipped\nwith gradients and Hessians to permit large-scale solution. By combining these\ntwo libraries, we created a robust, scalable, and efficient software framework\nthat realizes the benefits of each and allows us to tackle complex large-scale\nBayesian inverse problems. To illustrate the capabilities of hIPPYlib-MUQ, we\npresent a comparison of a number of MCMC methods on several inverse problems.\nThese include problems with linear and nonlinear PDEs, various noise models,\nand different parameter dimensions. The results demonstrate that large ($\\sim\n50\\times$) speedups over conventional black box and gradient-based MCMC\nalgorithms can be obtained by exploiting Hessian information (from the log\nposterior), underscoring the power of the integrated hIPPYlib-MUQ framework.",
          "arxiv_id": "2112.00713v2"
        }
      ],
      "7": [
        {
          "title": "chipfiring: A Python Package for Efficient Mathematical Analysis of Chip-Firing Games on Multigraphs",
          "year": "2025-08",
          "abstract": "This paper presents `chipfiring`, a comprehensive Python package for the\nmathematical analysis of chip-firing games on finite graphs. The package\nprovides a robust toolkit for defining graphs and chip configurations\n(divisors), performing chip-firing operations, and analyzing fundamental\nproperties such as winnability, linear equivalence, and divisor rank. We detail\nthe core components of the library, including its object-oriented graph and\ndivisor implementations, integrated Laplacian matrix computations, and an\nefficient implementation of Dhar's algorithm for determining the solvability of\nthe dollar game. The `chipfiring` package is designed for researchers and\nstudents in graph theory, combinatorics, and algebraic geometry, providing\nessential algorithms and data structures for exploring these rich mathematical\nmodels. We describe the library's architecture, illustrate its usage with\ncomprehensive examples, and highlight its specialized contributions compared to\ngeneral-purpose graph libraries.",
          "arxiv_id": "2508.00269v2"
        },
        {
          "title": "Graph4J -- A computationally efficient Java library for graph algorithms",
          "year": "2023-08",
          "abstract": "Graph algorithms play an important role in many computer science areas. In\norder to solve problems that can be modeled using graphs, it is necessary to\nuse a data structure that can represent those graphs in an efficient manner. On\ntop of this, an infrastructure should be build that will assist in implementing\ncommon algorithms or developing specialized ones. Here, a new Java library is\nintroduced, called Graph4J, that uses a different approach when compared to\nexisting, well-known Java libraries such as JGraphT, JUNG and Guava Graph.\nInstead of using object-oriented data structures for graph representation, a\nlower-level model based on arrays of primitive values is utilized, that\ndrastically reduces the required memory and the running times of the algorithm\nimplementations. The design of the library, the space complexity of the graph\nstructures and the time complexity of the most common graph operations are\npresented in detail, along with an experimental study that evaluates its\nperformance, when compared to the other libraries. Emphasis is given to\ninfrastructure related aspects, that is graph creation, inspection, alteration\nand traversal. The improvements obtained for other implemented algorithms are\nalso analyzed and it is shown that the proposed library significantly\noutperforms the existing ones.",
          "arxiv_id": "2308.09920v1"
        },
        {
          "title": "PlasmoData.jl -- A Julia Framework for Modeling and Analyzing Complex Data as Graphs",
          "year": "2024-01",
          "abstract": "Datasets encountered in scientific and engineering applications appear in\ncomplex formats (e.g., images, multivariate time series, molecules, video, text\nstrings, networks). Graph theory provides a unifying framework to model such\ndatasets and enables the use of powerful tools that can help analyze,\nvisualize, and extract value from data. In this work, we present\nPlasmoData$.$jl, an open-source, Julia framework that uses concepts of graph\ntheory to facilitate the modeling and analysis of complex datasets. The core of\nour framework is a general data modeling abstraction, which we call a\nDataGraph. We show how the abstraction and software implementation can be used\nto represent diverse data objects as graphs and to enable the use of tools from\ntopology, graph theory, and machine learning (e.g., graph neural networks) to\nconduct a variety of tasks. We illustrate the versatility of the framework by\nusing real datasets: i) an image classification problem using topological data\nanalysis to extract features from the graph model to train machine learning\nmodels; ii) a disease outbreak problem where we model multivariate time series\nas graphs to detect abnormal events; and iii) a technology pathway analysis\nproblem where we highlight how we can use graphs to navigate connectivity. Our\ndiscussion also highlights how PlasmoData$.$jl leverages native Julia\ncapabilities to enable compact syntax, scalable computations, and interfaces\nwith diverse packages.",
          "arxiv_id": "2401.11404v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:18:42Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}