{
  "topics": {
    "data": {
      "0": {
        "name": "0_learning_tasks_task_policy",
        "keywords": [
          [
            "learning",
            0.016969852866572575
          ],
          [
            "tasks",
            0.015942255256114434
          ],
          [
            "task",
            0.013185941488381831
          ],
          [
            "policy",
            0.011557266841848879
          ],
          [
            "manipulation",
            0.011197581814590766
          ],
          [
            "real",
            0.010715908445064091
          ],
          [
            "robot",
            0.010661984862549056
          ],
          [
            "world",
            0.010234003602581745
          ],
          [
            "language",
            0.010092252620126582
          ],
          [
            "data",
            0.009632512859403664
          ]
        ],
        "count": 5407
      },
      "1": {
        "name": "1_driving_autonomous_vehicle_traffic",
        "keywords": [
          [
            "driving",
            0.030371734526623357
          ],
          [
            "autonomous",
            0.01881375499013012
          ],
          [
            "vehicle",
            0.01763040978792128
          ],
          [
            "traffic",
            0.015576097644402412
          ],
          [
            "vehicles",
            0.015233695069540816
          ],
          [
            "safety",
            0.013231536729475232
          ],
          [
            "autonomous driving",
            0.01253088565044595
          ],
          [
            "scenarios",
            0.012447888366611865
          ],
          [
            "prediction",
            0.011593198516446076
          ],
          [
            "Autonomous",
            0.011487259986408975
          ]
        ],
        "count": 3430
      },
      "2": {
        "name": "2_SLAM_localization_LiDAR_map",
        "keywords": [
          [
            "SLAM",
            0.02888799934333346
          ],
          [
            "localization",
            0.016974217996932175
          ],
          [
            "LiDAR",
            0.015732846148257505
          ],
          [
            "map",
            0.012119563011853612
          ],
          [
            "estimation",
            0.01177750805027612
          ],
          [
            "odometry",
            0.011202949469156379
          ],
          [
            "accuracy",
            0.011172180297746465
          ],
          [
            "method",
            0.010525701370820836
          ],
          [
            "point",
            0.010233067388113463
          ],
          [
            "state",
            0.010013173136921944
          ]
        ],
        "count": 2686
      },
      "3": {
        "name": "3_robot_human_robots_social",
        "keywords": [
          [
            "robot",
            0.037966591722072104
          ],
          [
            "human",
            0.03276141486349349
          ],
          [
            "robots",
            0.026953078095722123
          ],
          [
            "social",
            0.025882304844495184
          ],
          [
            "trust",
            0.02341593775558813
          ],
          [
            "interaction",
            0.02044138396022219
          ],
          [
            "human robot",
            0.017058974790882024
          ],
          [
            "HRI",
            0.016984731259653985
          ],
          [
            "Robot",
            0.015692020722709137
          ],
          [
            "study",
            0.014985615295116038
          ]
        ],
        "count": 997
      },
      "4": {
        "name": "4_segmentation_3D_LiDAR_detection",
        "keywords": [
          [
            "segmentation",
            0.02353139370889225
          ],
          [
            "3D",
            0.02290317886995166
          ],
          [
            "LiDAR",
            0.019590830744780012
          ],
          [
            "detection",
            0.01824230771795781
          ],
          [
            "semantic",
            0.01684265703341715
          ],
          [
            "perception",
            0.01429373271329135
          ],
          [
            "point",
            0.01411894459606914
          ],
          [
            "object",
            0.0137993455741867
          ],
          [
            "data",
            0.012227696149903773
          ],
          [
            "object detection",
            0.011842163187715322
          ]
        ],
        "count": 812
      },
      "5": {
        "name": "5_tactile_contact_Tactile_sensing",
        "keywords": [
          [
            "tactile",
            0.06741774100936224
          ],
          [
            "contact",
            0.024781539214116403
          ],
          [
            "Tactile",
            0.02467509674423563
          ],
          [
            "sensing",
            0.022827667553667923
          ],
          [
            "sensor",
            0.022221918177330626
          ],
          [
            "sensors",
            0.01936981476595972
          ],
          [
            "object",
            0.018750387202948138
          ],
          [
            "force",
            0.016277470824598708
          ],
          [
            "manipulation",
            0.016249573145800347
          ],
          [
            "objects",
            0.015968188389934277
          ]
        ],
        "count": 750
      },
      "6": {
        "name": "6_surgical_tissue_surgery_robotic",
        "keywords": [
          [
            "surgical",
            0.04378297368422672
          ],
          [
            "tissue",
            0.01831691304293256
          ],
          [
            "surgery",
            0.01830714783257315
          ],
          [
            "robotic",
            0.016530663973562842
          ],
          [
            "Surgical",
            0.012862733370288912
          ],
          [
            "needle",
            0.012163765140282858
          ],
          [
            "ultrasound",
            0.010429335468269857
          ],
          [
            "Robotic",
            0.00963574641335353
          ],
          [
            "robot",
            0.00877224617310109
          ],
          [
            "Surgery",
            0.008611465790987757
          ]
        ],
        "count": 703
      },
      "7": {
        "name": "7_soft_Soft_robots_design",
        "keywords": [
          [
            "soft",
            0.05397792691429415
          ],
          [
            "Soft",
            0.021921420549127858
          ],
          [
            "robots",
            0.02122442506721665
          ],
          [
            "design",
            0.020044221988827736
          ],
          [
            "soft robots",
            0.018128185113067307
          ],
          [
            "control",
            0.01638388192240233
          ],
          [
            "actuators",
            0.014489569993139854
          ],
          [
            "gripper",
            0.013166855518787336
          ],
          [
            "robot",
            0.012756963635028583
          ],
          [
            "robotic",
            0.012463849815956558
          ]
        ],
        "count": 689
      },
      "8": {
        "name": "8_depth_pose_estimation_3D",
        "keywords": [
          [
            "depth",
            0.03396233659489551
          ],
          [
            "pose",
            0.025347488612610013
          ],
          [
            "estimation",
            0.022897920095979216
          ],
          [
            "3D",
            0.021121248356366067
          ],
          [
            "object",
            0.018684926550999695
          ],
          [
            "pose estimation",
            0.014863769090917443
          ],
          [
            "images",
            0.014007170942101863
          ],
          [
            "6D",
            0.013976290687496035
          ],
          [
            "reconstruction",
            0.012989764257620972
          ],
          [
            "depth estimation",
            0.01210430336203632
          ]
        ],
        "count": 607
      },
      "9": {
        "name": "9_control_safety_Control_systems",
        "keywords": [
          [
            "control",
            0.02906062612544128
          ],
          [
            "safety",
            0.022637123759505065
          ],
          [
            "Control",
            0.01910598613983304
          ],
          [
            "systems",
            0.015158077748436364
          ],
          [
            "constraints",
            0.014271471742289007
          ],
          [
            "CBF",
            0.013985025838850602
          ],
          [
            "nonlinear",
            0.01267677229264896
          ],
          [
            "MPC",
            0.012621207111040384
          ],
          [
            "safe",
            0.012257840998201899
          ],
          [
            "time",
            0.01197863392829587
          ]
        ],
        "count": 573
      },
      "10": {
        "name": "10_path_planning_MAPF_algorithm",
        "keywords": [
          [
            "path",
            0.026146269417656957
          ],
          [
            "planning",
            0.022091563418202752
          ],
          [
            "MAPF",
            0.022061760749387656
          ],
          [
            "algorithm",
            0.020219754266704488
          ],
          [
            "problem",
            0.01691237297570326
          ],
          [
            "Path",
            0.01598935585018978
          ],
          [
            "paths",
            0.015394584169679048
          ],
          [
            "robots",
            0.01503274795312298
          ],
          [
            "time",
            0.014239453188065245
          ],
          [
            "Multi",
            0.014111987171504081
          ]
        ],
        "count": 494
      },
      "11": {
        "name": "11_aerial_flight_control_Aerial",
        "keywords": [
          [
            "aerial",
            0.027336930315664382
          ],
          [
            "flight",
            0.024358757268672256
          ],
          [
            "control",
            0.021731038765995837
          ],
          [
            "Aerial",
            0.014374281186880824
          ],
          [
            "wing",
            0.013117084169648435
          ],
          [
            "controller",
            0.012848075481357963
          ],
          [
            "design",
            0.012595092546353175
          ],
          [
            "proposed",
            0.012173213183521879
          ],
          [
            "UAV",
            0.011890558273332219
          ],
          [
            "trajectory",
            0.011616244747934594
          ]
        ],
        "count": 493
      },
      "12": {
        "name": "12_locomotion_learning_policy_terrains",
        "keywords": [
          [
            "locomotion",
            0.036007359928814764
          ],
          [
            "learning",
            0.020401308069164167
          ],
          [
            "policy",
            0.01970590141896029
          ],
          [
            "terrains",
            0.01784440680364495
          ],
          [
            "robot",
            0.017623877794180457
          ],
          [
            "robots",
            0.01633331333214096
          ],
          [
            "Locomotion",
            0.015332977279667023
          ],
          [
            "legged",
            0.014697719595182363
          ],
          [
            "Learning",
            0.014489181898414649
          ],
          [
            "real",
            0.012864250132503445
          ]
        ],
        "count": 445
      },
      "13": {
        "name": "13_exoskeleton_gait_control_human",
        "keywords": [
          [
            "exoskeleton",
            0.024092406924415638
          ],
          [
            "gait",
            0.01837149017438618
          ],
          [
            "control",
            0.017920540938122474
          ],
          [
            "human",
            0.01723528995389382
          ],
          [
            "limb",
            0.01706037802374011
          ],
          [
            "muscle",
            0.01567788863671121
          ],
          [
            "rehabilitation",
            0.014483176219852798
          ],
          [
            "joint",
            0.01428900256735084
          ],
          [
            "hand",
            0.013726530554196748
          ],
          [
            "exoskeletons",
            0.013328941419951914
          ]
        ],
        "count": 429
      },
      "14": {
        "name": "14_agricultural_fruit_harvesting_crop",
        "keywords": [
          [
            "agricultural",
            0.023917502296359187
          ],
          [
            "fruit",
            0.021930559896904036
          ],
          [
            "harvesting",
            0.020618516279976976
          ],
          [
            "crop",
            0.020210317092425895
          ],
          [
            "plant",
            0.01687781534088215
          ],
          [
            "field",
            0.013186769368918383
          ],
          [
            "agriculture",
            0.012767621912217994
          ],
          [
            "robotic",
            0.011755219016817465
          ],
          [
            "plants",
            0.010893804487581368
          ],
          [
            "data",
            0.01009844599122543
          ]
        ],
        "count": 389
      },
      "15": {
        "name": "15_learning_control_reinforcement_reinforcement learning",
        "keywords": [
          [
            "learning",
            0.019871051836407413
          ],
          [
            "control",
            0.018308862133403013
          ],
          [
            "reinforcement",
            0.016590373712971847
          ],
          [
            "reinforcement learning",
            0.0163685706125139
          ],
          [
            "Reinforcement",
            0.016191342396327888
          ],
          [
            "Learning",
            0.015880688028267736
          ],
          [
            "RL",
            0.014465621909984017
          ],
          [
            "policy",
            0.013287527061687704
          ],
          [
            "flight",
            0.012643344140765303
          ],
          [
            "UAV",
            0.012133038967748744
          ]
        ],
        "count": 366
      },
      "16": {
        "name": "16_walking_robot_control_locomotion",
        "keywords": [
          [
            "walking",
            0.024587765920027774
          ],
          [
            "robot",
            0.023409981769230623
          ],
          [
            "control",
            0.020171797717521207
          ],
          [
            "locomotion",
            0.01991935994424533
          ],
          [
            "bipedal",
            0.019141885912831724
          ],
          [
            "body",
            0.016907600921903434
          ],
          [
            "controller",
            0.01546543147322257
          ],
          [
            "robots",
            0.015351036436056436
          ],
          [
            "model",
            0.015083825413389054
          ],
          [
            "dynamics",
            0.014984776485293099
          ]
        ],
        "count": 303
      },
      "17": {
        "name": "17_user_robot_Reality_human",
        "keywords": [
          [
            "user",
            0.02698433849079229
          ],
          [
            "robot",
            0.025370704761722845
          ],
          [
            "Reality",
            0.021315406377100454
          ],
          [
            "human",
            0.021148881171855913
          ],
          [
            "VR",
            0.01909938953939716
          ],
          [
            "interface",
            0.01672586162118754
          ],
          [
            "AR",
            0.016303736534877836
          ],
          [
            "control",
            0.016011865439565617
          ],
          [
            "virtual",
            0.015494343253004738
          ],
          [
            "users",
            0.014834911409550188
          ]
        ],
        "count": 300
      },
      "18": {
        "name": "18_learning_navigation_agent_environments",
        "keywords": [
          [
            "learning",
            0.018660499906764265
          ],
          [
            "navigation",
            0.018580769709016315
          ],
          [
            "agent",
            0.01793145466378309
          ],
          [
            "environments",
            0.01495228066920116
          ],
          [
            "agents",
            0.014245186267948793
          ],
          [
            "reinforcement",
            0.013899314333125709
          ],
          [
            "robot",
            0.013836095735845136
          ],
          [
            "reinforcement learning",
            0.013812798467333043
          ],
          [
            "Reinforcement",
            0.0136193583114911
          ],
          [
            "Learning",
            0.013087246908933741
          ]
        ],
        "count": 235
      },
      "19": {
        "name": "19_systems_ROS_software_robotic",
        "keywords": [
          [
            "systems",
            0.02730870933385261
          ],
          [
            "ROS",
            0.018930276931101515
          ],
          [
            "software",
            0.018548925422413907
          ],
          [
            "robotic",
            0.017569378707353117
          ],
          [
            "formal",
            0.016155256045415373
          ],
          [
            "security",
            0.01600305917692259
          ],
          [
            "robotics",
            0.014715285653581399
          ],
          [
            "verification",
            0.014467515088337688
          ],
          [
            "robot",
            0.01424504144698013
          ],
          [
            "BTs",
            0.013782268663896678
          ]
        ],
        "count": 232
      },
      "20": {
        "name": "20_UAV_underwater_detection_tracking",
        "keywords": [
          [
            "UAV",
            0.019633705614839004
          ],
          [
            "underwater",
            0.01845186998597901
          ],
          [
            "detection",
            0.0163566550266202
          ],
          [
            "tracking",
            0.01216234391992723
          ],
          [
            "UAVs",
            0.011661446815366948
          ],
          [
            "data",
            0.011575291272259336
          ],
          [
            "aerial",
            0.010548773795437791
          ],
          [
            "dataset",
            0.010482370429398816
          ],
          [
            "real",
            0.010289438855820947
          ],
          [
            "images",
            0.010000545367378412
          ]
        ],
        "count": 226
      },
      "21": {
        "name": "21_social_navigation_robot_human",
        "keywords": [
          [
            "social",
            0.0402002362596499
          ],
          [
            "navigation",
            0.04016152687024904
          ],
          [
            "robot",
            0.024764197927466387
          ],
          [
            "human",
            0.02380388326297757
          ],
          [
            "crowd",
            0.02343433232509426
          ],
          [
            "Social",
            0.01912357493834579
          ],
          [
            "social navigation",
            0.018322107347193514
          ],
          [
            "Navigation",
            0.01787505913934015
          ],
          [
            "pedestrian",
            0.0144865974632641
          ],
          [
            "robots",
            0.014482346395162212
          ]
        ],
        "count": 225
      }
    },
    "correlations": [
      [
        1.0,
        -0.7172380097110146,
        -0.7492785995868655,
        -0.6929957519332268,
        -0.7225590752019154,
        -0.7392018353879442,
        -0.7504216611324149,
        -0.7265334370965715,
        -0.7043298743244536,
        -0.7226642759909511,
        -0.7261898541521705,
        -0.7336300577290302,
        -0.5757175495818391,
        -0.7431501341988768,
        -0.7540745698919903,
        -0.5435130078861834,
        -0.7107631037366104,
        -0.7069709756258479,
        -0.5664465221679051,
        -0.7307797009074972,
        -0.7443655910577833,
        -0.7080840498803767
      ],
      [
        -0.7172380097110146,
        1.0,
        -0.7372296223099998,
        -0.7419646017028695,
        -0.5953763261152477,
        -0.7559711341596432,
        -0.7615096134455707,
        -0.7469744417421662,
        -0.7321879993262559,
        -0.6912122417502087,
        -0.7125698049548435,
        -0.7266035441276558,
        -0.7359793663411673,
        -0.7484373916642166,
        -0.7603295003243262,
        -0.6985314042842394,
        -0.7421187819404769,
        -0.7459976126501997,
        -0.6945971682136663,
        -0.7240810481197115,
        -0.7424171767594434,
        -0.725036469762641
      ],
      [
        -0.7492785995868655,
        -0.7372296223099998,
        1.0,
        -0.7498411888612924,
        -0.6836829002234124,
        -0.7545104494989596,
        -0.7606228196567368,
        -0.7547495703035099,
        -0.6815906239548013,
        -0.7588959671201974,
        -0.7419965062939373,
        -0.735965912274342,
        -0.7543883212002885,
        -0.7613430420319456,
        -0.7566486026885,
        -0.7578929342120794,
        -0.7525949031360688,
        -0.7481156063624677,
        -0.7488202531211826,
        -0.7420446913596274,
        -0.7343821447341835,
        -0.7315731630020941
      ],
      [
        -0.6929957519332268,
        -0.7419646017028695,
        -0.7498411888612924,
        1.0,
        -0.7433000732873374,
        -0.7315598634687595,
        -0.7413560201866374,
        -0.49633839042342776,
        -0.7246711440582945,
        -0.7136817951610546,
        -0.7094134201018764,
        -0.7204771160002069,
        -0.5253352875652304,
        -0.6339202433210978,
        -0.7556740033561227,
        -0.7086096371366248,
        -0.3997582856233981,
        -0.11367418691082493,
        -0.7071840766821089,
        -0.7062564803743916,
        -0.7489460744283898,
        -0.33735832314730263
      ],
      [
        -0.7225590752019154,
        -0.5953763261152477,
        -0.6836829002234124,
        -0.7433000732873374,
        1.0,
        -0.7478310951245156,
        -0.7487272099859552,
        -0.7500380894322862,
        -0.6467005429347688,
        -0.7520998755295056,
        -0.7406847901208671,
        -0.7338540061358898,
        -0.7491474869187957,
        -0.7589928409788009,
        -0.7424299343190026,
        -0.7473100122599268,
        -0.7517608827283546,
        -0.7432350810810658,
        -0.743353771845108,
        -0.7434309771702206,
        -0.5686135589427043,
        -0.731502719816297
      ],
      [
        -0.7392018353879442,
        -0.7559711341596432,
        -0.7545104494989596,
        -0.7315598634687595,
        -0.7478310951245156,
        1.0,
        -0.7519796411932762,
        -0.7110493979620911,
        -0.7200313250192718,
        -0.7541100936022294,
        -0.7544933014752584,
        -0.7456494157701927,
        -0.7377458734027226,
        -0.7488405171820522,
        -0.7549060639039109,
        -0.7475501002241929,
        -0.7380013623326436,
        -0.7343936390667329,
        -0.750217667423493,
        -0.7507845203699255,
        -0.7549511529037648,
        -0.7465176201352802
      ],
      [
        -0.7504216611324149,
        -0.7615096134455707,
        -0.7606228196567368,
        -0.7413560201866374,
        -0.7487272099859552,
        -0.7519796411932762,
        1.0,
        -0.7291475256835815,
        -0.7427615014152824,
        -0.7522110891170377,
        -0.7531709444187529,
        -0.751241489897595,
        -0.7476162560999114,
        -0.7520076065259071,
        -0.7598300992283868,
        -0.7469444443782032,
        -0.745288996981831,
        -0.7378182250011603,
        -0.7508625573494987,
        -0.739496446892667,
        -0.758491309122561,
        -0.7500479771315376
      ],
      [
        -0.7265334370965715,
        -0.7469744417421662,
        -0.7547495703035099,
        -0.49633839042342776,
        -0.7500380894322862,
        -0.7110493979620911,
        -0.7291475256835815,
        1.0,
        -0.7384058187948244,
        -0.7185182511809614,
        -0.7248626964996598,
        -0.7135348261825692,
        -0.6938064436161306,
        -0.7146385161145759,
        -0.7460226031569677,
        -0.7235333707989893,
        -0.6859160337694645,
        -0.7021013113858007,
        -0.7291813839418713,
        -0.7194332732241266,
        -0.7484041176216643,
        -0.7093873817587686
      ],
      [
        -0.7043298743244536,
        -0.7321879993262559,
        -0.6815906239548013,
        -0.7246711440582945,
        -0.6467005429347688,
        -0.7200313250192718,
        -0.7427615014152824,
        -0.7384058187948244,
        1.0,
        -0.7481173374603944,
        -0.737715934119843,
        -0.7262072531407947,
        -0.7363674701184078,
        -0.7502580686573788,
        -0.7508048953374657,
        -0.7385175148599412,
        -0.7336404669309876,
        -0.7244169779161187,
        -0.7365163970865756,
        -0.7403606020417179,
        -0.7240417715823941,
        -0.7235121771735735
      ],
      [
        -0.7226642759909511,
        -0.6912122417502087,
        -0.7588959671201974,
        -0.7136817951610546,
        -0.7520998755295056,
        -0.7541100936022294,
        -0.7522110891170377,
        -0.7185182511809614,
        -0.7481173374603944,
        1.0,
        -0.7062386830645284,
        -0.620107508015306,
        -0.6923782193836319,
        -0.601440625712039,
        -0.7569215918620888,
        -0.6356719477616752,
        -0.46400277502849474,
        -0.7228605416348552,
        -0.7055061174037318,
        -0.7001506508070414,
        -0.7472128816718404,
        -0.7122730320549151
      ],
      [
        -0.7261898541521705,
        -0.7125698049548435,
        -0.7419965062939373,
        -0.7094134201018764,
        -0.7406847901208671,
        -0.7544933014752584,
        -0.7531709444187529,
        -0.7248626964996598,
        -0.737715934119843,
        -0.7062386830645284,
        1.0,
        -0.7038051723430401,
        -0.7146659590109224,
        -0.7501202179657862,
        -0.7505919152228333,
        -0.7133719901454094,
        -0.7147453475660086,
        -0.7243274819635683,
        -0.6473480264589628,
        -0.7340165226358026,
        -0.726608938586247,
        -0.7042642552037026
      ],
      [
        -0.7336300577290302,
        -0.7266035441276558,
        -0.735965912274342,
        -0.7204771160002069,
        -0.7338540061358898,
        -0.7456494157701927,
        -0.751241489897595,
        -0.7135348261825692,
        -0.7262072531407947,
        -0.620107508015306,
        -0.7038051723430401,
        1.0,
        -0.7175513805207607,
        -0.6391463389723973,
        -0.7480526381947015,
        -0.6688293486608738,
        -0.6323835454027629,
        -0.7245682518695324,
        -0.7216497059046254,
        -0.7214033560806954,
        -0.6017761803876804,
        -0.7229070421280716
      ],
      [
        -0.5757175495818391,
        -0.7359793663411673,
        -0.7543883212002885,
        -0.5253352875652304,
        -0.7491474869187957,
        -0.7377458734027226,
        -0.7476162560999114,
        -0.6938064436161306,
        -0.7363674701184078,
        -0.6923782193836319,
        -0.7146659590109224,
        -0.7175513805207607,
        1.0,
        -0.7070842463077244,
        -0.7597926395859519,
        -0.3533747757039981,
        -0.268128495530503,
        -0.5220368571529947,
        -0.4274297086880817,
        -0.7300181085014169,
        -0.7569713100350075,
        -0.5556526516929728
      ],
      [
        -0.7431501341988768,
        -0.7484373916642166,
        -0.7613430420319456,
        -0.6339202433210978,
        -0.7589928409788009,
        -0.7488405171820522,
        -0.7520076065259071,
        -0.7146385161145759,
        -0.7502580686573788,
        -0.601440625712039,
        -0.7501202179657862,
        -0.6391463389723973,
        -0.7070842463077244,
        1.0,
        -0.7561636875332781,
        -0.6543683681833206,
        -0.5345764048762842,
        -0.6127514532110533,
        -0.7373553688971725,
        -0.7456992579543931,
        -0.7543810112093794,
        -0.6693693080700696
      ],
      [
        -0.7540745698919903,
        -0.7603295003243262,
        -0.7566486026885,
        -0.7556740033561227,
        -0.7424299343190026,
        -0.7549060639039109,
        -0.7598300992283868,
        -0.7460226031569677,
        -0.7508048953374657,
        -0.7569215918620888,
        -0.7505919152228333,
        -0.7480526381947015,
        -0.7597926395859519,
        -0.7561636875332781,
        1.0,
        -0.759871818663685,
        -0.7525744811958484,
        -0.757039236259584,
        -0.7587345496416125,
        -0.7545917719139964,
        -0.7458044885830288,
        -0.7494825143448018
      ],
      [
        -0.5435130078861834,
        -0.6985314042842394,
        -0.7578929342120794,
        -0.7086096371366248,
        -0.7473100122599268,
        -0.7475501002241929,
        -0.7469444443782032,
        -0.7235333707989893,
        -0.7385175148599412,
        -0.6356719477616752,
        -0.7133719901454094,
        -0.6688293486608738,
        -0.3533747757039981,
        -0.6543683681833206,
        -0.759871818663685,
        1.0,
        -0.6259087500255559,
        -0.718915958742276,
        0.11334451443585797,
        -0.7324751020464471,
        -0.7455901195575465,
        -0.6938628780930698
      ],
      [
        -0.7107631037366104,
        -0.7421187819404769,
        -0.7525949031360688,
        -0.3997582856233981,
        -0.7517608827283546,
        -0.7380013623326436,
        -0.745288996981831,
        -0.6859160337694645,
        -0.7336404669309876,
        -0.46400277502849474,
        -0.7147453475660086,
        -0.6323835454027629,
        -0.268128495530503,
        -0.5345764048762842,
        -0.7525744811958484,
        -0.6259087500255559,
        1.0,
        -0.3719267455755616,
        -0.7018077417672748,
        -0.726181328745507,
        -0.7503454768423639,
        -0.46350086946508773
      ],
      [
        -0.7069709756258479,
        -0.7459976126501997,
        -0.7481156063624677,
        -0.11367418691082493,
        -0.7432350810810658,
        -0.7343936390667329,
        -0.7378182250011603,
        -0.7021013113858007,
        -0.7244169779161187,
        -0.7228605416348552,
        -0.7243274819635683,
        -0.7245682518695324,
        -0.5220368571529947,
        -0.6127514532110533,
        -0.757039236259584,
        -0.718915958742276,
        -0.3719267455755616,
        1.0,
        -0.7200157630547559,
        -0.7267139583407654,
        -0.7471085309364777,
        -0.40561557582773156
      ],
      [
        -0.5664465221679051,
        -0.6945971682136663,
        -0.7488202531211826,
        -0.7071840766821089,
        -0.743353771845108,
        -0.750217667423493,
        -0.7508625573494987,
        -0.7291813839418713,
        -0.7365163970865756,
        -0.7055061174037318,
        -0.6473480264589628,
        -0.7216497059046254,
        -0.4274297086880817,
        -0.7373553688971725,
        -0.7587345496416125,
        0.11334451443585797,
        -0.7018077417672748,
        -0.7200157630547559,
        1.0,
        -0.7303633392421538,
        -0.7378964158368859,
        -0.6071283522910993
      ],
      [
        -0.7307797009074972,
        -0.7240810481197115,
        -0.7420446913596274,
        -0.7062564803743916,
        -0.7434309771702206,
        -0.7507845203699255,
        -0.739496446892667,
        -0.7194332732241266,
        -0.7403606020417179,
        -0.7001506508070414,
        -0.7340165226358026,
        -0.7214033560806954,
        -0.7300181085014169,
        -0.7456992579543931,
        -0.7545917719139964,
        -0.7324751020464471,
        -0.726181328745507,
        -0.7267139583407654,
        -0.7303633392421538,
        1.0,
        -0.7460869384913593,
        -0.7308884823721691
      ],
      [
        -0.7443655910577833,
        -0.7424171767594434,
        -0.7343821447341835,
        -0.7489460744283898,
        -0.5686135589427043,
        -0.7549511529037648,
        -0.758491309122561,
        -0.7484041176216643,
        -0.7240417715823941,
        -0.7472128816718404,
        -0.726608938586247,
        -0.6017761803876804,
        -0.7569713100350075,
        -0.7543810112093794,
        -0.7458044885830288,
        -0.7455901195575465,
        -0.7503454768423639,
        -0.7471085309364777,
        -0.7378964158368859,
        -0.7460869384913593,
        1.0,
        -0.7377053597958403
      ],
      [
        -0.7080840498803767,
        -0.725036469762641,
        -0.7315731630020941,
        -0.33735832314730263,
        -0.731502719816297,
        -0.7465176201352802,
        -0.7500479771315376,
        -0.7093873817587686,
        -0.7235121771735735,
        -0.7122730320549151,
        -0.7042642552037026,
        -0.7229070421280716,
        -0.5556526516929728,
        -0.6693693080700696,
        -0.7494825143448018,
        -0.6938628780930698,
        -0.46350086946508773,
        -0.40561557582773156,
        -0.6071283522910993,
        -0.7308884823721691,
        -0.7377053597958403,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        35,
        23,
        4,
        29,
        11,
        7,
        3,
        6,
        12,
        12,
        6,
        9,
        3,
        1,
        1,
        17,
        1,
        5,
        5,
        6,
        2,
        9
      ],
      "2020-02": [
        61,
        18,
        8,
        27,
        10,
        7,
        3,
        12,
        15,
        10,
        29,
        11,
        2,
        2,
        0,
        33,
        0,
        3,
        4,
        5,
        3,
        10
      ],
      "2020-03": [
        87,
        42,
        18,
        45,
        21,
        11,
        12,
        9,
        22,
        18,
        17,
        22,
        14,
        3,
        1,
        31,
        4,
        8,
        5,
        11,
        1,
        18
      ],
      "2020-04": [
        55,
        29,
        4,
        26,
        10,
        5,
        3,
        10,
        18,
        8,
        11,
        12,
        7,
        1,
        2,
        26,
        4,
        1,
        3,
        9,
        4,
        7
      ],
      "2020-05": [
        56,
        36,
        9,
        37,
        11,
        7,
        4,
        2,
        16,
        7,
        27,
        6,
        5,
        1,
        1,
        16,
        5,
        8,
        6,
        10,
        3,
        3
      ],
      "2020-06": [
        51,
        30,
        7,
        36,
        11,
        11,
        8,
        12,
        18,
        8,
        20,
        12,
        11,
        1,
        4,
        31,
        0,
        6,
        3,
        10,
        0,
        12
      ],
      "2020-07": [
        65,
        32,
        12,
        48,
        15,
        4,
        9,
        3,
        21,
        9,
        20,
        11,
        12,
        1,
        1,
        30,
        3,
        5,
        5,
        12,
        1,
        15
      ],
      "2020-08": [
        57,
        23,
        13,
        27,
        11,
        12,
        1,
        10,
        22,
        11,
        12,
        11,
        6,
        0,
        1,
        38,
        5,
        5,
        10,
        11,
        5,
        12
      ],
      "2020-09": [
        49,
        22,
        8,
        28,
        7,
        4,
        1,
        13,
        18,
        14,
        11,
        7,
        1,
        2,
        2,
        20,
        1,
        1,
        8,
        7,
        0,
        13
      ],
      "2020-10": [
        99,
        27,
        14,
        44,
        11,
        10,
        10,
        7,
        23,
        14,
        18,
        22,
        17,
        4,
        4,
        45,
        4,
        6,
        8,
        14,
        4,
        19
      ],
      "2020-11": [
        126,
        37,
        18,
        58,
        13,
        13,
        13,
        17,
        35,
        25,
        26,
        23,
        27,
        3,
        3,
        71,
        7,
        7,
        10,
        15,
        4,
        20
      ],
      "2020-12": [
        85,
        26,
        10,
        55,
        17,
        11,
        8,
        14,
        17,
        16,
        23,
        13,
        4,
        2,
        2,
        21,
        2,
        3,
        5,
        12,
        1,
        22
      ],
      "2021-01": [
        66,
        31,
        9,
        26,
        4,
        9,
        4,
        11,
        11,
        10,
        16,
        9,
        9,
        1,
        0,
        18,
        3,
        6,
        2,
        15,
        1,
        9
      ],
      "2021-02": [
        70,
        25,
        17,
        30,
        9,
        9,
        4,
        5,
        19,
        8,
        20,
        11,
        1,
        1,
        1,
        36,
        4,
        7,
        9,
        7,
        6,
        15
      ],
      "2021-03": [
        132,
        48,
        25,
        69,
        22,
        17,
        14,
        17,
        45,
        45,
        34,
        26,
        14,
        1,
        5,
        55,
        9,
        10,
        19,
        13,
        4,
        20
      ],
      "2021-04": [
        76,
        43,
        14,
        46,
        10,
        10,
        5,
        8,
        22,
        22,
        17,
        15,
        9,
        2,
        6,
        25,
        4,
        8,
        4,
        10,
        3,
        19
      ],
      "2021-05": [
        78,
        27,
        14,
        42,
        11,
        11,
        5,
        10,
        21,
        17,
        24,
        12,
        8,
        5,
        1,
        26,
        2,
        4,
        7,
        9,
        2,
        10
      ],
      "2021-06": [
        52,
        38,
        9,
        32,
        7,
        9,
        8,
        11,
        19,
        16,
        19,
        13,
        7,
        3,
        1,
        35,
        3,
        6,
        10,
        11,
        2,
        16
      ],
      "2021-07": [
        87,
        32,
        16,
        41,
        10,
        11,
        8,
        11,
        21,
        17,
        27,
        17,
        20,
        2,
        3,
        33,
        4,
        8,
        7,
        9,
        1,
        17
      ],
      "2021-08": [
        55,
        36,
        11,
        45,
        28,
        6,
        5,
        6,
        23,
        9,
        26,
        15,
        9,
        2,
        3,
        21,
        2,
        9,
        12,
        13,
        2,
        14
      ],
      "2021-09": [
        118,
        57,
        31,
        50,
        21,
        18,
        10,
        23,
        40,
        23,
        26,
        29,
        18,
        1,
        5,
        52,
        11,
        11,
        13,
        14,
        2,
        20
      ],
      "2021-10": [
        99,
        30,
        19,
        40,
        12,
        10,
        7,
        22,
        23,
        15,
        24,
        18,
        15,
        3,
        1,
        47,
        2,
        9,
        7,
        10,
        2,
        22
      ],
      "2021-11": [
        73,
        36,
        6,
        35,
        13,
        8,
        6,
        17,
        28,
        7,
        9,
        15,
        7,
        0,
        3,
        30,
        2,
        7,
        6,
        9,
        6,
        13
      ],
      "2021-12": [
        59,
        37,
        12,
        21,
        11,
        9,
        3,
        10,
        18,
        17,
        10,
        5,
        7,
        3,
        3,
        25,
        1,
        3,
        9,
        9,
        1,
        5
      ],
      "2022-01": [
        56,
        18,
        9,
        27,
        7,
        11,
        5,
        13,
        19,
        11,
        11,
        14,
        14,
        3,
        0,
        18,
        1,
        5,
        5,
        9,
        5,
        4
      ],
      "2022-02": [
        91,
        28,
        15,
        34,
        14,
        10,
        5,
        20,
        20,
        11,
        22,
        14,
        18,
        2,
        1,
        29,
        1,
        6,
        9,
        7,
        3,
        10
      ],
      "2022-03": [
        121,
        36,
        27,
        76,
        26,
        16,
        7,
        19,
        54,
        30,
        31,
        16,
        22,
        4,
        6,
        45,
        5,
        11,
        9,
        17,
        4,
        29
      ],
      "2022-04": [
        54,
        28,
        9,
        40,
        11,
        15,
        7,
        13,
        24,
        15,
        13,
        7,
        13,
        3,
        0,
        33,
        4,
        7,
        6,
        12,
        5,
        17
      ],
      "2022-05": [
        81,
        43,
        17,
        46,
        11,
        12,
        5,
        15,
        13,
        23,
        30,
        16,
        17,
        3,
        2,
        29,
        3,
        5,
        3,
        7,
        0,
        8
      ],
      "2022-06": [
        91,
        33,
        8,
        44,
        14,
        11,
        2,
        15,
        28,
        13,
        21,
        17,
        8,
        0,
        5,
        41,
        2,
        8,
        13,
        6,
        7,
        18
      ],
      "2022-07": [
        90,
        43,
        19,
        48,
        15,
        12,
        3,
        16,
        33,
        20,
        23,
        16,
        16,
        2,
        2,
        31,
        7,
        6,
        11,
        9,
        3,
        10
      ],
      "2022-08": [
        57,
        24,
        15,
        53,
        9,
        8,
        2,
        13,
        16,
        10,
        22,
        12,
        7,
        2,
        5,
        25,
        1,
        8,
        5,
        17,
        0,
        10
      ],
      "2022-09": [
        131,
        51,
        32,
        66,
        30,
        20,
        5,
        16,
        35,
        32,
        40,
        23,
        31,
        3,
        6,
        51,
        5,
        9,
        17,
        21,
        11,
        28
      ],
      "2022-10": [
        138,
        52,
        24,
        69,
        23,
        15,
        3,
        23,
        43,
        20,
        29,
        16,
        21,
        1,
        1,
        64,
        8,
        7,
        15,
        15,
        4,
        37
      ],
      "2022-11": [
        92,
        34,
        14,
        42,
        10,
        12,
        11,
        17,
        30,
        21,
        26,
        18,
        8,
        0,
        2,
        39,
        3,
        10,
        11,
        10,
        3,
        12
      ],
      "2022-12": [
        81,
        35,
        13,
        42,
        11,
        9,
        3,
        14,
        12,
        19,
        16,
        17,
        11,
        1,
        1,
        37,
        6,
        5,
        11,
        4,
        4,
        16
      ],
      "2023-01": [
        68,
        33,
        12,
        36,
        9,
        8,
        6,
        10,
        13,
        18,
        17,
        18,
        10,
        4,
        3,
        34,
        3,
        7,
        3,
        5,
        4,
        13
      ],
      "2023-02": [
        93,
        34,
        14,
        42,
        11,
        3,
        8,
        19,
        27,
        20,
        22,
        18,
        12,
        2,
        1,
        28,
        2,
        6,
        16,
        8,
        6,
        22
      ],
      "2023-03": [
        162,
        62,
        25,
        89,
        35,
        31,
        13,
        26,
        49,
        32,
        37,
        32,
        24,
        1,
        9,
        64,
        13,
        10,
        18,
        16,
        7,
        26
      ],
      "2023-04": [
        102,
        49,
        22,
        56,
        13,
        16,
        8,
        15,
        27,
        18,
        23,
        20,
        17,
        4,
        3,
        35,
        3,
        6,
        9,
        15,
        6,
        15
      ],
      "2023-05": [
        119,
        63,
        13,
        38,
        10,
        15,
        9,
        20,
        28,
        26,
        24,
        22,
        20,
        1,
        1,
        36,
        3,
        12,
        7,
        17,
        1,
        19
      ],
      "2023-06": [
        102,
        62,
        15,
        41,
        18,
        13,
        10,
        19,
        32,
        18,
        26,
        25,
        16,
        3,
        7,
        65,
        2,
        13,
        9,
        14,
        1,
        22
      ],
      "2023-07": [
        102,
        38,
        13,
        70,
        20,
        19,
        10,
        18,
        32,
        19,
        22,
        19,
        16,
        4,
        6,
        40,
        4,
        8,
        12,
        13,
        2,
        22
      ],
      "2023-08": [
        100,
        60,
        15,
        55,
        21,
        15,
        5,
        15,
        30,
        22,
        28,
        14,
        16,
        1,
        4,
        35,
        5,
        8,
        9,
        10,
        8,
        18
      ],
      "2023-09": [
        192,
        72,
        40,
        106,
        37,
        27,
        16,
        23,
        47,
        45,
        54,
        24,
        25,
        7,
        7,
        65,
        5,
        13,
        12,
        24,
        10,
        29
      ],
      "2023-10": [
        169,
        73,
        16,
        80,
        24,
        14,
        7,
        22,
        49,
        29,
        45,
        17,
        26,
        8,
        6,
        57,
        6,
        8,
        12,
        17,
        2,
        29
      ],
      "2023-11": [
        109,
        47,
        12,
        76,
        23,
        20,
        11,
        26,
        29,
        23,
        22,
        18,
        19,
        2,
        6,
        40,
        2,
        10,
        12,
        9,
        4,
        20
      ],
      "2023-12": [
        138,
        43,
        14,
        46,
        27,
        21,
        7,
        20,
        28,
        26,
        22,
        22,
        9,
        2,
        4,
        49,
        6,
        6,
        11,
        13,
        6,
        15
      ],
      "2024-01": [
        110,
        35,
        18,
        67,
        14,
        16,
        11,
        24,
        24,
        22,
        21,
        15,
        9,
        4,
        2,
        25,
        3,
        9,
        7,
        10,
        4,
        12
      ],
      "2024-02": [
        132,
        64,
        23,
        68,
        14,
        22,
        11,
        19,
        32,
        23,
        31,
        17,
        15,
        3,
        3,
        44,
        1,
        11,
        11,
        12,
        6,
        20
      ],
      "2024-03": [
        231,
        86,
        35,
        122,
        37,
        25,
        12,
        31,
        55,
        50,
        41,
        39,
        31,
        3,
        4,
        75,
        9,
        17,
        13,
        24,
        4,
        48
      ],
      "2024-04": [
        151,
        69,
        20,
        64,
        24,
        14,
        8,
        21,
        26,
        21,
        29,
        26,
        23,
        3,
        2,
        43,
        3,
        5,
        12,
        20,
        3,
        28
      ],
      "2024-05": [
        178,
        66,
        25,
        66,
        23,
        26,
        18,
        19,
        44,
        21,
        34,
        19,
        19,
        2,
        5,
        48,
        4,
        14,
        8,
        15,
        7,
        27
      ],
      "2024-06": [
        156,
        61,
        11,
        65,
        17,
        15,
        16,
        16,
        28,
        22,
        27,
        15,
        12,
        4,
        0,
        55,
        3,
        10,
        8,
        16,
        3,
        26
      ],
      "2024-07": [
        148,
        72,
        30,
        61,
        24,
        16,
        17,
        25,
        36,
        33,
        26,
        26,
        32,
        1,
        5,
        54,
        3,
        6,
        18,
        15,
        3,
        33
      ],
      "2024-08": [
        113,
        47,
        21,
        44,
        25,
        21,
        12,
        27,
        30,
        17,
        29,
        16,
        16,
        8,
        5,
        35,
        2,
        5,
        14,
        13,
        5,
        13
      ],
      "2024-09": [
        261,
        95,
        34,
        111,
        31,
        29,
        20,
        42,
        72,
        52,
        38,
        25,
        41,
        11,
        9,
        72,
        10,
        17,
        15,
        30,
        5,
        54
      ],
      "2024-10": [
        258,
        57,
        27,
        87,
        31,
        33,
        15,
        31,
        48,
        49,
        41,
        36,
        33,
        4,
        10,
        76,
        4,
        20,
        17,
        21,
        7,
        40
      ],
      "2024-11": [
        160,
        64,
        24,
        83,
        20,
        31,
        15,
        37,
        47,
        41,
        34,
        32,
        29,
        10,
        7,
        41,
        8,
        6,
        7,
        22,
        4,
        28
      ],
      "2024-12": [
        144,
        64,
        24,
        67,
        18,
        20,
        9,
        20,
        28,
        21,
        35,
        25,
        16,
        8,
        9,
        59,
        4,
        9,
        9,
        23,
        5,
        33
      ],
      "2025-01": [
        114,
        40,
        24,
        63,
        15,
        14,
        11,
        16,
        18,
        17,
        29,
        19,
        12,
        2,
        5,
        33,
        3,
        12,
        7,
        17,
        2,
        25
      ],
      "2025-02": [
        175,
        65,
        18,
        70,
        22,
        16,
        11,
        34,
        33,
        35,
        37,
        24,
        29,
        5,
        5,
        73,
        2,
        11,
        18,
        18,
        2,
        19
      ],
      "2025-03": [
        288,
        106,
        33,
        101,
        32,
        29,
        24,
        47,
        55,
        54,
        49,
        34,
        38,
        12,
        9,
        97,
        2,
        12,
        28,
        16,
        8,
        53
      ],
      "2025-04": [
        177,
        66,
        23,
        80,
        30,
        28,
        7,
        34,
        43,
        47,
        48,
        29,
        28,
        8,
        2,
        60,
        4,
        11,
        9,
        25,
        6,
        32
      ],
      "2025-05": [
        273,
        103,
        23,
        96,
        31,
        32,
        9,
        26,
        38,
        53,
        41,
        23,
        44,
        9,
        10,
        103,
        6,
        10,
        10,
        24,
        7,
        36
      ],
      "2025-06": [
        255,
        86,
        18,
        78,
        27,
        31,
        9,
        22,
        41,
        36,
        24,
        35,
        31,
        2,
        10,
        56,
        7,
        11,
        10,
        22,
        6,
        37
      ],
      "2025-07": [
        216,
        69,
        24,
        58,
        22,
        22,
        20,
        28,
        38,
        34,
        34,
        31,
        30,
        9,
        8,
        66,
        5,
        9,
        18,
        17,
        4,
        35
      ],
      "2025-08": [
        222,
        56,
        13,
        73,
        21,
        18,
        13,
        24,
        54,
        38,
        39,
        28,
        32,
        3,
        5,
        55,
        5,
        18,
        9,
        26,
        5,
        27
      ],
      "2025-09": [
        119,
        31,
        9,
        44,
        14,
        17,
        0,
        13,
        22,
        16,
        24,
        17,
        21,
        2,
        4,
        40,
        1,
        9,
        8,
        21,
        1,
        21
      ]
    },
    "papers": {
      "0": [
        {
          "title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks",
          "year": "2021-12",
          "abstract": "General-purpose robots coexisting with humans in their environment must learn\nto relate human language to their perceptions and actions to be useful in a\nrange of daily tasks. Moreover, they need to acquire a diverse repertoire of\ngeneral-purpose skills that allow composing long-horizon tasks by following\nunconstrained language instructions. In this paper, we present CALVIN\n(Composing Actions from Language and Vision), an open-source simulated\nbenchmark to learn long-horizon language-conditioned tasks. Our aim is to make\nit possible to develop agents that can solve many robotic manipulation tasks\nover a long horizon, from onboard sensors, and specified only via human\nlanguage. CALVIN tasks are more complex in terms of sequence length, action\nspace, and language than existing vision-and-language task datasets and\nsupports flexible specification of sensor suites. We evaluate the agents in\nzero-shot to novel language instructions and to novel environments and objects.\nWe show that a baseline model based on multi-context imitation learning\nperforms poorly on CALVIN, suggesting that there is significant room for\ndeveloping innovative agents that learn to relate human language to their world\nmodels with this benchmark.",
          "arxiv_id": "2112.03227v4"
        },
        {
          "title": "Robotic Programmer: Video Instructed Policy Code Generation for Robotic Manipulation",
          "year": "2025-01",
          "abstract": "Zero-shot generalization across various robots, tasks and environments\nremains a significant challenge in robotic manipulation. Policy code generation\nmethods use executable code to connect high-level task descriptions and\nlow-level action sequences, leveraging the generalization capabilities of large\nlanguage models and atomic skill libraries. In this work, we propose Robotic\nProgrammer (RoboPro), a robotic foundation model, enabling the capability of\nperceiving visual information and following free-form instructions to perform\nrobotic manipulation with policy code in a zero-shot manner. To address low\nefficiency and high cost in collecting runtime code data for robotic tasks, we\ndevise Video2Code to synthesize executable code from extensive videos\nin-the-wild with off-the-shelf vision-language model and code-domain large\nlanguage model. Extensive experiments show that RoboPro achieves the\nstate-of-the-art zero-shot performance on robotic manipulation in both\nsimulators and real-world environments. Specifically, the zero-shot success\nrate of RoboPro on RLBench surpasses the state-of-the-art model GPT-4o by\n11.6%, which is even comparable to a strong supervised training baseline.\nFurthermore, RoboPro is robust to variations on API formats and skill sets.",
          "arxiv_id": "2501.04268v1"
        },
        {
          "title": "Robot Fine-Tuning Made Easy: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning",
          "year": "2023-10",
          "abstract": "The pre-train and fine-tune paradigm in machine learning has had dramatic\nsuccess in a wide range of domains because the use of existing data or\npre-trained models on the internet enables quick and easy learning of new\ntasks. We aim to enable this paradigm in robotic reinforcement learning,\nallowing a robot to learn a new task with little human effort by leveraging\ndata and models from the Internet. However, reinforcement learning often\nrequires significant human effort in the form of manual reward specification or\nenvironment resets, even if the policy is pre-trained. We introduce RoboFuME, a\nreset-free fine-tuning system that pre-trains a multi-task manipulation policy\nfrom diverse datasets of prior experiences and self-improves online to learn a\ntarget task with minimal human intervention. Our insights are to utilize\ncalibrated offline reinforcement learning techniques to ensure efficient online\nfine-tuning of a pre-trained policy in the presence of distribution shifts and\nleverage pre-trained vision language models (VLMs) to build a robust reward\nclassifier for autonomously providing reward signals during the online\nfine-tuning process. In a diverse set of five real robot manipulation tasks, we\nshow that our method can incorporate data from an existing robot dataset\ncollected at a different institution and improve on a target task within as\nlittle as 3 hours of autonomous real-world experience. We also demonstrate in\nsimulation experiments that our method outperforms prior works that use\ndifferent RL algorithms or different approaches for predicting rewards. Project\nwebsite: https://robofume.github.io",
          "arxiv_id": "2310.15145v1"
        }
      ],
      "1": [
        {
          "title": "Exploring the Influence of Driving Context on Lateral Driving Style Preferences: A Simulator-Based Study",
          "year": "2024-02",
          "abstract": "Technological advancements focus on developing comfortable and acceptable\ndriving characteristics in autonomous vehicles. Present driving functions\npredominantly possess predefined parameters, and there is no universally\naccepted driving style for autonomous vehicles. While driving may be\ntechnically safe and the likelihood of road accidents is reduced, passengers\nmay still feel insecure due to a mismatch in driving styles between the human\nand the autonomous system. Incorporating driving style preferences into\nautomated vehicles enhances acceptance, reduces uncertainty, and poses the\nopportunity to expedite their adoption. Despite the increased research focus on\ndriving styles, there remains a need for comprehensive studies investigating\nhow variations in the driving context impact the assessment of automated\ndriving functions. Therefore, this work evaluates lateral driving style\npreferences for autonomous vehicles on rural roads, considering different\nweather and traffic situations. A controlled study was conducted with a variety\nof German participants utilizing a high-fidelity driving simulator. The\nsubjects experienced four different driving styles, including mimicking of\ntheir own driving behavior under two weather conditions. A notable preference\nfor a more passive driving style became evident based on statistical analyses\nof participants' responses during and after the drives. This study could not\nconfirm the hypothesis that subjects prefer to be driven by mimicking their own\ndriving behavior. Furthermore, the study illustrated that weather conditions\nand oncoming traffic substantially influence the perceived comfort during\nautonomous rides. The gathered dataset is openly accessible at\nhttps://www.kaggle.com/datasets/jhaselberger/idcld-subject-study-on-driving-style-preferences.",
          "arxiv_id": "2402.14432v3"
        },
        {
          "title": "Intention-based and Risk-Aware Trajectory Prediction for Autonomous Driving in Complex Traffic Scenarios",
          "year": "2024-09",
          "abstract": "Accurately predicting the trajectory of surrounding vehicles is a critical\nchallenge for autonomous vehicles. In complex traffic scenarios, there are two\nsignificant issues with the current autonomous driving system: the cognitive\nuncertainty of prediction and the lack of risk awareness, which limit the\nfurther development of autonomous driving. To address this challenge, we\nintroduce a novel trajectory prediction model that incorporates insights and\nprinciples from driving behavior, ethical decision-making, and risk assessment.\nBased on joint prediction, our model consists of interaction, intention, and\nrisk assessment modules. The dynamic variation of interaction between vehicles\ncan be comprehensively captured at each timestamp in the interaction module.\nBased on interaction information, our model considers primary intentions for\nvehicles to enhance the diversity of trajectory generation. The optimization of\npredicted trajectories follows the advanced risk-aware decision-making\nprinciples. Experimental results are evaluated on the DeepAccident dataset; our\napproach shows its remarkable prediction performance on normal and accident\nscenarios and outperforms the state-of-the-art algorithms by at least 28.9\\%\nand 26.5\\%, respectively. The proposed model improves the proficiency and\nadaptability of trajectory prediction in complex traffic scenarios. The code\nfor the proposed model is available at\nhttps://sites.google.com/view/ir-prediction.",
          "arxiv_id": "2409.15821v1"
        },
        {
          "title": "Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving",
          "year": "2024-02",
          "abstract": "In recent years, the expansion of internet technology and advancements in\nautomation have brought significant attention to autonomous driving technology.\nMajor automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have\nprogressively introduced products ranging from assisted-driving vehicles to\nsemi-autonomous vehicles. However, this period has also witnessed several\ntraffic safety incidents involving self-driving vehicles. For instance, in\nMarch 2016, a Google self-driving car was involved in a minor collision with a\nbus. At the time of the accident, the autonomous vehicle was attempting to\nmerge into the right lane but failed to dynamically respond to the real-time\nenvironmental information during the lane change. It incorrectly assumed that\nthe approaching bus would slow down to avoid it, leading to a low-speed\ncollision with the bus. This incident highlights the current technological\nshortcomings and safety concerns associated with autonomous lane-changing\nbehavior, despite the rapid advancements in autonomous driving technology.\nLane-changing is among the most common and hazardous behaviors in highway\ndriving, significantly impacting traffic safety and flow. Therefore,\nlane-changing is crucial for traffic safety, and accurately predicting drivers'\nlane change intentions can markedly enhance driving safety. This paper\nintroduces a deep learning-based prediction method for autonomous driving lane\nchange behavior, aiming to facilitate safe lane changes and thereby improve\nroad safety.",
          "arxiv_id": "2402.16036v1"
        }
      ],
      "2": [
        {
          "title": "CTE-MLO: Continuous-time and Efficient Multi-LiDAR Odometry with Localizability-aware Point Cloud Sampling",
          "year": "2024-08",
          "abstract": "In recent years, LiDAR-based localization and mapping methods have achieved\nsignificant progress thanks to their reliable and real-time localization\ncapability. Considering single LiDAR odometry often faces hardware failures and\ndegeneracy in practical scenarios, Multi-LiDAR Odometry (MLO), as an emerging\ntechnology, is studied to enhance the performance of LiDAR-based localization\nand mapping systems. However, MLO can suffer from high computational complexity\nintroduced by dense point clouds that are fused from multiple LiDARs, and the\ncontinuous-time measurement characteristic is constantly neglected by existing\nLiDAR odometry. This motivates us to develop a Continuous-Time and Efficient\nMLO, namely CTE-MLO, which can achieve accurate and real-time estimation using\nmulti-LiDAR measurements through a continuous-time perspective. In this paper,\nthe Gaussian process estimation is naturally combined with the Kalman filter,\nwhich enables each LiDAR point in a point stream to query the corresponding\ncontinuous-time trajectory using its time instants. A decentralized multi-LiDAR\nsynchronization scheme is also devised to combine points from separate LiDARs\ninto a single point cloud without the primary LiDAR assignment. Moreover, with\nthe aim of improving the real-time performance of MLO without sacrificing\nrobustness, a point cloud sampling strategy is designed with the consideration\nof localizability. To this end, CTE-MLO integrates synchronization,\nlocalizability-aware sampling, continuous-time estimation, and voxel map\nmanagement within a Kalman filter framework, which can achieve high accuracy\nand robust continuous-time estimation within only a few linear iterations. The\neffectiveness of the proposed method is demonstrated through various scenarios,\nincluding public datasets and real-world applications. The code is available at\nhttps://github.com/shenhm516/CTE-MLO to benefit the community.",
          "arxiv_id": "2408.04901v2"
        },
        {
          "title": "Multicam-SLAM: Non-overlapping Multi-camera SLAM for Indirect Visual Localization and Navigation",
          "year": "2024-06",
          "abstract": "This paper presents a novel approach to visual simultaneous localization and\nmapping (SLAM) using multiple RGB-D cameras. The proposed method,\nMulticam-SLAM, significantly enhances the robustness and accuracy of SLAM\nsystems by capturing more comprehensive spatial information from various\nperspectives. This method enables the accurate determination of pose\nrelationships among multiple cameras without the need for overlapping fields of\nview. The proposed Muticam-SLAM includes a unique multi-camera model, a\nmulti-keyframes structure, and several parallel SLAM threads. The multi-camera\nmodel allows for the integration of data from multiple cameras, while the\nmulti-keyframes and parallel SLAM threads ensure efficient and accurate pose\nestimation and mapping. Extensive experiments in various environments\ndemonstrate the superior accuracy and robustness of the proposed method\ncompared to conventional single-camera SLAM systems. The results highlight the\npotential of the proposed Multicam-SLAM for more complex and challenging\napplications. Code is available at\n\\url{https://github.com/AlterPang/Multi_ORB_SLAM}.",
          "arxiv_id": "2406.06374v2"
        },
        {
          "title": "Versatile LiDAR-Inertial Odometry With SE (2) Constraints for Ground Vehicles",
          "year": "2023-12",
          "abstract": "LiDAR SLAM has become one of the major localization systems for ground\nvehicles since LiDAR Odometry And Mapping (LOAM). Many extension works on LOAM\nmainly leverage one specific constraint to improve the performance, e.g.,\ninformation from on-board sensors such as loop closure and inertial state;\nprior conditions such as ground level and motion dynamics. In many robotic\napplications, these conditions are often known partially, hence a SLAM system\ncan be a comprehensive problem due to the existence of numerous constraints.\nTherefore, we can achieve a better SLAM result by fusing them properly. In this\npaper, we propose a hybrid LiDAR-inertial SLAM framework that leverages both\nthe on-board perception system and prior information such as motion dynamics to\nimprove localization performance. In particular, we consider the case for\nground vehicles, which are commonly used for autonomous driving and warehouse\nlogistics. We present a computationally efficient LiDAR-inertial odometry\nmethod that directly parameterizes ground vehicle poses on SE(2). The\nout-of-SE(2) motion perturbations are not neglected but incorporated into an\nintegrated noise term of a novel SE(2)-constraints model. For odometric\nmeasurement processing, we propose a versatile, tightly coupled LiDAR-inertial\nodometry to achieve better pose estimation than traditional LiDAR odometry.\nThorough experiments are performed to evaluate our proposed method's\nperformance in different scenarios, including localization for both indoor and\noutdoor environments. The proposed method achieves superior performance in\naccuracy and robustness.",
          "arxiv_id": "2404.01584v1"
        }
      ],
      "3": [
        {
          "title": "Egocentric Robots in a Human-Centric World? Exploring Group-Robot-Interaction in Public Spaces",
          "year": "2024-07",
          "abstract": "The deployment of social robots in real-world scenarios is increasing,\nsupporting humans in various contexts. However, they still struggle to grasp\nsocial dynamics, especially in public spaces, sometimes resulting in violations\nof social norms, such as interrupting human conversations. This behavior,\noriginating from a limited processing of social norms, might be perceived as\nrobot-centered. Understanding social dynamics, particularly in\ngroup-robot-interactions (GRI), underscores the need for further research and\ndevelopment in human-robot-interaction (HRI). Enhancing the interaction\nabilities of social robots, especially in GRIs, can improve their effectiveness\nin real-world applications on a micro-level, as group interactions lead to\nincreased motivation and comfort. In this study, we assessed the influence of\nthe interaction condition (dyadic vs. triadic) on the perceived extraversion\n(ext.) of social robots in public spaces. The research involved 40 HRIs,\nincluding 24 dyadic (i.e., one human and one robot) interactions and 16 triadic\ninteractions, which involve at least three entities, including the robot.",
          "arxiv_id": "2407.18009v1"
        },
        {
          "title": "Is a humorous robot more trustworthy?",
          "year": "2023-10",
          "abstract": "As more and more social robots are being used for collaborative activities\nwith humans, it is crucial to investigate mechanisms to facilitate trust in the\nhuman-robot interaction. One such mechanism is humour: it has been shown to\nincrease creativity and productivity in human-human interaction, which has an\nindirect influence on trust. In this study, we investigate if humour can\nincrease trust in human-robot interaction. We conducted a between-subjects\nexperiment with 40 participants to see if the participants are more likely to\naccept the robot's suggestion in the Three-card Monte game, as a trust check\ntask. Though we were unable to find a significant effect of humour, we discuss\nthe effect of possible confounding variables, and also report some interesting\nqualitative observations from our study: for instance, the participants\ninteracted effectively with the robot as a team member, regardless of the\nhumour or no-humour condition.",
          "arxiv_id": "2310.15862v1"
        },
        {
          "title": "A Review on Trust in Human-Robot Interaction",
          "year": "2021-05",
          "abstract": "Due to agile developments in the field of robotics and human-robot\ninteraction, prospective robotic agents are intended to play the role of\nteammates and partner with humans to perform operations, rather than tools that\nare replacing humans helping humans in a specific task. this notion of\npartnering with robots raises new challenges for human-robot interaction (HRI),\nwhich gives rise to a new field of research in HRI, namely human-robot trust.\nWhere humans and robots are working as partners, the performance of the work\ncan be diminished if humans do not trust robots appropriately. Considering the\nimpact of human-robot trust observed in different HRI fields, many researchers\nhave investigated the field of human-robot trust and examined various concerns\nrelated to human-robot trust. In this work, we review the past works on\nhuman-robot trust based on the research topics and discuss selected trends in\nthis field. Based on these reviews, we finally propose some ideas and areas of\npotential future research at the end of this paper.",
          "arxiv_id": "2105.10045v1"
        }
      ],
      "4": [
        {
          "title": "A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI",
          "year": "2020-03",
          "abstract": "Panoptic segmentation is the recently introduced task that tackles semantic\nsegmentation and instance segmentation jointly. In this paper, we present an\nextension of SemanticKITTI, which is a large-scale dataset providing dense\npoint-wise semantic labels for all sequences of the KITTI Odometry Benchmark,\nfor training and evaluation of laser-based panoptic segmentation. We provide\nthe data and discuss the processing steps needed to enrich a given semantic\nannotation with temporally consistent instance information, i.e., instance\ninformation that supplements the semantic labels and identifies the same\ninstance over sequences of LiDAR point clouds. Additionally, we present two\nstrong baselines that combine state-of-the-art LiDAR-based semantic\nsegmentation approaches with a state-of-the-art detector enriching the\nsegmentation with instance information and that allow other researchers to\ncompare their approaches against. We hope that our extension of SemanticKITTI\nwith strong baselines enables the creation of novel algorithms for LiDAR-based\npanoptic segmentation as much as it has for the original semantic segmentation\nand semantic scene completion tasks. Data, code, and an online evaluation using\na hidden test set will be published on http://semantic-kitti.org.",
          "arxiv_id": "2003.02371v1"
        },
        {
          "title": "3D Object Detection for Autonomous Driving: A Comprehensive Survey",
          "year": "2022-06",
          "abstract": "Autonomous driving, in recent years, has been receiving increasing attention\nfor its potential to relieve drivers' burdens and improve the safety of\ndriving. In modern autonomous driving pipelines, the perception system is an\nindispensable component, aiming to accurately estimate the status of\nsurrounding environments and provide reliable observations for prediction and\nplanning. 3D object detection, which intelligently predicts the locations,\nsizes, and categories of the critical 3D objects near an autonomous vehicle, is\nan important part of a perception system. This paper reviews the advances in 3D\nobject detection for autonomous driving. First, we introduce the background of\n3D object detection and discuss the challenges in this task. Second, we conduct\na comprehensive survey of the progress in 3D object detection from the aspects\nof models and sensory inputs, including LiDAR-based, camera-based, and\nmulti-modal detection approaches. We also provide an in-depth analysis of the\npotentials and challenges in each category of methods. Additionally, we\nsystematically investigate the applications of 3D object detection in driving\nsystems. Finally, we conduct a performance analysis of the 3D object detection\napproaches, and we further summarize the research trends over the years and\nprospect the future directions of this area.",
          "arxiv_id": "2206.09474v2"
        },
        {
          "title": "R-AGNO-RPN: A LIDAR-Camera Region Deep Network for Resolution-Agnostic Detection",
          "year": "2020-12",
          "abstract": "Current neural networks-based object detection approaches processing LiDAR\npoint clouds are generally trained from one kind of LiDAR sensors. However,\ntheir performances decrease when they are tested with data coming from a\ndifferent LiDAR sensor than the one used for training, i.e., with a different\npoint cloud resolution. In this paper, R-AGNO-RPN, a region proposal network\nbuilt on fusion of 3D point clouds and RGB images is proposed for 3D object\ndetection regardless of point cloud resolution. As our approach is designed to\nbe also applied on low point cloud resolutions, the proposed method focuses on\nobject localization instead of estimating refined boxes on reduced data. The\nresilience to low-resolution point cloud is obtained through image features\naccurately mapped to Bird's Eye View and a specific data augmentation procedure\nthat improves the contribution of the RGB images. To show the proposed\nnetwork's ability to deal with different point clouds resolutions, experiments\nare conducted on both data coming from the KITTI 3D Object Detection and the\nnuScenes datasets. In addition, to assess its performances, our method is\ncompared to PointPillars, a well-known 3D detection network. Experimental\nresults show that even on point cloud data reduced by $80\\%$ of its original\npoints, our method is still able to deliver relevant proposals localization.",
          "arxiv_id": "2012.05740v1"
        }
      ],
      "5": [
        {
          "title": "Vision-based Tactile Image Generation via Contact Condition-guided Diffusion Model",
          "year": "2024-12",
          "abstract": "Vision-based tactile sensors, through high-resolution optical measurements,\ncan effectively perceive the geometric shape of objects and the force\ninformation during the contact process, thus helping robots acquire\nhigher-dimensional tactile data. Vision-based tactile sensor simulation\nsupports the acquisition and understanding of tactile information without\nphysical sensors by accurately capturing and analyzing contact behavior and\nphysical properties. However, the complexity of contact dynamics and lighting\nmodeling limits the accurate reproduction of real sensor responses in\nsimulations, making it difficult to meet the needs of different sensor setups\nand affecting the reliability and effectiveness of strategy transfer to\npractical applications. In this letter, we propose a contact-condition guided\ndiffusion model that maps RGB images of objects and contact force data to\nhigh-fidelity, detail-rich vision-based tactile sensor images. Evaluations show\nthat the three-channel tactile images generated by this method achieve a 60.58%\nreduction in mean squared error and a 38.1% reduction in marker displacement\nerror compared to existing approaches based on lighting model and mechanical\nmodel, validating the effectiveness of our approach. The method is successfully\napplied to various types of tactile vision sensors and can effectively generate\ncorresponding tactile images under complex loads. Additionally, it demonstrates\noutstanding reconstruction of fine texture features of objects in a Montessori\ntactile board texture generation task.",
          "arxiv_id": "2412.01639v1"
        },
        {
          "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing",
          "year": "2025-08",
          "abstract": "Recent vision-language-action (VLA) models build upon vision-language\nfoundations, and have achieved promising results and exhibit the possibility of\ntask generalization in robot manipulation. However, due to the heterogeneity of\ntactile sensors and the difficulty of acquiring tactile data, current VLA\nmodels significantly overlook the importance of tactile perception and fail in\ncontact-rich tasks. To address this issue, this paper proposes OmniVTLA, a\nnovel architecture involving tactile sensing. Specifically, our contributions\nare threefold. First, our OmniVTLA features a dual-path tactile encoder\nframework. This framework enhances tactile perception across diverse\nvision-based and force-based tactile sensors by using a pretrained vision\ntransformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we\nintroduce ObjTac, a comprehensive force-based tactile dataset capturing\ntextual, visual, and tactile information for 56 objects across 10 categories.\nWith 135K tri-modal samples, ObjTac supplements existing visuo-tactile\ndatasets. Third, leveraging this dataset, we train a semantically-aligned\ntactile encoder to learn a unified tactile representation, serving as a better\ninitialization for OmniVTLA. Real-world experiments demonstrate substantial\nimprovements over state-of-the-art VLA baselines, achieving 96.9% success rates\nwith grippers, (21.9% higher over baseline) and 100% success rates with\ndexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides,\nOmniVTLA significantly reduces task completion time and generates smoother\ntrajectories through tactile sensing compared to existing VLA. Our ObjTac\ndataset can be found at https://readerek.github.io/Objtac.github.io",
          "arxiv_id": "2508.08706v2"
        },
        {
          "title": "Robotic Perception of Object Properties using Tactile Sensing",
          "year": "2021-12",
          "abstract": "The sense of touch plays a key role in enabling humans to understand and\ninteract with surrounding environments. For robots, tactile sensing is also\nirreplaceable. While interacting with objects, tactile sensing provides useful\ninformation for the robot to understand the object, such as distributed\npressure, temperature, vibrations and texture. During robot grasping, vision is\noften occluded by its end-effectors, whereas tactile sensing can measure areas\nthat are not accessible by vision. In the past decades, a number of tactile\nsensors have been developed for robots and used for different robotic tasks. In\nthis chapter, we focus on the use of tactile sensing for robotic grasping and\ninvestigate the recent trends in tactile perception of object properties. We\nfirst discuss works on tactile perception of three important object properties\nin grasping, i.e., shape, pose and material properties. We then review the\nrecent development in grasping stability prediction with tactile sensing. Among\nthese works, we identify the requirement for coordinating vision and tactile\nsensing in the robotic grasping. To demonstrate the use of tactile sensing to\nimprove the visual perception, our recent development of vision-guided tactile\nperception for crack reconstruction is presented. In the proposed framework,\nthe large receptive field of camera vision is first leveraged to achieve a\nquick search of candidate regions containing cracks, a high-resolution optical\ntactile sensor is then used to examine these candidate regions and reconstruct\na refined crack shape. The experiments show that our proposed method can\nachieve a significant reduction of mean distance error from 0.82 mm to 0.24 mm\nfor crack reconstruction. Finally, we conclude this chapter with a discussion\nof open issues and future directions for applying tactile sensing in robotic\ntasks.",
          "arxiv_id": "2112.14119v1"
        }
      ],
      "6": [
        {
          "title": "SurgRIPE challenge: Benchmark of Surgical Robot Instrument Pose Estimation",
          "year": "2025-01",
          "abstract": "Accurate instrument pose estimation is a crucial step towards the future of\nrobotic surgery, enabling applications such as autonomous surgical task\nexecution. Vision-based methods for surgical instrument pose estimation provide\na practical approach to tool tracking, but they often require markers to be\nattached to the instruments. Recently, more research has focused on the\ndevelopment of marker-less methods based on deep learning. However, acquiring\nrealistic surgical data, with ground truth instrument poses, required for deep\nlearning training, is challenging. To address the issues in surgical instrument\npose estimation, we introduce the Surgical Robot Instrument Pose Estimation\n(SurgRIPE) challenge, hosted at the 26th International Conference on Medical\nImage Computing and Computer-Assisted Intervention (MICCAI) in 2023. The\nobjectives of this challenge are: (1) to provide the surgical vision community\nwith realistic surgical video data paired with ground truth instrument poses,\nand (2) to establish a benchmark for evaluating markerless pose estimation\nmethods. The challenge led to the development of several novel algorithms that\nshowcased improved accuracy and robustness over existing methods. The\nperformance evaluation study on the SurgRIPE dataset highlights the potential\nof these advanced algorithms to be integrated into robotic surgery systems,\npaving the way for more precise and autonomous surgical procedures. The\nSurgRIPE challenge has successfully established a new benchmark for the field,\nencouraging further research and development in surgical robot instrument pose\nestimation.",
          "arxiv_id": "2501.02990v2"
        },
        {
          "title": "Gaze-based Learning from Demonstration In Surgical Robotics",
          "year": "2023-11",
          "abstract": "Surgical robotics is a rising field in medical technology and advanced\nrobotics. Robot assisted surgery, or robotic surgery, allows surgeons to\nperform complicated surgical tasks with more precision, automation, and\nflexibility than is possible for traditional surgical approaches. The main type\nof robot assisted surgery is minimally invasive surgery, which could be\nautomated and result in a faster healing time for the patient. The surgical\nrobot we are particularly interested in is the da Vinci surgical system, which\nis developed and manufactured by Intuitive Surgical. In the current iteration\nof the system, the endoscopic camera arm on the da Vinci robot has to be\nmanually controlled and calibrated by the surgeon during a surgical task, which\ninterrupts the flow of the operation. The main goal of this capstone project is\nto automate the motion of the camera arm using a probabilistic model based on\nsurgeon eye gaze data and da Vinci robot kinematic data.",
          "arxiv_id": "2311.00313v1"
        },
        {
          "title": "A Realistic Surgical Simulator for Non-Rigid and Contact-Rich Manipulation in Surgeries with the da Vinci Research Kit",
          "year": "2024-04",
          "abstract": "Realistic real-time surgical simulators play an increasingly important role\nin surgical robotics research, such as surgical robot learning and automation,\nand surgical skills assessment. Although there are a number of existing\nsurgical simulators for research, they generally lack the ability to simulate\nthe diverse types of objects and contact-rich manipulation tasks typically\npresent in surgeries, such as tissue cutting and blood suction. In this work,\nwe introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the\nda Vinci Research Kit (dVRK) that enables simulating various contact-rich\nsurgical tasks involving different surgical instruments, soft tissue, and body\nfluids. The real-world dVRK console and the master tool manipulator (MTM)\nrobots are incorporated into the system to allow for teleoperation through\nvirtual reality (VR). To showcase the advantages and potentials of the\nsimulator, we present three examples of surgical tasks, including tissue\ngrasping and deformation, blood suction, and tissue cutting. These tasks are\nperformed using the simulated surgical instruments, including the large needle\ndriver, suction irrigator, and curved scissor, through VR-based teleoperation.",
          "arxiv_id": "2404.05888v2"
        }
      ],
      "7": [
        {
          "title": "Stiffness Change for Reconfiguration of Inflated Beam Robots",
          "year": "2023-07",
          "abstract": "Active control of the shape of soft robots is challenging. Despite having an\ninfinite number of passive degrees of freedom (DOFs), soft robots typically\nonly have a few actively controllable DOFs, limited by the number of degrees of\nactuation (DOAs). The complexity of actuators restricts the number of DOAs that\ncan be incorporated into soft robots. Active shape control is further\ncomplicated by the buckling of soft robots under compressive forces; this is\nparticularly challenging for compliant continuum robots due to their long\naspect ratios. In this work, we show how variable stiffness can enable shape\ncontrol of soft robots by addressing these challenges. Dynamically changing the\nstiffness of sections along a compliant continuum robot can selectively\n\"activate\" discrete joints. By changing which joints are activated, the output\nof a single actuator can be reconfigured to actively control many different\njoints, thus decoupling the number of controllable DOFs from the number of\nDOAs. We demonstrate embedded positive pressure layer jamming as a simple\nmethod for stiffness change in inflated beam robots, its compatibility with\ngrowing robots, and its use as an \"activating\" technology. We experimentally\ncharacterize the stiffness change in a growing inflated beam robot and present\nfinite element models which serve as guides for robot design and fabrication.\nWe fabricate a multi-segment everting inflated beam robot and demonstrate how\nstiffness change is compatible with growth through tip eversion, enables an\nincrease in workspace, and achieves new actuation patterns not possible without\nstiffening.",
          "arxiv_id": "2307.03247v1"
        },
        {
          "title": "A Novel Design of Soft Robotic Hand with a Human-inspired Soft Palm for Dexterous Grasping",
          "year": "2020-09",
          "abstract": "Soft robotic hands and grippers are increasingly attracting attention as a\nrobotic end-effector. Compared with rigid counterparts, they are safer for\nhuman-robot and environment-robot interactions, easier to control, lower cost\nand weight, and more compliant. Current soft robotic hands have mostly focused\non the soft fingers and bending actuators. However, the palm is also essential\npart for grasping. In this work, we propose a novel design of soft humanoid\nhand with pneumatic soft fingers and soft palm. The hand is inexpensive to\nfabricate. The configuration of the soft palm is based on modular design which\ncan be easily applied into actuating all kinds of soft fingers before. The\nsplaying of the fingers, bending of the whole palm, abduction and adduction of\nthe thumb are implemented by the soft palm. Moreover, we present a new design\nof soft finger, called hybrid bending soft finger (HBSF). It can both bend in\nthe grasping axis and deflect in the side-to-side axis as human-like motion.\nThe functions of the HBSF and soft palm were simulated by SOFA framework. And\ntheir performance was tested in experiments. The 6 fingers with 1 to 11\nsegments were tested and analyzed. The versatility of the soft hand is\nevaluated and testified by the grasping experiments in real scenario according\nto Feix taxonomy. And the results present the diversity of grasps and show\npromise for grasping a variety of objects with different shapes and weights.",
          "arxiv_id": "2009.00979v1"
        },
        {
          "title": "Prismatic Soft Actuator Augments the Workspace of Soft Continuum Robots",
          "year": "2022-04",
          "abstract": "Soft robots are promising for manipulation tasks thanks to their compliance,\nsafety, and high degree of freedom. However, the commonly used bidirectional\ncontinuum segment design means soft robotic manipulators only function in a\nlimited hemispherical workspace. This work increases a soft robotic arm's\nworkspace by designing, fabricating, and controlling an additional soft\nprismatic actuator at the base of the soft arm. This actuator consists of\npneumatic artificial muscles and a piston, making the actuator back-driveable.\nWe increase the task space volume by 116\\%, and we are now able to perform\nmanipulation tasks that were previously impossible for soft robots, such as\npicking and placing objects at different positions on a surface and grabbing an\nobject out of a container. By combining a soft robotic arm with a prismatic\njoint, we greatly increase the usability of soft robots for object\nmanipulation. This work promotes the use of integrated and modular soft robotic\nsystems for practical manipulation applications in human-centered environments.",
          "arxiv_id": "2204.07630v2"
        }
      ],
      "8": [
        {
          "title": "Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset",
          "year": "2022-06",
          "abstract": "6D object pose estimation is one of the fundamental problems in computer\nvision and robotics research. While a lot of recent efforts have been made on\ngeneralizing pose estimation to novel object instances within the same\ncategory, namely category-level 6D pose estimation, it is still restricted in\nconstrained environments given the limited number of annotated data. In this\npaper, we collect Wild6D, a new unlabeled RGBD object video dataset with\ndiverse instances and backgrounds. We utilize this data to generalize\ncategory-level 6D object pose estimation in the wild with semi-supervised\nlearning. We propose a new model, called Rendering for Pose estimation network\nRePoNet, that is jointly trained using the free ground-truths with the\nsynthetic data, and a silhouette matching objective function on the real-world\ndata. Without using any 3D annotations on real data, our method outperforms\nstate-of-the-art methods on the previous dataset and our Wild6D test set (with\nmanual annotations for evaluation) by a large margin. Project page with Wild6D\ndata: https://oasisyang.github.io/semi-pose .",
          "arxiv_id": "2206.15436v1"
        },
        {
          "title": "Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames",
          "year": "2025-03",
          "abstract": "Estimating the 6D pose of textureless objects from RBG images is an important\nproblem in robotics. Due to appearance ambiguities, rotational symmetries, and\nsevere occlusions, single-view based 6D pose estimators are still unable to\nhandle a wide range of objects, motivating research towards multi-view pose\nestimation and next-best-view prediction that addresses these limitations. In\nthis work, we propose a comprehensive active perception framework for\nestimating the 6D poses of textureless objects using only RGB images. Our\napproach is built upon a key idea: decoupling the 6D pose estimation into a\nsequential two-step process can greatly improve both accuracy and efficiency.\nFirst, we estimate the 3D translation of each object, resolving scale and depth\nambiguities inherent to RGB images. These estimates are then used to simplify\nthe subsequent task of determining the 3D orientation, which we achieve through\ncanonical scale template matching. Building on this formulation, we then\nintroduce an active perception strategy that predicts the next best camera\nviewpoint to capture an RGB image, effectively reducing object pose uncertainty\nand enhancing pose accuracy. We evaluate our method on the public ROBI dataset\nas well as on a transparent object dataset that we created. When evaluated\nusing the same camera viewpoints, our multi-view pose estimation significantly\noutperforms state-of-the-art approaches. Furthermore, by leveraging our\nnext-best-view strategy, our method achieves high object pose accuracy with\nsubstantially fewer viewpoints than heuristic-based policies.",
          "arxiv_id": "2503.03726v1"
        },
        {
          "title": "INeRF: Inverting Neural Radiance Fields for Pose Estimation",
          "year": "2020-12",
          "abstract": "We present iNeRF, a framework that performs mesh-free pose estimation by\n\"inverting\" a Neural RadianceField (NeRF). NeRFs have been shown to be\nremarkably effective for the task of view synthesis - synthesizing\nphotorealistic novel views of real-world scenes or objects. In this work, we\ninvestigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,\nRGB-only 6DoF pose estimation - given an image, find the translation and\nrotation of a camera relative to a 3D object or scene. Our method assumes that\nno object mesh models are available during either training or test time.\nStarting from an initial pose estimate, we use gradient descent to minimize the\nresidual between pixels rendered from a NeRF and pixels in an observed image.\nIn our experiments, we first study 1) how to sample rays during pose refinement\nfor iNeRF to collect informative gradients and 2) how different batch sizes of\nrays affect iNeRF on a synthetic dataset. We then show that for complex\nreal-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating\nthe camera poses of novel images and using these images as additional training\ndata for NeRF. Finally, we show iNeRF can perform category-level object pose\nestimation, including object instances not seen during training, with RGB\nimages by inverting a NeRF model inferred from a single view.",
          "arxiv_id": "2012.05877v3"
        }
      ],
      "9": [
        {
          "title": "Enhancing Feasibility and Safety of Nonlinear Model Predictive Control with Discrete-Time Control Barrier Functions",
          "year": "2021-05",
          "abstract": "Safety is one of the fundamental problems in robotics. Recently, one-step or\nmulti-step optimal control problems for discrete-time nonlinear dynamical\nsystem were formulated to offer tracking stability using control Lyapunov\nfunctions (CLFs) while subject to input constraints as well as safety-critical\nconstraints using control barrier functions (CBFs). The limitations of these\nexisting approaches are mainly about feasibility and safety. In the existing\napproaches, the feasibility of the optimization and the system safety cannot be\nenhanced at the same time theoretically. In this paper, we propose two\nformulations that unifies CLFs and CBFs under the framework of nonlinear model\npredictive control (NMPC). In the proposed formulations, safety criteria is\ncommonly formulated as CBF constraints and stability performance is ensured\nwith either a terminal cost function or CLF constraints. Slack variables with\nrelaxing technique are introduced on the CBF constraints to resolve the\ntradeoff between feasibility and safety so that they can be enhanced at the\nsame. The advantages about feasibility and safety of proposed formulations\ncompared with existing methods are analyzed theoretically and validated with\nnumerical results.",
          "arxiv_id": "2105.10596v2"
        },
        {
          "title": "Iterative Convex Optimization for Model Predictive Control with Discrete-Time High-Order Control Barrier Functions",
          "year": "2022-10",
          "abstract": "Safety is one of the fundamental challenges in control theory. Recently,\nmulti-step optimal control problems for discrete-time dynamical systems were\nformulated to enforce stability, while subject to input constraints as well as\nsafety-critical requirements using discrete-time control barrier functions\nwithin a model predictive control (MPC) framework. Existing work usually focus\non the feasibility or the safety for the optimization problem, and the majority\nof the existing work restrict the discussions to relative-degree one control\nbarrier functions. Additionally, the real-time computation is challenging when\na large horizon is considered in the MPC problem for relative-degree one or\nhigh-order control barrier functions. In this paper, we propose a framework\nthat solves the safety-critical MPC problem in an iterative optimization, which\nis applicable for any relative-degree control barrier functions. In the\nproposed formulation, the nonlinear system dynamics as well as the safety\nconstraints modeled as discrete-time high-order control barrier functions\n(DHOCBF) are linearized at each time step. Our formulation is generally valid\nfor any control barrier function with an arbitrary relative-degree. The\nadvantages of fast computational performance with safety guarantee are analyzed\nand validated with numerical results.",
          "arxiv_id": "2210.04361v3"
        },
        {
          "title": "Safe and Performant Controller Synthesis using Gradient-based Model Predictive Control and Control Barrier Functions",
          "year": "2025-07",
          "abstract": "Ensuring both performance and safety is critical for autonomous systems\noperating in real-world environments. While safety filters such as Control\nBarrier Functions (CBFs) enforce constraints by modifying nominal controllers\nin real time, they can become overly conservative when the nominal policy lacks\nsafety awareness. Conversely, solving State-Constrained Optimal Control\nProblems (SC-OCPs) via dynamic programming offers formal guarantees but is\nintractable in high-dimensional systems. In this work, we propose a novel\ntwo-stage framework that combines gradient-based Model Predictive Control (MPC)\nwith CBF-based safety filtering for co-optimizing safety and performance. In\nthe first stage, we relax safety constraints as penalties in the cost function,\nenabling fast optimization via gradient-based methods. This step improves\nscalability and avoids feasibility issues associated with hard constraints. In\nthe second stage, we modify the resulting controller using a CBF-based\nQuadratic Program (CBF-QP), which enforces hard safety constraints with minimal\ndeviation from the reference. Our approach yields controllers that are both\nperformant and provably safe. We validate the proposed framework on two case\nstudies, showcasing its ability to synthesize scalable, safe, and\nhigh-performance controllers for complex, high-dimensional autonomous systems.",
          "arxiv_id": "2507.13872v1"
        }
      ],
      "10": [
        {
          "title": "Flexible and Explainable Solutions for Multi-Agent Path Finding Problems",
          "year": "2021-09",
          "abstract": "The multi-agent path finding (MAPF) problem is a combinatorial search problem\nthat aims at finding paths for multiple agents (e.g., robots) in an environment\n(e.g., an autonomous warehouse) such that no two agents collide with each\nother, and subject to some constraints on the lengths of paths. The real-world\napplications of MAPF require flexibility (e.g., solving variations of MAPF) as\nwell as explainability. In this study, both of these challenges are addressed\nand some flexible and explainable solutions for MAPF and its variants are\nintroduced.",
          "arxiv_id": "2109.08299v1"
        },
        {
          "title": "Transient Multi-Agent Path Finding for Lifelong Navigation in Dense Environments",
          "year": "2024-12",
          "abstract": "Multi-Agent Path Finding (MAPF) deals with finding conflict-free paths for a\nset of agents from an initial configuration to a given target configuration.\nThe Lifelong MAPF (LMAPF) problem is a well-studied online version of MAPF in\nwhich an agent receives a new target when it reaches its current target. The\ncommon approach for solving LMAPF is to treat it as a sequence of MAPF\nproblems, periodically replanning from the agents' current configurations to\ntheir current targets. A significant drawback in this approach is that in MAPF\nthe agents must reach a configuration in which all agents are at their targets\nsimultaneously, which is needlessly restrictive for LMAPF. Techniques have been\nproposed to indirectly mitigate this drawback. We describe cases where these\nmitigation techniques fail. As an alternative, we propose to solve LMAPF\nproblems by solving a sequence of modified MAPF problems, in which the\nobjective is for each agent to eventually visit its target, but not necessarily\nfor all agents to do so simultaneously. We refer to this MAPF variant as\nTransient MAPF (TMAPF) and propose several algorithms for solving it based on\nexisting MAPF algorithms. A limited experimental evaluation identifies some\ncases where using a TMAPF algorithm instead of a MAPF algorithm with an LMAPF\nframework can improve the system throughput significantly.",
          "arxiv_id": "2412.04256v1"
        },
        {
          "title": "CBS-Budget (CBSB): A Complete and Bounded Suboptimal Search for Multi-Agent Path Finding",
          "year": "2022-05",
          "abstract": "Multi-Agent Path Finding (MAPF) is the problem of finding a collection of\ncollision-free paths for a team of multiple agents while minimizing some global\ncost, such as the sum of the time travelled by all agents, or the time\ntravelled by the last agent. Conflict Based Search (CBS) is a leading complete\nand optimal MAPF solver which lazily explores the joint agent state space,\nusing an admissible heuristic joint plan. Such an admissible heuristic joint\nplan is computed by combining individual shortest paths found without\nconsidering inter-agent conflicts, and which becomes gradually more informed as\nconstraints are added to individual agents' path planning problems to avoid\ndiscovered conflicts. In this paper, we seek to speedup CBS by finding a more\ninformed heuristic joint plan which is bounded from above. We first propose the\nbudgeted Class-Ordered A* (bCOA*), a novel algorithm that finds the shortest\npath with minimal number of conflicts that is upper bounded in terms of length.\nThen, we propose a novel bounded-cost variant of CBS, called CBS-Budget (CBSB)\nby using a bCOA* search at the low-level search of the CBS and by using a\nmodified focal search at the high-level search of the CBS. We prove that CBSB\nis complete and bounded-suboptimal. In our numerical experiments, CBSB finds a\nnear optimal solution for hundreds of agents within a fraction of a second.\nCBSB shows state-of-the-art performance, comparable to Explicit Estimation CBS\n(EECBS), an enhanced recent version of CBS. On the other hand, CBSB is easier\nto implement than EECBS, since only two priority queues at the high-level\nsearch are needed as in Enhanced CBS (ECBS).",
          "arxiv_id": "2206.00130v1"
        }
      ],
      "11": [
        {
          "title": "Avian-Inspired High-Precision Tracking Control for Aerial Manipulators",
          "year": "2024-11",
          "abstract": "Aerial manipulators, composed of multirotors and robotic arms, have a\nstructure and function highly reminiscent of avian species. This paper studies\nthe tracking control problem for aerial manipulators. This paper studies the\ntracking control problem for aerial manipulators. We propose an avian-inspired\naerial manipulation system, which includes an avian-inspired robotic arm\ndesign, a Recursive Newton-Euler (RNE) method-based nonlinear flight\ncontroller, and a coordinated controller with two modes. Compared to existing\nmethods, our proposed approach offers several attractive features. First, the\nmorphological characteristics of avian species are used to determine the size\nproportion of the multirotor and the robotic arm in the aerial manipulator.\nSecond, the dynamic coupling of the aerial manipulator is addressed by the\nRNE-based flight controller and a dual-mode coordinated controller.\nSpecifically, under our proposed algorithm, the aerial manipulator can\nstabilize the end-effector's pose, similar to avian head stabilization. The\nproposed approach is verified through three numerical experiments. The results\nshow that even when the quadcopter is disturbed by different forces, the\nposition error of the end-effector achieves millimeter-level accuracy, and the\nattitude error remains within 1 degree. The limitation of this work is not\nconsidering aggressive manipulation like that seen in birds. Addressing this\nthrough future studies that explore real-world experiments will be a key\ndirection for research.",
          "arxiv_id": "2411.10966v1"
        },
        {
          "title": "Robust Control of An Aerial Manipulator Based on A Variable Inertia Parameters Model",
          "year": "2024-01",
          "abstract": "Aerial manipulator, which is composed of an UAV (Unmanned Aerial Vehicle) and\na multi-link manipulator and can perform aerial manipulation, has shown great\npotential of applications. However, dynamic coupling between the UAV and the\nmanipulator makes it difficult to control the aerial manipulator with high\nperformance. In this paper, system modeling and control problem of the aerial\nmanipulator are studied. Firstly, an UAV dynamic model is proposed with\nconsideration of the dynamic coupling from an attached manipulator, which is\ntreated as disturbance for the UAV. In the dynamic model, the disturbance is\naffected by the variable inertia parameters of the aerial manipulator system.\nThen, based on the proposed dynamic model, a disturbance compensation robust\n$H_{\\infty}$ controller is designed to stabilize flight of the UAV while the\nmanipulator is in operation. Finally, experiments are conducted and the\nexperimental results demonstrate the feasibility and validity of the proposed\ncontrol scheme.",
          "arxiv_id": "2401.04316v1"
        },
        {
          "title": "Lifting-wing Quadcopter Modeling and Unified Control",
          "year": "2023-01",
          "abstract": "Hybrid unmanned aerial vehicles (UAVs) integrate the efficient forward flight\nof fixed-wing and vertical takeoff and landing (VTOL) capabilities of\nmulticopter UAVs. This paper presents the modeling, control and simulation of a\nnew type of hybrid micro-small UAVs, coined as lifting-wing quadcopters. The\nairframe orientation of the lifting wing needs to tilt a specific angle often\nwithin $ 45$ degrees, neither nearly $ 90$ nor approximately $ 0$ degrees.\nCompared with some convertiplane and tail-sitter UAVs, the lifting-wing\nquadcopter has a highly reliable structure, robust wind resistance, low cruise\nspeed and reliable transition flight, making it potential to work\nfully-autonomous outdoor or some confined airspace indoor. In the modeling\npart, forces and moments generated by both lifting wing and rotors are\nconsidered. Based on the established model, a unified controller for the full\nflight phase is designed. The controller has the capability of uniformly\ntreating the hovering and forward flight, and enables a continuous transition\nbetween two modes, depending on the velocity command. What is more, by taking\nrotor thrust and aerodynamic force under consideration simultaneously, a\ncontrol allocation based on optimization is utilized to realize cooperative\ncontrol for energy saving. Finally, comprehensive Hardware-In-the-Loop (HIL)\nsimulations are performed to verify the advantages of the designed aircraft and\nthe proposed controller.",
          "arxiv_id": "2301.00730v1"
        }
      ],
      "12": [
        {
          "title": "Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains",
          "year": "2024-05",
          "abstract": "This paper addresses the challenge of terrain-adaptive dynamic locomotion in\nhumanoid robots, a problem traditionally tackled by optimization-based methods\nor reinforcement learning (RL). Optimization-based methods, such as\nmodel-predictive control, excel in finding optimal reaction forces and\nachieving agile locomotion, especially in quadruped, but struggle with the\nnonlinear hybrid dynamics of legged systems and the real-time computation of\nstep location, timing, and reaction forces. Conversely, RL-based methods show\npromise in navigating dynamic and rough terrains but are limited by their\nextensive data requirements. We introduce a novel locomotion architecture that\nintegrates a neural network policy, trained through RL in simplified\nenvironments, with a state-of-the-art motion controller combining\nmodel-predictive control (MPC) and whole-body impulse control (WBIC). The\npolicy efficiently learns high-level locomotion strategies, such as gait\nselection and step positioning, without the need for full dynamics simulations.\nThis control architecture enables humanoid robots to dynamically navigate\ndiscrete terrains, making strategic locomotion decisions (e.g., walking,\njumping, and leaping) based on ground height maps. Our results demonstrate that\nthis integrated control architecture achieves dynamic locomotion with\nsignificantly fewer training samples than conventional RL-based methods and can\nbe transferred to different humanoid platforms without additional training. The\ncontrol architecture has been extensively tested in dynamic simulations,\naccomplishing terrain height-based dynamic locomotion for three different\nrobots.",
          "arxiv_id": "2405.17227v2"
        },
        {
          "title": "Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild",
          "year": "2023-04",
          "abstract": "Recently, reinforcement learning has become a promising and polular solution\nfor robot legged locomotion. Compared to model-based control, reinforcement\nlearning based controllers can achieve better robustness against uncertainties\nof environments through sim-to-real learning. However, the corresponding\nlearned gaits are in general overly conservative and unatural. In this paper,\nwe propose a new framework for learning robust, agile and natural legged\nlocomotion skills over challenging terrain. We incorporate an adversarial\ntraining branch based on real animal locomotion data upon a teacher-student\ntraining pipeline for robust sim-to-real transfer. Empirical results on both\nsimulation and real world of a quadruped robot demonstrate that our proposed\nalgorithm enables robustly traversing challenging terrains such as stairs,\nrocky ground and slippery floor with only proprioceptive perception. Meanwhile,\nthe gaits are more agile, natural, and energy efficient compared to the\nbaselines. Both qualitative and quantitative results are presented in this\npaper.",
          "arxiv_id": "2304.10888v3"
        },
        {
          "title": "Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning",
          "year": "2025-04",
          "abstract": "For the deployment of legged robots in real-world environments, it is\nessential to develop robust locomotion control methods for challenging terrains\nthat may exhibit unexpected deformability and irregularity. In this paper, we\nexplore the application of sim-to-real deep reinforcement learning (RL) for the\ndesign of bipedal locomotion controllers for humanoid robots on compliant and\nuneven terrains. Our key contribution is to show that a simple training\ncurriculum for exposing the RL agent to randomized terrains in simulation can\nachieve robust walking on a real humanoid robot using only proprioceptive\nfeedback. We train an end-to-end bipedal locomotion policy using the proposed\napproach, and show extensive real-robot demonstration on the HRP-5P humanoid\nover several difficult terrains inside and outside the lab environment.\nFurther, we argue that the robustness of a bipedal walking policy can be\nimproved if the robot is allowed to exhibit aperiodic motion with variable\nstepping frequency. We propose a new control policy to enable modification of\nthe observed clock signal, leading to adaptive gait frequencies depending on\nthe terrain and command velocity. Through simulation experiments, we show the\neffectiveness of this policy specifically for walking over challenging terrains\nby controlling swing and stance durations. The code for training and evaluation\nis available online at https://github.com/rohanpsingh/LearningHumanoidWalking.\nDemo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q.",
          "arxiv_id": "2504.13619v1"
        }
      ],
      "13": [
        {
          "title": "Haptic Transparency and Interaction Force Control for a Lower-Limb Exoskeleton",
          "year": "2023-01",
          "abstract": "Controlling the interaction forces between a human and an exoskeleton is\ncrucial for providing transparency or adjusting assistance or resistance\nlevels. However, it is an open problem to control the interaction forces of\nlower-limb exoskeletons designed for unrestricted overground walking. For these\ntypes of exoskeletons, it is challenging to implement force/torque sensors at\nevery contact between the user and the exoskeleton for direct force\nmeasurement. Moreover, it is important to compensate for the exoskeleton's\nwhole-body gravitational and dynamical forces, especially for heavy lower-limb\nexoskeletons. Previous works either simplified the dynamic model by treating\nthe legs as independent double pendulums, or they did not close the loop with\ninteraction force feedback.\n  The proposed whole-exoskeleton closed-loop compensation (WECC) method\ncalculates the interaction torques during the complete gait cycle by using\nwhole-body dynamics and joint torque measurements on a hip-knee exoskeleton.\nFurthermore, it uses a constrained optimization scheme to track desired\ninteraction torques in a closed loop while considering physical and safety\nconstraints. We evaluated the haptic transparency and dynamic interaction\ntorque tracking of WECC control on three subjects. We also compared the\nperformance of WECC with a controller based on a simplified dynamic model and a\npassive version of the exoskeleton. The WECC controller results in a\nconsistently low absolute interaction torque error during the whole gait cycle\nfor both zero and nonzero desired interaction torques. In contrast, the\nsimplified controller yields poor performance in tracking desired interaction\ntorques during the stance phase.",
          "arxiv_id": "2301.06244v3"
        },
        {
          "title": "Ankle Exoskeletons in Walking and Load-Carrying Tasks: Insights into Biomechanics and Human-Robot Interaction",
          "year": "2025-04",
          "abstract": "Background: Lower limb exoskeletons can enhance quality of life, but\nwidespread adoption is limited by the lack of frameworks to assess their\nbiomechanical and human-robot interaction effects, which are essential for\ndeveloping adaptive and personalized control strategies. Understanding impacts\non kinematics, muscle activity, and HRI dynamics is key to achieve improved\nusability of wearable robots. Objectives: We propose a systematic methodology\nevaluate an ankle exoskeleton's effects on human movement during walking and\nload-carrying (10 kg front pack), focusing on joint kinematics, muscle\nactivity, and HRI torque signals. Materials and Methods: Using Xsens MVN\n(inertial motion capture), Delsys EMG, and a unilateral exoskeleton, three\nexperiments were conducted: (1) isolated dorsiflexion/plantarflexion; (2) gait\nanalysis (two subjects, passive/active modes); and (3) load-carrying under\nassistance. Results and Conclusions: The first experiment confirmed that the\nHRI sensor captured both voluntary and involuntary torques, providing\ndirectional torque insights. The second experiment showed that the device\nslightly restricted ankle range of motion (RoM) but supported normal gait\npatterns across all assistance modes. The exoskeleton reduced muscle activity,\nparticularly in active mode. HRI torque varied according to gait phases and\nhighlighted reduced synchronization, suggesting a need for improved support.\nThe third experiment revealed that load-carrying increased GM and TA muscle\nactivity, but the device partially mitigated user effort by reducing muscle\nactivity compared to unassisted walking. HRI increased during load-carrying,\nproviding insights into user-device dynamics. These results demonstrate the\nimportance of tailoring exoskeleton evaluation methods to specific devices and\nusers, while offering a framework for future studies on exoskeleton\nbiomechanics and HRI.",
          "arxiv_id": "2504.10294v1"
        },
        {
          "title": "EXOSMOOTH: Test of Innovative EXOskeleton Control for SMOOTH Assistance, With and Without Ankle Actuation",
          "year": "2022-03",
          "abstract": "This work presents a description of the EXOSMOOTH project, oriented to the\nbenchmarking of lower limb exoskeletons performance. In the field of assisted\nwalking by powered lower limb exoskeletons, the EXOSMOOTH project proposes an\nexperiment that targets two scientific questions. The first question is related\nto the effectiveness of a novel control strategy for smooth assistance. Current\nassist strategies are based on controllers that switch the assistance level\nbased on the gait segmentation provided by a finite state machine. The proposed\nstrategy aims at managing phase transitions to provide a smoother assistance to\nthe user, thus increasing the device transparency and comfort for the user. The\nsecond question is the role of the actuation at the ankle joint in assisted\nwalking. Many novel exoskeletons devised for industrial applications do not\nfeature an actuated ankle joint. In the EXOSMOOTH project, the ankle joint\nactuation will be one experimental factor to have a direct assessment of the\nrole of an actuated joint in assisted walking. Preliminary results of 15\nhealthy subjects walking at different speeds while wearing a lower limb\nexoskeleton supported the rationale behind this question: having an actuated\nankle joint could potentially reduce the torques applied by the user by a\nmaximum value of 85 Nm. The two aforementioned questions will be investigated\nin a protocol that includes walking on a treadmill and on flat ground, with or\nwithout slope, and with a load applied on the back. In addition, the\ninteraction forces measured at the exoskeleton harnesses will be used to assess\nthe comfort of the user and the effectiveness of the control strategy to\nimprove transparency.",
          "arxiv_id": "2203.04021v1"
        }
      ],
      "14": [
        {
          "title": "AHPPEBot: Autonomous Robot for Tomato Harvesting based on Phenotyping and Pose Estimation",
          "year": "2024-05",
          "abstract": "To address the limitations inherent to conventional automated harvesting\nrobots specifically their suboptimal success rates and risk of crop damage, we\ndesign a novel bot named AHPPEBot which is capable of autonomous harvesting\nbased on crop phenotyping and pose estimation. Specifically, In phenotyping,\nthe detection, association, and maturity estimation of tomato trusses and\nindividual fruits are accomplished through a multi-task YOLOv5 model coupled\nwith a detection-based adaptive DBScan clustering algorithm. In pose\nestimation, we employ a deep learning model to predict seven semantic keypoints\non the pedicel. These keypoints assist in the robot's path planning, minimize\ntarget contact, and facilitate the use of our specialized end effector for\nharvesting. In autonomous tomato harvesting experiments conducted in commercial\ngreenhouses, our proposed robot achieved a harvesting success rate of 86.67%,\nwith an average successful harvest time of 32.46 s, showcasing its continuous\nand robust harvesting capabilities. The result underscores the potential of\nharvesting robots to bridge the labor gap in agriculture.",
          "arxiv_id": "2405.06959v1"
        },
        {
          "title": "Machine Vision-Based Crop-Load Estimation Using YOLOv8",
          "year": "2023-04",
          "abstract": "Labor shortages in fruit crop production have prompted the development of\nmechanized and automated machines as alternatives to labor-intensive orchard\noperations such as harvesting, pruning, and thinning. Agricultural robots\ncapable of identifying tree canopy parts and estimating geometric and\ntopological parameters, such as branch diameter, length, and angles, can\noptimize crop yields through automated pruning and thinning platforms. In this\nstudy, we proposed a machine vision system to estimate canopy parameters in\napple orchards and determine an optimal number of fruit for individual\nbranches, providing a foundation for robotic pruning, flower thinning, and\nfruitlet thinning to achieve desired yield and quality.Using color and depth\ninformation from an RGB-D sensor (Microsoft Azure Kinect DK), a YOLOv8-based\ninstance segmentation technique was developed to identify trunks and branches\nof apple trees during the dormant season. Principal Component Analysis was\napplied to estimate branch diameter (used to calculate limb cross-sectional\narea, or LCSA) and orientation. The estimated branch diameter was utilized to\ncalculate LCSA, which served as an input for crop-load estimation, with larger\nLCSA values indicating a higher potential fruit-bearing capacity.RMSE for\nbranch diameter estimation was 2.08 mm, and for crop-load estimation, 3.95.\nBased on commercial apple orchard management practices, the target crop-load\n(number of fruit) for each segmented branch was estimated with a mean absolute\nerror (MAE) of 2.99 (ground truth crop-load was 6 apples per LCSA). This study\ndemonstrated a promising workflow with high performance in identifying trunks\nand branches of apple trees in dynamic commercial orchard environments and\nintegrating farm management practices into automated decision-making.",
          "arxiv_id": "2304.13282v1"
        },
        {
          "title": "Vision based Crop Row Navigation under Varying Field Conditions in Arable Fields",
          "year": "2022-09",
          "abstract": "Accurate crop row detection is often challenged by the varying field\nconditions present in real-world arable fields. Traditional colour based\nsegmentation is unable to cater for all such variations. The lack of\ncomprehensive datasets in agricultural environments limits the researchers from\ndeveloping robust segmentation models to detect crop rows. We present a dataset\nfor crop row detection with 11 field variations from Sugar Beet and Maize\ncrops. We also present a novel crop row detection algorithm for visual servoing\nin crop row fields. Our algorithm can detect crop rows against varying field\nconditions such as curved crop rows, weed presence, discontinuities, growth\nstages, tramlines, shadows and light levels. Our method only uses RGB images\nfrom a front-mounted camera on a Husky robot to predict crop rows. Our method\noutperformed the classic colour based crop row detection baseline. Dense weed\npresence within inter-row space and discontinuities in crop rows were the most\nchallenging field conditions for our crop row detection algorithm. Our method\ncan detect the end of the crop row and navigate the robot towards the headland\narea when it reaches the end of the crop row.",
          "arxiv_id": "2209.14003v1"
        }
      ],
      "15": [
        {
          "title": "AirPilot: Interpretable PPO-based DRL Auto-Tuned Nonlinear PID Drone Controller for Robust Autonomous Flights",
          "year": "2024-03",
          "abstract": "Navigation precision, speed and stability are crucial for safe Unmanned\nAerial Vehicle (UAV) flight maneuvers and effective flight mission executions\nin dynamic environments. Different flight missions may have varying objectives,\nsuch as minimizing energy consumption, achieving precise positioning, or\nmaximizing speed. A controller that can adapt to different objectives on the\nfly is highly valuable. Proportional Integral Derivative (PID) controllers are\none of the most popular and widely used control algorithms for drones and other\ncontrol systems, but their linear control algorithm fails to capture the\nnonlinear nature of the dynamic wind conditions and complex drone system.\nManually tuning the PID gains for various missions can be time-consuming and\nrequires significant expertise. This paper aims to revolutionize drone flight\ncontrol by presenting the AirPilot, a nonlinear Deep Reinforcement Learning\n(DRL) - enhanced Proportional Integral Derivative (PID) drone controller using\nProximal Policy Optimization (PPO). AirPilot controller combines the simplicity\nand effectiveness of traditional PID control with the adaptability, learning\ncapability, and optimization potential of DRL. This makes it better suited for\nmodern drone applications where the environment is dynamic, and\nmission-specific performance demands are high. We employed a COEX Clover\nautonomous drone for training the DRL agent within the simulator and\nimplemented it in a real-world lab setting, which marks a significant milestone\nas one of the first attempts to apply a DRL-based flight controller on an\nactual drone. Airpilot is capable of reducing the navigation error of the\ndefault PX4 PID position controller by 90%, improving effective navigation\nspeed of a fine-tuned PID controller by 21%, reducing settling time and\novershoot by 17% and 16% respectively.",
          "arxiv_id": "2404.00204v5"
        },
        {
          "title": "Model-Reference Reinforcement Learning for Collision-Free Tracking Control of Autonomous Surface Vehicles",
          "year": "2020-08",
          "abstract": "This paper presents a novel model-reference reinforcement learning algorithm\nfor the intelligent tracking control of uncertain autonomous surface vehicles\nwith collision avoidance. The proposed control algorithm combines a\nconventional control method with reinforcement learning to enhance control\naccuracy and intelligence. In the proposed control design, a nominal system is\nconsidered for the design of a baseline tracking controller using a\nconventional control approach. The nominal system also defines the desired\nbehaviour of uncertain autonomous surface vehicles in an obstacle-free\nenvironment. Thanks to reinforcement learning, the overall tracking controller\nis capable of compensating for model uncertainties and achieving collision\navoidance at the same time in environments with obstacles. In comparison to\ntraditional deep reinforcement learning methods, our proposed learning-based\ncontrol can provide stability guarantees and better sample efficiency. We\ndemonstrate the performance of the new algorithm using an example of autonomous\nsurface vehicles.",
          "arxiv_id": "2008.07240v1"
        },
        {
          "title": "Reinforcement Learning-Based Control of CrazyFlie 2.X Quadrotor",
          "year": "2023-06",
          "abstract": "The objective of the project is to explore synergies between classical\ncontrol algorithms such as PID and contemporary reinforcement learning\nalgorithms to come up with a pragmatic control mechanism to control the\nCrazyFlie 2.X quadrotor. The primary objective would be performing PID tuning\nusing reinforcement learning strategies. The secondary objective is to leverage\nthe learnings from the first task to implement control for navigation by\nintegrating with the lighthouse positioning system. Two approaches are\nconsidered for navigation, a discrete navigation problem using Deep Q-Learning\nwith finite predefined motion primitives, and deep reinforcement learning for a\ncontinuous navigation approach. Simulations for RL training will be performed\non gym-pybullet-drones, an open-source gym-based environment for reinforcement\nlearning, and the RL implementations are provided by stable-baselines3",
          "arxiv_id": "2306.03951v2"
        }
      ],
      "16": [
        {
          "title": "Robust Push Recovery on Bipedal Robots: Leveraging Multi-Domain Hybrid Systems with Reduced-Order Model Predictive Control",
          "year": "2025-04",
          "abstract": "In this paper, we present a novel control framework to achieve robust push\nrecovery on bipedal robots while locomoting. The key contribution is the\nunification of hybrid system models of locomotion with a reduced-order model\npredictive controller determining: foot placement, step timing, and ankle\ncontrol. The proposed reduced-order model is an augmented Linear Inverted\nPendulum model with zero moment point coordinates; this is integrated within a\nmodel predictive control framework for robust stabilization under external\ndisturbances. By explicitly leveraging the hybrid dynamics of locomotion, our\napproach significantly improves stability and robustness across varying walking\nheights, speeds, step durations, and is effective for both flat-footed and more\ncomplex multi-domain heel-to-toe walking patterns. The framework is validated\nwith high-fidelity simulation on Cassie, a 3D underactuated robot, showcasing\nreal-time feasibility and substantially improved stability. The results\ndemonstrate the robustness of the proposed method in dynamic environments.",
          "arxiv_id": "2504.18698v1"
        },
        {
          "title": "Time-Varying ALIP Model and Robust Foot-Placement Control for Underactuated Bipedal Robot Walking on a Swaying Rigid Surface",
          "year": "2022-10",
          "abstract": "Controller design for bipedal walking on dynamic rigid surfaces (DRSes),\nwhich are rigid surfaces moving in the inertial frame (e.g., ships and\nairplanes), remains largely uninvestigated. This paper introduces a\nhierarchical control approach that achieves stable underactuated bipedal robot\nwalking on a horizontally oscillating DRS. The highest layer of our approach is\na real-time motion planner that generates desired global behaviors (i.e., the\ncenter of mass trajectories and footstep locations) by stabilizing a\nreduced-order robot model. One key novelty of this layer is the derivation of\nthe reduced-order model by analytically extending the angular momentum based\nlinear inverted pendulum (ALIP) model from stationary to horizontally moving\nsurfaces. The other novelty is the development of a discrete-time\nfoot-placement controller that exponentially stabilizes the hybrid, linear,\ntime-varying ALIP model. The middle layer of the proposed approach is a walking\npattern generator that translates the desired global behaviors into the robot's\nfull-body reference trajectories for all directly actuated degrees of freedom.\nThe lowest layer is an input-output linearizing controller that exponentially\ntracks those full-body reference trajectories based on the full-order, hybrid,\nnonlinear robot dynamics. Simulations of planar underactuated bipedal walking\non a swaying DRS confirm that the proposed framework ensures the walking\nstability under different DRS motions and gait types.",
          "arxiv_id": "2210.13371v2"
        },
        {
          "title": "Terrain-Adaptive, ALIP-Based Bipedal Locomotion Controller via Model Predictive Control and Virtual Constraints",
          "year": "2021-09",
          "abstract": "This paper presents a gait controller for bipedal robots to achieve highly\nagile walking over various terrains given local slope and friction cone\ninformation. Without these considerations, untimely impacts can cause a robot\nto trip and inadequate tangential reaction forces at the stance foot can cause\nslippages. We address these challenges by combining, in a novel manner, a model\nbased on an Angular Momentum Linear Inverted Pendulum (ALIP) and a Model\nPredictive Control (MPC) foot placement planner that is executed by the method\nof virtual constraints. The process starts with abstracting from the full\ndynamics of a Cassie 3D bipedal robot, an exact low-dimensional representation\nof its center of mass dynamics, parameterized by angular momentum. Under a\npiecewise planar terrain assumption and the elimination of terms for the\nangular momentum about the robot's center of mass, the centroidal dynamics\nabout the contact point become linear and have dimension four. Importantly, we\ninclude the intra-step dynamics at uniformly-spaced intervals in the MPC\nformulation so that realistic workspace constraints on the robot's evolution\ncan be imposed from step-to-step. The output of the low-dimensional MPC\ncontroller is directly implemented on a high-dimensional Cassie robot through\nthe method of virtual constraints. In experiments, we validate the performance\nof our control strategy for the robot on a variety of surfaces with varied\ninclinations and textures.",
          "arxiv_id": "2109.14862v3"
        }
      ],
      "17": [
        {
          "title": "HoloSpot: Intuitive Object Manipulation via Mixed Reality Drag-and-Drop",
          "year": "2024-10",
          "abstract": "Human-robot interaction through mixed reality (MR) technologies enables\nnovel, intuitive interfaces to control robots in remote operations. Such\ninterfaces facilitate operations in hazardous environments, where human\npresence is risky, yet human oversight remains crucial. Potential environments\ninclude disaster response scenarios and areas with high radiation or toxic\nchemicals. In this paper we present an interface system projecting a 3D\nrepresentation of a scanned room as a scaled-down 'dollhouse' hologram,\nallowing users to select and manipulate objects using a straightforward\ndrag-and-drop interface. We then translate these drag-and-drop user commands\ninto real-time robot actions based on the recent Spot-Compose framework. The\nUnity-based application provides an interactive tutorial and a user-friendly\nexperience, ensuring ease of use. Through comprehensive end-to-end testing, we\nvalidate the system's capability in executing pick-and-place tasks and a\ncomplementary user study affirms the interface's intuitive controls. Our\nfindings highlight the advantages of this interface in improving user\nexperience and operational efficiency. This work lays the groundwork for a\nrobust framework that advances the potential for seamless human-robot\ncollaboration in diverse applications. Paper website:\nhttps://holospot.github.io/",
          "arxiv_id": "2410.11110v1"
        },
        {
          "title": "Multimodal \"Puppeteer\": An Exploration of Robot Teleoperation Via Virtual Counterpart with LLM-Driven Voice and Gesture Interaction in Augmented Reality",
          "year": "2025-06",
          "abstract": "The integration of robotics and augmented reality (AR) holds transformative\npotential for advancing human-robot interaction (HRI), offering enhancements in\nusability, intuitiveness, accessibility, and collaborative task performance.\nThis paper introduces and evaluates a novel multimodal AR-based robot puppeteer\nframework that enables intuitive teleoperation via virtual counterpart through\nlarge language model (LLM)-driven voice commands and hand gesture interactions.\nUtilizing the Meta Quest 3, users interact with a virtual counterpart robot in\nreal-time, effectively \"puppeteering\" its physical counterpart within an AR\nenvironment. We conducted a within-subject user study with 42 participants\nperforming robotic cube pick-and-place with pattern matching tasks under two\nconditions: gesture-only interaction and combined voice-and-gesture\ninteraction. Both objective performance metrics and subjective user experience\n(UX) measures were assessed, including an extended comparative analysis between\nroboticists and non-roboticists. The results provide key insights into how\nmultimodal input influences contextual task efficiency, usability, and user\nsatisfaction in AR-based HRI. Our findings offer practical design implications\nfor designing effective AR-enhanced HRI systems.",
          "arxiv_id": "2506.13189v1"
        },
        {
          "title": "Design and Evaluation of an Augmented Reality Head-Mounted Display Interface for Human Robot Teams Collaborating in Physically Shared Manufacturing Tasks",
          "year": "2022-03",
          "abstract": "We provide an experimental evaluation of a wearable augmented reality (AR)\nsystem we have developed for human-robot teams working on tasks requiring\ncollaboration in shared physical workspace. Recent advances in AR technology\nhave facilitated the development of more intuitive user interfaces for many\nhuman-robot interaction applications. While it has been anticipated that AR can\nprovided a more intuitive interface to robot assistants helping human workers\nin various manufacturing scenarios, existing studies in robotics have been\nlargely limited to teleoperation and programming. Industry 5.0 envisions\ncooperation between human and robot working in teams. Indeed, there exist many\nindustrial task that can benefit from human-robot collaboration. A prime\nexample is high-value composite manufacturing. Working with our industry\npartner towards this example application, we evaluated our AR interface design\nfor shared physical workspace collaboration in human-robot teams. We conducted\na multi-dimensional analysis of our interface using establish metrics. Results\nfrom our user study (n=26) show that subjectively, the AR interface feels more\nnovel and a standard joystick interface feels more dependable to users.\nHowever, the AR interface was found to reduce physical demand and task\ncompletion time, while increasing robot utilization. Furthermore, user's\nfreedom of choice to collaborate with the robot may also affect the perceived\nusability of the system.",
          "arxiv_id": "2203.08343v1"
        }
      ],
      "18": [
        {
          "title": "SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially Observable Multi-Agent Path Finding",
          "year": "2023-07",
          "abstract": "Multi-Agent Path Finding (MAPF) is a crucial component for many large-scale\nrobotic systems, where agents must plan their collision-free paths to their\ngiven goal positions. Recently, multi-agent reinforcement learning has been\nintroduced to solve the partially observable variant of MAPF by learning a\ndecentralized single-agent policy in a centralized fashion based on each\nagent's partial observation. However, existing learning-based methods are\nineffective in achieving complex multi-agent cooperation, especially in\ncongested environments, due to the non-stationarity of this setting. To tackle\nthis challenge, we propose a multi-agent actor-critic method called Soft\nActor-Critic with Heuristic-Based Attention (SACHA), which employs novel\nheuristic-based attention mechanisms for both the actors and critics to\nencourage cooperation among agents. SACHA learns a neural network for each\nagent to selectively pay attention to the shortest path heuristic guidance from\nmultiple agents within its field of view, thereby allowing for more scalable\nlearning of cooperation. SACHA also extends the existing multi-agent\nactor-critic framework by introducing a novel critic centered on each agent to\napproximate $Q$-values. Compared to existing methods that use a fully\nobservable critic, our agent-centered multi-agent actor-critic method results\nin more impartial credit assignment and better generalizability of the learned\npolicy to MAPF instances with varying numbers of agents and types of\nenvironments. We also implement SACHA(C), which embeds a communication module\nin the agent's policy network to enable information exchange among agents. We\nevaluate both SACHA and SACHA(C) on a variety of MAPF instances and demonstrate\ndecent improvements over several state-of-the-art learning-based MAPF methods\nwith respect to success rate and solution quality.",
          "arxiv_id": "2307.02691v1"
        },
        {
          "title": "Learning Graph-Enhanced Commander-Executor for Multi-Agent Navigation",
          "year": "2023-02",
          "abstract": "This paper investigates the multi-agent navigation problem, which requires\nmultiple agents to reach the target goals in a limited time. Multi-agent\nreinforcement learning (MARL) has shown promising results for solving this\nissue. However, it is inefficient for MARL to directly explore the (nearly)\noptimal policy in the large search space, which is exacerbated as the agent\nnumber increases (e.g., 10+ agents) or the environment is more complex (e.g.,\n3D simulator). Goal-conditioned hierarchical reinforcement learning (HRL)\nprovides a promising direction to tackle this challenge by introducing a\nhierarchical structure to decompose the search space, where the low-level\npolicy predicts primitive actions in the guidance of the goals derived from the\nhigh-level policy. In this paper, we propose Multi-Agent Graph-Enhanced\nCommander-Executor (MAGE-X), a graph-based goal-conditioned hierarchical method\nfor multi-agent navigation tasks. MAGE-X comprises a high-level Goal Commander\nand a low-level Action Executor. The Goal Commander predicts the probability\ndistribution of goals and leverages them to assign each agent the most\nappropriate final target. The Action Executor utilizes graph neural networks\n(GNN) to construct a subgraph for each agent that only contains crucial\npartners to improve cooperation. Additionally, the Goal Encoder in the Action\nExecutor captures the relationship between the agent and the designated goal to\nencourage the agent to reach the final target. The results show that MAGE-X\noutperforms the state-of-the-art MARL baselines with a 100% success rate with\nonly 3 million training steps in multi-agent particle environments (MPE) with\n50 agents, and at least a 12% higher success rate and 2x higher data efficiency\nin a more complicated quadrotor 3D navigation task.",
          "arxiv_id": "2302.04094v1"
        },
        {
          "title": "MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement Learning in Mixed Dynamic Environments",
          "year": "2020-07",
          "abstract": "Multi-agent navigation in dynamic environments is of great industrial value\nwhen deploying a large scale fleet of robot to real-world applications. This\npaper proposes a decentralized partially observable multi-agent path planning\nwith evolutionary reinforcement learning (MAPPER) method to learn an effective\nlocal planning policy in mixed dynamic environments. Reinforcement\nlearning-based methods usually suffer performance degradation on long-horizon\ntasks with goal-conditioned sparse rewards, so we decompose the long-range\nnavigation task into many easier sub-tasks under the guidance of a global\nplanner, which increases agents' performance in large environments. Moreover,\nmost existing multi-agent planning approaches assume either perfect information\nof the surrounding environment or homogeneity of nearby dynamic agents, which\nmay not hold in practice. Our approach models dynamic obstacles' behavior with\nan image-based representation and trains a policy in mixed dynamic environments\nwithout homogeneity assumption. To ensure multi-agent training stability and\nperformance, we propose an evolutionary training approach that can be easily\nscaled to large and complex environments. Experiments show that MAPPER is able\nto achieve higher success rates and more stable performance when exposed to a\nlarge number of non-cooperative dynamic obstacles compared with traditional\nreaction-based planner LRA* and the state-of-the-art learning-based method.",
          "arxiv_id": "2007.15724v1"
        }
      ],
      "19": [
        {
          "title": "Proceedings Second Workshop on Formal Methods for Autonomous Systems",
          "year": "2020-12",
          "abstract": "Autonomous systems are highly complex and present unique challenges for the\napplication of formal methods. Autonomous systems act without human\nintervention, and are often embedded in a robotic system, so that they can\ninteract with the real world. As such, they exhibit the properties of\nsafety-critical, cyber-physical, hybrid, and real-time systems.\n  The goal of FMAS is to bring together leading researchers who are tackling\nthe unique challenges of autonomous systems using formal methods, to present\nrecent and ongoing work. We are interested in the use of formal methods to\nspecify, model, or verify autonomous or robotic systems; in whole or in part.\nWe are also interested in successful industrial applications and potential\nfuture directions for this emerging application of formal methods.",
          "arxiv_id": "2012.01176v1"
        },
        {
          "title": "AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy",
          "year": "2025-08",
          "abstract": "Designing robotic systems to act autonomously in unforeseen environments is a\nchallenging task. This work presents a novel approach to use formal\nverification, specifically Statistical Model Checking (SMC), to verify system\nproperties of autonomous robots at design-time. We introduce an extension of\nthe SCXML format, designed to model system components including both Robot\nOperating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we\ncontribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the\nfull system model into JANI. The use of JANI, a standard format for\nquantitative model checking, enables verification of system properties with\noff-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both\nin terms of applicability to real-world autonomous robotic control systems, and\nin terms of verification runtime scaling. We provide a case study, where we\nsuccessfully identify problems in a ROS 2-based robotic manipulation use case\nthat is verifiable in less than one second using consumer hardware.\nAdditionally, we compare to the state of the art and demonstrate that our\nmethod is more comprehensive in system feature support, and that the\nverification runtime scales linearly with the size of the model, instead of\nexponentially.",
          "arxiv_id": "2508.18820v1"
        },
        {
          "title": "Runtime Verification and Field-based Testing for ROS-based Robotic Systems",
          "year": "2024-04",
          "abstract": "Robotic systems are becoming pervasive and adopted in increasingly many\ndomains, such as manufacturing, healthcare, and space exploration. To this end,\nengineering software has emerged as a crucial discipline for building\nmaintainable and reusable robotic systems. The robotics software engineering\nresearch field has received increasing attention, fostering autonomy as a\nfundamental goal. However, robotics developers are still challenged to achieve\nthis goal because simulation cannot realistically deliver solutions to emulate\nreal-world phenomena. Robots also need to operate in unpredictable and\nuncontrollable environments, which require safe and trustworthy self-adaptation\ncapabilities implemented in software. Typical techniques to address the\nchallenges are runtime verification, field-based testing, and mitigation\ntechniques that enable fail-safe solutions. However, no clear guidance exists\nfor architecting ROS-based systems to enable and facilitate runtime\nverification and field-based testing. This paper aims to fill this gap by\nproviding guidelines to help developers and quality assurance (QA) teams\ndevelop, verify, or test their robots in the field. These guidelines are\ncarefully tailored to address the challenges and requirements of testing\nrobotics systems in real-world scenarios. We conducted (i) a literature review\non studies addressing runtime verification and field-based testing for robotic\nsystems, (ii) mined ROS-based applications repositories, and (iii) validated\nthe applicability, clarity, and usefulness via two questionnaires with 55\nanswers overall. We contribute 20 guidelines: 8 for developers and 12 for QA\nteams formulated for researchers and practitioners in robotic software\nengineering. Finally, we map our guidelines to open challenges in runtime\nverification and field-based testing for ROS-based systems, and we outline\npromising research directions in the field.",
          "arxiv_id": "2404.11498v3"
        }
      ],
      "20": [
        {
          "title": "Vision-based Safe Autonomous UAV Docking with Panoramic Sensors",
          "year": "2023-05",
          "abstract": "The remarkable growth of unmanned aerial vehicles (UAVs) has also sparked\nconcerns about safety measures during their missions. To advance towards safer\nautonomous aerial robots, this work presents a vision-based solution to\nensuring safe autonomous UAV landings with minimal infrastructure. During\ndocking maneuvers, UAVs pose a hazard to people in the vicinity. In this paper,\nwe propose the use of a single omnidirectional panoramic camera pointing\nupwards from a landing pad to detect and estimate the position of people around\nthe landing area. The images are processed in real-time in an embedded\ncomputer, which communicates with the onboard computer of approaching UAVs to\ntransition between landing, hovering or emergency landing states. While\nlanding, the ground camera also aids in finding an optimal position, which can\nbe required in case of low-battery or when hovering is no longer possible. We\nuse a YOLOv7-based object detection model and a XGBooxt model for localizing\nnearby people, and the open-source ROS and PX4 frameworks for communication,\ninterfacing, and control of the UAV. We present both simulation and real-world\nindoor experimental results to show the efficiency of our methods.",
          "arxiv_id": "2305.16008v2"
        },
        {
          "title": "An Integrated Visual System for Unmanned Aerial Vehicles Tracking and Landing on the Ground Vehicles",
          "year": "2022-12",
          "abstract": "The vision of unmanned aerial vehicles is very significant for UAV-related\napplications such as search and rescue, landing on a moving platform, etc. In\nthis work, we have developed an integrated system for the UAV landing on the\nmoving platform, and the UAV object detection with tracking in the complicated\nenvironment. Firstly, we have proposed a robust LoG-based deep neural network\nfor object detection and tracking, which has great advantages in robustness to\nobject scale and illuminations compared with typical deep network-based\napproaches. Then, we have also improved based on the original Kalman filter and\ndesigned an iterative multi-model-based filter to tackle the problem of unknown\ndynamics in real circumstances of motion estimations. Next, we implemented the\nwhole system and do ROS Gazebo-based testing in two complicated circumstances\nto verify the effectiveness of our design. Finally, we have deployed the\nproposed detection, tracking, and motion estimation strategies into real\napplications to do UAV tracking of a pillar and obstacle avoidance. It is\ndemonstrated that our system shows great accuracy and robustness in real\napplications.",
          "arxiv_id": "2301.00198v1"
        },
        {
          "title": "Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches",
          "year": "2025-08",
          "abstract": "Underwater 3D object detection remains one of the most challenging frontiers\nin computer vision, where traditional approaches struggle with the harsh\nacoustic environment and scarcity of training data. While deep learning has\nrevolutionized terrestrial 3D detection, its application underwater faces a\ncritical bottleneck: obtaining sufficient annotated sonar data is prohibitively\nexpensive and logistically complex, often requiring specialized vessels, expert\nsurveyors, and favorable weather conditions. This work addresses a fundamental\nquestion: Can we achieve reliable underwater 3D object detection without\nreal-world training data? We tackle this challenge by developing and comparing\ntwo paradigms for training-free detection of artificial structures in multibeam\necho-sounder point clouds. Our dual approach combines a physics-based sonar\nsimulation pipeline that generates synthetic training data for state-of-the-art\nneural networks, with a robust model-based template matching system that\nleverages geometric priors of target objects. Evaluation on real bathymetry\nsurveys from the Baltic Sea reveals surprising insights: while neural networks\ntrained on synthetic data achieve 98% mean Average Precision (mAP) on simulated\nscenes, they drop to 40% mAP on real sonar data due to domain shift.\nConversely, our template matching approach maintains 83% mAP on real data\nwithout requiring any training, demonstrating remarkable robustness to acoustic\nnoise and environmental variations. Our findings challenge conventional wisdom\nabout data-hungry deep learning in underwater domains and establish the first\nlarge-scale benchmark for training-free underwater 3D detection. This work\nopens new possibilities for autonomous underwater vehicle navigation, marine\narchaeology, and offshore infrastructure monitoring in data-scarce environments\nwhere traditional machine learning approaches fail.",
          "arxiv_id": "2508.18293v1"
        }
      ],
      "21": [
        {
          "title": "Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning",
          "year": "2024-03",
          "abstract": "Mobile robots are being used on a large scale in various crowded situations\nand become part of our society. The socially acceptable navigation behavior of\na mobile robot with individual human consideration is an essential requirement\nfor scalable applications and human acceptance. Deep Reinforcement Learning\n(DRL) approaches are recently used to learn a robot's navigation policy and to\nmodel the complex interactions between robots and humans. We propose to divide\nexisting DRL-based navigation approaches based on the robot's exhibited social\nbehavior and distinguish between social collision avoidance with a lack of\nsocial behavior and socially aware approaches with explicit predefined social\nbehavior. In addition, we propose a novel socially integrated navigation\napproach where the robot's social behavior is adaptive and emerges from the\ninteraction with humans. The formulation of our approach is derived from a\nsociological definition, which states that social acting is oriented toward the\nacting of others. The DRL policy is trained in an environment where other\nagents interact socially integrated and reward the robot's behavior\nindividually. The simulation results indicate that the proposed socially\nintegrated navigation approach outperforms a socially aware approach in terms\nof ego navigation performance while significantly reducing the negative impact\non all agents within the environment.",
          "arxiv_id": "2403.09793v3"
        },
        {
          "title": "Principles and Guidelines for Evaluating Social Robot Navigation Algorithms",
          "year": "2023-06",
          "abstract": "A major challenge to deploying robots widely is navigation in human-populated\nenvironments, commonly referred to as social robot navigation. While the field\nof social navigation has advanced tremendously in recent years, the fair\nevaluation of algorithms that tackle social navigation remains hard because it\ninvolves not just robotic agents moving in static environments but also dynamic\nhuman agents and their perceptions of the appropriateness of robot behavior. In\ncontrast, clear, repeatable, and accessible benchmarks have accelerated\nprogress in fields like computer vision, natural language processing and\ntraditional robot navigation by enabling researchers to fairly compare\nalgorithms, revealing limitations of existing solutions and illuminating\npromising new directions. We believe the same approach can benefit social\nnavigation. In this paper, we pave the road towards common, widely accessible,\nand repeatable benchmarking criteria to evaluate social robot navigation. Our\ncontributions include (a) a definition of a socially navigating robot as one\nthat respects the principles of safety, comfort, legibility, politeness, social\ncompetency, agent understanding, proactivity, and responsiveness to context,\n(b) guidelines for the use of metrics, development of scenarios, benchmarks,\ndatasets, and simulators to evaluate social navigation, and (c) a design of a\nsocial navigation metrics framework to make it easier to compare results from\ndifferent simulators, robots and datasets.",
          "arxiv_id": "2306.16740v4"
        },
        {
          "title": "SOCIALGYM: A Framework for Benchmarking Social Robot Navigation",
          "year": "2021-09",
          "abstract": "Robots moving safely and in a socially compliant manner in dynamic human\nenvironments is an essential benchmark for long-term robot autonomy. However,\nit is not feasible to learn and benchmark social navigation behaviors entirely\nin the real world, as learning is data-intensive, and it is challenging to make\nsafety guarantees during training. Therefore, simulation-based benchmarks that\nprovide abstractions for social navigation are required. A framework for these\nbenchmarks would need to support a wide variety of learning approaches, be\nextensible to the broad range of social navigation scenarios, and abstract away\nthe perception problem to focus on social navigation explicitly. While there\nhave been many proposed solutions, including high fidelity 3D simulators and\ngrid world approximations, no existing solution satisfies all of the\naforementioned properties for learning and evaluating social navigation\nbehaviors. In this work, we propose SOCIALGYM, a lightweight 2D simulation\nenvironment for robot social navigation designed with extensibility in mind,\nand a benchmark scenario built on SOCIALGYM. Further, we present benchmark\nresults that compare and contrast human-engineered and model-based learning\napproaches to a suite of off-the-shelf Learning from Demonstration (LfD) and\nReinforcement Learning (RL) approaches applied to social robot navigation.\nThese results demonstrate the data efficiency, task performance, social\ncompliance, and environment transfer capabilities for each of the policies\nevaluated to provide a solid grounding for future social navigation research.",
          "arxiv_id": "2109.11011v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:21:12Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}