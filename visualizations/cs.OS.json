{
  "topics": {
    "data": {
      "0": {
        "name": "0_memory_performance_data_storage",
        "keywords": [
          [
            "memory",
            0.051361890912721274
          ],
          [
            "performance",
            0.03205795023717289
          ],
          [
            "data",
            0.030359005158975917
          ],
          [
            "storage",
            0.02264290536611332
          ],
          [
            "applications",
            0.02239230243241939
          ],
          [
            "page",
            0.02058207473208948
          ],
          [
            "systems",
            0.02040490318476795
          ],
          [
            "latency",
            0.015398042592011234
          ],
          [
            "Memory",
            0.015347666419271771
          ],
          [
            "application",
            0.015112496569393803
          ]
        ],
        "count": 211
      },
      "1": {
        "name": "1_kernel_security_applications_memory",
        "keywords": [
          [
            "kernel",
            0.0440431742230201
          ],
          [
            "security",
            0.031940725523650584
          ],
          [
            "applications",
            0.020533268091183027
          ],
          [
            "memory",
            0.019700979211331376
          ],
          [
            "attacks",
            0.019474808105926004
          ],
          [
            "Linux",
            0.019218750188009708
          ],
          [
            "user",
            0.01921010553070773
          ],
          [
            "software",
            0.019165907467027232
          ],
          [
            "hardware",
            0.01882736645457955
          ],
          [
            "OS",
            0.01868925580813825
          ]
        ],
        "count": 137
      },
      "2": {
        "name": "2_time_scheduling_real_task",
        "keywords": [
          [
            "time",
            0.04694617639558833
          ],
          [
            "scheduling",
            0.03872642082269799
          ],
          [
            "real",
            0.02847355903509769
          ],
          [
            "task",
            0.02801506031952169
          ],
          [
            "tasks",
            0.02710358542387324
          ],
          [
            "systems",
            0.026997029214924653
          ],
          [
            "schedulability",
            0.022703825229961054
          ],
          [
            "criticality",
            0.021252107398431225
          ],
          [
            "resource",
            0.02071806253864152
          ],
          [
            "low",
            0.019781498329082214
          ]
        ],
        "count": 84
      },
      "3": {
        "name": "3_GPU_LLM_inference_memory",
        "keywords": [
          [
            "GPU",
            0.06048031975286409
          ],
          [
            "LLM",
            0.03556576076227716
          ],
          [
            "inference",
            0.03130022967209975
          ],
          [
            "memory",
            0.02572493986122613
          ],
          [
            "KV",
            0.025291880888337084
          ],
          [
            "CPU",
            0.02261886610370941
          ],
          [
            "model",
            0.021256987979921173
          ],
          [
            "GPUs",
            0.02117723582178381
          ],
          [
            "latency",
            0.019998789100515835
          ],
          [
            "models",
            0.019837085834315323
          ]
        ],
        "count": 45
      },
      "4": {
        "name": "4_ROS_AI_agents_OS",
        "keywords": [
          [
            "ROS",
            0.04262526943642418
          ],
          [
            "AI",
            0.04046161904032029
          ],
          [
            "agents",
            0.03591955901239289
          ],
          [
            "OS",
            0.02782913163718581
          ],
          [
            "Operating",
            0.027139526960382944
          ],
          [
            "agent",
            0.024956937570519387
          ],
          [
            "LLM",
            0.022797132030346483
          ],
          [
            "Agent",
            0.02252660175511529
          ],
          [
            "time",
            0.02199299545385171
          ],
          [
            "models",
            0.021142418446751094
          ]
        ],
        "count": 28
      }
    },
    "correlations": [
      [
        1.0,
        -0.43469381250067884,
        -0.5094096250428188,
        -0.6446813586572373,
        -0.6875408160510652
      ],
      [
        -0.43469381250067884,
        1.0,
        -0.5262889356499519,
        -0.7128988657945744,
        -0.6618584613122878
      ],
      [
        -0.5094096250428188,
        -0.5262889356499519,
        1.0,
        -0.6814388366869119,
        -0.5497821752490619
      ],
      [
        -0.6446813586572373,
        -0.7128988657945744,
        -0.6814388366869119,
        1.0,
        -0.6374706913503208
      ],
      [
        -0.6875408160510652,
        -0.6618584613122878,
        -0.5497821752490619,
        -0.6374706913503208,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        2,
        2,
        3,
        0,
        0
      ],
      "2020-02": [
        3,
        1,
        1,
        0,
        1
      ],
      "2020-03": [
        3,
        0,
        7,
        1,
        0
      ],
      "2020-04": [
        6,
        2,
        2,
        0,
        0
      ],
      "2020-05": [
        4,
        4,
        2,
        0,
        0
      ],
      "2020-06": [
        3,
        3,
        1,
        0,
        0
      ],
      "2020-07": [
        3,
        0,
        4,
        0,
        0
      ],
      "2020-08": [
        6,
        0,
        0,
        0,
        0
      ],
      "2020-09": [
        4,
        2,
        2,
        0,
        0
      ],
      "2020-10": [
        2,
        1,
        1,
        0,
        1
      ],
      "2020-11": [
        7,
        1,
        1,
        0,
        0
      ],
      "2020-12": [
        2,
        0,
        3,
        0,
        0
      ],
      "2021-01": [
        6,
        1,
        5,
        0,
        0
      ],
      "2021-02": [
        3,
        4,
        0,
        0,
        0
      ],
      "2021-03": [
        3,
        0,
        0,
        0,
        0
      ],
      "2021-04": [
        3,
        2,
        5,
        0,
        0
      ],
      "2021-05": [
        1,
        2,
        1,
        0,
        0
      ],
      "2021-06": [
        3,
        2,
        0,
        0,
        0
      ],
      "2021-07": [
        1,
        1,
        0,
        0,
        0
      ],
      "2021-08": [
        1,
        1,
        0,
        0,
        0
      ],
      "2021-09": [
        2,
        0,
        1,
        0,
        0
      ],
      "2021-10": [
        4,
        0,
        3,
        0,
        0
      ],
      "2021-11": [
        1,
        1,
        2,
        1,
        0
      ],
      "2021-12": [
        3,
        3,
        2,
        0,
        0
      ],
      "2022-01": [
        1,
        4,
        1,
        0,
        2
      ],
      "2022-02": [
        3,
        0,
        1,
        0,
        0
      ],
      "2022-03": [
        8,
        1,
        2,
        1,
        0
      ],
      "2022-04": [
        3,
        0,
        1,
        0,
        0
      ],
      "2022-05": [
        3,
        1,
        3,
        0,
        1
      ],
      "2022-06": [
        3,
        4,
        0,
        0,
        0
      ],
      "2022-07": [
        1,
        0,
        1,
        0,
        0
      ],
      "2022-08": [
        0,
        0,
        1,
        0,
        0
      ],
      "2022-09": [
        2,
        2,
        2,
        0,
        0
      ],
      "2022-10": [
        4,
        2,
        1,
        0,
        1
      ],
      "2022-11": [
        1,
        1,
        3,
        0,
        0
      ],
      "2022-12": [
        4,
        0,
        2,
        0,
        0
      ],
      "2023-01": [
        2,
        4,
        1,
        0,
        0
      ],
      "2023-02": [
        5,
        2,
        1,
        0,
        0
      ],
      "2023-03": [
        3,
        4,
        3,
        0,
        0
      ],
      "2023-04": [
        2,
        1,
        2,
        1,
        0
      ],
      "2023-05": [
        3,
        0,
        2,
        2,
        0
      ],
      "2023-06": [
        4,
        2,
        3,
        1,
        0
      ],
      "2023-07": [
        4,
        2,
        1,
        0,
        0
      ],
      "2023-08": [
        1,
        1,
        2,
        0,
        0
      ],
      "2023-09": [
        4,
        8,
        3,
        0,
        0
      ],
      "2023-10": [
        7,
        4,
        4,
        1,
        0
      ],
      "2023-11": [
        5,
        4,
        1,
        0,
        1
      ],
      "2023-12": [
        5,
        2,
        3,
        2,
        3
      ],
      "2024-01": [
        6,
        4,
        1,
        1,
        0
      ],
      "2024-02": [
        5,
        1,
        2,
        2,
        0
      ],
      "2024-03": [
        5,
        1,
        1,
        2,
        1
      ],
      "2024-04": [
        3,
        3,
        0,
        0,
        0
      ],
      "2024-05": [
        3,
        1,
        3,
        2,
        0
      ],
      "2024-06": [
        7,
        0,
        2,
        1,
        2
      ],
      "2024-07": [
        1,
        2,
        1,
        0,
        1
      ],
      "2024-08": [
        6,
        2,
        5,
        1,
        1
      ],
      "2024-09": [
        7,
        5,
        4,
        1,
        1
      ],
      "2024-10": [
        8,
        0,
        4,
        4,
        1
      ],
      "2024-11": [
        5,
        1,
        3,
        2,
        1
      ],
      "2024-12": [
        9,
        1,
        4,
        1,
        1
      ],
      "2025-01": [
        10,
        4,
        3,
        1,
        1
      ],
      "2025-02": [
        7,
        3,
        2,
        3,
        0
      ],
      "2025-03": [
        9,
        6,
        1,
        2,
        3
      ],
      "2025-04": [
        7,
        8,
        1,
        3,
        1
      ],
      "2025-05": [
        6,
        0,
        7,
        1,
        2
      ],
      "2025-06": [
        8,
        1,
        4,
        6,
        2
      ],
      "2025-07": [
        2,
        3,
        2,
        2,
        0
      ],
      "2025-08": [
        6,
        0,
        9,
        5,
        3
      ],
      "2025-09": [
        3,
        1,
        1,
        1,
        3
      ]
    },
    "papers": {
      "0": [
        {
          "title": "On the Applicability of PEBS based Online Memory Access Tracking for Heterogeneous Memory Management at Scale",
          "year": "2020-11",
          "abstract": "Operating systems have historically had to manage only a single type of\nmemory device. The imminent availability of heterogeneous memory devices based\non emerging memory technologies confronts the classic single memory model and\nopens a new spectrum of possibilities for memory management. Transparent data\nmovement between different memory devices based on access patterns of\napplications is a desired feature to make optimal use of such devices and to\nhide the complexity of memory management to the end-user. However, capturing\nmemory access patterns of an application at runtime comes at a cost, which is\nparticularly challenging for large scale parallel applications that may be\nsensitive to system noise.\n  In this work, we focus on the access pattern profiling phase prior to the\nactual memory relocation. We study the feasibility of using Intel's Processor\nEvent-Based Sampling (PEBS) feature to record memory accesses by sampling at\nruntime and study the overhead at scale. We have implemented a custom PEBS\ndriver in the IHK/McKernel lightweight multi-kernel operating system, one of\nwhose advantages is minimal system interference due to the lightweight kernel's\nsimple design compared to other OS kernels such as Linux. We present the PEBS\noverhead of a set of scientific applications and show the access patterns\nidentified in noise-sensitive HPC applications. Our results show that clear\naccess patterns can be captured with a 10% overhead in the worst-case and 1% in\nthe best case when running on up to 128k CPU cores (2,048 Intel Xeon Phi\nKnights Landing nodes). We conclude that online memory access profiling using\nPEBS at large scale is promising for memory management in heterogeneous memory\nenvironments.",
          "arxiv_id": "2011.13432v1"
        },
        {
          "title": "Nomad: Non-Exclusive Memory Tiering via Transactional Page Migration",
          "year": "2024-01",
          "abstract": "With the advent of byte-addressable memory devices, such as CXL memory,\npersistent memory, and storage-class memory, tiered memory systems have become\na reality. Page migration is the de facto method within operating systems for\nmanaging tiered memory. It aims to bring hot data whenever possible into fast\nmemory to optimize the performance of data accesses while using slow memory to\naccommodate data spilled from fast memory. While the existing research has\ndemonstrated the effectiveness of various optimizations on page migration, it\nfalls short of addressing a fundamental question: Is exclusive memory tiering,\nin which a page is either present in fast memory or slow memory, but not both\nsimultaneously, the optimal strategy for tiered memory management?\n  We demonstrate that page migration-based exclusive memory tiering suffers\nsignificant performance degradation when fast memory is under pressure. In this\npaper, we propose non-exclusive memory tiering, a page management strategy that\nretains a copy of pages recently promoted from slow memory to fast memory to\nmitigate memory thrashing. To enable non-exclusive memory tiering, we develop\nNomad, a new page management mechanism for Linux that features transactional\npage migration and page shadowing. Nomad helps remove page migration off the\ncritical path of program execution and makes migration completely asynchronous.\nEvaluations with carefully crafted micro-benchmarks and real-world applications\nshow that Nomad is able to achieve up to 6x performance improvement over the\nstate-of-the-art transparent page placement (TPP) approach in Linux when under\nmemory pressure. We also compare Nomad with a recently proposed\nhardware-assisted, access sampling-based page migration approach and\ndemonstrate Nomad's strengths and potential weaknesses in various scenarios.",
          "arxiv_id": "2401.13154v2"
        },
        {
          "title": "TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory",
          "year": "2022-06",
          "abstract": "The increasing demand for memory in hyperscale applications has led to memory\nbecoming a large portion of the overall datacenter spend. The emergence of\ncoherent interfaces like CXL enables main memory expansion and offers an\nefficient solution to this problem. In such systems, the main memory can\nconstitute different memory technologies with varied characteristics. In this\npaper, we characterize memory usage patterns of a wide range of datacenter\napplications across the server fleet of Meta. We, therefore, demonstrate the\nopportunities to offload colder pages to slower memory tiers for these\napplications. Without efficient memory management, however, such systems can\nsignificantly degrade performance.\n  We propose a novel OS-level application-transparent page placement mechanism\n(TPP) for CXL-enabled memory. TPP employs a lightweight mechanism to identify\nand place hot/cold pages to appropriate memory tiers. It enables a proactive\npage demotion from local memory to CXL-Memory. This technique ensures a memory\nheadroom for new page allocations that are often related to request processing\nand tend to be short-lived and hot. At the same time, TPP can promptly promote\nperformance-critical hot pages trapped in the slow CXL-Memory to the fast local\nmemory, while minimizing both sampling overhead and unnecessary migrations. TPP\nworks transparently without any application-specific knowledge and can be\ndeployed globally as a kernel release.\n  We evaluate TPP in the production server fleet with early samples of new x86\nCPUs with CXL 1.1 support. TPP makes a tiered memory system performant as an\nideal baseline (<1% gap) that has all the memory in the local tier. It is 18%\nbetter than today's Linux, and 5-17% better than existing solutions including\nNUMA Balancing and AutoTiering. Most of the TPP patches have been merged in the\nLinux v5.18 release.",
          "arxiv_id": "2206.02878v2"
        }
      ],
      "1": [
        {
          "title": "Case Study: Securing MMU-less Linux Using CHERI",
          "year": "2023-10",
          "abstract": "MMU-less Linux variant lacks security because it does not have protection or\nisolation mechanisms. It also does not use MPUs as they do not fit with its\nsoftware model because of the design drawbacks of MPUs (\\ie coarse-grained\nprotection with fixed number of protected regions). We secure the existing\nMMU-less Linux version of the RISC-V port using CHERI. CHERI is a\nhardware-software capability-based system that extends the ISA, toolchain,\nprogramming languages, operating systems, and applications in order to provide\ncomplete pointer and memory safety. We believe that CHERI could provide\nsignificant security guarantees for high-end dynamic MMU-less embedded systems\nat lower costs, compared to MMUs and MPUs, by: 1) building the entire software\nstack in pure-capability CHERI C mode which provides complete spatial memory\nsafety at the kernel and user-level, 2) isolating user programs as separate\nELFs, each with its own CHERI-based capability table; this provides spatial\nmemory safety similar to what the MMU offers (\\ie user programs cannot access\neach other's memory), 3) isolating user programs from the kernel as the kernel\nhas its own capability table from the users and vice versa, and 4)\ncompartmentalising kernel modules using CompartOS' linkage-based\ncompartmentalisation. This offers a new security front that is not possible\nusing the current MMU-based Linux, where vulnerable/malicious kernel modules\n(\\eg device drivers) executing in the kernel space would not compromise or take\ndown the entire system. These are the four main contributions of this paper,\npresenting novel CHERI-based mechanisms to secure MMU-less embedded Linux.",
          "arxiv_id": "2310.00933v2"
        },
        {
          "title": "BULKHEAD: Secure, Scalable, and Efficient Kernel Compartmentalization with PKS",
          "year": "2024-09",
          "abstract": "The endless stream of vulnerabilities urgently calls for principled\nmitigation to confine the effect of exploitation. However, the monolithic\narchitecture of commodity OS kernels, like the Linux kernel, allows an attacker\nto compromise the entire system by exploiting a vulnerability in any kernel\ncomponent. Kernel compartmentalization is a promising approach that follows the\nleast-privilege principle. However, existing mechanisms struggle with the\ntrade-off on security, scalability, and performance, given the challenges\nstemming from mutual untrustworthiness among numerous and complex components.\n  In this paper, we present BULKHEAD, a secure, scalable, and efficient kernel\ncompartmentalization technique that offers bi-directional isolation for\nunlimited compartments. It leverages Intel's new hardware feature PKS to\nisolate data and code into mutually untrusted compartments and benefits from\nits fast compartment switching. With untrust in mind, BULKHEAD introduces a\nlightweight in-kernel monitor that enforces multiple important security\ninvariants, including data integrity, execute-only memory, and compartment\ninterface integrity. In addition, it provides a locality-aware two-level scheme\nthat scales to unlimited compartments. We implement a prototype system on Linux\nv6.1 to compartmentalize loadable kernel modules (LKMs). Extensive evaluation\nconfirms the effectiveness of our approach. As the system-wide impacts,\nBULKHEAD incurs an average performance overhead of 2.44% for real-world\napplications with 160 compartmentalized LKMs. While focusing on a specific\ncompartment, ApacheBench tests on ipv6 show an overhead of less than 2%.\nMoreover, the performance is almost unaffected by the number of compartments,\nwhich makes it highly scalable.",
          "arxiv_id": "2409.09606v1"
        },
        {
          "title": "SFP: Providing System Call Flow Protection against Software and Fault Attacks",
          "year": "2023-01",
          "abstract": "With the improvements in computing technologies, edge devices in the\nInternet-of-Things have become more complex. The enabler technology for these\ncomplex systems are powerful application core processors with operating system\nsupport, such as Linux. While the isolation of applications through the\noperating system increases the security, the interface to the kernel poses a\nnew threat. Different attack vectors, including fault attacks and memory\nvulnerabilities, exploit the kernel interface to escalate privileges and take\nover the system.\n  In this work, we present SFP, a mechanism to protect the execution of system\ncalls against software and fault attacks providing integrity to user-kernel\ntransitions. SFP provides system call flow integrity by a two-step linking\napproach, which links the system call and its origin to the state of\ncontrol-flow integrity. A second linking step within the kernel ensures that\nthe right system call is executed in the kernel. Combining both linking steps\nensures that only the correct system call is executed at the right location in\nthe program and cannot be skipped. Furthermore, SFP provides dynamic CFI\ninstrumentation and a new CFI checking policy at the edge of the kernel to\nverify the control-flow state of user programs before entering the kernel. We\nintegrated SFP into FIPAC, a CFI protection scheme exploiting ARM pointer\nauthentication. Our prototype is based on a custom LLVM-based toolchain with an\ninstrumented runtime library combined with a custom Linux kernel to protect\nsystem calls. The evaluation of micro- and macrobenchmarks based on SPEC 2017\nshow an average runtime overhead of 1.9 % and 20.6 %, which is only an increase\nof 1.8 % over plain control-flow protection. This small impact on the\nperformance shows the efficiency of SFP for protecting all system calls and\nproviding integrity for the user-kernel transitions.",
          "arxiv_id": "2301.02915v2"
        }
      ],
      "2": [
        {
          "title": "Real-Time Aware IP-Networking for Resource-Constrained Embedded Devices",
          "year": "2024-11",
          "abstract": "This dissertation explores the area of real-time IP networking for embedded\ndevices, especially those with limited computational resources. With the\nincreasing convergence of information and operational technologies in various\nindustries, and the growing complexity of communication requirements in\n(semi-)autonomous machines, there is a need for more advanced and reliable\nnetworking solutions. This research focuses on the challenge of integrating\nreal-time embedded devices into packet-switched networks. Through a\ncomprehensive review of current real-time communication technologies,\nstandards, and practices in the context of Industry 4.0, a notable gap is\nidentified: the lack of a robust real-time communication standard tailored for\nwireless mobile machines, and insufficient research on real-time embedded\ndevices in highly networked environments. The study includes detailed\nexperimentation with commercially available off-the-shelf networked\nmicrocontrollers, revealing a priority inversion problem where network packet\nprocessing interrupts real-time tasks, potentially causing real-time\nviolations. To address this challenge, this thesis proposes mitigation methods\nand system designs that include software and hardware implementations. These\ninclude a new embedded network subsystem that prioritizes packet processing\nbased on task priority, and a real-time-aware network interface controller that\nmoderates interrupt requests. In addition, a hybrid hardware-software co-design\napproach is developed to ensure predictable and reliable real-time task\nexecution despite network congestion. Furthermore, the research extends to task\noffloading in wireless Industrial Internet of Things environments, presenting a\nsystem architecture and scheduler capable of maintaining real-time constraints\neven under heavy loads and network uncertainties.",
          "arxiv_id": "2411.15150v1"
        },
        {
          "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with Generative Profiling",
          "year": "2025-01",
          "abstract": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
          "arxiv_id": "2501.08484v1"
        },
        {
          "title": "Global Scheduling of Weakly-Hard Real-Time Tasks using Job-Level Priority Classes",
          "year": "2024-10",
          "abstract": "Real-time systems are intrinsic components of many pivotal applications, such\nas self-driving vehicles, aerospace and defense systems. The trend in these\napplications is to incorporate multiple tasks onto fewer, more powerful\nhardware platforms, e.g., multi-core systems, mainly for reducing cost and\npower consumption. Many real-time tasks, like control tasks, can tolerate\noccasional deadline misses due to robust algorithms. These tasks can be modeled\nusing the weakly-hard model. Literature shows that leveraging the weakly-hard\nmodel can relax the over-provisioning associated with designed real-time\nsystems. However, a wide-range of the research focuses on single-core\nplatforms. Therefore, we strive to extend the state-of-the-art of scheduling\nweakly-hard real-time tasks to multi-core platforms. We present a global\njob-level fixed priority scheduling algorithm together with its schedulability\nanalysis. The scheduling algorithm leverages the tolerable continuous deadline\nmisses to assigning priorities to jobs. The proposed analysis extends the\nResponse Time Analysis (RTA) for global scheduling to test the schedulability\nof tasks. Hence, our analysis scales with the number of tasks and number of\ncores because, unlike literature, it depends neither on Integer Linear\nProgramming nor reachability trees. Schedulability analyses show that the\nschedulability ratio is improved by 40% comparing to the global Rate Monotonic\n(RM) scheduling and up to 60% more than the global EDF scheduling, which are\nthe state-of-the-art schedulers on the RTEMS real-time operating system. Our\nevaluation on industrial embedded multi-core platform running RTEMS shows that\nthe scheduling overhead of our proposal does not exceed 60 Nanosecond.",
          "arxiv_id": "2410.01528v1"
        }
      ],
      "3": [
        {
          "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU",
          "year": "2023-12",
          "abstract": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM)\ninference engine on a personal computer (PC) equipped with a single\nconsumer-grade GPU. The key principle underlying the design of PowerInfer is\nexploiting the high locality inherent in LLM inference, characterized by a\npower-law distribution in neuron activation. This distribution indicates that a\nsmall subset of neurons, termed hot neurons, are consistently activated across\ninputs, while the majority, cold neurons, vary based on specific inputs.\nPowerInfer exploits such an insight to design a GPU-CPU hybrid inference\nengine: hot-activated neurons are preloaded onto the GPU for fast access, while\ncold-activated neurons are computed on the CPU, thus significantly reducing GPU\nmemory demands and CPU-GPU data transfers. PowerInfer further integrates\nadaptive predictors and neuron-aware sparse operators, optimizing the\nefficiency of neuron activation and computational sparsity. The evaluation\nshows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while\nretaining model accuracy across various LLMs (including OPT-175B) on a single\nNVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance\ncomparable to that of a high-end server-grade A100 GPU, reaching 82% of its\ntoken generation rate on a single consumer-grade RTX 4090 GPU.",
          "arxiv_id": "2312.12456v2"
        },
        {
          "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models",
          "year": "2024-02",
          "abstract": "Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures\nhave shown promising performance on various tasks. However, due to the huge\nmodel sizes, running them in resource-constrained environments where the GPU\nmemory is not abundant is challenging. Some existing systems propose to use CPU\nresources to solve that, but they either suffer from the significant overhead\nof frequently moving data between CPU and GPU, or fail to consider distinct\ncharacteristics of CPUs and GPUs. This paper proposes Fiddler, a\nresource-efficient inference system for MoE models with limited GPU resources.\nFiddler strategically utilizes CPU and GPU resources by determining the optimal\nexecution strategy. Our evaluation shows that, unlike state-of-the-art systems\nthat optimize for specific scenarios such as single batch inference or long\nprefill, Fiddler performs better in all scenarios. Compared against different\nbaselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30\ntimes in long prefill processing, and 11.57 times in beam search inference. The\ncode of Fiddler is publicly available at https://github.com/efeslab/fiddler.",
          "arxiv_id": "2402.07033v3"
        },
        {
          "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV Management on a Single Commodity GPU",
          "year": "2025-06",
          "abstract": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
          "arxiv_id": "2506.20187v2"
        }
      ],
      "4": [
        {
          "title": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem",
          "year": "2023-12",
          "abstract": "This paper envisions a revolutionary AIOS-Agent ecosystem, where Large\nLanguage Model (LLM) serves as the (Artificial) Intelligent Operating System\n(IOS, or AIOS)--an operating system \"with soul\". Upon this foundation, a\ndiverse range of LLM-based AI Agent Applications (Agents, or AAPs) are\ndeveloped, enriching the AIOS-Agent ecosystem and signaling a paradigm shift\nfrom the traditional OS-APP ecosystem. We envision that LLM's impact will not\nbe limited to the AI application level, instead, it will in turn revolutionize\nthe design and implementation of computer system, architecture, software, and\nprogramming language, featured by several main concepts: LLM as OS\n(system-level), Agents as Applications (application-level), Natural Language as\nProgramming Interface (user-level), and Tools as Devices/Libraries\n(hardware/middleware-level). We begin by introducing the architecture of\ntraditional OS. Then we formalize a conceptual framework for AIOS through \"LLM\nas OS (LLMOS)\", drawing analogies between AIOS and traditional OS: LLM is\nlikened to OS kernel, context window to memory, external storage to file\nsystem, hardware tools to peripheral devices, software tools to programming\nlibraries, and user prompts to user commands. Subsequently, we introduce the\nnew AIOS-Agent Ecosystem, where users can easily program Agent Applications\n(AAPs) using natural language, democratizing the development of software, which\nis different from the traditional OS-APP ecosystem. Following this, we explore\nthe diverse scope of Agent Applications. We delve into both single-agent and\nmulti-agent systems, as well as human-agent interaction. Lastly, drawing on the\ninsights from traditional OS-APP ecosystem, we propose a roadmap for the\nevolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the\nfuture research and development, suggesting systematic progresses of AIOS and\nits Agent applications.",
          "arxiv_id": "2312.03815v2"
        },
        {
          "title": "CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and Complex Automation",
          "year": "2024-09",
          "abstract": "The underlying framework for controlling autonomous robots and complex\nautomation applications are Operating Systems (OS) capable of scheduling\nperception-and-control tasks, as well as providing real-time data communication\nto other robotic peers and remote cloud computers. In this paper, we introduce\nCyberCortex AI, a robotics OS designed to enable heterogeneous AI-based\nrobotics and complex automation applications. CyberCortex AI is a decentralized\ndistributed OS which enables robots to talk to each other, as well as to High\nPerformance Computers (HPC) in the cloud. Sensory and control data from the\nrobots is streamed towards HPC systems with the purpose of training AI\nalgorithms, which are afterwards deployed on the robots. Each functionality of\na robot (e.g. sensory data acquisition, path planning, motion control, etc.) is\nexecuted within a so-called DataBlock of Filters shared through the internet,\nwhere each filter is computed either locally on the robot itself, or remotely\non a different robotic system. The data is stored and accessed via a so-called\nTemporal Addressable Memory (TAM), which acts as a gateway between each\nfilter's input and output. CyberCortex AI has two main components: i) the\nCyberCortex AI inference system, which is a real-time implementation of the\nDataBlock running on the robots' embedded hardware, and ii) the CyberCortex AI\ndojo, which runs on an HPC computer in the cloud, and it is used to design,\ntrain and deploy AI algorithms. We present a quantitative and qualitative\nperformance analysis of the proposed approach using two collaborative robotics\napplications: i) a forest fires prevention system based on an Unitree A1 legged\nrobot and an Anafi Parrot 4K drone, as well as ii) an autonomous driving system\nwhich uses CyberCortex AI for collaborative perception and motion control.",
          "arxiv_id": "2409.01241v3"
        },
        {
          "title": "AIOS: LLM Agent Operating System",
          "year": "2024-03",
          "abstract": "LLM-based intelligent agents face significant deployment challenges,\nparticularly related to resource management. Allowing unrestricted access to\nLLM or tool resources can lead to inefficient or even potentially harmful\nresource allocation and utilization for agents. Furthermore, the absence of\nproper scheduling and resource management mechanisms in current agent designs\nhinders concurrent processing and limits overall system efficiency. To address\nthese challenges, this paper proposes the architecture of AIOS (LLM-based AI\nAgent Operating System) under the context of managing LLM-based agents. It\nintroduces a novel architecture for serving LLM-based agents by isolating\nresources and LLM-specific services from agent applications into an AIOS\nkernel. This AIOS kernel provides fundamental services (e.g., scheduling,\ncontext management, memory management, storage management, access control) for\nruntime agents. To enhance usability, AIOS also includes an AIOS SDK, a\ncomprehensive suite of APIs designed for utilizing functionalities provided by\nthe AIOS kernel. Experimental results demonstrate that using AIOS can achieve\nup to 2.1x faster execution for serving agents built by various agent\nframeworks. The source code is available at\nhttps://github.com/agiresearch/AIOS.",
          "arxiv_id": "2403.16971v5"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:19:55Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}