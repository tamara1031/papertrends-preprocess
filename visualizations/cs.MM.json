{
  "topics": {
    "data": {
      "0": {
        "name": "0_video_model_models_image",
        "keywords": [
          [
            "video",
            0.030262564045947227
          ],
          [
            "model",
            0.0258894017169137
          ],
          [
            "models",
            0.02336547087901479
          ],
          [
            "image",
            0.02209704041983496
          ],
          [
            "visual",
            0.02167005831307116
          ],
          [
            "data",
            0.020897673263751745
          ],
          [
            "audio",
            0.01980667033124672
          ],
          [
            "performance",
            0.01838894281442856
          ],
          [
            "methods",
            0.018284674845348122
          ],
          [
            "learning",
            0.018231176911445175
          ]
        ],
        "count": 5493
      },
      "1": {
        "name": "1_image_watermarking_detection_images",
        "keywords": [
          [
            "image",
            0.03662386181498039
          ],
          [
            "watermarking",
            0.0252235787251767
          ],
          [
            "detection",
            0.02474144575537188
          ],
          [
            "images",
            0.024675394728320487
          ],
          [
            "methods",
            0.018553656000723517
          ],
          [
            "data",
            0.01850619597954223
          ],
          [
            "method",
            0.01814718888281271
          ],
          [
            "model",
            0.017202177255309085
          ],
          [
            "deepfake",
            0.01667371738714835
          ],
          [
            "attacks",
            0.015776898938024222
          ]
        ],
        "count": 484
      }
    },
    "correlations": [
      [
        1.0,
        -0.22920215286946777
      ],
      [
        -0.22920215286946777,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        15,
        19
      ],
      "2020-02": [
        24,
        16
      ],
      "2020-03": [
        26,
        20
      ],
      "2020-04": [
        32,
        28
      ],
      "2020-05": [
        33,
        24
      ],
      "2020-06": [
        24,
        23
      ],
      "2020-07": [
        34,
        23
      ],
      "2020-08": [
        35,
        29
      ],
      "2020-09": [
        34,
        27
      ],
      "2020-10": [
        51,
        28
      ],
      "2020-11": [
        27,
        21
      ],
      "2020-12": [
        25,
        22
      ],
      "2021-01": [
        36,
        25
      ],
      "2021-02": [
        25,
        21
      ],
      "2021-03": [
        34,
        20
      ],
      "2021-04": [
        51,
        29
      ],
      "2021-05": [
        33,
        21
      ],
      "2021-06": [
        33,
        26
      ],
      "2021-07": [
        46,
        19
      ],
      "2021-08": [
        39,
        33
      ],
      "2021-09": [
        33,
        22
      ],
      "2021-10": [
        40,
        26
      ],
      "2021-11": [
        26,
        14
      ],
      "2021-12": [
        39,
        28
      ],
      "2022-01": [
        32,
        36
      ],
      "2022-02": [
        27,
        18
      ],
      "2022-03": [
        52,
        36
      ],
      "2022-04": [
        37,
        27
      ],
      "2022-05": [
        40,
        27
      ],
      "2022-06": [
        44,
        27
      ],
      "2022-07": [
        69,
        40
      ],
      "2022-08": [
        52,
        31
      ],
      "2022-09": [
        37,
        37
      ],
      "2022-10": [
        59,
        40
      ],
      "2022-11": [
        55,
        35
      ],
      "2022-12": [
        35,
        33
      ],
      "2023-01": [
        37,
        15
      ],
      "2023-02": [
        39,
        31
      ],
      "2023-03": [
        60,
        47
      ],
      "2023-04": [
        54,
        28
      ],
      "2023-05": [
        88,
        60
      ],
      "2023-06": [
        45,
        34
      ],
      "2023-07": [
        49,
        29
      ],
      "2023-08": [
        71,
        61
      ],
      "2023-09": [
        82,
        42
      ],
      "2023-10": [
        64,
        45
      ],
      "2023-11": [
        72,
        27
      ],
      "2023-12": [
        61,
        34
      ],
      "2024-01": [
        56,
        28
      ],
      "2024-02": [
        57,
        29
      ],
      "2024-03": [
        78,
        41
      ],
      "2024-04": [
        74,
        41
      ],
      "2024-05": [
        68,
        36
      ],
      "2024-06": [
        78,
        38
      ],
      "2024-07": [
        95,
        48
      ],
      "2024-08": [
        77,
        44
      ],
      "2024-09": [
        92,
        56
      ],
      "2024-10": [
        96,
        40
      ],
      "2024-11": [
        69,
        30
      ],
      "2024-12": [
        111,
        52
      ],
      "2025-01": [
        50,
        19
      ],
      "2025-02": [
        64,
        24
      ],
      "2025-03": [
        82,
        50
      ],
      "2025-04": [
        92,
        47
      ],
      "2025-05": [
        115,
        37
      ],
      "2025-06": [
        92,
        45
      ],
      "2025-07": [
        114,
        30
      ],
      "2025-08": [
        117,
        45
      ],
      "2025-09": [
        43,
        17
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning",
          "year": "2022-08",
          "abstract": "Despite the recent developments in the field of cross-modal retrieval, there\nhas been less research focusing on low-resource languages due to the lack of\nmanually annotated datasets. In this paper, we propose a noise-robust\ncross-lingual cross-modal retrieval method for low-resource languages. To this\nend, we use Machine Translation (MT) to construct pseudo-parallel sentence\npairs for low-resource languages. However, as MT is not perfect, it tends to\nintroduce noise during translation, rendering textual embeddings corrupted and\nthereby compromising the retrieval performance. To alleviate this, we introduce\na multi-view self-distillation method to learn noise-robust target-language\nrepresentations, which employs a cross-attention module to generate soft\npseudo-targets to provide direct supervision from the similarity-based view and\nfeature-based view. Besides, inspired by the back-translation in unsupervised\nMT, we minimize the semantic discrepancies between origin sentences and\nback-translated sentences to further improve the noise robustness of the\ntextual encoder. Extensive experiments are conducted on three video-text and\nimage-text cross-modal retrieval benchmarks across different languages, and the\nresults demonstrate that our method significantly improves the overall\nperformance without using extra human-labeled data. In addition, equipped with\na pre-trained visual encoder from a recent vision-and-language pre-training\nframework, i.e., CLIP, our model achieves a significant performance gain,\nshowing that our method is compatible with popular pre-training models. Code\nand data are available at https://github.com/HuiGuanLab/nrccr.",
          "arxiv_id": "2208.12526v1"
        },
        {
          "title": "ChinaOpen: A Dataset for Open-world Multimodal Learning",
          "year": "2023-05",
          "abstract": "This paper introduces ChinaOpen, a dataset sourced from Bilibili, a popular\nChinese video-sharing website, for open-world multimodal learning. While the\nstate-of-the-art multimodal learning networks have shown impressive performance\nin automated video annotation and cross-modal video retrieval, their training\nand evaluation are primarily conducted on YouTube videos with English text.\nTheir effectiveness on Chinese data remains to be verified. In order to support\nmultimodal learning in the new context, we construct ChinaOpen-50k, a webly\nannotated training set of 50k Bilibili videos associated with user-generated\ntitles and tags. Both text-based and content-based data cleaning are performed\nto remove low-quality videos in advance. For a multi-faceted evaluation, we\nbuild ChinaOpen-1k, a manually labeled test set of 1k videos. Each test video\nis accompanied with a manually checked user title and a manually written\ncaption. Besides, each video is manually tagged to describe objects / actions /\nscenes shown in the visual content. The original user tags are also manually\nchecked. Moreover, with all the Chinese text translated into English,\nChinaOpen-1k is also suited for evaluating models trained on English data. In\naddition to ChinaOpen, we propose Generative Video-to-text Transformer (GVT)\nfor Chinese video captioning. We conduct an extensive evaluation of the\nstate-of-the-art single-task / multi-task models on the new dataset, resulting\nin a number of novel findings and insights.",
          "arxiv_id": "2305.05880v2"
        },
        {
          "title": "MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions",
          "year": "2024-07",
          "abstract": "Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.",
          "arxiv_id": "2407.20962v3"
        }
      ],
      "1": [
        {
          "title": "LSB Based Non Blind Predictive Edge Adaptive Image Steganography",
          "year": "2022-01",
          "abstract": "Image steganography is the art of hiding secret message in grayscale or color\nimages. Easy detection of secret message for any state-of-art image\nsteganography can break the stego system. To prevent the breakdown of the stego\nsystem data is embedded in the selected area of an image which reduces the\nprobability of detection. Most of the existing adaptive image steganography\ntechniques achieve low embedding capacity. In this paper a high capacity\nPredictive Edge Adaptive image steganography technique is proposed where\nselective area of cover image is predicted using Modified Median Edge Detector\n(MMED) predictor to embed the binary payload (data). The cover image used to\nembed the payload is a grayscale image. Experimental results show that the\nproposed scheme achieves better embedding capacity with minimum level of\ndistortion and higher level of security. The proposed scheme is compared with\nthe existing image steganography schemes. Results show that the proposed scheme\nachieves better embedding rate with lower level of distortion.",
          "arxiv_id": "2201.01277v1"
        },
        {
          "title": "An Automated and Robust Image Watermarking Scheme Based on Deep Neural Networks",
          "year": "2020-07",
          "abstract": "Digital image watermarking is the process of embedding and extracting a\nwatermark covertly on a cover-image. To dynamically adapt image watermarking\nalgorithms, deep learning-based image watermarking schemes have attracted\nincreased attention during recent years. However, existing deep learning-based\nwatermarking methods neither fully apply the fitting ability to learn and\nautomate the embedding and extracting algorithms, nor achieve the properties of\nrobustness and blindness simultaneously. In this paper, a robust and blind\nimage watermarking scheme based on deep learning neural networks is proposed.\nTo minimize the requirement of domain knowledge, the fitting ability of deep\nneural networks is exploited to learn and generalize an automated image\nwatermarking algorithm. A deep learning architecture is specially designed for\nimage watermarking tasks, which will be trained in an unsupervised manner to\navoid human intervention and annotation. To facilitate flexible applications,\nthe robustness of the proposed scheme is achieved without requiring any prior\nknowledge or adversarial examples of possible attacks. A challenging case of\nwatermark extraction from phone camera-captured images demonstrates the\nrobustness and practicality of the proposal. The experiments, evaluation, and\napplication cases confirm the superiority of the proposed scheme.",
          "arxiv_id": "2007.02460v1"
        },
        {
          "title": "Robust Watermarking using Diffusion of Logo into Autoencoder Feature Maps",
          "year": "2021-05",
          "abstract": "Digital contents have grown dramatically in recent years, leading to\nincreased attention to copyright. Image watermarking has been considered one of\nthe most popular methods for copyright protection. With the recent advancements\nin applying deep neural networks in image processing, these networks have also\nbeen used in image watermarking. Robustness and imperceptibility are two\nchallenging features of watermarking methods that the trade-off between them\nshould be satisfied. In this paper, we propose to use an end-to-end network for\nwatermarking. We use a convolutional neural network (CNN) to control the\nembedding strength based on the image content. Dynamic embedding helps the\nnetwork to have the lowest effect on the visual quality of the watermarked\nimage. Different image processing attacks are simulated as a network layer to\nimprove the robustness of the model. Our method is a blind watermarking\napproach that replicates the watermark string to create a matrix of the same\nsize as the input image. Instead of diffusing the watermark data into the input\nimage, we inject the data into the feature space and force the network to do\nthis in regions that increase the robustness against various attacks.\nExperimental results show the superiority of the proposed method in terms of\nimperceptibility and robustness compared to the state-of-the-art algorithms.",
          "arxiv_id": "2105.11095v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:18:39Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}