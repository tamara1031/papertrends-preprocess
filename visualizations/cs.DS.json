{
  "topics": {
    "data": {
      "0": {
        "name": "0_graph_graphs_time_problem",
        "keywords": [
          [
            "graph",
            0.02220670422678416
          ],
          [
            "graphs",
            0.021605034974643524
          ],
          [
            "time",
            0.016266429102004574
          ],
          [
            "problem",
            0.015745932461415608
          ],
          [
            "algorithm",
            0.014972551806000049
          ],
          [
            "vertex",
            0.013221629729471916
          ],
          [
            "edge",
            0.012157750312547243
          ],
          [
            "vertices",
            0.010762967187434984
          ],
          [
            "algorithms",
            0.009887843612301524
          ],
          [
            "approximation",
            0.009421922582458211
          ]
        ],
        "count": 2737
      },
      "1": {
        "name": "1_string_time_strings_length",
        "keywords": [
          [
            "string",
            0.02806528985324624
          ],
          [
            "time",
            0.023045362719981574
          ],
          [
            "strings",
            0.020716372756110314
          ],
          [
            "length",
            0.02045535617012495
          ],
          [
            "pattern",
            0.015194896107901864
          ],
          [
            "space",
            0.013794619496468484
          ],
          [
            "distance",
            0.013586403117517234
          ],
          [
            "algorithm",
            0.01323110122179105
          ],
          [
            "edit",
            0.012918209313821535
          ],
          [
            "problem",
            0.012887711300444304
          ]
        ],
        "count": 731
      },
      "2": {
        "name": "2_data_sorting_queries_tree",
        "keywords": [
          [
            "data",
            0.023663935563734327
          ],
          [
            "sorting",
            0.016658346144577948
          ],
          [
            "queries",
            0.013519379846732722
          ],
          [
            "tree",
            0.01258454496543335
          ],
          [
            "time",
            0.01245771591211771
          ],
          [
            "structures",
            0.012274807255032376
          ],
          [
            "trees",
            0.01221054621678356
          ],
          [
            "data structures",
            0.011944627378303775
          ],
          [
            "space",
            0.01183966265444004
          ],
          [
            "structure",
            0.011807402171223932
          ]
        ],
        "count": 592
      },
      "3": {
        "name": "3_matrix_rank_algorithm_regression",
        "keywords": [
          [
            "matrix",
            0.021360457505578908
          ],
          [
            "rank",
            0.01311024270465493
          ],
          [
            "algorithm",
            0.01301163500873757
          ],
          [
            "regression",
            0.011766610598169993
          ],
          [
            "Gaussian",
            0.011035233708359498
          ],
          [
            "low",
            0.010347898574340552
          ],
          [
            "matrices",
            0.010057313281092228
          ],
          [
            "linear",
            0.009940553849106004
          ],
          [
            "algorithms",
            0.009760427941508571
          ],
          [
            "error",
            0.00930468846550556
          ]
        ],
        "count": 484
      },
      "4": {
        "name": "4_quantum_Quantum_classical_state",
        "keywords": [
          [
            "quantum",
            0.08483078974738412
          ],
          [
            "Quantum",
            0.026703622310789044
          ],
          [
            "classical",
            0.022205901041382953
          ],
          [
            "state",
            0.017172387755941595
          ],
          [
            "algorithm",
            0.016093022980268646
          ],
          [
            "algorithms",
            0.014602571367962329
          ],
          [
            "states",
            0.014041067946046523
          ],
          [
            "quantum algorithms",
            0.012927622938621864
          ],
          [
            "quantum algorithm",
            0.012193047862236125
          ],
          [
            "circuit",
            0.011981544219037498
          ]
        ],
        "count": 472
      },
      "5": {
        "name": "5_online_competitive_algorithm_problem",
        "keywords": [
          [
            "online",
            0.03929093163905574
          ],
          [
            "competitive",
            0.02458361251139299
          ],
          [
            "algorithm",
            0.017653497876393247
          ],
          [
            "problem",
            0.01564436466720172
          ],
          [
            "ratio",
            0.015617737823762799
          ],
          [
            "matching",
            0.015564436834946092
          ],
          [
            "competitive ratio",
            0.015305531599253753
          ],
          [
            "Online",
            0.015024007410003913
          ],
          [
            "optimal",
            0.014321968478883373
          ],
          [
            "regret",
            0.013807129722261256
          ]
        ],
        "count": 355
      },
      "6": {
        "name": "6_random_degree_graphs_mixing",
        "keywords": [
          [
            "random",
            0.020293958420377983
          ],
          [
            "degree",
            0.01736790248115628
          ],
          [
            "graphs",
            0.01712051780874576
          ],
          [
            "mixing",
            0.016661769068805475
          ],
          [
            "model",
            0.016204987961591123
          ],
          [
            "graph",
            0.01500648268193935
          ],
          [
            "dynamics",
            0.01457201273200716
          ],
          [
            "spectral",
            0.014148508090615197
          ],
          [
            "sampling",
            0.01381004876549934
          ],
          [
            "Glauber",
            0.011964935974713247
          ]
        ],
        "count": 353
      },
      "7": {
        "name": "7_privacy_private_DP_differential privacy",
        "keywords": [
          [
            "privacy",
            0.058962371699272806
          ],
          [
            "private",
            0.03830244296692429
          ],
          [
            "DP",
            0.026915695296446916
          ],
          [
            "differential privacy",
            0.024972994753176825
          ],
          [
            "differential",
            0.024534442979541226
          ],
          [
            "data",
            0.018438979976446643
          ],
          [
            "Private",
            0.017670432433324907
          ],
          [
            "mechanism",
            0.015979403961474292
          ],
          [
            "error",
            0.014114814805645692
          ],
          [
            "Privacy",
            0.01309548647289368
          ]
        ],
        "count": 349
      },
      "8": {
        "name": "8_learning_testing_distribution_sample",
        "keywords": [
          [
            "learning",
            0.03502286523409971
          ],
          [
            "testing",
            0.02182315965644228
          ],
          [
            "distribution",
            0.02122212335919401
          ],
          [
            "sample",
            0.01614556905815925
          ],
          [
            "complexity",
            0.015703999792289338
          ],
          [
            "samples",
            0.013463448370299662
          ],
          [
            "distributions",
            0.01272287334365249
          ],
          [
            "algorithm",
            0.012672807192638217
          ],
          [
            "PAC",
            0.012142847054156405
          ],
          [
            "sample complexity",
            0.012126587722852144
          ]
        ],
        "count": 329
      },
      "9": {
        "name": "9_clustering_approximation_means_center",
        "keywords": [
          [
            "clustering",
            0.03657272271467076
          ],
          [
            "approximation",
            0.024569615847308993
          ],
          [
            "means",
            0.022330478605954263
          ],
          [
            "center",
            0.019368557844950087
          ],
          [
            "points",
            0.019013978726137318
          ],
          [
            "median",
            0.018609342831090163
          ],
          [
            "Clustering",
            0.01662663759521091
          ],
          [
            "algorithm",
            0.015756007782050184
          ],
          [
            "problem",
            0.015510119631571253
          ],
          [
            "metric",
            0.014160964091009549
          ]
        ],
        "count": 326
      },
      "10": {
        "name": "10_graph_graphs_algorithm_algorithms",
        "keywords": [
          [
            "graph",
            0.028198103948860236
          ],
          [
            "graphs",
            0.017879798220471865
          ],
          [
            "algorithm",
            0.014096986604244287
          ],
          [
            "algorithms",
            0.013804531632121989
          ],
          [
            "subgraph",
            0.013062075861281355
          ],
          [
            "real",
            0.012771662749909382
          ],
          [
            "networks",
            0.012482536211976817
          ],
          [
            "parallel",
            0.011276889701461583
          ],
          [
            "world",
            0.010773878917656758
          ],
          [
            "large",
            0.01050819900573528
          ]
        ],
        "count": 296
      },
      "11": {
        "name": "11_algorithm_time_Subset_Sum",
        "keywords": [
          [
            "algorithm",
            0.022513409717593098
          ],
          [
            "time",
            0.01859770999445936
          ],
          [
            "Subset",
            0.01771106066343794
          ],
          [
            "Sum",
            0.016782511559880184
          ],
          [
            "problem",
            0.01240549659413359
          ],
          [
            "matrix",
            0.011631075990903855
          ],
          [
            "polynomial",
            0.01058443377360783
          ],
          [
            "integers",
            0.01056296469686506
          ],
          [
            "algorithms",
            0.010150463473556722
          ],
          [
            "multiplication",
            0.009313750284116854
          ]
        ],
        "count": 245
      },
      "12": {
        "name": "12_time_robots_problem_agent",
        "keywords": [
          [
            "time",
            0.02218586208929317
          ],
          [
            "robots",
            0.018741504822628402
          ],
          [
            "problem",
            0.017457330775497142
          ],
          [
            "agent",
            0.016856053833598224
          ],
          [
            "algorithm",
            0.016440707004192864
          ],
          [
            "set",
            0.0157802057267347
          ],
          [
            "points",
            0.014760179347552752
          ],
          [
            "plane",
            0.011987262636293107
          ],
          [
            "graph",
            0.011130559063284928
          ],
          [
            "point",
            0.010387750534536425
          ]
        ],
        "count": 226
      },
      "13": {
        "name": "13_jobs_scheduling_machines_job",
        "keywords": [
          [
            "jobs",
            0.051809503037869756
          ],
          [
            "scheduling",
            0.047237455949950734
          ],
          [
            "machines",
            0.03377060351455174
          ],
          [
            "job",
            0.029958311250468456
          ],
          [
            "time",
            0.024399252171286558
          ],
          [
            "Scheduling",
            0.023355604263714032
          ],
          [
            "machine",
            0.022562832453427356
          ],
          [
            "makespan",
            0.022452785431056378
          ],
          [
            "problem",
            0.021286939554332168
          ],
          [
            "processing",
            0.0192590645129247
          ]
        ],
        "count": 208
      },
      "14": {
        "name": "14_agents_stable_matching_allocation",
        "keywords": [
          [
            "agents",
            0.045480332995099314
          ],
          [
            "stable",
            0.037067228570108426
          ],
          [
            "matching",
            0.031662192729914655
          ],
          [
            "allocation",
            0.024685957495946535
          ],
          [
            "problem",
            0.02162193606131557
          ],
          [
            "stable matching",
            0.02014637647417255
          ],
          [
            "matchings",
            0.018841809194673747
          ],
          [
            "allocations",
            0.018656163305485897
          ],
          [
            "agent",
            0.018189812837219602
          ],
          [
            "preferences",
            0.017781901327969474
          ]
        ],
        "count": 196
      },
      "15": {
        "name": "15_submodular_maximization_Submodular_monotone",
        "keywords": [
          [
            "submodular",
            0.05977443843128605
          ],
          [
            "maximization",
            0.029583098771247946
          ],
          [
            "Submodular",
            0.02916767637328922
          ],
          [
            "monotone",
            0.026700680402998837
          ],
          [
            "function",
            0.02503942856591392
          ],
          [
            "approximation",
            0.02469053588171689
          ],
          [
            "functions",
            0.022792717808273
          ],
          [
            "submodular maximization",
            0.020880412660059695
          ],
          [
            "constraint",
            0.019158481010631154
          ],
          [
            "submodular function",
            0.018971137456904767
          ]
        ],
        "count": 193
      },
      "16": {
        "name": "16_stream_data_streaming_space",
        "keywords": [
          [
            "stream",
            0.028455821336224315
          ],
          [
            "data",
            0.02427304518793303
          ],
          [
            "streaming",
            0.022670575668452916
          ],
          [
            "space",
            0.021190922148824953
          ],
          [
            "sketch",
            0.01789841196202233
          ],
          [
            "estimation",
            0.016725465623899205
          ],
          [
            "sketches",
            0.015341269649899876
          ],
          [
            "streams",
            0.015119665819346257
          ],
          [
            "algorithm",
            0.013695565292351516
          ],
          [
            "algorithms",
            0.01360667830115814
          ]
        ],
        "count": 177
      },
      "17": {
        "name": "17_temporal_Temporal_time_network",
        "keywords": [
          [
            "temporal",
            0.07705763560594182
          ],
          [
            "Temporal",
            0.026630281872003947
          ],
          [
            "time",
            0.022439540140678466
          ],
          [
            "network",
            0.020541623325508622
          ],
          [
            "temporal graphs",
            0.01982396241724755
          ],
          [
            "graphs",
            0.01943099768710946
          ],
          [
            "temporal graph",
            0.017850775162281788
          ],
          [
            "graph",
            0.017400992559289667
          ],
          [
            "networks",
            0.016762569583239125
          ],
          [
            "edges",
            0.014492432565878115
          ]
        ],
        "count": 138
      },
      "18": {
        "name": "18_SAT_CSPs_CSP_Boolean",
        "keywords": [
          [
            "SAT",
            0.04959746143873951
          ],
          [
            "CSPs",
            0.022705061486181902
          ],
          [
            "CSP",
            0.021140252208521604
          ],
          [
            "Boolean",
            0.017800685312734085
          ],
          [
            "problems",
            0.01725891885253169
          ],
          [
            "variables",
            0.016439091207922224
          ],
          [
            "problem",
            0.015539381708441317
          ],
          [
            "constraints",
            0.014898978187330427
          ],
          [
            "formula",
            0.014731533880439652
          ],
          [
            "constraint",
            0.013767887700965882
          ]
        ],
        "count": 133
      },
      "19": {
        "name": "19_search_nearest_neighbor_data",
        "keywords": [
          [
            "search",
            0.03687479441762968
          ],
          [
            "nearest",
            0.029526800932179142
          ],
          [
            "neighbor",
            0.025647949732987507
          ],
          [
            "data",
            0.0245958013388593
          ],
          [
            "query",
            0.022658776407175143
          ],
          [
            "LSH",
            0.0206297894459443
          ],
          [
            "nearest neighbor",
            0.020028384088367403
          ],
          [
            "neighbor search",
            0.017927204102116095
          ],
          [
            "Nearest",
            0.01671268892304077
          ],
          [
            "Search",
            0.01456240190922207
          ]
        ],
        "count": 119
      },
      "20": {
        "name": "20_streaming_space_pass_passes",
        "keywords": [
          [
            "streaming",
            0.04913777383028607
          ],
          [
            "space",
            0.03476735863378034
          ],
          [
            "pass",
            0.027042867068618703
          ],
          [
            "passes",
            0.019654897526341378
          ],
          [
            "algorithms",
            0.019007767845634307
          ],
          [
            "semi",
            0.017384635517194352
          ],
          [
            "graph",
            0.01711963902081487
          ],
          [
            "algorithm",
            0.01626375535022542
          ],
          [
            "streaming model",
            0.015241153466532723
          ],
          [
            "matching",
            0.014985737289099792
          ]
        ],
        "count": 117
      }
    },
    "correlations": [
      [
        1.0,
        -0.5046411968004759,
        -0.6641242055577539,
        -0.6778924183493121,
        -0.7217111245540236,
        -0.3309079956135951,
        -0.47081383737117155,
        -0.7354598648519848,
        -0.619826090992269,
        -0.5949721776458237,
        -0.1429773346662284,
        0.028362494061151232,
        0.0027084679423732046,
        -0.5815123101579088,
        -0.22192827099207207,
        -0.6228080420767927,
        -0.6910531655161504,
        -0.5876837051581435,
        -0.7330310426003461,
        -0.6837441694537914,
        -0.6880345351486437
      ],
      [
        -0.5046411968004759,
        1.0,
        -0.6540748774950988,
        -0.7409673146078564,
        -0.7358909181330069,
        -0.6557321865673602,
        -0.709214344374639,
        -0.750644031673406,
        -0.7199334515665577,
        -0.7015903255419311,
        -0.6948333669455831,
        -0.45368558332715037,
        -0.3942814569838542,
        -0.5417261201573871,
        -0.6580231583095029,
        -0.7193022316465509,
        -0.7002734370362839,
        -0.5056976254136811,
        -0.7521960452407981,
        -0.6982271625069763,
        -0.6962503410426273
      ],
      [
        -0.6641242055577539,
        -0.6540748774950988,
        1.0,
        -0.7374147090848215,
        -0.7488500714944528,
        -0.6968502099200387,
        -0.7173462396043391,
        -0.7379942459876125,
        -0.7177693269775576,
        -0.7112331133974299,
        -0.6997987766377777,
        -0.6749900652756975,
        -0.5256031270712458,
        -0.7103642784539441,
        -0.7107459807168419,
        -0.7376835036478683,
        -0.5136685414343355,
        -0.7129561639221014,
        -0.7635999311252133,
        -0.4287154551205591,
        -0.6919335751574212
      ],
      [
        -0.6778924183493121,
        -0.7409673146078564,
        -0.7374147090848215,
        1.0,
        -0.7379314263356447,
        -0.6808056349470513,
        -0.7367535670874,
        -0.7496343907759093,
        -0.6526714027441112,
        -0.733565707661376,
        -0.7440592379714696,
        -0.6524002513052575,
        -0.719782075524106,
        -0.7486185156134479,
        -0.7361545389750314,
        -0.7352641652141306,
        -0.7365762456085874,
        -0.7502661448732784,
        -0.7539610388614213,
        -0.7498017264320309,
        -0.7448602272874221
      ],
      [
        -0.7217111245540236,
        -0.7358909181330069,
        -0.7488500714944528,
        -0.7379314263356447,
        1.0,
        -0.7345581373601415,
        -0.7400768953158711,
        -0.7585410725821292,
        -0.7244507697617656,
        -0.750296366636279,
        -0.7449406979031834,
        -0.7215232781780688,
        -0.734244727591244,
        -0.7544756418214031,
        -0.74109509420006,
        -0.7527170311850949,
        -0.757837350582051,
        -0.7538651800230709,
        -0.7462728637735522,
        -0.7379928983680386,
        -0.7538258094111199
      ],
      [
        -0.3309079956135951,
        -0.6557321865673602,
        -0.6968502099200387,
        -0.6808056349470513,
        -0.7345581373601415,
        1.0,
        -0.6717560039458712,
        -0.7350138723608253,
        -0.48232969097903927,
        -0.6595857690369464,
        -0.6776259519052197,
        -0.13210516912525996,
        -0.5579976035707989,
        -0.6361659856782274,
        -0.5904110087820116,
        -0.6635243959027779,
        -0.7022608429891002,
        -0.7031512951188323,
        -0.7541497954833243,
        -0.7165126962206656,
        -0.7123276731611838
      ],
      [
        -0.47081383737117155,
        -0.709214344374639,
        -0.7173462396043391,
        -0.7367535670874,
        -0.7400768953158711,
        -0.6717560039458712,
        1.0,
        -0.7487116546828516,
        -0.6823368415624222,
        -0.7118986507318739,
        -0.44248073048704584,
        -0.6615172591536729,
        -0.647668227821263,
        -0.7320183919599577,
        -0.6805965083850938,
        -0.7167157525001979,
        -0.7279458384178957,
        -0.7252446157039447,
        -0.7526702309815021,
        -0.7300139381631612,
        -0.7344042280279243
      ],
      [
        -0.7354598648519848,
        -0.750644031673406,
        -0.7379942459876125,
        -0.7496343907759093,
        -0.7585410725821292,
        -0.7350138723608253,
        -0.7487116546828516,
        1.0,
        -0.7048023680050702,
        -0.7458402127565977,
        -0.7481533202857935,
        -0.7313089275484241,
        -0.7409339677824001,
        -0.7573551347962709,
        -0.7461643899194235,
        -0.7441811498513285,
        -0.7006980076446603,
        -0.7525020310270409,
        -0.7637945727502338,
        -0.7391046436771745,
        -0.7429383922633507
      ],
      [
        -0.619826090992269,
        -0.7199334515665577,
        -0.7177693269775576,
        -0.6526714027441112,
        -0.7244507697617656,
        -0.48232969097903927,
        -0.6823368415624222,
        -0.7048023680050702,
        1.0,
        -0.7125224255155369,
        -0.7208610614048815,
        -0.6215543792347438,
        -0.6692226854349914,
        -0.727562294694624,
        -0.6856889728185886,
        -0.7041691402146226,
        -0.721780836409905,
        -0.7397662589521041,
        -0.7581642902815279,
        -0.7199493181551548,
        -0.7287864867448608
      ],
      [
        -0.5949721776458237,
        -0.7015903255419311,
        -0.7112331133974299,
        -0.733565707661376,
        -0.750296366636279,
        -0.6595857690369464,
        -0.7118986507318739,
        -0.7458402127565977,
        -0.7125224255155369,
        1.0,
        -0.6880799031717422,
        -0.6365867468569266,
        -0.5909252153063551,
        -0.7096149155195302,
        -0.6409151025836475,
        -0.4595313033477111,
        -0.7028113833451626,
        -0.7276850926222833,
        -0.752814032036811,
        -0.7059787173946991,
        -0.7039141431198628
      ],
      [
        -0.1429773346662284,
        -0.6948333669455831,
        -0.6997987766377777,
        -0.7440592379714696,
        -0.7449406979031834,
        -0.6776259519052197,
        -0.44248073048704584,
        -0.7481533202857935,
        -0.7208610614048815,
        -0.6880799031717422,
        1.0,
        -0.6341005987088577,
        -0.6160194291285873,
        -0.720953583079631,
        -0.6513669359418383,
        -0.7043356400369247,
        -0.7201498460883003,
        -0.6924734066881111,
        -0.7533584593943814,
        -0.7012862817346318,
        -0.7163861185913163
      ],
      [
        0.028362494061151232,
        -0.45368558332715037,
        -0.6749900652756975,
        -0.6524002513052575,
        -0.7215232781780688,
        -0.13210516912525996,
        -0.6615172591536729,
        -0.7313089275484241,
        -0.6215543792347438,
        -0.6365867468569266,
        -0.6341005987088577,
        1.0,
        -0.37023950722304433,
        -0.5406658453461947,
        -0.6004715981971562,
        -0.6601355575775572,
        -0.689667068527835,
        -0.5432184059903191,
        -0.7383558737095941,
        -0.6990779240160501,
        -0.6966421702948906
      ],
      [
        0.0027084679423732046,
        -0.3942814569838542,
        -0.5256031270712458,
        -0.719782075524106,
        -0.734244727591244,
        -0.5579976035707989,
        -0.647668227821263,
        -0.7409339677824001,
        -0.6692226854349914,
        -0.5909252153063551,
        -0.6160194291285873,
        -0.37023950722304433,
        1.0,
        -0.5163097807294659,
        -0.03155377270479621,
        -0.639881890520832,
        -0.6958709097467994,
        -0.5197761723360336,
        -0.7388749067272025,
        -0.6771974603335498,
        -0.6956373796150717
      ],
      [
        -0.5815123101579088,
        -0.5417261201573871,
        -0.7103642784539441,
        -0.7486185156134479,
        -0.7544756418214031,
        -0.6361659856782274,
        -0.7320183919599577,
        -0.7573551347962709,
        -0.727562294694624,
        -0.7096149155195302,
        -0.720953583079631,
        -0.5406658453461947,
        -0.5163097807294659,
        1.0,
        -0.6820497650021959,
        -0.711500550067466,
        -0.7368117498372018,
        -0.49567994684163597,
        -0.7583977120809174,
        -0.7315982053853893,
        -0.7384815714019557
      ],
      [
        -0.22192827099207207,
        -0.6580231583095029,
        -0.7107459807168419,
        -0.7361545389750314,
        -0.74109509420006,
        -0.5904110087820116,
        -0.6805965083850938,
        -0.7461643899194235,
        -0.6856889728185886,
        -0.6409151025836475,
        -0.6513669359418383,
        -0.6004715981971562,
        -0.03155377270479621,
        -0.6820497650021959,
        1.0,
        -0.6601938334654169,
        -0.7232309615585575,
        -0.7077473687971554,
        -0.7469074984594255,
        -0.7092217260652798,
        -0.7234852492231234
      ],
      [
        -0.6228080420767927,
        -0.7193022316465509,
        -0.7376835036478683,
        -0.7352641652141306,
        -0.7527170311850949,
        -0.6635243959027779,
        -0.7167157525001979,
        -0.7441811498513285,
        -0.7041691402146226,
        -0.4595313033477111,
        -0.7043356400369247,
        -0.6601355575775572,
        -0.639881890520832,
        -0.711500550067466,
        -0.6601938334654169,
        1.0,
        -0.7282072203294303,
        -0.7311211462672347,
        -0.7541705091276556,
        -0.7316586463630192,
        -0.7069193448017963
      ],
      [
        -0.6910531655161504,
        -0.7002734370362839,
        -0.5136685414343355,
        -0.7365762456085874,
        -0.757837350582051,
        -0.7022608429891002,
        -0.7279458384178957,
        -0.7006980076446603,
        -0.721780836409905,
        -0.7028113833451626,
        -0.7201498460883003,
        -0.689667068527835,
        -0.6958709097467994,
        -0.7368117498372018,
        -0.7232309615585575,
        -0.7282072203294303,
        1.0,
        -0.7278133738631638,
        -0.7545387042304301,
        -0.5383334711535491,
        -0.18994091927921708
      ],
      [
        -0.5876837051581435,
        -0.5056976254136811,
        -0.7129561639221014,
        -0.7502661448732784,
        -0.7538651800230709,
        -0.7031512951188323,
        -0.7252446157039447,
        -0.7525020310270409,
        -0.7397662589521041,
        -0.7276850926222833,
        -0.6924734066881111,
        -0.5432184059903191,
        -0.5197761723360336,
        -0.49567994684163597,
        -0.7077473687971554,
        -0.7311211462672347,
        -0.7278133738631638,
        1.0,
        -0.7584004935238199,
        -0.7263740372108132,
        -0.732899705089358
      ],
      [
        -0.7330310426003461,
        -0.7521960452407981,
        -0.7635999311252133,
        -0.7539610388614213,
        -0.7462728637735522,
        -0.7541497954833243,
        -0.7526702309815021,
        -0.7637945727502338,
        -0.7581642902815279,
        -0.752814032036811,
        -0.7533584593943814,
        -0.7383558737095941,
        -0.7388749067272025,
        -0.7583977120809174,
        -0.7469074984594255,
        -0.7541705091276556,
        -0.7545387042304301,
        -0.7584004935238199,
        1.0,
        -0.7591662904711263,
        -0.7447261739267639
      ],
      [
        -0.6837441694537914,
        -0.6982271625069763,
        -0.4287154551205591,
        -0.7498017264320309,
        -0.7379928983680386,
        -0.7165126962206656,
        -0.7300139381631612,
        -0.7391046436771745,
        -0.7199493181551548,
        -0.7059787173946991,
        -0.7012862817346318,
        -0.6990779240160501,
        -0.6771974603335498,
        -0.7315982053853893,
        -0.7092217260652798,
        -0.7316586463630192,
        -0.5383334711535491,
        -0.7263740372108132,
        -0.7591662904711263,
        1.0,
        -0.722838337897715
      ],
      [
        -0.6880345351486437,
        -0.6962503410426273,
        -0.6919335751574212,
        -0.7448602272874221,
        -0.7538258094111199,
        -0.7123276731611838,
        -0.7344042280279243,
        -0.7429383922633507,
        -0.7287864867448608,
        -0.7039141431198628,
        -0.7163861185913163,
        -0.6966421702948906,
        -0.6956373796150717,
        -0.7384815714019557,
        -0.7234852492231234,
        -0.7069193448017963,
        -0.18994091927921708,
        -0.732899705089358,
        -0.7447261739267639,
        -0.722838337897715,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        46,
        5,
        12,
        0,
        6,
        5,
        3,
        2,
        4,
        4,
        4,
        7,
        4,
        10,
        2,
        4,
        0,
        4,
        1,
        3,
        4
      ],
      "2020-02": [
        75,
        14,
        11,
        7,
        5,
        14,
        7,
        6,
        8,
        21,
        4,
        10,
        12,
        4,
        4,
        11,
        0,
        1,
        2,
        5,
        10
      ],
      "2020-03": [
        44,
        13,
        7,
        1,
        3,
        5,
        3,
        7,
        4,
        9,
        7,
        7,
        8,
        5,
        4,
        4,
        0,
        2,
        1,
        3,
        6
      ],
      "2020-04": [
        76,
        15,
        8,
        4,
        8,
        7,
        3,
        3,
        10,
        16,
        6,
        11,
        6,
        7,
        6,
        8,
        5,
        6,
        3,
        5,
        5
      ],
      "2020-05": [
        50,
        15,
        9,
        3,
        6,
        11,
        3,
        2,
        6,
        9,
        5,
        9,
        5,
        8,
        9,
        4,
        1,
        0,
        1,
        6,
        3
      ],
      "2020-06": [
        58,
        12,
        8,
        1,
        3,
        14,
        10,
        5,
        18,
        17,
        13,
        12,
        3,
        4,
        5,
        16,
        0,
        2,
        0,
        4,
        2
      ],
      "2020-07": [
        73,
        14,
        11,
        3,
        7,
        13,
        7,
        4,
        7,
        20,
        11,
        17,
        8,
        6,
        8,
        8,
        3,
        5,
        2,
        2,
        9
      ],
      "2020-08": [
        60,
        7,
        7,
        3,
        2,
        11,
        11,
        3,
        4,
        12,
        10,
        11,
        6,
        1,
        3,
        3,
        2,
        0,
        3,
        3,
        1
      ],
      "2020-09": [
        54,
        6,
        10,
        5,
        8,
        10,
        5,
        8,
        5,
        12,
        6,
        6,
        6,
        2,
        4,
        5,
        0,
        2,
        2,
        4,
        8
      ],
      "2020-10": [
        53,
        4,
        13,
        4,
        5,
        15,
        5,
        16,
        7,
        7,
        7,
        18,
        5,
        3,
        2,
        10,
        0,
        5,
        1,
        5,
        11
      ],
      "2020-11": [
        65,
        9,
        8,
        4,
        7,
        22,
        8,
        4,
        10,
        15,
        6,
        16,
        7,
        7,
        7,
        5,
        4,
        1,
        6,
        3,
        7
      ],
      "2020-12": [
        54,
        8,
        6,
        1,
        5,
        15,
        4,
        5,
        8,
        14,
        5,
        8,
        3,
        5,
        1,
        3,
        1,
        1,
        1,
        6,
        2
      ],
      "2021-01": [
        42,
        6,
        6,
        4,
        2,
        5,
        2,
        1,
        3,
        6,
        3,
        5,
        4,
        2,
        3,
        2,
        1,
        5,
        0,
        5,
        5
      ],
      "2021-02": [
        54,
        9,
        6,
        3,
        7,
        18,
        10,
        7,
        9,
        11,
        5,
        7,
        6,
        3,
        5,
        15,
        0,
        3,
        2,
        2,
        9
      ],
      "2021-03": [
        44,
        4,
        3,
        1,
        10,
        10,
        2,
        4,
        1,
        8,
        2,
        8,
        2,
        3,
        3,
        7,
        0,
        1,
        0,
        3,
        5
      ],
      "2021-04": [
        37,
        1,
        6,
        1,
        0,
        7,
        3,
        3,
        1,
        8,
        7,
        12,
        6,
        3,
        3,
        6,
        0,
        1,
        1,
        3,
        4
      ],
      "2021-05": [
        70,
        14,
        8,
        3,
        6,
        8,
        9,
        3,
        7,
        12,
        2,
        16,
        4,
        6,
        7,
        7,
        0,
        4,
        1,
        5,
        10
      ],
      "2021-06": [
        58,
        7,
        6,
        3,
        6,
        11,
        10,
        13,
        9,
        27,
        5,
        15,
        4,
        8,
        8,
        9,
        4,
        3,
        2,
        6,
        7
      ],
      "2021-07": [
        55,
        9,
        12,
        5,
        4,
        15,
        4,
        1,
        11,
        20,
        4,
        11,
        6,
        6,
        6,
        8,
        2,
        0,
        5,
        5,
        12
      ],
      "2021-08": [
        39,
        6,
        8,
        1,
        10,
        5,
        3,
        5,
        5,
        10,
        2,
        9,
        4,
        5,
        3,
        7,
        1,
        1,
        2,
        4,
        3
      ],
      "2021-09": [
        35,
        8,
        6,
        0,
        5,
        12,
        5,
        2,
        2,
        4,
        3,
        7,
        5,
        1,
        7,
        4,
        0,
        2,
        2,
        3,
        8
      ],
      "2021-10": [
        43,
        5,
        6,
        3,
        10,
        13,
        5,
        8,
        7,
        10,
        5,
        14,
        5,
        4,
        7,
        6,
        1,
        4,
        3,
        6,
        4
      ],
      "2021-11": [
        64,
        15,
        6,
        4,
        5,
        16,
        8,
        9,
        6,
        19,
        6,
        16,
        5,
        10,
        6,
        15,
        1,
        2,
        3,
        7,
        8
      ],
      "2021-12": [
        38,
        7,
        3,
        1,
        13,
        15,
        2,
        5,
        6,
        22,
        5,
        11,
        7,
        4,
        7,
        3,
        1,
        4,
        2,
        7,
        5
      ],
      "2022-01": [
        52,
        4,
        10,
        1,
        8,
        6,
        1,
        5,
        8,
        2,
        3,
        5,
        1,
        3,
        4,
        9,
        2,
        4,
        2,
        4,
        2
      ],
      "2022-02": [
        39,
        13,
        4,
        5,
        14,
        10,
        4,
        4,
        3,
        16,
        2,
        7,
        5,
        9,
        6,
        3,
        2,
        6,
        1,
        1,
        4
      ],
      "2022-03": [
        46,
        7,
        7,
        3,
        10,
        11,
        5,
        10,
        5,
        8,
        8,
        7,
        3,
        8,
        3,
        5,
        5,
        2,
        0,
        2,
        4
      ],
      "2022-04": [
        46,
        9,
        4,
        5,
        5,
        9,
        5,
        2,
        12,
        11,
        5,
        13,
        7,
        6,
        8,
        9,
        1,
        5,
        1,
        1,
        6
      ],
      "2022-05": [
        45,
        5,
        7,
        2,
        4,
        13,
        7,
        7,
        7,
        11,
        4,
        8,
        5,
        12,
        3,
        5,
        2,
        2,
        4,
        2,
        8
      ],
      "2022-06": [
        43,
        10,
        8,
        3,
        9,
        9,
        3,
        9,
        11,
        11,
        6,
        9,
        6,
        4,
        7,
        7,
        0,
        2,
        2,
        4,
        6
      ],
      "2022-07": [
        58,
        12,
        6,
        2,
        7,
        10,
        2,
        6,
        5,
        10,
        3,
        14,
        2,
        6,
        4,
        6,
        0,
        0,
        4,
        4,
        12
      ],
      "2022-08": [
        58,
        12,
        10,
        4,
        4,
        9,
        4,
        5,
        8,
        14,
        5,
        8,
        5,
        6,
        8,
        6,
        3,
        1,
        2,
        3,
        3
      ],
      "2022-09": [
        58,
        10,
        11,
        4,
        11,
        13,
        5,
        1,
        6,
        9,
        7,
        10,
        4,
        5,
        9,
        9,
        1,
        1,
        0,
        3,
        4
      ],
      "2022-10": [
        52,
        6,
        9,
        2,
        11,
        12,
        2,
        4,
        7,
        12,
        8,
        11,
        3,
        5,
        5,
        11,
        2,
        1,
        2,
        3,
        4
      ],
      "2022-11": [
        70,
        14,
        13,
        7,
        9,
        17,
        7,
        12,
        12,
        9,
        8,
        15,
        4,
        6,
        9,
        10,
        1,
        2,
        5,
        5,
        12
      ],
      "2022-12": [
        45,
        5,
        9,
        3,
        8,
        3,
        2,
        4,
        6,
        5,
        2,
        11,
        2,
        4,
        2,
        4,
        0,
        1,
        1,
        5,
        4
      ],
      "2023-01": [
        41,
        8,
        3,
        1,
        7,
        5,
        1,
        10,
        9,
        11,
        5,
        9,
        7,
        5,
        5,
        3,
        2,
        3,
        1,
        3,
        4
      ],
      "2023-02": [
        55,
        9,
        5,
        2,
        7,
        13,
        4,
        10,
        10,
        12,
        9,
        20,
        5,
        3,
        9,
        7,
        1,
        4,
        2,
        4,
        3
      ],
      "2023-03": [
        49,
        10,
        6,
        2,
        8,
        10,
        2,
        5,
        2,
        8,
        6,
        8,
        5,
        1,
        5,
        8,
        2,
        3,
        1,
        5,
        3
      ],
      "2023-04": [
        56,
        6,
        10,
        6,
        4,
        14,
        8,
        6,
        4,
        14,
        4,
        7,
        6,
        2,
        5,
        7,
        1,
        5,
        1,
        3,
        8
      ],
      "2023-05": [
        56,
        11,
        9,
        2,
        10,
        14,
        8,
        10,
        7,
        16,
        4,
        17,
        6,
        4,
        7,
        11,
        3,
        3,
        0,
        3,
        9
      ],
      "2023-06": [
        47,
        16,
        12,
        5,
        9,
        13,
        2,
        13,
        12,
        10,
        7,
        12,
        4,
        5,
        4,
        6,
        3,
        4,
        2,
        6,
        5
      ],
      "2023-07": [
        87,
        14,
        10,
        4,
        8,
        15,
        13,
        6,
        8,
        11,
        10,
        11,
        6,
        8,
        5,
        10,
        1,
        2,
        3,
        6,
        10
      ],
      "2023-08": [
        63,
        10,
        6,
        4,
        9,
        10,
        6,
        7,
        4,
        22,
        5,
        12,
        4,
        4,
        1,
        9,
        0,
        2,
        1,
        1,
        9
      ],
      "2023-09": [
        50,
        7,
        5,
        4,
        4,
        7,
        4,
        5,
        5,
        17,
        3,
        6,
        3,
        3,
        4,
        8,
        1,
        3,
        3,
        5,
        6
      ],
      "2023-10": [
        65,
        6,
        11,
        0,
        6,
        15,
        2,
        3,
        6,
        20,
        7,
        16,
        5,
        2,
        3,
        6,
        2,
        2,
        4,
        3,
        6
      ],
      "2023-11": [
        71,
        7,
        13,
        9,
        18,
        11,
        4,
        5,
        10,
        9,
        6,
        11,
        3,
        3,
        6,
        11,
        2,
        0,
        4,
        4,
        7
      ],
      "2023-12": [
        41,
        6,
        6,
        1,
        3,
        9,
        4,
        5,
        4,
        10,
        7,
        9,
        6,
        5,
        8,
        6,
        1,
        3,
        3,
        5,
        5
      ],
      "2024-01": [
        35,
        5,
        6,
        0,
        7,
        6,
        2,
        1,
        5,
        7,
        6,
        7,
        2,
        4,
        6,
        9,
        2,
        1,
        3,
        3,
        4
      ],
      "2024-02": [
        76,
        10,
        8,
        2,
        14,
        18,
        5,
        6,
        10,
        12,
        9,
        14,
        5,
        10,
        10,
        7,
        1,
        3,
        2,
        6,
        7
      ],
      "2024-03": [
        40,
        8,
        6,
        5,
        5,
        10,
        5,
        11,
        10,
        10,
        5,
        17,
        10,
        2,
        7,
        6,
        3,
        3,
        3,
        4,
        4
      ],
      "2024-04": [
        66,
        13,
        9,
        2,
        8,
        9,
        5,
        6,
        11,
        9,
        8,
        13,
        5,
        10,
        2,
        9,
        2,
        1,
        1,
        6,
        2
      ],
      "2024-05": [
        52,
        11,
        11,
        2,
        7,
        14,
        7,
        9,
        5,
        14,
        7,
        13,
        4,
        6,
        4,
        11,
        1,
        2,
        0,
        9,
        11
      ],
      "2024-06": [
        47,
        11,
        7,
        2,
        10,
        14,
        5,
        14,
        8,
        13,
        5,
        7,
        7,
        5,
        4,
        7,
        3,
        1,
        3,
        5,
        8
      ],
      "2024-07": [
        54,
        14,
        8,
        4,
        9,
        16,
        8,
        7,
        10,
        12,
        8,
        12,
        4,
        6,
        5,
        10,
        3,
        2,
        0,
        3,
        3
      ],
      "2024-08": [
        48,
        9,
        5,
        3,
        14,
        13,
        6,
        9,
        2,
        6,
        4,
        6,
        6,
        6,
        1,
        6,
        3,
        1,
        1,
        5,
        2
      ],
      "2024-09": [
        48,
        14,
        10,
        0,
        8,
        4,
        7,
        5,
        7,
        6,
        7,
        12,
        7,
        6,
        10,
        9,
        2,
        2,
        2,
        7,
        2
      ],
      "2024-10": [
        70,
        12,
        8,
        5,
        25,
        14,
        9,
        6,
        15,
        14,
        7,
        16,
        7,
        4,
        13,
        5,
        1,
        0,
        1,
        3,
        8
      ],
      "2024-11": [
        59,
        14,
        8,
        4,
        16,
        11,
        9,
        8,
        20,
        18,
        4,
        10,
        4,
        3,
        7,
        6,
        5,
        3,
        4,
        8,
        6
      ],
      "2024-12": [
        45,
        6,
        9,
        2,
        5,
        4,
        2,
        13,
        6,
        14,
        7,
        15,
        2,
        5,
        3,
        5,
        2,
        3,
        3,
        4,
        7
      ],
      "2025-01": [
        34,
        3,
        2,
        1,
        4,
        10,
        2,
        4,
        6,
        9,
        4,
        6,
        3,
        4,
        6,
        4,
        0,
        4,
        2,
        5,
        5
      ],
      "2025-02": [
        59,
        10,
        6,
        2,
        5,
        17,
        8,
        7,
        18,
        8,
        10,
        8,
        6,
        7,
        10,
        6,
        2,
        3,
        1,
        6,
        10
      ],
      "2025-03": [
        55,
        9,
        8,
        1,
        11,
        14,
        2,
        6,
        7,
        11,
        6,
        9,
        7,
        3,
        6,
        3,
        1,
        2,
        0,
        2,
        8
      ],
      "2025-04": [
        71,
        9,
        10,
        3,
        13,
        14,
        5,
        7,
        12,
        20,
        6,
        10,
        5,
        4,
        6,
        5,
        1,
        9,
        3,
        6,
        17
      ],
      "2025-05": [
        57,
        11,
        6,
        6,
        8,
        11,
        12,
        12,
        12,
        6,
        7,
        8,
        0,
        5,
        7,
        9,
        1,
        2,
        1,
        5,
        3
      ],
      "2025-06": [
        72,
        11,
        7,
        2,
        5,
        4,
        5,
        7,
        13,
        12,
        6,
        13,
        8,
        4,
        5,
        6,
        2,
        2,
        2,
        1,
        5
      ],
      "2025-07": [
        66,
        9,
        7,
        5,
        9,
        21,
        4,
        5,
        8,
        19,
        12,
        11,
        6,
        6,
        10,
        13,
        1,
        5,
        4,
        9,
        6
      ],
      "2025-08": [
        48,
        9,
        6,
        4,
        9,
        11,
        3,
        3,
        4,
        11,
        5,
        15,
        7,
        7,
        6,
        7,
        1,
        2,
        3,
        5,
        7
      ],
      "2025-09": [
        27,
        6,
        7,
        1,
        7,
        1,
        2,
        3,
        1,
        3,
        3,
        7,
        2,
        3,
        2,
        4,
        2,
        1,
        1,
        5,
        5
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Partial Vertex Cover on Graphs of Bounded Degeneracy",
          "year": "2022-01",
          "abstract": "In the Partial Vertex Cover (PVC) problem, we are given an $n$-vertex graph\n$G$ and a positive integer $k$, and the objective is to find a vertex subset\n$S$ of size $k$ maximizing the number of edges with at least one end-point in\n$S$. This problem is W[1]-hard on general graphs, but admits a parameterized\nsubexponential time algorithm with running time $2^{O(\\sqrt{k})}n^{O(1)}$ on\nplanar and apex-minor free graphs [Fomin et al. (FSTTCS 2009, IPL 2011)], and a\n$k^{O(k)}n^{O(1)}$ time algorithm on bounded degeneracy graphs [Amini et al.\n(FSTTCS 2009, JCSS 2011)]. Graphs of bounded degeneracy contain many sparse\ngraph classes like planar graphs, $H$-minor free graphs, and bounded tree-width\ngraphs. In this work, we prove the following results:\n  1) There is an algorithm for PVC with running time $2^{O(k)}n^{O(1)}$ on\ngraphs of bounded degeneracy which is an improvement on the previous\n$k^{O(k)}n^{O(1)}$ time algorithm by Amini et al.\n  2) PVC admits a polynomial compression on graphs of bounded degeneracy,\nresolving an open problem posed by Amini et al.",
          "arxiv_id": "2201.03876v1"
        },
        {
          "title": "Path Cover, Hamiltonicity, and Independence Number: An FPT Perspective",
          "year": "2024-03",
          "abstract": "The classic theorem of Gallai and Milgram (1960) asserts that for every graph\nG, the vertex set of G can be partitioned into at most \\alpha(G)\nvertex-disjoint paths, where \\alpha(G) is the maximum size of an independent\nset in G. The proof of Gallai--Milgram's theorem is constructive and yields a\npolynomial-time algorithm that computes a covering of G by at most \\alpha(G)\nvertex-disjoint paths.\n  We prove the following algorithmic extension of Gallai--Milgram's theorem for\nundirected graphs: determining whether an undirected graph can be covered by\nfewer than \\alpha(G) - k vertex-disjoint paths is fixed-parameter tractable\n(FPT) when parameterized by k. More precisely, we provide an algorithm that,\nfor an n-vertex graph G and an integer parameter k \\ge 1, runs in time\n2^{k^{O(k^4)}} \\cdot n^{O(1)}, and outputs a path cover P of G. Furthermore,\nit:\n  - either correctly reports that P is a minimum-size path cover,\n  - or outputs, together with P, an independent set of size |P| + k certifying\nthat P contains at most \\alpha(G) - k paths.\n  A key subroutine in our algorithm is an FPT algorithm, parameterized by\n\\alpha(G), for deciding whether G contains a Hamiltonian path. This result is\nof independent interest -- prior to our work, no polynomial-time algorithm for\ndeciding Hamiltonicity was known even for graphs with independence number at\nmost 3. Moreover, the techniques we develop apply to a wide array of problems\nin undirected graphs, including Hamiltonian Cycle, Path Cover, Largest Linkage,\nand Topological Minor Containment. We show that all these problems are FPT when\nparameterized by the independence number of the graph.\n  Notably, the independence number parameterization, which describes graph's\ndensity, departs from the typical flow of research in parameterized complexity,\nwhich focuses on parameters describing graph's sparsity, like treewidth or\nvertex cover.",
          "arxiv_id": "2403.05943v2"
        },
        {
          "title": "Subexponential Parameterized Directed Steiner Network Problems on Planar Graphs: a Complete Classification",
          "year": "2022-08",
          "abstract": "In the Directed Steiner Network problem, the input is a directed graph G, a\nsubset T of k vertices of G called the terminals, and a demand graph D on T.\nThe task is to find a subgraph H of G with the minimum number of edges such\nthat for every edge (s,t) in D, the solution H contains a directed s to t path.\nIn this paper we investigate how the complexity of the problem depends on the\ndemand pattern when G is planar. Formally, if \\mathcal{D} is a class of\ndirected graphs closed under identification of vertices, then the\n\\mathcal{D}-Steiner Network (\\mathcal{D}-SN) problem is the special case where\nthe demand graph D is restricted to be from \\mathcal{D}. For general graphs,\nFeldmann and Marx [ICALP 2016] characterized those families of demand graphs\nwhere the problem is fixed-parameter tractable (FPT) parameterized by the\nnumber k of terminals. They showed that if \\mathcal{D} is a superset of one of\nthe five hard families, then \\mathcal{D}-SN is W[1]-hard parameterized by k,\notherwise it can be solved in time f(k)n^{O(1)}.\n  For planar graphs an interesting question is whether the W[1]-hard cases can\nbe solved by subexponential parameterized algorithms. Chitnis et al. [SICOMP\n2020] showed that, assuming the ETH, there is no f(k)n^{o(k)} time algorithm\nfor the general \\mathcal{D}-SN problem on planar graphs, but the special case\ncalled Strongly Connected Steiner Subgraph can be solved in time f(k)\nn^{O(\\sqrt{k})} on planar graphs. We present a far-reaching generalization and\nunification of these two results: we give a complete characterization of the\nbehavior of every $\\mathcal{D}$-SN problem on planar graphs. We show that\nassuming ETH, either the problem is (1) solvable in time 2^{O(k)}n^{O(1)}, and\nnot in time 2^{o(k)}n^{O(1)}, or (2) solvable in time f(k)n^{O(\\sqrt{k})}, but\nnot in time f(k)n^{o(\\sqrt{k})}, or (3) solvable in time f(k)n^{O(k)}, but not\nin time f(k)n^{o({k})}.",
          "arxiv_id": "2208.06015v1"
        }
      ],
      "1": [
        {
          "title": "An Efficient Data Structure and Algorithm for Long-Match Query in Run-Length Compressed BWT",
          "year": "2025-05",
          "abstract": "In this paper, we describe a new type of match between a pattern and a text\nthat aren't necessarily maximal in the query, but still contain useful matching\ninformation: locally maximal exact matches (LEMs). There are usually a large\namount of LEMs, so we only consider those above some length threshold\n$\\mathcal{L}$. These are referred to as long LEMs. The purpose of long LEMs is\nto capture substring matches between a query and a text that are not\nnecessarily maximal in the pattern but still long enough to be important.\nTherefore efficient long LEMs finding algorithms are desired for these\ndatasets. However, these datasets are too large to query on traditional string\nindexes. Fortunately, these datasets are very repetitive. Recently, compressed\nstring indexes that take advantage of the redundancy in the data but retain\nefficient querying capability have been proposed as a solution. We therefore\ngive an efficient algorithm for computing all the long LEMs of a query and a\ntext in a BWT runs compressed string index. We describe an $O(m+occ)$ expected\ntime algorithm that relies on an $O(r)$ words space string index for outputting\nall long LEMs of a pattern with respect to a text given the matching statistics\nof the pattern with respect to the text. Here $m$ is the length of the query,\n$occ$ is the number of long LEMs outputted, and $r$ is the number of runs in\nthe BWT of the text. The $O(r)$ space string index we describe relies on an\nadaptation of the move data structure by Nishimoto and Tabei. We are able to\nsupport $LCP[i]$ queries in constant time given $SA[i]$. In other words, we\nanswer $PLCP[i]$ queries in constant time. Long LEMs may provide useful\nsimilarity information between a pattern and a text that MEMs may ignore. This\ninformation is particularly useful in pangenome and biobank scale haplotype\npanel contexts.",
          "arxiv_id": "2505.15698v1"
        },
        {
          "title": "Compressed Dictionary Matching on Run-Length Encoded Strings",
          "year": "2025-09",
          "abstract": "Given a set of pattern strings $\\mathcal{P}=\\{P_1, P_2,\\ldots P_k\\}$ and a\ntext string $S$, the classic dictionary matching problem is to report all\noccurrences of each pattern in $S$. We study the dictionary problem in the\ncompressed setting, where the pattern strings and the text string are\ncompressed using run-length encoding, and the goal is to solve the problem\nwithout decompression and achieve efficient time and space in the size of the\ncompressed strings. Let $m$ and $n$ be the total length of the patterns\n$\\mathcal{P}$ and the length of the text string $S$, respectively, and let\n$\\overline{m}$ and $\\overline{n}$ be the total number of runs in the run-length\nencoding of the patterns in $\\mathcal{P}$ and $S$, respectively. Our main\nresult is an algorithm that achieves $O( (\\overline{m} + \\overline{n})\\log \\log\nm + \\mathrm{occ})$ expected time, and $O(\\overline{m})$ space, where\n$\\mathrm{occ}$ is the total number of occurrences of patterns in $S$. This is\nthe first non-trivial solution to the problem. Since any solution must read the\ninput, our time bound is optimal within an $\\log \\log m$ factor. We introduce\nseveral new techniques to achieve our bounds, including a new compressed\nrepresentation of the classic Aho-Corasick automaton and a new efficient string\nindex that supports fast queries in run-length encoded strings.",
          "arxiv_id": "2509.03265v1"
        },
        {
          "title": "Linear Time Online Algorithms for Constructing Linear-size Suffix Trie",
          "year": "2023-01",
          "abstract": "The suffix trees are fundamental data structures for various kinds of string\nprocessing. The suffix tree of a text string $T$ of length $n$ has $O(n)$ nodes\nand edges, and the string label of each edge is encoded by a pair of positions\nin $T$. Thus, even after the tree is built, the input string $T$ needs to be\nkept stored and random access to $T$ is still needed. The \\emph{linear-size\nsuffix tries} (\\emph{LSTs}), proposed by Crochemore et al. [Linear-size suffix\ntries, TCS 638:171-178, 2016], are a \"stand-alone\" alternative to the suffix\ntrees. Namely, the LST of an input text string $T$ of length $n$ occupies\n$O(n)$ total space, and supports pattern matching and other tasks with the same\nefficiency as the suffix tree without the need to store the input text string\n$T$. Crochemore et al. proposed an \\emph{offline} algorithm which transforms\nthe suffix tree of $T$ into the LST of $T$ in $O(n \\log \\sigma)$ time and\n$O(n)$ space, where $\\sigma$ is the alphabet size. In this paper, we present\ntwo types of \\emph{online} algorithms which \"directly\" construct the LST, from\nright to left, and from left to right, without constructing the suffix tree as\nan intermediate structure. Both algorithms construct the LST incrementally when\na new symbol is read, and do not access the previously read symbols. Both of\nthe right-to-left construction algorithm and the left-to-right construction\nalgorithm work in $O(n \\log \\sigma)$ time and $O(n)$ space. The main feature of\nour algorithms is that the input text string does not need to be stored.",
          "arxiv_id": "2301.04295v3"
        }
      ],
      "2": [
        {
          "title": "Concurrent Size",
          "year": "2022-09",
          "abstract": "The size of a data structure (i.e., the number of elements in it) is a widely\nused property of a data set. However, for concurrent programs, obtaining a\ncorrect size efficiently is non-trivial. In fact, the literature does not offer\na mechanism to obtain a correct (linearizable) size of a concurrent data set\nwithout resorting to inefficient solutions, such as taking a full snapshot of\nthe data structure to count the elements, or acquiring one global lock in all\nupdate and size operations. This paper presents a methodology for adding a\nconcurrent linearizable size operation to sets and dictionaries with a\nrelatively low performance overhead. Theoretically, the proposed size operation\nis wait-free with asymptotic complexity linear in the number of threads\n(independently of data-structure size). Practically, we evaluated the\nperformance overhead by adding size to various concurrent data structures in\nJava$-$a skip list, a hash table and a tree. The proposed linearizable size\noperation executes faster by orders of magnitude compared to the existing\noption of taking a snapshot, while incurring a throughput loss of $1\\%-20\\%$ on\nthe original data structure's operations.",
          "arxiv_id": "2209.07100v1"
        },
        {
          "title": "Lock-Free Augmented Trees",
          "year": "2024-05",
          "abstract": "Augmenting an existing sequential data structure with extra information to\nsupport greater functionality is a widely used technique. For example, search\ntrees are augmented to build sequential data structures like order-statistic\ntrees, interval trees, tango trees, link/cut trees and many others.\n  We study how to design concurrent augmented tree data structures. We present\na new, general technique that can augment a lock-free tree to add any new\nfields to each tree node, provided the new fields' values can be computed from\ninformation in the node and its children. This enables the design of lock-free,\nlinearizable analogues of a wide variety of classical augmented data\nstructures. As a first example, we give a wait-free trie that stores a set $S$\nof elements drawn from $\\{1,\\ldots,N\\}$ and supports linearizable\norder-statistic queries such as finding the $k$th smallest element of $S$.\nUpdates and queries take $O(\\log N)$ steps. We also apply our technique to a\nlock-free binary search tree (BST), where changes to the structure of the tree\nmake the linearization argument more challenging. Our augmented BST supports\norder statistic queries in $O(h)$ steps on a tree of height $h$. The\naugmentation does not affect the asymptotic running time of the updates.\n  For both our trie and BST, we give an alternative augmentation to improve\nsearches and order-statistic queries to run in $O(\\log |S|)$ steps (with a\nsmall increase in step complexity of updates). As an added bonus, our technique\nsupports arbitrary multi-point queries (such as range queries) with the same\ntime complexity as they would have in the corresponding sequential data\nstructure.",
          "arxiv_id": "2405.10506v1"
        },
        {
          "title": "Dynamic \"Succincter\"",
          "year": "2023-09",
          "abstract": "Augmented B-trees (aB-trees) are a broad class of data structures. The\nseminal work \"succincter\" by Patrascu showed that any aB-tree can be stored\nusing only two bits of redundancy, while supporting queries to the tree in time\nproportional to its depth. It has been a versatile building block for\nconstructing succinct data structures, including rank/select data structures,\ndictionaries, locally decodable arithmetic coding, storing balanced\nparenthesis, etc.\n  In this paper, we show how to \"dynamize\" an aB-tree. Our main result is the\ndesign of dynamic aB-trees (daB-trees) with branching factor two using only\nthree bits of redundancy (with the help of lookup tables that are of negligible\nsize in applications), while supporting updates and queries in time polynomial\nin its depth. As an application, we present a dynamic rank/select data\nstructure for $n$-bit arrays, also known as a dynamic fully indexable\ndictionary (FID). It supports updates and queries in $O(\\log n/\\log\\log n)$\ntime, and when the array has $m$ ones, the data structure occupies \\[\n  \\log\\binom{n}{m} + O(n/2^{\\log^{0.199}n}) \\] bits. Note that the update and\nquery times are optimal even without space constraints due to a lower bound by\nFredman and Saks. Prior to our work, no dynamic FID with near-optimal update\nand query times and redundancy $o(n/\\log n)$ was known. We further show that a\ndynamic sequence supporting insertions, deletions and rank/select queries can\nbe maintained in (optimal) $O(\\log n/\\log\\log n)$ time and with $O(n \\cdot\n\\text{poly}\\log\\log n/\\log^2 n)$ bits of redundancy.",
          "arxiv_id": "2309.12950v1"
        }
      ],
      "3": [
        {
          "title": "Low-Rank Approximation with $1/ε^{1/3}$ Matrix-Vector Products",
          "year": "2022-02",
          "abstract": "We study iterative methods based on Krylov subspaces for low-rank\napproximation under any Schatten-$p$ norm. Here, given access to a matrix $A$\nthrough matrix-vector products, an accuracy parameter $\\epsilon$, and a target\nrank $k$, the goal is to find a rank-$k$ matrix $Z$ with orthonormal columns\nsuch that $\\| A(I -ZZ^\\top)\\|_{S_p} \\leq (1+\\epsilon)\\min_{U^\\top U = I_k}\n\\|A(I - U U^\\top)\\|_{S_p}$, where $\\|M\\|_{S_p}$ denotes the $\\ell_p$ norm of\nthe the singular values of $M$. For the special cases of $p=2$ (Frobenius norm)\nand $p = \\infty$ (Spectral norm), Musco and Musco (NeurIPS 2015) obtained an\nalgorithm based on Krylov methods that uses $\\tilde{O}(k/\\sqrt{\\epsilon})$\nmatrix-vector products, improving on the na\\\"ive $\\tilde{O}(k/\\epsilon)$\ndependence obtainable by the power method, where $\\tilde{O}$ suppresses\npoly$(\\log(dk/\\epsilon))$ factors.\n  Our main result is an algorithm that uses only\n$\\tilde{O}(kp^{1/6}/\\epsilon^{1/3})$ matrix-vector products, and works for all\n$p \\geq 1$. For $p = 2$ our bound improves the previous\n$\\tilde{O}(k/\\epsilon^{1/2})$ bound to $\\tilde{O}(k/\\epsilon^{1/3})$. Since the\nSchatten-$p$ and Schatten-$\\infty$ norms are the same up to a $(1+\n\\epsilon)$-factor when $p \\geq (\\log d)/\\epsilon$, our bound recovers the\nresult of Musco and Musco for $p = \\infty$. Further, we prove a matrix-vector\nquery lower bound of $\\Omega(1/\\epsilon^{1/3})$ for any fixed constant $p \\geq\n1$, showing that surprisingly $\\tilde{\\Theta}(1/\\epsilon^{1/3})$ is the optimal\ncomplexity for constant~$k$.\n  To obtain our results, we introduce several new techniques, including\noptimizing over multiple Krylov subspaces simultaneously, and pinching\ninequalities for partitioned operators. Our lower bound for $p \\in [1,2]$ uses\nthe Araki-Lieb-Thirring trace inequality, whereas for $p>2$, we appeal to a\nnorm-compression inequality for aligned partitioned operators.",
          "arxiv_id": "2202.05120v4"
        },
        {
          "title": "Near-optimal hierarchical matrix approximation from matrix-vector products",
          "year": "2024-07",
          "abstract": "We describe a randomized algorithm for producing a near-optimal hierarchical\noff-diagonal low-rank (HODLR) approximation to an $n\\times n$ matrix\n$\\mathbf{A}$, accessible only though matrix-vector products with $\\mathbf{A}$\nand $\\mathbf{A}^{\\mathsf{T}}$. We prove that, for the rank-$k$ HODLR\napproximation problem, our method achieves a $(1+\\beta)^{\\log(n)}$-optimal\napproximation in expected Frobenius norm using $O(k\\log(n)/\\beta^3)$\nmatrix-vector products. In particular, the algorithm obtains a\n$(1+\\varepsilon)$-optimal approximation with $O(k\\log^4(n)/\\varepsilon^3)$\nmatrix-vector products, and for any constant $c$, an $n^c$-optimal\napproximation with $O(k \\log(n))$ matrix-vector products. Apart from\nmatrix-vector products, the additional computational cost of our method is just\n$O(n \\operatorname{poly}(\\log(n), k, \\beta))$. We complement the upper bound\nwith a lower bound, which shows that any matrix-vector query algorithm requires\nat least $\\Omega(k\\log(n) + k/\\varepsilon)$ queries to obtain a\n$(1+\\varepsilon)$-optimal approximation.\n  Our algorithm can be viewed as a robust version of widely used \"peeling\"\nmethods for recovering HODLR matrices and is, to the best of our knowledge, the\nfirst matrix-vector query algorithm to enjoy theoretical worst-case guarantees\nfor approximation by any hierarchical matrix class. To control the propagation\nof error between levels of hierarchical approximation, we introduce a new\nperturbation bound for low-rank approximation, which shows that the widely used\nGeneralized Nystr\\\"om method enjoys inherent stability when implemented with\nnoisy matrix-vector products. We also introduce a novel randomly perforated\nmatrix sketching method to further control the error in the peeling algorithm.",
          "arxiv_id": "2407.04686v2"
        },
        {
          "title": "Reduced-Rank Regression with Operator Norm Error",
          "year": "2020-11",
          "abstract": "A common data analysis task is the reduced-rank regression problem:\n$$\\min_{\\textrm{rank-}k \\ X} \\|AX-B\\|,$$ where $A \\in \\mathbb{R}^{n \\times c}$\nand $B \\in \\mathbb{R}^{n \\times d}$ are given large matrices and $\\|\\cdot\\|$ is\nsome norm. Here the unknown matrix $X \\in \\mathbb{R}^{c \\times d}$ is\nconstrained to be of rank $k$ as it results in a significant parameter\nreduction of the solution when $c$ and $d$ are large. In the case of Frobenius\nnorm error, there is a standard closed form solution to this problem and a fast\nalgorithm to find a $(1+\\varepsilon)$-approximate solution. However, for the\nimportant case of operator norm error, no closed form solution is known and the\nfastest known algorithms take singular value decomposition time.\n  We give the first randomized algorithms for this problem running in time\n$$(nnz{(A)} + nnz{(B)} + c^2) \\cdot k/\\varepsilon^{1.5} + (n+d)k^2/\\epsilon +\nc^{\\omega},$$ up to a polylogarithmic factor involving condition numbers,\nmatrix dimensions, and dependence on $1/\\varepsilon$. Here $nnz{(M)}$ denotes\nthe number of non-zero entries of a matrix $M$, and $\\omega$ is the exponent of\nmatrix multiplication. As both (1) spectral low rank approximation ($A = B$)\nand (2) linear system solving ($n = c$ and $d = 1$) are special cases, our time\ncannot be improved by more than a $1/\\varepsilon$ factor (up to polylogarithmic\nfactors) without a major breakthrough in linear algebra. Interestingly, known\ntechniques for low rank approximation, such as alternating minimization or\nsketch-and-solve, provably fail for this problem. Instead, our algorithm uses\nan existential characterization of a solution, together with Krylov methods,\nlow degree polynomial approximation, and sketching-based preconditioning.",
          "arxiv_id": "2011.04564v2"
        }
      ],
      "4": [
        {
          "title": "Learning k-qubit Quantum Operators via Pauli Decomposition",
          "year": "2021-02",
          "abstract": "Motivated by the limited qubit capacity of current quantum systems, we study\nthe quantum sample complexity of $k$-qubit quantum operators, i.e., operations\napplicable on only $k$ out of $d$ qubits. The problem is studied according to\nthe quantum probably approximately correct (QPAC) model abiding by quantum\nmechanical laws such as no-cloning, state collapse, and measurement\nincompatibility. With the delicacy of quantum samples and the richness of\nquantum operations, one expects a significantly larger quantum sample\ncomplexity.\n  This paper proves the contrary. We show that the quantum sample complexity of\n$k$-qubit quantum operations is comparable to the classical sample complexity\nof their counterparts (juntas), at least when $\\frac{k}{d}\\ll 1$. This is\nsurprising, especially since sample duplication is prohibited, and measurement\nincompatibility would lead to an exponentially larger sample complexity with\nstandard methods. Our approach is based on the Pauli decomposition of quantum\noperators and a technique that we name Quantum Shadow Sampling (QSS) to reduce\nthe sample complexity exponentially. The results are proved by developing (i) a\nconnection between the learning loss and the Pauli decomposition; (ii) a\nscalable QSS circuit for estimating the Pauli coefficients; and (iii) a quantum\nalgorithm for learning $k$-qubit operators with sample complexity\n$O(\\frac{k4^k}{\\epsilon^2}\\log d)$.",
          "arxiv_id": "2102.05209v4"
        },
        {
          "title": "Probing Quantum Telecloning on Superconducting Quantum Processors",
          "year": "2023-08",
          "abstract": "Quantum information can not be perfectly cloned, but approximate copies of\nquantum information can be generated. Quantum telecloning combines approximate\nquantum cloning, more typically referred as quantum cloning, and quantum\nteleportation. Quantum telecloning allows approximate copies of quantum\ninformation to be constructed by separate parties, using the classical results\nof a Bell measurement made on a prepared quantum telecloning state. Quantum\ntelecloning can be implemented as a circuit on quantum computers using a\nclassical co-processor to compute classical feed forward instructions using if\nstatements based on the results of a mid-circuit Bell measurement in real time.\nWe present universal, symmetric, optimal $1 \\rightarrow M$ telecloning\ncircuits, and experimentally demonstrate these quantum telecloning circuits for\n$M=2$ up to $M=10$, natively executed with real time classical control systems\non IBM Quantum superconducting processors, known as dynamic circuits. We\nperform the cloning procedure on many different message states across the Bloch\nsphere, on $7$ IBM Quantum processors, optionally using the error suppression\ntechnique X-X sequence digital dynamical decoupling. Two circuit optimizations\nare utilized, one which removes ancilla qubits for $M=2, 3$, and one which\nreduces the total number of gates in the circuit but still uses ancilla qubits.\nParallel single qubit tomography with MLE density matrix reconstruction is used\nin order to compute the mixed state density matrices of the clone qubits, and\nclone quality is measured using quantum fidelity. These results present one of\nthe largest and most comprehensive NISQ computer experimental analyses on\n(single qubit) quantum telecloning to date. The clone fidelity sharply\ndecreases to $0.5$ for $M > 5$, but for $M=2$ we are able to achieve a mean\nclone fidelity of up to $0.79$ using dynamical decoupling.",
          "arxiv_id": "2308.15579v3"
        },
        {
          "title": "Parametric Synthesis of Computational Circuits for Complex Quantum Algorithms",
          "year": "2022-09",
          "abstract": "At the moment, quantum circuits are created mainly by manually placing logic\nelements on lines that symbolize quantum bits. The purpose of creating Quantum\nCircuit Synthesizer \"Naginata\" was due to the fact that even with a slight\nincrease in the number of operations in a quantum algorithm, leads to the\nsignificant increase in size of the corresponding quantum circuit. This causes\nserious difficulties both in creating and debugging these quantum circuits. The\npurpose of our quantum synthesizer is enabling users an opportunity to\nimplement quantum algorithms using higher-level commands. This is achieved by\ncreating generic blocks for frequently used operations such as: the adder,\nmultiplier, digital comparator (comparison operator), etc. Thus, the user could\nimplement a quantum algorithm by using these generic blocks, and the quantum\nsynthesizer would create a suitable circuit for this algorithm, in a format\nthat is supported by the chosen quantum computation environment. This approach\ngreatly simplifies the processes of development and debugging a quantum\nalgorithm. The proposed approach for implementing quantum algorithms has a\npotential application in the field of machine learning, in this regard, we\nprovided an example of creating a circuit for training a simple neural network.\nNeural networks have a significant impact on the technological development of\nthe transport and road complex, and there is a potential for improving the\nreliability and efficiency of their learning process by utilizing quantum\ncomputation, through the introduction of quantum computing.",
          "arxiv_id": "2209.09903v1"
        }
      ],
      "5": [
        {
          "title": "Improved Competitive Ratios for Online Bipartite Matching on Degree Bounded Graphs",
          "year": "2023-06",
          "abstract": "We consider the online bipartite matching problem on $(k,d)$-bounded graphs,\nwhere each online vertex has at most $d$ neighbors, each offline vertex has at\nleast $k$ neighbors, and $k\\geq d\\geq 2$. The model of $(k,d)$-bounded graphs\nis proposed by Naor and Wajc (EC 2015 and TEAC 2018) to model the online\nadvertising applications in which offline advertisers are interested in a large\nnumber of ad slots, while each online ad slot is interesting to a small number\nof advertisers. They proposed deterministic and randomized algorithms with a\ncompetitive ratio of $1 - (1-1/d)^k$ for the problem, and show that the\ncompetitive ratio is optimal for deterministic algorithms. They also raised the\nopen questions of whether strictly better competitive ratios can be achieved\nusing randomized algorithms, for both the adversarial and stochastic arrival\nmodels. In this paper we answer both of their open problems affirmatively. For\nthe adversarial arrival model, we propose a randomized algorithm with\ncompetitive ratio $1 - (1-1/d)^k + \\Omega(d^{-4}\\cdot e^{-\\frac{k}{d}})$ for\nall $k\\geq d\\geq 2$. We also consider the stochastic model and show that even\nbetter competitive ratios can be achieved. We show that for all $k\\geq d\\geq\n2$, the competitive ratio is always at least $0.8237$. We further consider the\n$b$-matching problem when each offline vertex can be matched at most $b$ times,\nand provide several competitive ratio lower bounds for the adversarial and\nstochastic model.",
          "arxiv_id": "2306.13387v2"
        },
        {
          "title": "Online Bipartite Matching with Reusable Resources",
          "year": "2021-10",
          "abstract": "We study the classic online bipartite matching problem with a twist: offline\nvertices, called resources, are $\\textit{reusable}$. In particular, when a\nresource is matched to an online vertex it is unavailable for a deterministic\ntime duration $d$ after which it becomes available again for a re-match. Thus,\na resource can be matched to many different online vertices over a period of\ntime. While recent work on the problem have resolved the asymptotic case where\nwe have large starting inventory (i.e., many copies) of every resource, we\nconsider the (more general) case of $\\textit{unit inventory}$ and give the\nfirst algorithms that are provably better than the naive greedy approach which\nhas a competitive ratio of (exactly) 0.5. Our first algorithm, which achieves a\ncompetitive ratio of $0.589$, generalizes the classic RANKING algorithm for\nonline bipartite matching of non-reusable resources (Karp et al., 1990), by\n$\\textit{reranking}$ resources independently over time. While reranking\nresources frequently has the same worst case performance as greedy, we show\nthat reranking intermittently on a periodic schedule succeeds in addressing\nreusability of resources and performs significantly better than greedy in the\nworst case. Our second algorithm, which achieves a competitive ratio of\n$0.505$, is a primal-dual randomized algorithm that works by suggesting up to\ntwo resources as candidate matches for every online vertex, and then breaking\nthe tie to make the final matching selection in a randomized correlated fashion\nover time. As a key component of our algorithm, we suitably adapt and extend\nthe powerful technique of online correlated selection (Fahrbach et al., 2020)\nto reusable resources, in order to induce negative correlation in our tie\nbreaking step and to beat the competitive ratio of $0.5$. Both of our results\nalso extend to the case where offline vertices have weights.",
          "arxiv_id": "2110.07084v2"
        },
        {
          "title": "Online Bipartite Matching in the Probe-Commit Model",
          "year": "2023-03",
          "abstract": "We consider the classical online bipartite matching problem in the\nprobe-commit model. In this problem, when an online vertex arrives, its edges\nmust be probed to determine if they exist, based on known edge probabilities. A\nprobing algorithm must respect commitment, meaning that if a probed edge\nexists, it must be used in the matching. Additionally, each online vertex has a\npatience constraint which limits the number of probes that can be made to an\nonline vertex's adjacent edges. We introduce a new configuration linear program\n(LP) which we prove is a relaxation of an optimal offline probing algorithm.\nUsing this LP, we establish the following competitive ratios which depend on\nthe model used to generate the instance graph, and the arrival order of its\nonline vertices:\n  - In the worst-case instance model, an optimal $1/e$ ratio when the vertices\narrive in uniformly at random (u.a.r.) order.\n  - In the known independently distributed (i.d.) instance model, an optimal\n$1/2$ ratio when the vertices arrive in adversarial order, and a $1-1/e$ ratio\nwhen the vertices arrive in u.a.r. order.\n  The latter two results improve upon the previous best competitive ratio of\n$0.46$ due to Brubach et al. (Algorithmica 2020), which only held in the more\nrestricted known i.i.d. (independent and identically distributed) instance\nmodel. Our $1-1/e$-competitive algorithm matches the best known result for the\nprophet secretary matching problem due to Ehsani et al. (SODA 2018). Our\nalgorithm is efficient and implies a $1-1/e$ approximation ratio for the\nspecial case when the graph is known. This is the offline stochastic matching\nproblem, and we improve upon the $0.42$ approximation ratio for one-sided\npatience due to Pollner et al. (EC 2022), while also generalizing the $1-1/e$\napproximation ratio for unbounded patience due to Gamlath et al. (SODA 2019).",
          "arxiv_id": "2303.08908v2"
        }
      ],
      "6": [
        {
          "title": "Rapid Mixing on Random Regular Graphs beyond Uniqueness",
          "year": "2025-04",
          "abstract": "The hardcore model is a fundamental probabilistic model extensively studied\nin statistical physics, probability theory, and computer science. For graphs of\nmaximum degree $\\Delta$, a well-known computational phase transition occurs at\nthe tree-uniqueness threshold $\\lambda_c(\\Delta) =\n\\frac{(\\Delta-1)^{\\Delta-1}}{(\\Delta-2)^\\Delta}$, where the mixing behavior of\nthe Glauber dynamics (a simple Markov chain) undergoes a sharp transition.\n  It is conjectured that random regular graphs exhibit different mixing\nbehavior, with the slowdown occurring far beyond the uniqueness threshold. We\nconfirm this conjecture by showing that, for the hardcore model on random\n$\\Delta$-regular graphs, the Glauber dynamics mixes rapidly with high\nprobability when $\\lambda = O(1/\\sqrt{\\Delta})$, which is significantly beyond\nthe uniqueness threshold $\\lambda_c(\\Delta) \\approx e/\\Delta$. Our result\nestablishes a sharp distinction between the hardcore model on worst-case and\nbeyond-worst-case instances, showing that the worst-case and average-case\ncomplexities of sampling and counting are fundamentally different.\n  This result of rapid mixing on random instances follows from a new criterion\nwe establish for rapid mixing of Glauber dynamics for any distribution\nsupported on a downward closed set family. Our criterion is simple, general,\nand easy to check. In addition to proving new mixing conditions for the\nhardcore model, we also establish improved mixing time bounds for sampling\nuniform matchings or $b$ matchings on graphs, the random cluster model on\nmatroids with $q \\in [0,1)$, and the determinantal point process. Our proof of\nthis new criterion for rapid mixing combines and generalizes several recent\ntools in a novel way, including a trickle down theorem for field dynamics,\nspectral/entropic stability, and a new comparison result between field dynamics\nand Glauber dynamics.",
          "arxiv_id": "2504.03406v1"
        },
        {
          "title": "Rapid mixing of global Markov chains via spectral independence: the unbounded degree case",
          "year": "2023-07",
          "abstract": "We consider spin systems on general $n$-vertex graphs of unbounded degree and\nexplore the effects of spectral independence on the rate of convergence to\nequilibrium of global Markov chains. Spectral independence is a novel way of\nquantifying the decay of correlations in spin system models, which has\nsignificantly advanced the study of Markov chains for spin systems. We prove\nthat whenever spectral independence holds, the popular Swendsen--Wang dynamics\nfor the $q$-state ferromagnetic Potts model on graphs of maximum degree\n$\\Delta$, where $\\Delta$ is allowed to grow with $n$, converges in $O((\\Delta\n\\log n)^c)$ steps where $c > 0$ is a constant independent of $\\Delta$ and $n$.\nWe also show a similar mixing time bound for the block dynamics of general spin\nsystems, again assuming that spectral independence holds. Finally, for monotone\nspin systems such as the Ising model and the hardcore model on bipartite\ngraphs, we show that spectral independence implies that the mixing time of the\nsystematic scan dynamics is $O(\\Delta^c \\log n)$ for a constant $c>0$\nindependent of $\\Delta$ and $n$. Systematic scan dynamics are widely popular\nbut are notoriously difficult to analyze. Our result implies optimal $O(\\log\nn)$ mixing time bounds for any systematic scan dynamics of the ferromagnetic\nIsing model on general graphs up to the tree uniqueness threshold. Our main\ntechnical contribution is an improved factorization of the entropy functional:\nthis is the common starting point for all our proofs. Specifically, we\nestablish the so-called $k$-partite factorization of entropy with a constant\nthat depends polynomially on the maximum degree of the graph.",
          "arxiv_id": "2307.00683v2"
        },
        {
          "title": "Optimal Mixing of Glauber Dynamics: Entropy Factorization via High-Dimensional Expansion",
          "year": "2020-11",
          "abstract": "We prove an optimal mixing time bound on the single-site update Markov chain\nknown as the Glauber dynamics or Gibbs sampling in a variety of settings. Our\nwork presents an improved version of the spectral independence approach of\nAnari et al. (2020) and shows $O(n\\log{n})$ mixing time on any $n$-vertex graph\nof bounded degree when the maximum eigenvalue of an associated influence matrix\nis bounded. As an application of our results, for the hard-core model on\nindependent sets weighted by a fugacity $\\lambda$, we establish $O(n\\log{n})$\nmixing time for the Glauber dynamics on any $n$-vertex graph of constant\nmaximum degree $\\Delta$ when $\\lambda<\\lambda_c(\\Delta)$ where\n$\\lambda_c(\\Delta)$ is the critical point for the uniqueness/non-uniqueness\nphase transition on the $\\Delta$-regular tree. More generally, for any\nantiferromagnetic 2-spin system we prove $O(n\\log{n})$ mixing time of the\nGlauber dynamics on any bounded degree graph in the corresponding tree\nuniqueness region. Our results apply more broadly; for example, we also obtain\n$O(n\\log{n})$ mixing for $q$-colorings of triangle-free graphs of maximum\ndegree $\\Delta$ when the number of colors satisfies $q > \\alpha \\Delta$ where\n$\\alpha \\approx 1.763$, and $O(m\\log{n})$ mixing for generating random\nmatchings of any graph with bounded degree and $m$ edges.",
          "arxiv_id": "2011.02075v4"
        }
      ],
      "7": [
        {
          "title": "What Do Our Choices Say About Our Preferences?",
          "year": "2020-05",
          "abstract": "Taking online decisions is a part of everyday life. Think of buying a house,\nparking a car or taking part in an auction. We often take those decisions\npublicly, which may breach our privacy - a party observing our choices may\nlearn a lot about our preferences. In this paper we investigate the online\nstopping algorithms from the privacy preserving perspective, using a\nmathematically rigorous differential privacy notion. In differentially private\nalgorithms there is usually an issue of balancing the privacy and utility. In\nthis regime, in most cases, having both optimality and high level of privacy at\nthe same time is impossible. We propose a natural mechanism to achieve a\ncontrollable trade-off, quantified by a parameter, between the accuracy of the\nonline algorithm and its privacy. Depending on the parameter, our mechanism can\nbe optimal with weaker differential privacy or suboptimal, yet more\nprivacy-preserving. We conduct a detailed accuracy and privacy analysis of our\nmechanism applied to the optimal algorithm for the classical secretary problem.\nThereby the classical notions from two distinct areas - optimal stopping and\ndifferential privacy - meet for the first time.",
          "arxiv_id": "2005.01586v4"
        },
        {
          "title": "Adaptive Privacy Composition for Accuracy-first Mechanisms",
          "year": "2023-06",
          "abstract": "In many practical applications of differential privacy, practitioners seek to\nprovide the best privacy guarantees subject to a target level of accuracy. A\nrecent line of work by Ligett et al. '17 and Whitehouse et al. '22 has\ndeveloped such accuracy-first mechanisms by leveraging the idea of noise\nreduction that adds correlated noise to the sufficient statistic in a private\ncomputation and produces a sequence of increasingly accurate answers. A major\nadvantage of noise reduction mechanisms is that the analysts only pay the\nprivacy cost of the least noisy or most accurate answer released. Despite this\nappealing property in isolation, there has not been a systematic study on how\nto use them in conjunction with other differentially private mechanisms. A\nfundamental challenge is that the privacy guarantee for noise reduction\nmechanisms is (necessarily) formulated as ex-post privacy that bounds the\nprivacy loss as a function of the released outcome. Furthermore, there has yet\nto be any study on how ex-post private mechanisms compose, which allows us to\ntrack the accumulated privacy over several mechanisms. We develop privacy\nfilters [Rogers et al. '16, Feldman and Zrnic '21, and Whitehouse et al. '22']\nthat allow an analyst to adaptively switch between differentially private and\nex-post private mechanisms subject to an overall differential privacy\nguarantee.",
          "arxiv_id": "2306.13824v2"
        },
        {
          "title": "Concurrent Composition for Differentially Private Continual Mechanisms",
          "year": "2024-11",
          "abstract": "Many intended uses of differential privacy involve a $\\textit{continual\nmechanism}$ that is set up to run continuously over a long period of time,\nmaking more statistical releases as either queries come in or the dataset is\nupdated. In this paper, we give the first general treatment of privacy against\n$\\textit{adaptive}$ adversaries for mechanisms that support dataset updates and\na variety of queries, all arbitrarily interleaved. It also models a very\ngeneral notion of neighboring, that includes both event-level and user-level\nprivacy.\n  We prove several $\\textit{concurrent}$ composition theorems for continual\nmechanisms, which ensure privacy even when an adversary can interleave queries\nand dataset updates to the different composed mechanisms. Previous concurrent\ncomposition theorems for differential privacy were only for the case when the\ndataset is static, with no adaptive updates. Moreover, we also give the first\ninteractive and continual generalizations of the \"parallel composition theorem\"\nfor noninteractive differential privacy. Specifically, we show that the\nanalogue of the noninteractive parallel composition theorem holds if either\nthere are no adaptive dataset updates or each of the composed mechanisms\nsatisfies pure differential privacy, but it fails to hold for composing\napproximately differentially private mechanisms with dataset updates.\n  We then formalize a set of general conditions on a continual mechanism $M$\nthat runs multiple continual sub-mechanisms such that the privacy guarantees of\n$M$ follow directly using the above concurrent composition theorems on the\nsub-mechanisms, without further privacy loss. This enables us to give a simpler\nand more modular privacy analysis of a recent continual histogram mechanism of\nHenzinger, Sricharan, and Steiner. In the case of approximate DP, ours is the\nfirst proof showing that its privacy holds against adaptive adversaries.",
          "arxiv_id": "2411.03299v2"
        }
      ],
      "8": [
        {
          "title": "A Theory of Universal Learning",
          "year": "2020-11",
          "abstract": "How quickly can a given class of concepts be learned from examples? It is\ncommon to measure the performance of a supervised machine learning algorithm by\nplotting its \"learning curve\", that is, the decay of the error rate as a\nfunction of the number of training examples. However, the classical theoretical\nframework for understanding learnability, the PAC model of Vapnik-Chervonenkis\nand Valiant, does not explain the behavior of learning curves: the\ndistribution-free PAC model of learning can only bound the upper envelope of\nthe learning curves over all possible data distributions. This does not match\nthe practice of machine learning, where the data source is typically fixed in\nany given scenario, while the learner may choose the number of training\nexamples on the basis of factors such as computational resources and desired\naccuracy.\n  In this paper, we study an alternative learning model that better captures\nsuch practical aspects of machine learning, but still gives rise to a complete\ntheory of the learnable in the spirit of the PAC model. More precisely, we\nconsider the problem of universal learning, which aims to understand the\nperformance of learning algorithms on every data distribution, but without\nrequiring uniformity over the distribution. The main result of this paper is a\nremarkable trichotomy: there are only three possible rates of universal\nlearning. More precisely, we show that the learning curves of any given concept\nclass decay either at an exponential, linear, or arbitrarily slow rates.\nMoreover, each of these cases is completely characterized by appropriate\ncombinatorial parameters, and we exhibit optimal learning algorithms that\nachieve the best possible rate in each case.\n  For concreteness, we consider in this paper only the realizable case, though\nanalogous results are expected to extend to more general learning scenarios.",
          "arxiv_id": "2011.04483v1"
        },
        {
          "title": "VC Dimension and Distribution-Free Sample-Based Testing",
          "year": "2020-12",
          "abstract": "We consider the problem of determining which classes of functions can be\ntested more efficiently than they can be learned, in the distribution-free\nsample-based model that corresponds to the standard PAC learning setting. Our\nmain result shows that while VC dimension by itself does not always provide\ntight bounds on the number of samples required to test a class of functions in\nthis model, it can be combined with a closely-related variant that we call\n\"lower VC\" (or LVC) dimension to obtain strong lower bounds on this sample\ncomplexity.\n  We use this result to obtain strong and in many cases nearly optimal lower\nbounds on the sample complexity for testing unions of intervals, halfspaces,\nintersections of halfspaces, polynomial threshold functions, and decision\ntrees. Conversely, we show that two natural classes of functions, juntas and\nmonotone functions, can be tested with a number of samples that is polynomially\nsmaller than the number of samples required for PAC learning.\n  Finally, we also use the connection between VC dimension and property testing\nto establish new lower bounds for testing radius clusterability and testing\nfeasibility of linear constraint systems.",
          "arxiv_id": "2012.03923v1"
        },
        {
          "title": "Downsampling for Testing and Learning in Product Distributions",
          "year": "2020-07",
          "abstract": "We study distribution-free property testing and learning problems where the\nunknown probability distribution is a product distribution over $\\mathbb{R}^d$.\nFor many important classes of functions, such as intersections of halfspaces,\npolynomial threshold functions, convex sets, and $k$-alternating functions, the\nknown algorithms either have complexity that depends on the support size of the\ndistribution, or are proven to work only for specific examples of product\ndistributions. We introduce a general method, which we call downsampling, that\nresolves these issues. Downsampling uses a notion of \"rectilinear isoperimetry\"\nfor product distributions, which further strengthens the connection between\nisoperimetry, testing, and learning. Using this technique, we attain new\nefficient distribution-free algorithms under product distributions on\n$\\mathbb{R}^d$:\n  1. A simpler proof for non-adaptive, one-sided monotonicity testing of\nfunctions $[n]^d \\to \\{0,1\\}$, and improved sample complexity for testing\nmonotonicity over unknown product distributions, from $O(d^7)$ [Black,\nChakrabarty, & Seshadhri, SODA 2020] to $\\widetilde O(d^3)$.\n  2. Polynomial-time agnostic learning algorithms for functions of a constant\nnumber of halfspaces, and constant-degree polynomial threshold functions.\n  3. An $\\exp(O(d \\log(dk)))$-time agnostic learning algorithm, and an\n$\\exp(O(d \\log(dk)))$-sample tolerant tester, for functions of $k$ convex sets;\nand a $2^{\\widetilde O(d)}$ sample-based one-sided tester for convex sets.\n  4. An $\\exp(\\widetilde O(k \\sqrt d))$-time agnostic learning algorithm for\n$k$-alternating functions, and a sample-based tolerant tester with the same\ncomplexity.",
          "arxiv_id": "2007.07449v2"
        }
      ],
      "9": [
        {
          "title": "Achieving anonymity via weak lower bound constraints for k-median and k-means",
          "year": "2020-09",
          "abstract": "We study $k$-clustering problems with lower bounds, including $k$-median and\n$k$-means clustering with lower bounds. In addition to the point set $P$ and\nthe number of centers $k$, a $k$-clustering problem with (uniform) lower bounds\ngets a number $B$. The solution space is restricted to clusterings where every\ncluster has at least $B$ points. We demonstrate how to approximate $k$-median\nwith lower bounds via a reduction to facility location with lower bounds, for\nwhich $O(1)$-approximation algorithms are known.\n  Then we propose a new constrained clustering problem with lower bounds where\nwe allow points to be assigned multiple times (to different centers). This\nmeans that for every point, the clustering specifies a set of centers to which\nit is assigned. We call this clustering with weak lower bounds. We give a\n$(6.5+\\epsilon)$-approximation for $k$-median clustering with weak lower bounds\nand an $O(1)$-approximation for $k$-means with weak lower bounds.\n  We conclude by showing that at a constant increase in the approximation\nfactor, we can restrict the number of assignments of every point to $2$ (or, if\nwe allow fractional assignments, to $1+\\epsilon$). This also leads to the first\nbicritera approximation algorithm for $k$-means with (standard) lower bounds\nwhere bicriteria is interpreted in the sense that the lower bounds are violated\nby a constant factor.\n  All algorithms in this paper run in time that is polynomial in $n$ and $k$\n(and $d$ for the Euclidean variants considered).",
          "arxiv_id": "2009.03078v3"
        },
        {
          "title": "FPT Approximations for Capacitated/Fair Clustering with Outliers",
          "year": "2023-05",
          "abstract": "Clustering problems such as $k$-Median, and $k$-Means, are motivated from\napplications such as location planning, unsupervised learning among others. In\nsuch applications, it is important to find the clustering of points that is not\n``skewed'' in terms of the number of points, i.e., no cluster should contain\ntoo many points. This is modeled by capacity constraints on the sizes of\nclusters. In an orthogonal direction, another important consideration in\nclustering is how to handle the presence of outliers in the data. Indeed, these\nclustering problems have been generalized in the literature to separately\nhandle capacity constraints and outliers. To the best of our knowledge, there\nhas been very little work on studying the approximability of clustering\nproblems that can simultaneously handle both capacities and outliers.\n  We initiate the study of the Capacitated $k$-Median with Outliers (C$k$MO)\nproblem. Here, we want to cluster all except $m$ outlier points into at most\n$k$ clusters, such that (i) the clusters respect the capacity constraints, and\n(ii) the cost of clustering, defined as the sum of distances of each\nnon-outlier point to its assigned cluster-center, is minimized.\n  We design the first constant-factor approximation algorithms for C$k$MO. In\nparticular, our algorithm returns a (3+\\epsilon)-approximation for C$k$MO in\ngeneral metric spaces, and a (1+\\epsilon)-approximation in Euclidean spaces of\nconstant dimension, that runs in time in time $f(k, m, \\epsilon) \\cdot\n|I_m|^{O(1)}$, where $|I_m|$ denotes the input size. We can also extend these\nresults to a broader class of problems, including Capacitated\nk-Means/k-Facility Location with Outliers, and Size-Balanced Fair Clustering\nproblems with Outliers. For each of these problems, we obtain an approximation\nratio that matches the best known guarantee of the corresponding outlier-free\nproblem.",
          "arxiv_id": "2305.01471v1"
        },
        {
          "title": "Parameterized Approximation Schemes for Clustering with General Norm Objectives",
          "year": "2023-04",
          "abstract": "This paper considers the well-studied algorithmic regime of designing a\n$(1+\\epsilon)$-approximation algorithm for a $k$-clustering problem that runs\nin time $f(k,\\epsilon)poly(n)$ (sometimes called an efficient parameterized\napproximation scheme or EPAS for short). Notable results of this kind include\nEPASes in the high-dimensional Euclidean setting for $k$-center [Bad\\u{o}iu,\nHar-Peled, Indyk; STOC'02] as well as $k$-median, and $k$-means [Kumar,\nSabharwal, Sen; J. ACM 2010]. However, existing EPASes handle only basic\nobjectives (such as $k$-center, $k$-median, and $k$-means) and are tailored to\nthe specific objective and metric space.\n  Our main contribution is a clean and simple EPAS that settles more than ten\nclustering problems (across multiple well-studied objectives as well as metric\nspaces) and unifies well-known EPASes. Our algorithm gives EPASes for a large\nvariety of clustering objectives (for example, $k$-means, $k$-center,\n$k$-median, priority $k$-center, $\\ell$-centrum, ordered $k$-median, socially\nfair $k$-median aka robust $k$-median, or more generally monotone norm\n$k$-clustering) and metric spaces (for example, continuous high-dimensional\nEuclidean spaces, metrics of bounded doubling dimension, bounded treewidth\nmetrics, and planar metrics).\n  Key to our approach is a new concept that we call bounded $\\epsilon$-scatter\ndimension--an intrinsic complexity measure of a metric space that is a\nrelaxation of the standard notion of bounded doubling dimension. Our main\ntechnical result shows that two conditions are essentially sufficient for our\nalgorithm to yield an EPAS on the input metric $M$ for any clustering\nobjective: (i) The objective is described by a monotone (not necessarily\nsymmetric!) norm, and (ii) the $\\epsilon$-scatter dimension of $M$ is upper\nbounded by a function of $\\epsilon$.",
          "arxiv_id": "2304.03146v1"
        }
      ],
      "10": [
        {
          "title": "Accelerating Clique Counting in Sparse Real-World Graphs via Communication-Reducing Optimizations",
          "year": "2021-12",
          "abstract": "Counting instances of specific subgraphs in a larger graph is an important\nproblem in graph mining. Finding cliques of size k (k-cliques) is one example\nof this NP-hard problem. Different algorithms for clique counting avoid\ncounting the same clique multiple times by pivoting or ordering the graph.\nOrdering-based algorithms include an ordering step to direct the edges in the\ninput graph, and a counting step, which is dominated by building node or\nedge-induced subgraphs. Of the ordering-based algorithms, kClist is the\nstate-of-the art algorithm designed to work on sparse real-world graphs.\nDespite its leading overall performance, kClist's vertex-parallel\nimplementation does not scale well in practice on graphs with a few million\nvertices.\n  We present CITRON (Clique counting with Traffic Reducing Optimizations) to\nimprove the parallel scalability and thus overall performance of clique\ncounting. We accelerate the ordering phase by abandoning kClist's sequential\ncore ordering and using a parallelized degree ordering. We accelerate the\ncounting phase with our reorganized subgraph data structures that reduce memory\ntraffic to improve scaling bottlenecks. Our sorted, compact neighbor lists\nimprove locality and communication efficiency which results in near-linear\nparallel scaling. CITRON significantly outperforms kClist while counting\nmoderately sized cliques, and thus increases the size of graph practical for\nclique counting.\n  We have recently become aware of ArbCount (arXiv:2002.10047), which often\noutperforms us. However, we believe that the analysis included in this paper\nwill be helpful for anyone who wishes to understand the performance\ncharacteristics of k-clique counting.",
          "arxiv_id": "2112.10913v2"
        },
        {
          "title": "BFS based distributed algorithm for parallel local directed sub-graph enumeration",
          "year": "2022-01",
          "abstract": "Estimating the frequency of sub-graphs is of importance for many tasks,\nincluding sub-graph isomorphism, kernel-based anomaly detection, and network\nstructure analysis. While multiple algorithms were proposed for full\nenumeration or sampling-based estimates, these methods fail in very large\ngraphs. Recent advances in parallelization allow for estimates of total\nsub-graphs counts in very large graphs. The task of counting the frequency of\neach sub-graph associated with each vertex also received excellent solutions\nfor undirected graphs. However, there is currently no good solution for very\nlarge directed graphs.\n  We here propose VDMC (Vertex specific Distributed Motif Counting) -- a fully\ndistributed algorithm to optimally count all the 3 and 4 vertices connected\ndirected graphs (sub-graph motifs) associated with each vertex of a graph. VDMC\ncounts each motif only once and its efficacy is linear in the number of counted\nmotifs. It is fully parallelized to be efficient in GPU-based computation. VDMC\nis based on three main elements: 1) Ordering the vertices and only counting\nmotifs containing increasing order vertices, 2) sub-ordering motifs based on\nthe average length of the BFS composing the motif, and 3) removing isomorphisms\nonly once for the entire graph. We here compare VDMC to analytical estimates of\nthe expected number of motifs and show its accuracy. VDMC is available as a\nhighly efficient CPU and GPU code with a novel data structure for efficient\ngraph manipulation. We show the efficacy of VDMC and real-world graphs. VDMC\nallows for the precise analysis of sub-graph frequency around each vertex in\nlarge graphs and opens the way for the extension of methods until now limited\nto graphs of thousands of edges to graphs with millions of edges and above.\n  GIT: https://github.com/louzounlab/graph-measures",
          "arxiv_id": "2201.11655v1"
        },
        {
          "title": "Scaling Up Maximal k-plex Enumeration",
          "year": "2022-03",
          "abstract": "Finding all maximal $k$-plexes on networks is a fundamental research problem\nin graph analysis due to many important applications, such as community\ndetection, biological graph analysis, and so on. A $k$-plex is a subgraph in\nwhich every vertex is adjacent to all but at most $k$ vertices within the\nsubgraph. In this paper, we study the problem of enumerating all large maximal\n$k$-plexes of a graph and develop several new and efficient techniques to solve\nthe problem. Specifically, we first propose several novel upper-bounding\ntechniques to prune unnecessary computations during the enumeration procedure.\nWe show that the proposed upper bounds can be computed in linear time. Then, we\ndevelop a new branch-and-bound algorithm with a carefully-designed pivot\nre-selection strategy to enumerate all $k$-plexes, which outputs all $k$-plexes\nin $O(n^2\\gamma_k^n)$ time theoretically, where $n$ is the number of vertices\nof the graph and $\\gamma_k$ is strictly smaller than 2. In addition, a parallel\nversion of the proposed algorithm is further developed to scale up to process\nlarge real-world graphs. Finally, extensive experimental results show that the\nproposed sequential algorithm can achieve up to $2\\times$ to $100\\times$\nspeedup over the state-of-the-art sequential algorithms on most benchmark\ngraphs. The results also demonstrate the high scalability of the proposed\nparallel algorithm. For example, on a large real-world graph with more than 200\nmillion edges, our parallel algorithm can finish the computation within two\nminutes, while the state-of-the-art parallel algorithm cannot terminate within\n24 hours.",
          "arxiv_id": "2203.10760v3"
        }
      ],
      "11": [
        {
          "title": "Approximating Subset Sum Ratio faster than Subset Sum",
          "year": "2023-10",
          "abstract": "Subset Sum Ratio is the following optimization problem: Given a set of $n$\npositive numbers $I$, find disjoint subsets $X,Y \\subseteq I$ minimizing the\nratio $\\max\\{\\Sigma(X)/\\Sigma(Y),\\Sigma(Y)/\\Sigma(X)\\}$, where $\\Sigma(Z)$\ndenotes the sum of all elements of $Z$. Subset Sum Ratio is an optimization\nvariant of the Equal Subset Sum problem. It was introduced by Woeginger and Yu\nin '92 and is known to admit an FPTAS [Bazgan, Santha, Tuza '98]. The best\napproximation schemes before this work had running time $O(n^4/\\varepsilon)$\n[Melissinos, Pagourtzis '18], $\\tilde O(n^{2.3}/\\varepsilon^{2.6})$ and $\\tilde\nO(n^2/\\varepsilon^3)$ [Alonistiotis et al. '22].\n  In this work, we present an improved approximation scheme for Subset Sum\nRatio running in time $O(n / \\varepsilon^{0.9386})$. Here we assume that the\nitems are given in sorted order, otherwise we need an additional running time\nof $O(n \\log n)$ for sorting. Our improved running time simultaneously improves\nthe dependence on $n$ to linear and the dependence on $1/\\varepsilon$ to\nsublinear.\n  For comparison, the related Subset Sum problem admits an approximation scheme\nrunning in time $O(n/\\varepsilon)$ [Gens, Levner '79]. If one would achieve an\napproximation scheme with running time $\\tilde O(n / \\varepsilon^{0.99})$ for\nSubset Sum, then one would falsify the Strong Exponential Time Hypothesis\n[Abboud, Bringmann, Hermelin, Shabtay '19] as well as the Min-Plus-Convolution\nHypothesis [Bringmann, Nakos '21]. We thus establish that Subset Sum Ratio\nadmits faster approximation schemes than Subset Sum. This comes as a surprise,\nsince at any point in time before this work the best known approximation scheme\nfor Subset Sum Ratio had a worse running time than the best known approximation\nscheme for Subset Sum.",
          "arxiv_id": "2310.07595v1"
        },
        {
          "title": "On Near-Linear-Time Algorithms for Dense Subset Sum",
          "year": "2020-10",
          "abstract": "In the Subset Sum problem we are given a set of $n$ positive integers $X$ and\na target $t$ and are asked whether some subset of $X$ sums to $t$. Natural\nparameters for this problem that have been studied in the literature are $n$\nand $t$ as well as the maximum input number $\\rm{mx}_X$ and the sum of all\ninput numbers $\\Sigma_X$. In this paper we study the dense case of Subset Sum,\nwhere all these parameters are polynomial in $n$. In this regime, standard\npseudo-polynomial algorithms solve Subset Sum in polynomial time $n^{O(1)}$.\n  Our main question is: When can dense Subset Sum be solved in near-linear time\n$\\tilde{O}(n)$? We provide an essentially complete dichotomy by designing\nimproved algorithms and proving conditional lower bounds, thereby determining\nessentially all settings of the parameters $n,t,\\rm{mx}_X,\\Sigma_X$ for which\ndense Subset Sum is in time $\\tilde{O}(n)$. For notational convenience we\nassume without loss of generality that $t \\ge \\rm{mx}_X$ (as larger numbers can\nbe ignored) and $t \\le \\Sigma_X/2$ (using symmetry). Then our dichotomy reads\nas follows:\n  - By reviving and improving an additive-combinatorics-based approach by Galil\nand Margalit [SICOMP'91], we show that Subset Sum is in near-linear time\n$\\tilde{O}(n)$ if $t \\gg \\rm{mx}_X \\Sigma_X/n^2$.\n  - We prove a matching conditional lower bound: If Subset Sum is in\nnear-linear time for any setting with $t \\ll \\rm{mx}_X \\Sigma_X/n^2$, then the\nStrong Exponential Time Hypothesis and the Strong k-Sum Hypothesis fail.\n  We also generalize our algorithm from sets to multi-sets, albeit with\nnon-matching upper and lower bounds.",
          "arxiv_id": "2010.09096v1"
        },
        {
          "title": "Fast Low-Space Algorithms for Subset Sum",
          "year": "2020-11",
          "abstract": "We consider the canonical Subset Sum problem: given a list of positive\nintegers $a_1,\\ldots,a_n$ and a target integer $t$ with $t > a_i$ for all $i$,\ndetermine if there is an $S \\subseteq [n]$ such that $\\sum_{i \\in S} a_i = t$.\nThe well-known pseudopolynomial-time dynamic programming algorithm [Bellman,\n1957] solves Subset Sum in $O(nt)$ time, while requiring $\\Omega(t)$ space.\n  In this paper we present algorithms for Subset Sum with $\\tilde O(nt)$\nrunning time and much lower space requirements than Bellman's algorithm, as\nwell as that of prior work. We show that Subset Sum can be solved in $\\tilde\nO(nt)$ time and $O(\\log(nt))$ space with access to $O(\\log n \\log \\log n+\\log\nt)$ random bits. This significantly improves upon the $\\tilde O(n\nt^{1+\\varepsilon})$-time, $\\tilde O(n\\log t)$-space algorithm of Bringmann\n(SODA 2017). We also give an $\\tilde O(n^{1+\\varepsilon}t)$-time,\n$O(\\log(nt))$-space randomized algorithm, improving upon previous\n$(nt)^{O(1)}$-time $O(\\log(nt))$-space algorithms by Elberfeld, Jakoby, and\nTantau (FOCS 2010), and Kane (2010). In addition, we also give a $\\mathrm{poly}\n\\log(nt)$-space, $\\tilde O(n^2 t)$-time deterministic algorithm.\n  We also study time-space trade-offs for Subset Sum. For parameter $1\\le k\\le\n\\min\\{n,t\\}$, we present a randomized algorithm running in $\\tilde O((n+t)\\cdot\nk)$ time and $O((t/k) \\mathrm{polylog} (nt))$ space.\n  As an application of our results, we give an\n$\\tilde{O}(\\min\\{n^2/\\varepsilon, n/\\varepsilon^2\\})$-time and\n$\\mathrm{polylog}(nt)$-space algorithm for \"weak\" $\\varepsilon$-approximations\nof Subset Sum.",
          "arxiv_id": "2011.03819v1"
        }
      ],
      "12": [
        {
          "title": "Unweighted Geometric Hitting Set for Line-Constrained Disks and Related Problems",
          "year": "2024-06",
          "abstract": "Given a set $P$ of $n$ points and a set $S$ of $m$ disks in the plane, the\ndisk hitting set problem asks for a smallest subset of $P$ such that every disk\nof $S$ contains at least one point in the subset. The problem is NP-hard. In\nthis paper, we consider a line-constrained version in which all disks have\ntheir centers on a line. We present an $O(m\\log^2n+(n+m)\\log(n+m))$ time\nalgorithm for the problem. This improves the previously best result of\n$O(m^2\\log m+(n+m)\\log(n+m))$ time for the weighted case of the problem where\nevery point of $P$ has a weight and the objective is to minimize the total\nweight of the hitting set. Our algorithm actually solves a more general\nline-separable problem with a single intersection property: The points of $P$\nand the disk centers are separated by a line $\\ell$ and the boundary of every\ntwo disks intersect at most once on the side of $\\ell$ containing $P$.",
          "arxiv_id": "2407.00331v1"
        },
        {
          "title": "On Line-Separable Weighted Unit-Disk Coverage and Related Problems",
          "year": "2024-06",
          "abstract": "Given a set $P$ of $n$ points and a set $S$ of $n$ weighted disks in the\nplane, the disk coverage problem is to compute a subset of disks of smallest\ntotal weight such that the union of the disks in the subset covers all points\nof $P$. The problem is NP-hard. In this paper, we consider a line-separable\nunit-disk version of the problem where all disks have the same radius and their\ncenters are separated from the points of $P$ by a line $\\ell$. We present an\n$O(n^{3/2}\\log^2 n)$ time algorithm for the problem. This improves the\npreviously best work of $O(n^2\\log n)$ time. Our result leads to an algorithm\nof $O(n^{{7}/{2}}\\log^2 n)$ time for the halfplane coverage problem (i.e.,\nusing $n$ weighted halfplanes to cover $n$ points), an improvement over the\nprevious $O(n^4\\log n)$ time solution. If all halfplanes are lower ones, our\nalgorithm runs in $O(n^{{3}/{2}}\\log^2 n)$ time, while the previous best\nalgorithm takes $O(n^2\\log n)$ time. Using duality, the hitting set problems\nunder the same settings can be solved with similar time complexities.",
          "arxiv_id": "2407.00329v1"
        },
        {
          "title": "Algorithms for the Line-Constrained Disk Coverage and Related Problems",
          "year": "2021-04",
          "abstract": "Given a set $P$ of $n$ points and a set $S$ of $m$ weighted disks in the\nplane, the disk coverage problem asks for a subset of disks of minimum total\nweight that cover all points of $P$. The problem is NP-hard. In this paper, we\nconsider a line-constrained version in which all disks are centered on a line\n$L$ (while points of $P$ can be anywhere in the plane). We present an\n$O((m+n)\\log(m+n)+\\kappa\\log m)$ time algorithm for the problem, where $\\kappa$\nis the number of pairs of disks that intersect. Alternatively, we can also\nsolve the problem in $O(nm\\log(m+n))$ time. For the unit-disk case where all\ndisks have the same radius, the running time can be reduced to\n$O((n+m)\\log(m+n))$. In addition, we solve in $O((m+n)\\log(m+n))$ time the\n$L_{\\infty}$ and $L_1$ cases of the problem, in which the disks are squares and\ndiamonds, respectively. As a by-product, the 1D version of the problem where\nall points of $P$ are on $L$ and the disks are line segments on $L$ is also\nsolved in $O((m+n)\\log(m+n))$ time. We also show that the problem has an\n$\\Omega((m+n)\\log (m+n))$ time lower bound even for the 1D case.\n  We further demonstrate that our techniques can also be used to solve other\ngeometric coverage problems. For example, given in the plane a set $P$ of $n$\npoints and a set $S$ of $n$ weighted half-planes, we solve in $O(n^4\\log n)$\ntime the problem of finding a subset of half-planes to cover $P$ so that their\ntotal weight is minimized. This improves the previous best algorithm of\n$O(n^5)$ time by almost a linear factor. If all half-planes are lower ones,\nthen our algorithm runs in $O(n^2\\log n)$ time, which improves the previous\nbest algorithm of $O(n^4)$ time by almost a quadratic factor.",
          "arxiv_id": "2104.14680v1"
        }
      ],
      "13": [
        {
          "title": "Analysis of Busy-Time Scheduling on Heterogeneous Machines",
          "year": "2021-05",
          "abstract": "This paper studies a generalized busy-time scheduling model on heterogeneous\nmachines. The input to the model includes a set of jobs and a set of machine\ntypes. Each job has a size and a time interval during which it should be\nprocessed. Each job is to be placed on a machine for execution. Different types\nof machines have distinct capacities and cost rates. The total size of the jobs\nrunning on a machine must always be kept within the machine's capacity, giving\nrise to placement restrictions for jobs of various sizes among the machine\ntypes. Each machine used is charged according to the time duration in which it\nis busy, i.e., it is processing jobs. The objective is to schedule the jobs\nonto machines to minimize the total cost of all the machines used. We develop\nan $O(1)$-approximation algorithm in the offline setting and an\n$O(\\mu)$-competitive algorithm in the online setting (where $\\mu$ is the\nmax/min job length ratio), both of which are asymptotically optimal.",
          "arxiv_id": "2105.06287v1"
        },
        {
          "title": "Online Total Completion Time Scheduling on Parallel Identical Machines",
          "year": "2022-07",
          "abstract": "We investigate deterministic non-preemptive online scheduling with delayed\ncommitment for total completion time minimization on parallel identical\nmachines. In this problem, jobs arrive one-by-one and their processing times\nare revealed upon arrival. An online algorithm can assign a job to a machine at\nany time after its arrival. We neither allow preemption nor a restart of the\njob, that is, once started, the job occupies the assigned machine until its\ncompletion. Our objective is the minimization of the sum of the completion\ntimes of all jobs. In the more general weighted version of the problem, we\nmultiply the completion time of a job by the individual weight of the job. We\napply competitive analysis to evaluate our algorithms.\n  We improve 25-year-old lower bounds for the competitive ratio of this problem\nby optimizing a simple job pattern. These lower bounds decrease with growing\nnumbers of machines. Based on the job pattern, we develop an online algorithm\nwhich is an extension of the delayed-SPT (Shortest Processing Time first)\napproach to the parallel machine environment. We show that the competitive\nratio is at most 1.546 for any even machine number which is a significant\nimprovement over the best previously known competitive ratio of 1.791. For the\ntwo-machine environment, this algorithm achieves a tight competitive ratio.\nThis is the first algorithm which optimally solves an online total completion\ntime problem in a parallel machine environment. Finally, we give the first\nseparation between the weighted and unweighted versions of the problem by\nshowing that in the two-machine environment, the competitive ratio of the\nweighted completion time objective is strictly larger than 1.546.",
          "arxiv_id": "2207.08550v1"
        },
        {
          "title": "Online busy time scheduling with flexible jobs",
          "year": "2024-05",
          "abstract": "We present several competitive ratios for the online busy time scheduling\nproblem with flexible jobs. The busy time scheduling problem is a fundamental\nscheduling problem motivated by energy efficiency with the goal of minimizing\nthe total time that machines with multiple processors are enabled. In the busy\ntime scheduling problem, an unbounded number of machines is given, where each\nmachine has $g$ processors. No more than $g$ jobs can be scheduled\nsimultaneously on each machine. A machine consumes energy whenever at least one\njob is scheduled at any time on the machine. Scheduling a single job at some\ntime $t$ consumes the same amount of energy as scheduling $g$ jobs at time $t$.\nIn the online setting, jobs are revealed when they are released.\n  We consider the cases where $g$ is unbounded and bounded. In this paper, we\nrevisit the bounds of the unbounded general setting from the literature and\ntighten it significantly. We also consider agreeable jobs. For the bounded\nsetting, we show a tightened upper bound. Furthermore, we show the first\nconstant competitive ratio in the bounded setting that does not require\nlookahead.",
          "arxiv_id": "2405.08595v1"
        }
      ],
      "14": [
        {
          "title": "Almost Envy-Free Allocations of Indivisible Goods or Chores with Entitlements",
          "year": "2023-05",
          "abstract": "We here address the problem of fairly allocating indivisible goods or chores\nto $n$ agents with weights that define their entitlement to the set of\nindivisible resources. Stemming from well-studied fairness concepts such as\nenvy-freeness up to one good (EF1) and envy-freeness up to any good (EFX) for\nagents with equal entitlements, we present, in this study, the first set of\nimpossibility results alongside algorithmic guarantees for fairness among\nagents with unequal entitlements.\n  Within this paper, we expand the concept of envy-freeness up to any good or\nchore to the weighted context (WEFX and XWEF respectively), demonstrating that\nthese allocations are not guaranteed to exist for two or three agents. Despite\nthese negative results, we develop a WEFX procedure for two agents with integer\nweights, and furthermore, we devise an approximate WEFX procedure for two\nagents with normalized weights. We further present a polynomial-time algorithm\nthat guarantees a weighted envy-free allocation up to one chore (1WEF) for any\nnumber of agents with additive cost functions. Our work underscores the\nheightened complexity of the weighted fair division problem when compared to\nits unweighted counterpart.",
          "arxiv_id": "2305.16081v2"
        },
        {
          "title": "The Three-Dimensional Stable Roommates Problem with Additively Separable Preferences",
          "year": "2021-07",
          "abstract": "The Stable Roommates problem involves matching a set of agents into pairs\nbased on the agents' strict ordinal preference lists. The matching must be\nstable, meaning that no two agents strictly prefer each other to their assigned\npartners. A number of three-dimensional variants exist, in which agents are\ninstead matched into triples. Both the original problem and these variants can\nalso be viewed as hedonic games. We formalise a three-dimensional variant using\ngeneral additively separable preferences, in which each agent provides an\ninteger valuation of every other agent. In this variant, we show that a stable\nmatching may not exist and that the related decision problem is NP-complete,\neven when the valuations are binary. In contrast, we show that if the\nvaluations are binary and symmetric then a stable matching must exist and can\nbe found in polynomial time. We also consider the related problem of finding a\nstable matching with maximum utilitarian welfare when valuations are binary and\nsymmetric. We show that this optimisation problem is NP-hard and present a\nnovel 2-approximation algorithm.",
          "arxiv_id": "2107.04368v1"
        },
        {
          "title": "On Achieving Leximin Fairness and Stability in Many-to-One Matchings",
          "year": "2020-09",
          "abstract": "The past few years have seen a surge of work on fairness in allocation\nproblems where items must be fairly divided among agents having individual\npreferences. In comparison, fairness in settings with preferences on both\nsides, that is, where agents have to be matched to other agents, has received\nmuch less attention. Moreover, two-sided matching literature has largely\nfocused on ordinal preferences. This paper initiates the study of fairness in\nstable many-to-one matchings under cardinal valuations. Motivated by real-world\nsettings, we study leximin optimality over stable many-to-one matchings. We\nfirst investigate matching problems with ranked valuations where all agents on\neach side have the same preference orders or rankings over the agents on the\nother side (but not necessarily the same valuations). Here, we provide a\ncomplete characterisation of the space of stable matchings. This leads to FaSt,\na novel and efficient algorithm to compute a leximin optimal stable matching\nunder ranked isometric valuations (where, for each pair of agents, the\nvaluation of one agent for the other is the same). Building upon FaSt, we\npresent an efficient algorithm, FaSt-Gen, that finds the leximin optimal stable\nmatching for a more general ranked setting. When there are exactly two agents\non one side who may be matched to many agents on the other, strict preferences\nare enough to guarantee an efficient algorithm. We next establish that, in the\nabsence of rankings and under strict preferences (with no restriction on the\nnumber of agents on either side), finding a leximin optimal stable matching is\nNP-Hard. Further, with weak rankings, the problem is strongly NP-Hard, even\nunder isometric valuations. In fact, when additivity and non-negativity are the\nonly assumptions, we show that, unless P=NP, no efficient polynomial factor\napproximation is possible.",
          "arxiv_id": "2009.05823v4"
        }
      ],
      "15": [
        {
          "title": "Regularized Non-monotone Submodular Maximization",
          "year": "2021-03",
          "abstract": "In this paper, we present a thorough study of maximizing a regularized\nnon-monotone submodular function subject to various constraints, i.e., $\\max \\{\ng(A) - \\ell(A) : A \\in \\mathcal{F} \\}$, where $g \\colon 2^\\Omega \\to\n\\mathbb{R}_+$ is a non-monotone submodular function, $\\ell \\colon 2^\\Omega \\to\n\\mathbb{R}_+$ is a normalized modular function and $\\mathcal{F}$ is the\nconstraint set. Though the objective function $f := g - \\ell$ is still\nsubmodular, the fact that $f$ could potentially take on negative values\nprevents the existing methods for submodular maximization from providing a\nconstant approximation ratio for the regularized submodular maximization\nproblem. To overcome the obstacle, we propose several algorithms which can\nprovide a relatively weak approximation guarantee for maximizing regularized\nnon-monotone submodular functions. More specifically, we propose a continuous\ngreedy algorithm for the relaxation of maximizing $g - \\ell$ subject to a\nmatroid constraint. Then, the pipage rounding procedure can produce an integral\nsolution $S$ such that $\\mathbb{E} [g(S) - \\ell(S)] \\geq e^{-1}g(OPT) -\n\\ell(OPT) - O(\\epsilon)$. Moreover, we present a much faster algorithm for\nmaximizing $g - \\ell$ subject to a cardinality constraint, which can output a\nsolution $S$ with $\\mathbb{E} [g(S) - \\ell(S)] \\geq (e^{-1} - \\epsilon) g(OPT)\n- \\ell(OPT)$ using $O(\\frac{n}{\\epsilon^2} \\ln \\frac 1\\epsilon)$ value oracle\nqueries. We also consider the unconstrained maximization problem and give an\nalgorithm which can return a solution $S$ with $\\mathbb{E} [g(S) - \\ell(S)]\n\\geq e^{-1} g(OPT) - \\ell(OPT)$ using $O(n)$ value oracle queries.",
          "arxiv_id": "2103.10008v1"
        },
        {
          "title": "Partial-Monotone Adaptive Submodular Maximization",
          "year": "2022-07",
          "abstract": "Many sequential decision making problems, including pool-based active\nlearning and adaptive viral marketing, can be formulated as an adaptive\nsubmodular maximization problem. Most of existing studies on adaptive\nsubmodular optimization focus on either monotone case or non-monotone case.\nSpecifically, if the utility function is monotone and adaptive submodular,\n\\cite{golovin2011adaptive} developed a greedy policy that achieves a $(1-1/e)$\napproximation ratio subject to a cardinality constraint. If the utility\nfunction is non-monotone and adaptive submodular, \\cite{tang2021beyond} showed\nthat a random greedy policy achieves a $1/e$ approximation ratio subject to a\ncardinality constraint. In this work, we aim to generalize the above mentioned\nresults by studying the partial-monotone adaptive submodular maximization\nproblem. To this end, we introduce the notation of adaptive monotonicity ratio\n$m\\in[0,1]$ to measure the degree of monotonicity of a function. Our main\nresult is to show that a random greedy policy achieves an approximation ratio\nof $m(1-1/e)+(1-m)(1/e)$ if the utility function is $m$-adaptive monotone and\nadaptive submodular. Notably this result recovers the aforementioned $(1-1/e)$\nand $1/e$ approximation ratios when $m = 0$ and $m = 1$, respectively. We\nfurther extend our results to consider a knapsack constraint. We show that a\nsampling-based policy achieves an approximation ratio of $(m+1)/10$ if the\nutility function is $m$-adaptive monotone and adaptive submodular. One\nimportant implication of our results is that even for a non-monotone utility\nfunction, we still can achieve an approximation ratio close to $(1-1/e)$ if\nthis function is ``close'' to a monotone function. This leads to improved\nperformance bounds for many machine learning applications whose utility\nfunctions are almost adaptive monotone.",
          "arxiv_id": "2207.12840v2"
        },
        {
          "title": "Practical Parallel Algorithms for Non-Monotone Submodular Maximization",
          "year": "2023-08",
          "abstract": "Submodular maximization has found extensive applications in various domains\nwithin the field of artificial intelligence, including but not limited to\nmachine learning, computer vision, and natural language processing. With the\nincreasing size of datasets in these domains, there is a pressing need to\ndevelop efficient and parallelizable algorithms for submodular maximization.\nOne measure of the parallelizability of a submodular maximization algorithm is\nits adaptive complexity, which indicates the number of sequential rounds where\na polynomial number of queries to the objective function can be executed in\nparallel. In this paper, we study the problem of non-monotone submodular\nmaximization subject to a knapsack constraint, and propose the first\ncombinatorial algorithm achieving an $(8+\\epsilon)$-approximation under\n$\\mathcal{O}(\\log n)$ adaptive complexity, which is \\textit{optimal} up to a\nfactor of $\\mathcal{O}(\\log\\log n)$. Moreover, we also propose the first\nalgorithm with both provable approximation ratio and sublinear adaptive\ncomplexity for the problem of non-monotone submodular maximization subject to a\n$k$-system constraint. As a by-product, we show that our two algorithms can\nalso be applied to the special case of submodular maximization subject to a\ncardinality constraint, and achieve performance bounds comparable with those of\nstate-of-the-art algorithms. Finally, the effectiveness of our approach is\ndemonstrated by extensive experiments on real-world applications.",
          "arxiv_id": "2308.10656v2"
        }
      ],
      "16": [
        {
          "title": "Tight Bounds for Adversarially Robust Streams and Sliding Windows via Difference Estimators",
          "year": "2020-11",
          "abstract": "In the adversarially robust streaming model, a stream of elements is\npresented to an algorithm and is allowed to depend on the output of the\nalgorithm at earlier times during the stream. In the classic insertion-only\nmodel of data streams, Ben-Eliezer et. al. (PODS 2020, best paper award) show\nhow to convert a non-robust algorithm into a robust one with a roughly\n$1/\\varepsilon$ factor overhead. This was subsequently improved to a\n$1/\\sqrt{\\varepsilon}$ factor overhead by Hassidim et. al. (NeurIPS 2020, oral\npresentation), suppressing logarithmic factors. For general functions the\nlatter is known to be best-possible, by a result of Kaplan et. al. (CRYPTO\n2021). We show how to bypass this impossibility result by developing data\nstream algorithms for a large class of streaming problems, with no overhead in\nthe approximation factor. Our class of streaming problems includes the most\nwell-studied problems such as the $L_2$-heavy hitters problem, $F_p$-moment\nestimation, as well as empirical entropy estimation. We substantially improve\nupon all prior work on these problems, giving the first optimal dependence on\nthe approximation factor.\n  As in previous work, we obtain a general transformation that applies to any\nnon-robust streaming algorithm and depends on the so-called twist number.\nHowever, the key technical innovation is that we apply the transformation to\nwhat we call a difference estimator for the streaming problem, rather than an\nestimator for the streaming problem itself. We then develop the first\ndifference estimators for a wide range of problems. Our difference estimator\nmethodology is not only applicable to the adversarially robust model, but to\nother streaming models where temporal properties of the data play a central\nrole. (Abstract shortened to meet arXiv limit.)",
          "arxiv_id": "2011.07471v5"
        },
        {
          "title": "A Framework for Adversarially Robust Streaming Algorithms",
          "year": "2020-03",
          "abstract": "We investigate the adversarial robustness of streaming algorithms. In this\ncontext, an algorithm is considered robust if its performance guarantees hold\neven if the stream is chosen adaptively by an adversary that observes the\noutputs of the algorithm along the stream and can react in an online manner.\nWhile deterministic streaming algorithms are inherently robust, many central\nproblems in the streaming literature do not admit sublinear-space deterministic\nalgorithms; on the other hand, classical space-efficient randomized algorithms\nfor these problems are generally not adversarially robust. This raises the\nnatural question of whether there exist efficient adversarially robust\n(randomized) streaming algorithms for these problems.\n  In this work, we show that the answer is positive for various important\nstreaming problems in the insertion-only model, including distinct elements and\nmore generally $F_p$-estimation, $F_p$-heavy hitters, entropy estimation, and\nothers. For all of these problems, we develop adversarially robust\n$(1+\\varepsilon)$-approximation algorithms whose required space matches that of\nthe best known non-robust algorithms up to a $\\text{poly}(\\log n,\n1/\\varepsilon)$ multiplicative factor (and in some cases even up to a constant\nfactor). Towards this end, we develop several generic tools allowing one to\nefficiently transform a non-robust streaming algorithm into a robust one in\nvarious scenarios.",
          "arxiv_id": "2003.14265v3"
        },
        {
          "title": "Double-Hashing Algorithm for Frequency Estimation in Data Streams",
          "year": "2022-04",
          "abstract": "Frequency estimation of elements is an important task for summarizing data\nstreams and machine learning applications. The problem is often addressed by\nusing streaming algorithms with sublinear space data structures. These\nalgorithms allow processing of large data while using limited data storage.\nCommonly used streaming algorithms, such as count-min sketch, have many\nadvantages, but do not take into account properties of a data stream for\nperformance optimization. In the present paper we introduce a novel\ndouble-hashing algorithm that provides flexibility to optimize streaming\nalgorithms depending on the properties of a given stream. In the double-hashing\napproach, first a standard streaming algorithm is employed to obtain an\nestimate of the element frequencies. This estimate is derived using a fraction\nof the stream and allows identification of the heavy hitters. Next, it uses a\nmodified hash table where the heavy hitters are mapped into individual buckets\nand other stream elements are mapped into the remaining buckets. Finally, the\nelement frequencies are estimated based on the constructed hash table over the\nentire data stream with any streaming algorithm. We demonstrate on both\nsynthetic data and an internet query log dataset that our approach is capable\nof improving frequency estimation due to removing heavy hitters from the\nhashing process and, thus, reducing collisions in the hash table. Our approach\navoids employing additional machine learning models to identify heavy hitters\nand, thus, reduces algorithm complexity and streamlines implementation.\nMoreover, because it is not dependent on specific features of the stream\nelements for identifying heavy hitters, it is applicable to a large variety of\nstreams. In addition, we propose a procedure on how to dynamically adjust the\nproposed double-hashing algorithm when frequencies of the elements in a stream\nare changing over time.",
          "arxiv_id": "2204.00650v1"
        }
      ],
      "17": [
        {
          "title": "Resolving Sets in Temporal Graphs",
          "year": "2024-03",
          "abstract": "A \\emph{resolving set} $R$ in a graph $G$ is a set of vertices such that\nevery vertex of $G$ is uniquely identified by its distances to the vertices of\n$R$. Introduced in the 1970s, this concept has been since then extensively\nstudied from both combinatorial and algorithmic points of view. We propose a\ngeneralization of the concept of resolving sets to temporal graphs,\n\\emph{i.e.}, graphs with edge sets that change over discrete time-steps. In\nthis setting, the \\emph{temporal distance from $u$ to $v$} is the earliest\npossible time-step at which a journey with strictly increasing time-steps on\nedges leaving $u$ reaches $v$, \\emph{i.e.}, the first time-step at which $v$\ncould receive a message broadcast from $u$. A \\emph{temporal resolving set} of\na temporal graph $\\mathcal{G}$ is a subset $R$ of its vertices such that every\nvertex of $\\mathcal{G}$ is uniquely identified by its temporal distances from\nvertices of $R$.\n  We study the problem of finding a minimum-size temporal resolving set, and\nshow that it is NP-complete even on very restricted graph classes and with\nstrong constraints on the time-steps: temporal complete graphs where every edge\nappears in either time-step~1 or~2, temporal trees where every edge appears in\nat most two consecutive time-steps, and even temporal subdivided stars where\nevery edge appears in at most two (not necessarily consecutive) time-steps. On\nthe other hand, we give polynomial-time algorithms for temporal paths and\ntemporal stars where every edge appears in exactly one time-step, and give a\ncombinatorial analysis and algorithms for several temporal graph classes where\nthe edges appear in periodic time-steps.",
          "arxiv_id": "2403.13183v3"
        },
        {
          "title": "Temporal Walk Centrality: Ranking Nodes in Evolving Networks",
          "year": "2022-02",
          "abstract": "We propose the Temporal Walk Centrality, which quantifies the importance of a\nnode by measuring its ability to obtain and distribute information in a\ntemporal network. In contrast to the widely-used betweenness centrality, we\nassume that information does not necessarily spread on shortest paths but on\ntemporal random walks that satisfy the time constraints of the network. We show\nthat temporal walk centrality can identify nodes playing central roles in\ndissemination processes that might not be detected by related betweenness\nconcepts and other common static and temporal centrality measures. We propose\nexact and approximation algorithms with different running times depending on\nthe properties of the temporal network and parameters of our new centrality\nmeasure. A technical contribution is a general approach to lift existing\nalgebraic methods for counting walks in static networks to temporal networks.\nOur experiments on real-world temporal networks show the efficiency and\naccuracy of our algorithms. Finally, we demonstrate that the rankings by\ntemporal walk centrality often differ significantly from those of other\nstate-of-the-art temporal centralities.",
          "arxiv_id": "2202.03706v1"
        },
        {
          "title": "Sparse Temporal Spanners with Low Stretch",
          "year": "2022-06",
          "abstract": "A temporal graph is an undirected graph $G=(V,E)$ along with a function that\nassigns a time-label to each edge in $E$. A path in $G$ with non-decreasing\ntime-labels is called temporal path and the distance from $u$ to $v$ is the\nminimum length (i.e., the number of edges) of a temporal path from $u$ to $v$.\nA temporal $\\alpha$-spanner of $G$ is a (temporal) subgraph $H$ that preserves\nthe distances between any pair of vertices in $V$, up to a multiplicative\nstretch factor of $\\alpha$. The size of $H$ is the number of its edges.\n  In this work we study the size-stretch trade-offs of temporal spanners. We\nshow that temporal cliques always admit a temporal $(2k-1)-$spanner with\n$\\tilde{O}(kn^{1+\\frac{1}{k}})$ edges, where $k>1$ is an integer parameter of\nchoice. Choosing $k=\\lfloor\\log n\\rfloor$, we obtain a temporal $O(\\log\nn)$-spanner with $\\tilde{O}(n)$ edges that has almost the same size (up to\nlogarithmic factors) as the temporal spanner in [Casteigts et al., JCSS 2021]\nwhich only preserves temporal connectivity.\n  We then consider general temporal graphs. Since $\\Omega(n^2)$ edges might be\nneeded by any connectivity-preserving temporal subgraph [Axiotis et al.,\nICALP'16], we focus on approximating distances from a single source. We show\nthat $\\tilde{O}(n/\\log(1+\\varepsilon))$ edges suffice to obtain a stretch of\n$(1+\\varepsilon)$, for any small $\\varepsilon>0$. This result is essentially\ntight since there are temporal graphs for which any temporal subgraph\npreserving exact distances from a single-source must use $\\Omega(n^2)$ edges.\nWe extend our analysis to prove an upper bound of $\\tilde{O}(n^2/\\beta)$ on the\nsize of any temporal $\\beta$-additive spanner, which is tight up to\npolylogarithmic factors.\n  Finally, we investigate how the lifetime of $G$, i.e., the number of its\ndistinct time-labels, affects the trade-off between the size and the stretch of\na temporal spanner.",
          "arxiv_id": "2206.11113v1"
        }
      ],
      "18": [
        {
          "title": "Flow-augmentation III: Complexity dichotomy for Boolean CSPs parameterized by the number of unsatisfied constraints",
          "year": "2022-07",
          "abstract": "We study the parameterized problem of satisfying ``almost all'' constraints\nof a given formula $F$ over a fixed, finite Boolean constraint language\n$\\Gamma$, with or without weights. More precisely, for each finite Boolean\nconstraint language $\\Gamma$, we consider the following two problems. In Min\nSAT$(\\Gamma)$, the input is a formula $F$ over $\\Gamma$ and an integer $k$, and\nthe task is to find an assignment $\\alpha \\colon V(F) \\to \\{0,1\\}$ that\nsatisfies all but at most $k$ constraints of $F$, or determine that no such\nassignment exists. In Weighted Min SAT$(\\Gamma$), the input additionally\ncontains a weight function $w \\colon F \\to \\mathbb{Z}_+$ and an integer $W$,\nand the task is to find an assignment $\\alpha$ such that (1) $\\alpha$ satisfies\nall but at most $k$ constraints of $F$, and (2) the total weight of the\nviolated constraints is at most $W$. We give a complete dichotomy for the\nfixed-parameter tractability of these problems: We show that for every Boolean\nconstraint language $\\Gamma$, either Weighted Min SAT$(\\Gamma)$ is FPT; or\nWeighted Min SAT$(\\Gamma)$ is W[1]-hard but Min SAT$(\\Gamma)$ is FPT; or Min\nSAT$(\\Gamma)$ is W[1]-hard. This generalizes recent work of Kim et al. (SODA\n2021) which did not consider weighted problems, and only considered languages\n$\\Gamma$ that cannot express implications $(u \\to v)$ (as is used to, e.g.,\nmodel digraph cut problems). Our result generalizes and subsumes multiple\nprevious results, including the FPT algorithms for Weighted Almost 2-SAT,\nweighted and unweighted $\\ell$-Chain SAT, and Coupled Min-Cut, as well as\nweighted and directed versions of the latter. The main tool used in our\nalgorithms is the recently developed method of directed flow-augmentation (Kim\net al., STOC 2022).",
          "arxiv_id": "2207.07422v3"
        },
        {
          "title": "MAJORITY-3SAT (and Related Problems) in Polynomial Time",
          "year": "2021-07",
          "abstract": "Majority-SAT is the problem of determining whether an input $n$-variable\nformula in conjunctive normal form (CNF) has at least $2^{n-1}$ satisfying\nassignments. Majority-SAT and related problems have been studied extensively in\nvarious AI communities interested in the complexity of probabilistic planning\nand inference. Although Majority-SAT has been known to be PP-complete for over\n40 years, the complexity of a natural variant has remained open:\nMajority-$k$SAT, where the input CNF formula is restricted to have clause width\nat most $k$.\n  We prove that for every $k$, Majority-$k$SAT is in P. In fact, for any\npositive integer $k$ and rational $\\rho \\in (0,1)$ with bounded denominator, we\ngive an algorithm that can determine whether a given $k$-CNF has at least $\\rho\n\\cdot 2^n$ satisfying assignments, in deterministic linear time (whereas the\nprevious best-known algorithm ran in exponential time). Our algorithms have\ninteresting positive implications for counting complexity and the complexity of\ninference, significantly reducing the known complexities of related problems\nsuch as E-MAJ-$k$SAT and MAJ-MAJ-$k$SAT. At the heart of our approach is an\nefficient method for solving threshold counting problems by extracting\nsunflowers found in the corresponding set system of a $k$-CNF.\n  We also show that the tractability of Majority-$k$SAT is somewhat fragile.\nFor the closely related GtMajority-SAT problem (where we ask whether a given\nformula has greater than $2^{n-1}$ satisfying assignments) which is known to be\nPP-complete, we show that GtMajority-$k$SAT is in P for $k\\le 3$, but becomes\nNP-complete for $k\\geq 4$. These results are counterintuitive, because the\n``natural'' classifications of these problems would have been PP-completeness,\nand because there is a stark difference in the complexity of GtMajority-$k$SAT\nand Majority-$k$SAT for all $k\\ge 4$.",
          "arxiv_id": "2107.02748v2"
        },
        {
          "title": "On the Parameterized Complexity of Diverse SAT",
          "year": "2024-12",
          "abstract": "We study the Boolean Satisfiability problem (SAT) in the framework of\ndiversity, where one asks for multiple solutions that are mutually far apart\n(i.e., sufficiently dissimilar from each other) for a suitable notion of\ndistance/dissimilarity between solutions. Interpreting assignments as bit\nvectors, we take their Hamming distance to quantify dissimilarity, and we focus\non problem of finding two solutions. Specifically, we define the problem MAX\nDIFFER SAT (resp. EXACT DIFFER SAT) as follows: Given a Boolean formula $\\phi$\non $n$ variables, decide whether $\\phi$ has two satisfying assignments that\ndiffer on at least (resp. exactly) $d$ variables. We study classical and\nparameterized (in parameters $d$ and $n-d$) complexities of MAX DIFFER SAT and\nEXACT DIFFER SAT, when restricted to some formula-classes on which SAT is known\nto be polynomial-time solvable. In particular, we consider affine formulas,\n$2$-CNF formulas and hitting formulas.\n  For affine formulas, we show the following: Both problems are polynomial-time\nsolvable when each equation has at most two variables. EXACT DIFFER SAT is\nNP-hard, even when each equation has at most three variables and each variable\nappears in at most four equations. Also, MAX DIFFER SAT is NP-hard, even when\neach equation has at most four variables. Both problems are W[1]-hard in the\nparameter $n-d$. In contrast, when parameterized by $d$, EXACT DIFFER SAT is\nW[1]-hard, but MAX DIFFER SAT admits a single-exponential FPT algorithm and a\npolynomial-kernel.\n  For 2-CNF formulas, we show the following: Both problems are polynomial-time\nsolvable when each variable appears in at most two clauses. Also, both problems\nare W[1]-hard in the parameter $d$ (and therefore, it turns out, also NP-hard),\neven on monotone inputs (i.e., formulas with no negative literals). Finally,\nfor hitting formulas, we show that both problems are polynomial-time solvable.",
          "arxiv_id": "2412.09717v1"
        }
      ],
      "19": [
        {
          "title": "Approximate Nearest Neighbor Search with Window Filters",
          "year": "2024-02",
          "abstract": "We define and investigate the problem of $\\textit{c-approximate window\nsearch}$: approximate nearest neighbor search where each point in the dataset\nhas a numeric label, and the goal is to find nearest neighbors to queries\nwithin arbitrary label ranges. Many semantic search problems, such as image and\ndocument search with timestamp filters, or product search with cost filters,\nare natural examples of this problem. We propose and theoretically analyze a\nmodular tree-based framework for transforming an index that solves the\ntraditional c-approximate nearest neighbor problem into a data structure that\nsolves window search. On standard nearest neighbor benchmark datasets equipped\nwith random label values, adversarially constructed embeddings, and image\nsearch embeddings with real timestamps, we obtain up to a $75\\times$ speedup\nover existing solutions at the same level of recall.",
          "arxiv_id": "2402.00943v2"
        },
        {
          "title": "Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations",
          "year": "2023-10",
          "abstract": "Graph-based approaches to nearest neighbor search are popular and powerful\ntools for handling large datasets in practice, but they have limited\ntheoretical guarantees. We study the worst-case performance of recent\ngraph-based approximate nearest neighbor search algorithms, such as HNSW, NSG\nand DiskANN. For DiskANN, we show that its \"slow preprocessing\" version\nprovably supports approximate nearest neighbor search query with constant\napproximation ratio and poly-logarithmic query time, on data sets with bounded\n\"intrinsic\" dimension. For the other data structure variants studied, including\nDiskANN with \"fast preprocessing\", HNSW and NSG, we present a family of\ninstances on which the empirical query time required to achieve a \"reasonable\"\naccuracy is linear in instance size. For example, for DiskANN, we show that the\nquery procedure can take at least $0.1 n$ steps on instances of size $n$ before\nit encounters any of the $5$ nearest neighbors of the query.",
          "arxiv_id": "2310.19126v1"
        },
        {
          "title": "Efficient Approximate Nearest Neighbor Search for Multiple Weighted $l_{p\\leq2}$ Distance Functions",
          "year": "2020-11",
          "abstract": "Nearest neighbor search is fundamental to a wide range of applications. Since\nthe exact nearest neighbor search suffers from the \"curse of dimensionality\",\napproximate approaches, such as Locality-Sensitive Hashing (LSH), are widely\nused to trade a little query accuracy for a much higher query efficiency. In\nmany scenarios, it is necessary to perform nearest neighbor search under\nmultiple weighted distance functions in high-dimensional spaces. This paper\nconsiders the important problem of supporting efficient approximate nearest\nneighbor search for multiple weighted distance functions in high-dimensional\nspaces. To the best of our knowledge, prior work can only solve the problem for\nthe $l_2$ distance. However, numerous studies have shown that the $l_p$\ndistance with $p\\in(0,2)$ could be more effective than the $l_2$ distance in\nhigh-dimensional spaces. We propose a novel method, WLSH, to address the\nproblem for the $l_p$ distance for $p\\in(0,2]$. WLSH takes the LSH approach and\ncan theoretically guarantee both the efficiency of processing queries and the\naccuracy of query results while minimizing the required total number of hash\ntables. We conduct extensive experiments on synthetic and real data sets, and\nthe results show that WLSH achieves high performance in terms of query\nefficiency, query accuracy and space consumption.",
          "arxiv_id": "2011.11907v1"
        }
      ],
      "20": [
        {
          "title": "Optimal Multi-Pass Lower Bounds for MST in Dynamic Streams",
          "year": "2023-12",
          "abstract": "The seminal work of Ahn, Guha, and McGregor in 2012 introduced the graph\nsketching technique and used it to present the first streaming algorithms for\nvarious graph problems over dynamic streams with both insertions and deletions\nof edges. This includes algorithms for cut sparsification, spanners, matchings,\nand minimum spanning trees (MSTs). These results have since been improved or\ngeneralized in various directions, leading to a vastly rich host of efficient\nalgorithms for processing dynamic graph streams.\n  A curious omission from the list of improvements has been the MST problem.\nThe best algorithm for this problem remains the original AGM algorithm that for\nevery integer $p \\geq 1$, uses $n^{1+O(1/p)}$ space in $p$ passes on $n$-vertex\ngraphs, and thus achieves the desired semi-streaming space of $\\tilde{O}(n)$ at\na relatively high cost of $O(\\frac{\\log{n}}{\\log\\log{n}})$ passes. On the other\nhand, no lower bounds beyond a folklore one-pass lower bound is known for this\nproblem.\n  We provide a simple explanation for this lack of improvements: The AGM\nalgorithm for MSTs is optimal for the entire range of its number of passes! We\nprove that even for the simplest decision version of the problem -- deciding\nwhether the weight of MSTs is at least a given threshold or not -- any $p$-pass\ndynamic streaming algorithm requires $n^{1+\\Omega(1/p)}$ space. This implies\nthat semi-streaming algorithms do need $\\Omega(\\frac{\\log{n}}{\\log\\log{n}})$\npasses.\n  Our result relies on proving new multi-round communication complexity lower\nbounds for a variant of the universal relation problem that has been\ninstrumental in proving prior lower bounds for single-pass dynamic streaming\nalgorithms. The proof also involves proving new composition theorems in\ncommunication complexity, including majority lemmas and multi-party XOR lemmas,\nvia information complexity approaches.",
          "arxiv_id": "2312.04674v1"
        },
        {
          "title": "Polynomial Pass Semi-Streaming Lower Bounds for K-Cores and Degeneracy",
          "year": "2024-05",
          "abstract": "The following question arises naturally in the study of graph streaming\nalgorithms:\n  \"Is there any graph problem which is \"not too hard\", in that it can be solved\nefficiently with total communication (nearly) linear in the number $n$ of\nvertices, and for which, nonetheless, any streaming algorithm with\n$\\tilde{O}(n)$ space (i.e., a semi-streaming algorithm) needs a polynomial\n$n^{\\Omega(1)}$ number of passes?\"\n  Assadi, Chen, and Khanna [STOC 2019] were the first to prove that this is\nindeed the case. However, the lower bounds that they obtained are for rather\nnon-standard graph problems.\n  Our first main contribution is to present the first polynomial-pass lower\nbounds for natural \"not too hard\" graph problems studied previously in the\nstreaming model: $k$-cores and degeneracy. We devise a novel communication\nprotocol for both problems with near-linear communication, thus showing that\n$k$-cores and degeneracy are natural examples of \"not too hard\" problems.\nIndeed, previous work have developed single-pass semi-streaming algorithms for\napproximating these problems. In contrast, we prove that any semi-streaming\nalgorithm for exactly solving these problems requires (almost)\n$\\Omega(n^{1/3})$ passes.\n  Our second main contribution is improved round-communication lower bounds for\nthe underlying communication problems at the basis of these reductions:\n  * We improve the previous lower bound of Assadi, Chen, and Khanna for hidden\npointer chasing (HPC) to achieve optimal bounds.\n  * We observe that all current reductions from HPC can also work with a\ngeneralized version of this problem that we call MultiHPC, and prove an even\nstronger and optimal lower bound for this generalization.\n  These two results collectively allow us to improve the resulting pass lower\nbounds for semi-streaming algorithms by a polynomial factor, namely, from\n$n^{1/5}$ to $n^{1/3}$ passes.",
          "arxiv_id": "2405.14835v1"
        },
        {
          "title": "Settling the Pass Complexity of Approximate Matchings in Dynamic Graph Streams",
          "year": "2024-07",
          "abstract": "A semi-streaming algorithm in dynamic graph streams processes any $n$-vertex\ngraph by making one or multiple passes over a stream of insertions and\ndeletions to edges of the graph and using $O(n \\cdot \\mbox{polylog}(n))$ space.\nSemi-streaming algorithms for dynamic streams were first obtained in the\nseminal work of Ahn, Guha, and McGregor in 2012, alongside the introduction of\nthe graph sketching technique, which remains the de facto way of designing\nalgorithms in this model and a highly popular technique for designing graph\nalgorithms in general.\n  We settle the pass complexity of approximating maximum matchings in dynamic\nstreams via semi-streaming algorithms by improving the state-of-the-art in both\nupper and lower bounds.\n  We present a randomized sketching based semi-streaming algorithm for\n$O(1)$-approximation of maximum matching in dynamic streams using\n$O(\\log\\log{n})$ passes. The approximation ratio of this algorithm can be\nimproved to $(1+\\epsilon)$ for any fixed $\\epsilon > 0$ even on weighted graphs\nusing standard techniques. This exponentially improves upon several\n$O(\\log{n})$ pass algorithms developed for this problem since the introduction\nof the dynamic graph streaming model.\n  In addition, we prove that any semi-streaming algorithm (not only sketching\nbased) for $O(1)$-approximation of maximum matching in dynamic streams requires\n$\\Omega(\\log\\log{n})$ passes. This presents the first multi-pass lower bound\nfor this problem, which is already also optimal, settling a longstanding open\nquestion in this area.",
          "arxiv_id": "2407.21005v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:41:11Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}