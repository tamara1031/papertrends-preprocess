{
  "topics": {
    "data": {
      "0": {
        "name": "0_blockchain_Blockchain_smart_contracts",
        "keywords": [
          [
            "blockchain",
            0.02991446772601258
          ],
          [
            "Blockchain",
            0.014150928240691753
          ],
          [
            "smart",
            0.012776892738678951
          ],
          [
            "contracts",
            0.011297772138192597
          ],
          [
            "transactions",
            0.011195380128421369
          ],
          [
            "transaction",
            0.010594913767107363
          ],
          [
            "chain",
            0.010589131457271026
          ],
          [
            "Ethereum",
            0.01032858703217443
          ],
          [
            "data",
            0.0090575560250022
          ],
          [
            "decentralized",
            0.00893976178183655
          ]
        ],
        "count": 3164
      },
      "1": {
        "name": "1_LLMs_LLM_models_AI",
        "keywords": [
          [
            "LLMs",
            0.02638918917009904
          ],
          [
            "LLM",
            0.018036861598310854
          ],
          [
            "models",
            0.013912469857861999
          ],
          [
            "AI",
            0.012687771807580578
          ],
          [
            "language",
            0.012630859027599351
          ],
          [
            "Language",
            0.012239949132849502
          ],
          [
            "Large",
            0.011885288804850522
          ],
          [
            "Models",
            0.010399016792349407
          ],
          [
            "safety",
            0.010220080413880455
          ],
          [
            "model",
            0.009836085344283487
          ]
        ],
        "count": 2171
      },
      "2": {
        "name": "2_quantum_key_Quantum_classical",
        "keywords": [
          [
            "quantum",
            0.05071660352351523
          ],
          [
            "key",
            0.017285443777144874
          ],
          [
            "Quantum",
            0.016733354220820142
          ],
          [
            "classical",
            0.014245558855473609
          ],
          [
            "cryptography",
            0.011730107432784308
          ],
          [
            "security",
            0.011032242607980988
          ],
          [
            "secure",
            0.010698255960733366
          ],
          [
            "scheme",
            0.010696242223994325
          ],
          [
            "protocol",
            0.010658431004583207
          ],
          [
            "cryptographic",
            0.010134242124025788
          ]
        ],
        "count": 1922
      },
      "3": {
        "name": "3_privacy_DP_private_differential privacy",
        "keywords": [
          [
            "privacy",
            0.034940775946525046
          ],
          [
            "DP",
            0.02871567385935105
          ],
          [
            "private",
            0.020932498183852052
          ],
          [
            "differential privacy",
            0.017695477856910072
          ],
          [
            "differential",
            0.017530161662666233
          ],
          [
            "data",
            0.015980974547727997
          ],
          [
            "Privacy",
            0.012255260573134035
          ],
          [
            "utility",
            0.012075569934724091
          ],
          [
            "Differential",
            0.011314874440203931
          ],
          [
            "Private",
            0.010929406137403735
          ]
        ],
        "count": 1472
      },
      "4": {
        "name": "4_hardware_security_memory_attacks",
        "keywords": [
          [
            "hardware",
            0.016572023395922945
          ],
          [
            "security",
            0.014383438743034374
          ],
          [
            "memory",
            0.013148247574147927
          ],
          [
            "attacks",
            0.010681181272204727
          ],
          [
            "execution",
            0.010547026585652186
          ],
          [
            "channel",
            0.010351233723510204
          ],
          [
            "design",
            0.009492469686521936
          ],
          [
            "software",
            0.00885640849866013
          ],
          [
            "code",
            0.008045852418812768
          ],
          [
            "cache",
            0.007762236391936689
          ]
        ],
        "count": 1471
      },
      "5": {
        "name": "5_FL_Federated_learning_model",
        "keywords": [
          [
            "FL",
            0.031050517792134975
          ],
          [
            "Federated",
            0.025968561398945947
          ],
          [
            "learning",
            0.02120028569162961
          ],
          [
            "model",
            0.019515333097804916
          ],
          [
            "privacy",
            0.019109905187594332
          ],
          [
            "clients",
            0.01897323546772632
          ],
          [
            "Learning",
            0.017403520488007012
          ],
          [
            "data",
            0.016708488919355426
          ],
          [
            "training",
            0.012667442274663734
          ],
          [
            "local",
            0.011680393856394333
          ]
        ],
        "count": 1286
      },
      "6": {
        "name": "6_adversarial_robustness_attacks_Adversarial",
        "keywords": [
          [
            "adversarial",
            0.043103976900902424
          ],
          [
            "robustness",
            0.019004110148684333
          ],
          [
            "attacks",
            0.01861737689764491
          ],
          [
            "Adversarial",
            0.018592098464619223
          ],
          [
            "examples",
            0.017444293965978856
          ],
          [
            "adversarial examples",
            0.016072853705159085
          ],
          [
            "attack",
            0.0140642525525123
          ],
          [
            "models",
            0.013761841710669208
          ],
          [
            "adversarial attacks",
            0.013406725025898026
          ],
          [
            "perturbations",
            0.012230208352621856
          ]
        ],
        "count": 1055
      },
      "7": {
        "name": "7_detection_network_traffic_IoT",
        "keywords": [
          [
            "detection",
            0.021818015341404637
          ],
          [
            "network",
            0.020316369244419638
          ],
          [
            "traffic",
            0.018704649015107117
          ],
          [
            "IoT",
            0.01702213455583731
          ],
          [
            "intrusion",
            0.013761345642231476
          ],
          [
            "Detection",
            0.013027699524338958
          ],
          [
            "learning",
            0.012021332482548297
          ],
          [
            "intrusion detection",
            0.011597976411501629
          ],
          [
            "Intrusion",
            0.011334781223671974
          ],
          [
            "IDS",
            0.010968317362135003
          ]
        ],
        "count": 1043
      },
      "8": {
        "name": "8_code_vulnerability_software_vulnerabilities",
        "keywords": [
          [
            "code",
            0.03759808068006647
          ],
          [
            "vulnerability",
            0.021481771247395257
          ],
          [
            "software",
            0.021358207898338612
          ],
          [
            "vulnerabilities",
            0.018243499469390304
          ],
          [
            "security",
            0.0144310765827946
          ],
          [
            "source",
            0.012297425183585203
          ],
          [
            "LLMs",
            0.011533605963250558
          ],
          [
            "vulnerability detection",
            0.010246046580840499
          ],
          [
            "detection",
            0.009788318467680879
          ],
          [
            "Code",
            0.009602742459683832
          ]
        ],
        "count": 761
      },
      "9": {
        "name": "9_malware_detection_Malware_malware detection",
        "keywords": [
          [
            "malware",
            0.06355154348241336
          ],
          [
            "detection",
            0.021946019508557558
          ],
          [
            "Malware",
            0.021062776143971902
          ],
          [
            "malware detection",
            0.018575472691950312
          ],
          [
            "Android",
            0.015593077748114857
          ],
          [
            "ransomware",
            0.01480131271811555
          ],
          [
            "learning",
            0.012521514149936134
          ],
          [
            "classification",
            0.010793092614755742
          ],
          [
            "features",
            0.009923784049891556
          ],
          [
            "analysis",
            0.009869378046467831
          ]
        ],
        "count": 728
      },
      "10": {
        "name": "10_cyber_cybersecurity_security_Cyber",
        "keywords": [
          [
            "cyber",
            0.029874697380510323
          ],
          [
            "cybersecurity",
            0.018999016492102276
          ],
          [
            "security",
            0.01847808234952777
          ],
          [
            "Cyber",
            0.013252551741045584
          ],
          [
            "threats",
            0.009874991799007103
          ],
          [
            "research",
            0.008927472093401773
          ],
          [
            "risk",
            0.008606223743598249
          ],
          [
            "systems",
            0.008548549928528106
          ],
          [
            "threat",
            0.008015836730754134
          ],
          [
            "network",
            0.00789145551872645
          ]
        ],
        "count": 705
      },
      "11": {
        "name": "11_watermarking_watermark_image_watermarks",
        "keywords": [
          [
            "watermarking",
            0.040043786571352886
          ],
          [
            "watermark",
            0.03196000730123098
          ],
          [
            "image",
            0.02081497893002871
          ],
          [
            "watermarks",
            0.01669173923034226
          ],
          [
            "Watermarking",
            0.014742657525570435
          ],
          [
            "embedding",
            0.014576295407482382
          ],
          [
            "model",
            0.014244422105685287
          ],
          [
            "steganography",
            0.013090652736897143
          ],
          [
            "images",
            0.012630055431010024
          ],
          [
            "models",
            0.01096057928447843
          ]
        ],
        "count": 621
      },
      "12": {
        "name": "12_backdoor_attacks_model_backdoor attacks",
        "keywords": [
          [
            "backdoor",
            0.044095669612727574
          ],
          [
            "attacks",
            0.021030499241389316
          ],
          [
            "model",
            0.018780572528588117
          ],
          [
            "backdoor attacks",
            0.018651478059312665
          ],
          [
            "Backdoor",
            0.016647710922619158
          ],
          [
            "training",
            0.016567724213909805
          ],
          [
            "attack",
            0.016484150674693613
          ],
          [
            "trigger",
            0.016356225348541007
          ],
          [
            "poisoning",
            0.014061800690632686
          ],
          [
            "clean",
            0.013544813625561506
          ]
        ],
        "count": 540
      },
      "13": {
        "name": "13_biometric_face_recognition_authentication",
        "keywords": [
          [
            "biometric",
            0.025037977150988506
          ],
          [
            "face",
            0.024805599416535724
          ],
          [
            "recognition",
            0.018416925824266944
          ],
          [
            "authentication",
            0.01818158330494555
          ],
          [
            "facial",
            0.014531512223765837
          ],
          [
            "images",
            0.013473467795646402
          ],
          [
            "user",
            0.011603016004286692
          ],
          [
            "Face",
            0.010360863231653882
          ],
          [
            "systems",
            0.009690350054988382
          ],
          [
            "privacy",
            0.009544899581161092
          ]
        ],
        "count": 529
      },
      "14": {
        "name": "14_vehicles_vehicle_CAN_security",
        "keywords": [
          [
            "vehicles",
            0.022855024011897058
          ],
          [
            "vehicle",
            0.02170006253091397
          ],
          [
            "CAN",
            0.018316804498070643
          ],
          [
            "security",
            0.013174967165761019
          ],
          [
            "attacks",
            0.012891238081024296
          ],
          [
            "communication",
            0.011868712463755654
          ],
          [
            "detection",
            0.008911261262864205
          ],
          [
            "vehicular",
            0.008429528282061845
          ],
          [
            "Vehicles",
            0.008421185045821658
          ],
          [
            "attack",
            0.00834730771266326
          ]
        ],
        "count": 457
      },
      "15": {
        "name": "15_wireless_security_network_networks",
        "keywords": [
          [
            "wireless",
            0.01618144225603761
          ],
          [
            "security",
            0.01562801100115633
          ],
          [
            "network",
            0.013975852479308545
          ],
          [
            "networks",
            0.013966138205652321
          ],
          [
            "jamming",
            0.011943209722711652
          ],
          [
            "channel",
            0.011816382543837722
          ],
          [
            "communication",
            0.011232210343005216
          ],
          [
            "layer",
            0.008467324002549067
          ],
          [
            "signal",
            0.008033025172863136
          ],
          [
            "authentication",
            0.007976346873745748
          ]
        ],
        "count": 453
      },
      "16": {
        "name": "16_FHE_encryption_data_encrypted",
        "keywords": [
          [
            "FHE",
            0.025355673167790905
          ],
          [
            "encryption",
            0.02230321780318519
          ],
          [
            "data",
            0.02117839988222331
          ],
          [
            "encrypted",
            0.018550591444507228
          ],
          [
            "Encryption",
            0.018209668448180536
          ],
          [
            "cloud",
            0.015048869851283496
          ],
          [
            "HE",
            0.014868576740098984
          ],
          [
            "Homomorphic",
            0.013903926969579131
          ],
          [
            "computation",
            0.012948608114005939
          ],
          [
            "homomorphic",
            0.012924275290267729
          ]
        ],
        "count": 379
      },
      "17": {
        "name": "17_speech_audio_voice_speaker",
        "keywords": [
          [
            "speech",
            0.04686557657620578
          ],
          [
            "audio",
            0.03706176897827683
          ],
          [
            "voice",
            0.028731485230678595
          ],
          [
            "speaker",
            0.02510156260635711
          ],
          [
            "ASR",
            0.016347266338900355
          ],
          [
            "Speech",
            0.013778520923987327
          ],
          [
            "adversarial",
            0.013426625928566534
          ],
          [
            "attacks",
            0.01311564363002275
          ],
          [
            "systems",
            0.012217452043504813
          ],
          [
            "recognition",
            0.011973694892048406
          ]
        ],
        "count": 330
      },
      "18": {
        "name": "18_inference_HE_computation_privacy",
        "keywords": [
          [
            "inference",
            0.025783213567381603
          ],
          [
            "HE",
            0.01633071514868226
          ],
          [
            "computation",
            0.01454340331998746
          ],
          [
            "privacy",
            0.013664791847671604
          ],
          [
            "MPC",
            0.013072843138152377
          ],
          [
            "secure",
            0.012984083075213218
          ],
          [
            "data",
            0.012702509709013057
          ],
          [
            "neural",
            0.0117481061544386
          ],
          [
            "model",
            0.011446588226004358
          ],
          [
            "encrypted",
            0.01098639373749697
          ]
        ],
        "count": 326
      },
      "19": {
        "name": "19_IoT_devices_security_Things",
        "keywords": [
          [
            "IoT",
            0.05634008434133146
          ],
          [
            "devices",
            0.02858518059580614
          ],
          [
            "security",
            0.021248439038344034
          ],
          [
            "Things",
            0.020337226288445663
          ],
          [
            "Internet",
            0.02019467738508758
          ],
          [
            "IoT devices",
            0.015407092427336688
          ],
          [
            "smart",
            0.013508510720340325
          ],
          [
            "home",
            0.012280905849217873
          ],
          [
            "device",
            0.011551915785941991
          ],
          [
            "trust",
            0.009614754962319432
          ]
        ],
        "count": 318
      },
      "20": {
        "name": "20_phishing_Phishing_emails_detection",
        "keywords": [
          [
            "phishing",
            0.07039168411078243
          ],
          [
            "Phishing",
            0.026614284671711796
          ],
          [
            "emails",
            0.022261352822776033
          ],
          [
            "detection",
            0.01913053217369518
          ],
          [
            "email",
            0.01738284268360001
          ],
          [
            "URL",
            0.015777302496665882
          ],
          [
            "spam",
            0.014381945648821394
          ],
          [
            "URLs",
            0.013506846517003809
          ],
          [
            "phishing emails",
            0.011699614748091766
          ],
          [
            "attacks",
            0.010916757944521847
          ]
        ],
        "count": 309
      },
      "21": {
        "name": "21_graph_GNNs_Graph_node",
        "keywords": [
          [
            "graph",
            0.06032004668261623
          ],
          [
            "GNNs",
            0.033192982524423265
          ],
          [
            "Graph",
            0.03240062389779144
          ],
          [
            "node",
            0.022952618753204503
          ],
          [
            "GNN",
            0.022078813815472605
          ],
          [
            "attack",
            0.01609756022875757
          ],
          [
            "graphs",
            0.015635717185902252
          ],
          [
            "nodes",
            0.014062124553975104
          ],
          [
            "attacks",
            0.014009976903831352
          ],
          [
            "Neural",
            0.013779933767816947
          ]
        ],
        "count": 299
      },
      "22": {
        "name": "22_grid_power_smart_grids",
        "keywords": [
          [
            "grid",
            0.03822762792408698
          ],
          [
            "power",
            0.02692778667707056
          ],
          [
            "smart",
            0.022772162979899615
          ],
          [
            "grids",
            0.016863638664529182
          ],
          [
            "cyber",
            0.016814675569788877
          ],
          [
            "smart grid",
            0.016017211218454744
          ],
          [
            "energy",
            0.015369990620429446
          ],
          [
            "Smart",
            0.014046969279619665
          ],
          [
            "data",
            0.012726996824532048
          ],
          [
            "power grid",
            0.012210689321842874
          ]
        ],
        "count": 298
      },
      "23": {
        "name": "23_DNS_Tor_traffic_Internet",
        "keywords": [
          [
            "DNS",
            0.038485754268419485
          ],
          [
            "Tor",
            0.023968876853846903
          ],
          [
            "traffic",
            0.02334188781658211
          ],
          [
            "Internet",
            0.017224919525991696
          ],
          [
            "network",
            0.015705195525733528
          ],
          [
            "TLS",
            0.013659303765118367
          ],
          [
            "WF",
            0.011652389468120247
          ],
          [
            "attacks",
            0.010156234874383373
          ],
          [
            "protocol",
            0.010114735579703021
          ],
          [
            "In",
            0.009379526711330712
          ]
        ],
        "count": 291
      },
      "24": {
        "name": "24_models_image_images_diffusion",
        "keywords": [
          [
            "models",
            0.030036017063853337
          ],
          [
            "image",
            0.029426001279217802
          ],
          [
            "images",
            0.024635511485500444
          ],
          [
            "diffusion",
            0.02461584227632423
          ],
          [
            "diffusion models",
            0.019691033164684978
          ],
          [
            "Diffusion",
            0.018977874518953327
          ],
          [
            "text",
            0.018465259442875595
          ],
          [
            "generation",
            0.015706738229032714
          ],
          [
            "generative",
            0.014509719577444397
          ],
          [
            "prompts",
            0.013346351709111024
          ]
        ],
        "count": 249
      },
      "25": {
        "name": "25_unlearning_inference_model_membership",
        "keywords": [
          [
            "unlearning",
            0.03004571490268426
          ],
          [
            "inference",
            0.025712201875206248
          ],
          [
            "model",
            0.025324324650567873
          ],
          [
            "membership",
            0.025259989834788728
          ],
          [
            "training",
            0.021388212322414092
          ],
          [
            "data",
            0.019724822696104986
          ],
          [
            "membership inference",
            0.019155774842445277
          ],
          [
            "models",
            0.017422053025799075
          ],
          [
            "attacks",
            0.01723356209736231
          ],
          [
            "Membership",
            0.01646748378041207
          ]
        ],
        "count": 243
      },
      "26": {
        "name": "26_apps_Android_app_mobile",
        "keywords": [
          [
            "apps",
            0.06540627473140424
          ],
          [
            "Android",
            0.04718056656686727
          ],
          [
            "app",
            0.04030293637806452
          ],
          [
            "mobile",
            0.020073102058051937
          ],
          [
            "privacy",
            0.017886231541411474
          ],
          [
            "user",
            0.013963390075122347
          ],
          [
            "users",
            0.013142483907618852
          ],
          [
            "analysis",
            0.013121248325200723
          ],
          [
            "security",
            0.012894001031657679
          ],
          [
            "developers",
            0.012555928794492535
          ]
        ],
        "count": 239
      },
      "27": {
        "name": "27_synthetic_data_synthetic data_privacy",
        "keywords": [
          [
            "synthetic",
            0.04627120543816745
          ],
          [
            "data",
            0.04172277537804285
          ],
          [
            "synthetic data",
            0.04003912011800354
          ],
          [
            "privacy",
            0.02903803655993381
          ],
          [
            "private",
            0.01884676068566575
          ],
          [
            "Synthetic",
            0.01834863393075931
          ],
          [
            "DP",
            0.017082903058702404
          ],
          [
            "generative",
            0.015370601388073632
          ],
          [
            "data generation",
            0.01495339801465737
          ],
          [
            "generation",
            0.013180364284688351
          ]
        ],
        "count": 231
      },
      "28": {
        "name": "28_privacy_data_GDPR_Privacy",
        "keywords": [
          [
            "privacy",
            0.035089772907479985
          ],
          [
            "data",
            0.03250340325387547
          ],
          [
            "GDPR",
            0.020682642452148545
          ],
          [
            "Privacy",
            0.017107976821830607
          ],
          [
            "Data",
            0.016800081891699167
          ],
          [
            "policies",
            0.01507708619259751
          ],
          [
            "privacy policies",
            0.011120437350727465
          ],
          [
            "research",
            0.01024300271707464
          ],
          [
            "personal",
            0.010159525430666751
          ],
          [
            "personal data",
            0.00983510270451819
          ]
        ],
        "count": 216
      },
      "29": {
        "name": "29_fuzzing_fuzzers_bugs_fuzzer",
        "keywords": [
          [
            "fuzzing",
            0.05616302228967037
          ],
          [
            "fuzzers",
            0.02741259890342302
          ],
          [
            "bugs",
            0.026987201674194584
          ],
          [
            "fuzzer",
            0.024822148400819095
          ],
          [
            "coverage",
            0.023474948038342526
          ],
          [
            "Fuzzing",
            0.02331179071794903
          ],
          [
            "programs",
            0.019461675853534498
          ],
          [
            "program",
            0.018982894495533642
          ],
          [
            "code",
            0.016696693411340217
          ],
          [
            "vulnerabilities",
            0.016467646819202885
          ]
        ],
        "count": 188
      },
      "30": {
        "name": "30_password_passwords_authentication_user",
        "keywords": [
          [
            "password",
            0.07603636806814952
          ],
          [
            "passwords",
            0.046084247204015195
          ],
          [
            "authentication",
            0.03843874821198453
          ],
          [
            "user",
            0.02124651305645048
          ],
          [
            "Password",
            0.017900928132087945
          ],
          [
            "users",
            0.017272730679448407
          ],
          [
            "security",
            0.016104778439148742
          ],
          [
            "RBA",
            0.015418222334478803
          ],
          [
            "guessing",
            0.013706543906128578
          ],
          [
            "Authentication",
            0.012973475349660368
          ]
        ],
        "count": 188
      },
      "31": {
        "name": "31_contact_tracing_contact tracing_COVID",
        "keywords": [
          [
            "contact",
            0.061772452661666796
          ],
          [
            "tracing",
            0.059009063053085356
          ],
          [
            "contact tracing",
            0.053543799548582245
          ],
          [
            "COVID",
            0.034409026719831814
          ],
          [
            "privacy",
            0.024948085359825876
          ],
          [
            "Contact",
            0.022168902552074116
          ],
          [
            "Tracing",
            0.018813675099162604
          ],
          [
            "pandemic",
            0.016861905892805035
          ],
          [
            "health",
            0.015707528876217774
          ],
          [
            "spread",
            0.015403474060640378
          ]
        ],
        "count": 178
      }
    },
    "correlations": [
      [
        1.0,
        -0.7530656690867772,
        -0.7527395127803682,
        -0.7447215306462153,
        -0.7300866876658654,
        -0.7240149860611917,
        -0.7485707378785105,
        -0.7291878902196491,
        -0.7203057832190092,
        -0.761990058289439,
        -0.7347626903422366,
        -0.763678422501372,
        -0.7418542075528001,
        -0.7507009967644,
        -0.7521095962011889,
        -0.7310473054786232,
        -0.7419215837743279,
        -0.7645512892213937,
        -0.7543215274268595,
        -0.7036502280629746,
        -0.7569492483262975,
        -0.7384139839184956,
        -0.7132480789077602,
        -0.7500735022891138,
        -0.7597595895247746,
        -0.7405374869797221,
        -0.7590374845759911,
        -0.7370701026752241,
        -0.7199181224421815,
        -0.7499879240874285,
        -0.7483611829375636,
        -0.7399872535099777
      ],
      [
        -0.7530656690867772,
        1.0,
        -0.7625445711323748,
        -0.7296797634894592,
        -0.7439795409980702,
        -0.7231677595557546,
        -0.6819129220817458,
        -0.7370258453117506,
        -0.6446710872610283,
        -0.7392419196009241,
        -0.7313955484628082,
        -0.7113629685758764,
        -0.6719548649901088,
        -0.7508261605629857,
        -0.7597561676793714,
        -0.746421594715159,
        -0.7425915902710788,
        -0.7532163983460687,
        -0.731872484283127,
        -0.7470679931176192,
        -0.7326398885827845,
        -0.7445316918399362,
        -0.7548463066706541,
        -0.7585693906920263,
        -0.5880306562818958,
        -0.6624730500640221,
        -0.7555271628687278,
        -0.7039696490357371,
        -0.7296750856320329,
        -0.7443012379170636,
        -0.753647983728097,
        -0.7506887437251791
      ],
      [
        -0.7527395127803682,
        -0.7625445711323748,
        1.0,
        -0.752001086394257,
        -0.7080153693006602,
        -0.7432687427717569,
        -0.7521511073953262,
        -0.7553888102259732,
        -0.7431884351570327,
        -0.760028464023604,
        -0.7293404945974244,
        -0.7607683491773236,
        -0.7432067173743332,
        -0.7571453478410293,
        -0.7601891211366005,
        -0.7158328618893168,
        -0.7184017478364655,
        -0.7649882088832003,
        -0.7477489878342383,
        -0.730609404752012,
        -0.7611027858827113,
        -0.7568639024215924,
        -0.7505738801838969,
        -0.7553886626256401,
        -0.7590944050911395,
        -0.7477021610552679,
        -0.7634730233733498,
        -0.7529075715291237,
        -0.751397232276539,
        -0.7644454460573719,
        -0.7445124923421471,
        -0.7594638984900628
      ],
      [
        -0.7447215306462153,
        -0.7296797634894592,
        -0.752001086394257,
        1.0,
        -0.751008014369939,
        -0.4833494632253078,
        -0.7387882249001971,
        -0.7360569773456122,
        -0.7470951840975468,
        -0.7600755252127134,
        -0.7494579801431033,
        -0.7607257713162371,
        -0.7136633077838876,
        -0.7458417753187285,
        -0.7571418164910468,
        -0.7491479065949689,
        -0.6094401594840999,
        -0.7556990912348849,
        -0.6438440304243312,
        -0.7373636538931901,
        -0.7616500602612617,
        -0.7229882792654947,
        -0.7399632661697184,
        -0.7551107054052313,
        -0.7239020904004709,
        -0.5634403007479916,
        -0.7543047364096531,
        -0.20906648733432737,
        -0.318758302474919,
        -0.7628654217033399,
        -0.7421214887482035,
        -0.6191854061142172
      ],
      [
        -0.7300866876658654,
        -0.7439795409980702,
        -0.7080153693006602,
        -0.751008014369939,
        1.0,
        -0.7341541076456578,
        -0.7271096851612442,
        -0.7339426152735715,
        -0.6988587314428192,
        -0.752410215747843,
        -0.4355830390496375,
        -0.7556157283139684,
        -0.7087621753275661,
        -0.7410203607363802,
        -0.7501466449272831,
        -0.3439859161624533,
        -0.7221672980850578,
        -0.7603239004210877,
        -0.742832131154738,
        -0.5074717101591836,
        -0.7581106302044328,
        -0.7487535424762249,
        -0.7197113862914775,
        -0.7479456309131323,
        -0.7437569689011443,
        -0.7314620062128854,
        -0.7516772726882586,
        -0.7397263815628278,
        -0.7258572048779717,
        -0.7421786462640743,
        -0.7356947368633557,
        -0.7509561650009442
      ],
      [
        -0.7240149860611917,
        -0.7231677595557546,
        -0.7432687427717569,
        -0.4833494632253078,
        -0.7341541076456578,
        1.0,
        -0.6966591577868564,
        -0.6735802624331828,
        -0.7313279558174721,
        -0.7442931616959041,
        -0.7289694827765217,
        -0.7444835425133989,
        -0.5510206366457304,
        -0.7425938362915272,
        -0.7494351663514176,
        -0.7295058259366192,
        -0.5768160966150125,
        -0.7524503268212708,
        -0.6204299316902144,
        -0.7061915986489582,
        -0.7538510353410142,
        -0.7335937720004055,
        -0.7392003352704661,
        -0.7431047113944417,
        -0.7127020869282448,
        -0.35237829074980154,
        -0.7510141607717298,
        -0.4391123995017183,
        -0.436487110036742,
        -0.760283104100399,
        -0.7438786020178053,
        -0.6639005666375428
      ],
      [
        -0.7485707378785105,
        -0.6819129220817458,
        -0.7521511073953262,
        -0.7387882249001971,
        -0.7271096851612442,
        -0.6966591577868564,
        1.0,
        -0.7006980941949603,
        -0.703276287371658,
        -0.7251904342248912,
        -0.7263032894555057,
        -0.7235201690781325,
        -0.44671858525555386,
        -0.7267687000032947,
        -0.7315181835616907,
        -0.7328885157045926,
        -0.7390627044520971,
        -0.7209922937215674,
        -0.7314753982341551,
        -0.7303414513147105,
        -0.7441680004361482,
        -0.720575691942876,
        -0.7346252686118644,
        -0.7468554687300888,
        -0.6423258330318466,
        -0.6314302064461086,
        -0.7571790753920015,
        -0.7119679431334304,
        -0.7340451038775715,
        -0.7609182431994668,
        -0.7521606876348449,
        -0.752405934078039
      ],
      [
        -0.7291878902196491,
        -0.7370258453117506,
        -0.7553888102259732,
        -0.7360569773456122,
        -0.7339426152735715,
        -0.6735802624331828,
        -0.7006980941949603,
        1.0,
        -0.7343101864942336,
        -0.6564523036395944,
        -0.6881155592915871,
        -0.7578352552935845,
        -0.6984711680675235,
        -0.7449713183468509,
        -0.7086022762745293,
        -0.7150088173722375,
        -0.7332913120382794,
        -0.7523524328138946,
        -0.667868111729572,
        -0.5999519698506051,
        -0.6681623456973425,
        -0.7080789778701997,
        -0.7213379130760742,
        -0.551926048887663,
        -0.7309846309646784,
        -0.6530312443974591,
        -0.7517785275926641,
        -0.7137621714415163,
        -0.7250076275428945,
        -0.7552149923040501,
        -0.74840061479968,
        -0.7528006934186513
      ],
      [
        -0.7203057832190092,
        -0.6446710872610283,
        -0.7431884351570327,
        -0.7470951840975468,
        -0.6988587314428192,
        -0.7313279558174721,
        -0.703276287371658,
        -0.7343101864942336,
        1.0,
        -0.7258489056951309,
        -0.7114852582671676,
        -0.7465260985647764,
        -0.6939846779972332,
        -0.7486123904744149,
        -0.7508535388624793,
        -0.7239700821545636,
        -0.7403852783872842,
        -0.7550613325109176,
        -0.7440830806410904,
        -0.7191726480398832,
        -0.7456125519830206,
        -0.721391132627849,
        -0.7438635077833148,
        -0.7523080527900351,
        -0.7193647776977143,
        -0.7145968392021954,
        -0.7390973131532479,
        -0.7279052694415671,
        -0.7373441931347195,
        -0.699530501787948,
        -0.7394710557719512,
        -0.7513920939642877
      ],
      [
        -0.761990058289439,
        -0.7392419196009241,
        -0.760028464023604,
        -0.7600755252127134,
        -0.752410215747843,
        -0.7442931616959041,
        -0.7251904342248912,
        -0.6564523036395944,
        -0.7258489056951309,
        1.0,
        -0.7261516374695394,
        -0.7599609994211718,
        -0.7370165601588764,
        -0.7557597745330806,
        -0.7602692106682138,
        -0.7564589187422186,
        -0.7552040782438723,
        -0.759933731993228,
        -0.7480065020351587,
        -0.7264175366892669,
        -0.6589265389207477,
        -0.7306288006013706,
        -0.7518518097417264,
        -0.7338018153843939,
        -0.7400687776045458,
        -0.728761032248002,
        -0.6549306315596652,
        -0.7478802960250509,
        -0.7538091943724854,
        -0.7576021117425951,
        -0.7583042214499498,
        -0.7582001512589135
      ],
      [
        -0.7347626903422366,
        -0.7313955484628082,
        -0.7293404945974244,
        -0.7494579801431033,
        -0.4355830390496375,
        -0.7289694827765217,
        -0.7263032894555057,
        -0.6881155592915871,
        -0.7114852582671676,
        -0.7261516374695394,
        1.0,
        -0.7596716711694531,
        -0.71296641231328,
        -0.7377110075872451,
        -0.7275362701738677,
        -0.4406469776806493,
        -0.7366460867900688,
        -0.7602447320634743,
        -0.7473012300088242,
        -0.5432810744892616,
        -0.739453933554066,
        -0.7340845367394657,
        -0.6083070961108502,
        -0.7371111598568908,
        -0.7414763372897752,
        -0.7255088585677546,
        -0.7547153402437093,
        -0.7371326936943754,
        -0.7245983013515178,
        -0.7579276594278777,
        -0.7376464613500984,
        -0.7431817093082596
      ],
      [
        -0.763678422501372,
        -0.7113629685758764,
        -0.7607683491773236,
        -0.7607257713162371,
        -0.7556157283139684,
        -0.7444835425133989,
        -0.7235201690781325,
        -0.7578352552935845,
        -0.7465260985647764,
        -0.7599609994211718,
        -0.7596716711694531,
        1.0,
        -0.7280052617257422,
        -0.7478693651619635,
        -0.7646555299118061,
        -0.7585640432147547,
        -0.7536625342009999,
        -0.7349554053208442,
        -0.7597595191705535,
        -0.76045277318124,
        -0.7594228453038019,
        -0.7552871264377901,
        -0.7597178911685487,
        -0.7641820684093199,
        -0.5617099875113876,
        -0.7296547980007648,
        -0.763757149448723,
        -0.7432904722600343,
        -0.755841865058503,
        -0.7653481490318834,
        -0.7569032813201484,
        -0.7459600656456736
      ],
      [
        -0.7418542075528001,
        -0.6719548649901088,
        -0.7432067173743332,
        -0.7136633077838876,
        -0.7087621753275661,
        -0.5510206366457304,
        -0.44671858525555386,
        -0.6984711680675235,
        -0.6939846779972332,
        -0.7370165601588764,
        -0.71296641231328,
        -0.7280052617257422,
        1.0,
        -0.7299499342264333,
        -0.7344576688040102,
        -0.7234371634463971,
        -0.7263411278818883,
        -0.7333820421873933,
        -0.7197746527276601,
        -0.7205049710436439,
        -0.7431346243996009,
        -0.7158125638987429,
        -0.7276538498970744,
        -0.7395874329365224,
        -0.6667999734199519,
        -0.419155115369142,
        -0.7522669048384801,
        -0.6526007798961881,
        -0.7073158171600321,
        -0.7595410690893162,
        -0.7433629558905235,
        -0.7479054322461721
      ],
      [
        -0.7507009967644,
        -0.7508261605629857,
        -0.7571453478410293,
        -0.7458417753187285,
        -0.7410203607363802,
        -0.7425938362915272,
        -0.7267687000032947,
        -0.7449713183468509,
        -0.7486123904744149,
        -0.7557597745330806,
        -0.7377110075872451,
        -0.7478693651619635,
        -0.7299499342264333,
        1.0,
        -0.753819056039341,
        -0.7446789832696967,
        -0.7337494555923213,
        -0.7272528498529918,
        -0.7443109611008885,
        -0.7426124109558483,
        -0.7527165007018048,
        -0.7598033827401566,
        -0.753962564388653,
        -0.7552222672690285,
        -0.7150161117923304,
        -0.730893916233802,
        -0.7557780220542734,
        -0.7341255886633236,
        -0.7317399436408238,
        -0.7641435796792149,
        -0.6115507376050235,
        -0.7459075216016637
      ],
      [
        -0.7521095962011889,
        -0.7597561676793714,
        -0.7601891211366005,
        -0.7571418164910468,
        -0.7501466449272831,
        -0.7494351663514176,
        -0.7315181835616907,
        -0.7086022762745293,
        -0.7508535388624793,
        -0.7602692106682138,
        -0.7275362701738677,
        -0.7646555299118061,
        -0.7344576688040102,
        -0.753819056039341,
        1.0,
        -0.7364378131699143,
        -0.7548215107147,
        -0.7626166751666504,
        -0.7586066114113689,
        -0.7457230898613341,
        -0.7604722443302843,
        -0.7577601392620487,
        -0.7437453811219513,
        -0.7464152923328571,
        -0.7576870760983261,
        -0.7507901221315254,
        -0.7585760900399026,
        -0.7516044954488723,
        -0.7452632633528262,
        -0.755398715528417,
        -0.7503885571744247,
        -0.7596495685431661
      ],
      [
        -0.7310473054786232,
        -0.746421594715159,
        -0.7158328618893168,
        -0.7491479065949689,
        -0.3439859161624533,
        -0.7295058259366192,
        -0.7328885157045926,
        -0.7150088173722375,
        -0.7239700821545636,
        -0.7564589187422186,
        -0.4406469776806493,
        -0.7585640432147547,
        -0.7234371634463971,
        -0.7446789832696967,
        -0.7364378131699143,
        1.0,
        -0.7341195130676421,
        -0.7618674294568584,
        -0.7522979469576374,
        -0.5061986058583022,
        -0.7596230920612403,
        -0.752864746791917,
        -0.734384681014956,
        -0.7383669638498538,
        -0.7462273766267323,
        -0.738790263375441,
        -0.7501342214914921,
        -0.7393916978863636,
        -0.7245618540283278,
        -0.7504902484352329,
        -0.7287337697305705,
        -0.7477609358102779
      ],
      [
        -0.7419215837743279,
        -0.7425915902710788,
        -0.7184017478364655,
        -0.6094401594840999,
        -0.7221672980850578,
        -0.5768160966150125,
        -0.7390627044520971,
        -0.7332913120382794,
        -0.7403852783872842,
        -0.7552040782438723,
        -0.7366460867900688,
        -0.7536625342009999,
        -0.7263411278818883,
        -0.7337494555923213,
        -0.7548215107147,
        -0.7341195130676421,
        1.0,
        -0.7562647872279289,
        -0.5491941782294627,
        -0.7243051048836726,
        -0.7595744833287326,
        -0.7485685838880791,
        -0.746186251884682,
        -0.7499230800178698,
        -0.7360474357074154,
        -0.5747366672731438,
        -0.7525789979676454,
        -0.4753247291815439,
        -0.4693508582631445,
        -0.7641157198681194,
        -0.7428919139720043,
        -0.7343960500533095
      ],
      [
        -0.7645512892213937,
        -0.7532163983460687,
        -0.7649882088832003,
        -0.7556990912348849,
        -0.7603239004210877,
        -0.7524503268212708,
        -0.7209922937215674,
        -0.7523524328138946,
        -0.7550613325109176,
        -0.759933731993228,
        -0.7602447320634743,
        -0.7349554053208442,
        -0.7333820421873933,
        -0.7272528498529918,
        -0.7626166751666504,
        -0.7618674294568584,
        -0.7562647872279289,
        1.0,
        -0.756450409015915,
        -0.7527819024431904,
        -0.7578058653835094,
        -0.7622856644619014,
        -0.761812966787383,
        -0.7531148430210277,
        -0.7321026312525044,
        -0.7438566025985943,
        -0.7549200077232894,
        -0.7425422772148962,
        -0.7476906436351897,
        -0.7655764556207769,
        -0.7501068319286237,
        -0.7552304826939866
      ],
      [
        -0.7543215274268595,
        -0.731872484283127,
        -0.7477489878342383,
        -0.6438440304243312,
        -0.742832131154738,
        -0.6204299316902144,
        -0.7314753982341551,
        -0.667868111729572,
        -0.7440830806410904,
        -0.7480065020351587,
        -0.7473012300088242,
        -0.7597595191705535,
        -0.7197746527276601,
        -0.7443109611008885,
        -0.7586066114113689,
        -0.7522979469576374,
        -0.5491941782294627,
        -0.756450409015915,
        1.0,
        -0.7445050747732771,
        -0.756287792304726,
        -0.7373821270447036,
        -0.7447408915481049,
        -0.7571621680895065,
        -0.7305886505466358,
        -0.5535550780879637,
        -0.7587841557003088,
        -0.6814186637844317,
        -0.6791277631721421,
        -0.7590887754284998,
        -0.7508973730179371,
        -0.7383028153306523
      ],
      [
        -0.7036502280629746,
        -0.7470679931176192,
        -0.730609404752012,
        -0.7373636538931901,
        -0.5074717101591836,
        -0.7061915986489582,
        -0.7303414513147105,
        -0.5999519698506051,
        -0.7191726480398832,
        -0.7264175366892669,
        -0.5432810744892616,
        -0.76045277318124,
        -0.7205049710436439,
        -0.7426124109558483,
        -0.7457230898613341,
        -0.5061986058583022,
        -0.7243051048836726,
        -0.7527819024431904,
        -0.7445050747732771,
        1.0,
        -0.7562429117932987,
        -0.7494526921339654,
        -0.712887417353048,
        -0.5921464135755787,
        -0.7458313562577021,
        -0.7226990647975423,
        -0.7365944400667852,
        -0.7205001034085541,
        -0.7008858250117452,
        -0.7544164291426307,
        -0.724861720149247,
        -0.7423111878371945
      ],
      [
        -0.7569492483262975,
        -0.7326398885827845,
        -0.7611027858827113,
        -0.7616500602612617,
        -0.7581106302044328,
        -0.7538510353410142,
        -0.7441680004361482,
        -0.6681623456973425,
        -0.7456125519830206,
        -0.6589265389207477,
        -0.739453933554066,
        -0.7594228453038019,
        -0.7431346243996009,
        -0.7527165007018048,
        -0.7604722443302843,
        -0.7596230920612403,
        -0.7595744833287326,
        -0.7578058653835094,
        -0.756287792304726,
        -0.7562429117932987,
        1.0,
        -0.7476828301908492,
        -0.7563371583835339,
        -0.7458562160002329,
        -0.7477780501052406,
        -0.7463774300183146,
        -0.754717099631361,
        -0.7546302809348182,
        -0.757933937112158,
        -0.7600466165036454,
        -0.7399374215767931,
        -0.7546710785449795
      ],
      [
        -0.7384139839184956,
        -0.7445316918399362,
        -0.7568639024215924,
        -0.7229882792654947,
        -0.7487535424762249,
        -0.7335937720004055,
        -0.720575691942876,
        -0.7080789778701997,
        -0.721391132627849,
        -0.7306288006013706,
        -0.7340845367394657,
        -0.7552871264377901,
        -0.7158125638987429,
        -0.7598033827401566,
        -0.7577601392620487,
        -0.752864746791917,
        -0.7485685838880791,
        -0.7622856644619014,
        -0.7373821270447036,
        -0.7494526921339654,
        -0.7476828301908492,
        1.0,
        -0.749572410487316,
        -0.7570639077725101,
        -0.7423107985017703,
        -0.7183319089465205,
        -0.7568992230879374,
        -0.7299388956924522,
        -0.740234974760165,
        -0.7591971864137743,
        -0.7588290233036921,
        -0.7522002070096397
      ],
      [
        -0.7132480789077602,
        -0.7548463066706541,
        -0.7505738801838969,
        -0.7399632661697184,
        -0.7197113862914775,
        -0.7392003352704661,
        -0.7346252686118644,
        -0.7213379130760742,
        -0.7438635077833148,
        -0.7518518097417264,
        -0.6083070961108502,
        -0.7597178911685487,
        -0.7276538498970744,
        -0.753962564388653,
        -0.7437453811219513,
        -0.734384681014956,
        -0.746186251884682,
        -0.761812966787383,
        -0.7447408915481049,
        -0.712887417353048,
        -0.7563371583835339,
        -0.749572410487316,
        1.0,
        -0.7520607909783308,
        -0.7534227720254099,
        -0.7386065625630771,
        -0.761669611830551,
        -0.7388435788770882,
        -0.7415880778310872,
        -0.7635843571210765,
        -0.7480529711337296,
        -0.7517273279520882
      ],
      [
        -0.7500735022891138,
        -0.7585693906920263,
        -0.7553886626256401,
        -0.7551107054052313,
        -0.7479456309131323,
        -0.7431047113944417,
        -0.7468554687300888,
        -0.551926048887663,
        -0.7523080527900351,
        -0.7338018153843939,
        -0.7371111598568908,
        -0.7641820684093199,
        -0.7395874329365224,
        -0.7552222672690285,
        -0.7464152923328571,
        -0.7383669638498538,
        -0.7499230800178698,
        -0.7531148430210277,
        -0.7571621680895065,
        -0.5921464135755787,
        -0.7458562160002329,
        -0.7570639077725101,
        -0.7520607909783308,
        1.0,
        -0.7595223294903675,
        -0.7407068615909382,
        -0.7534851522496858,
        -0.7435405319986659,
        -0.7387587379244733,
        -0.7609603509445911,
        -0.745088598483046,
        -0.7548945022662543
      ],
      [
        -0.7597595895247746,
        -0.5880306562818958,
        -0.7590944050911395,
        -0.7239020904004709,
        -0.7437569689011443,
        -0.7127020869282448,
        -0.6423258330318466,
        -0.7309846309646784,
        -0.7193647776977143,
        -0.7400687776045458,
        -0.7414763372897752,
        -0.5617099875113876,
        -0.6667999734199519,
        -0.7150161117923304,
        -0.7576870760983261,
        -0.7462273766267323,
        -0.7360474357074154,
        -0.7321026312525044,
        -0.7305886505466358,
        -0.7458313562577021,
        -0.7477780501052406,
        -0.7423107985017703,
        -0.7534227720254099,
        -0.7595223294903675,
        1.0,
        -0.6452583292055027,
        -0.7574567283707273,
        -0.6763672152611566,
        -0.7297873998149935,
        -0.7616493981252177,
        -0.7522190757700022,
        -0.7511890747391872
      ],
      [
        -0.7405374869797221,
        -0.6624730500640221,
        -0.7477021610552679,
        -0.5634403007479916,
        -0.7314620062128854,
        -0.35237829074980154,
        -0.6314302064461086,
        -0.6530312443974591,
        -0.7145968392021954,
        -0.728761032248002,
        -0.7255088585677546,
        -0.7296547980007648,
        -0.419155115369142,
        -0.730893916233802,
        -0.7507901221315254,
        -0.738790263375441,
        -0.5747366672731438,
        -0.7438566025985943,
        -0.5535550780879637,
        -0.7226990647975423,
        -0.7463774300183146,
        -0.7183319089465205,
        -0.7386065625630771,
        -0.7407068615909382,
        -0.6452583292055027,
        1.0,
        -0.7509847390463837,
        -0.41278766837205205,
        -0.513898876426985,
        -0.7579945969277699,
        -0.7436210396877456,
        -0.7277622747219615
      ],
      [
        -0.7590374845759911,
        -0.7555271628687278,
        -0.7634730233733498,
        -0.7543047364096531,
        -0.7516772726882586,
        -0.7510141607717298,
        -0.7571790753920015,
        -0.7517785275926641,
        -0.7390973131532479,
        -0.6549306315596652,
        -0.7547153402437093,
        -0.763757149448723,
        -0.7522669048384801,
        -0.7557780220542734,
        -0.7585760900399026,
        -0.7501342214914921,
        -0.7525789979676454,
        -0.7549200077232894,
        -0.7587841557003088,
        -0.7365944400667852,
        -0.754717099631361,
        -0.7568992230879374,
        -0.761669611830551,
        -0.7534851522496858,
        -0.7574567283707273,
        -0.7509847390463837,
        1.0,
        -0.7482076082855655,
        -0.7115317432957635,
        -0.7567631399727184,
        -0.7450905265509575,
        -0.711448660289901
      ],
      [
        -0.7370701026752241,
        -0.7039696490357371,
        -0.7529075715291237,
        -0.20906648733432737,
        -0.7397263815628278,
        -0.4391123995017183,
        -0.7119679431334304,
        -0.7137621714415163,
        -0.7279052694415671,
        -0.7478802960250509,
        -0.7371326936943754,
        -0.7432904722600343,
        -0.6526007798961881,
        -0.7341255886633236,
        -0.7516044954488723,
        -0.7393916978863636,
        -0.4753247291815439,
        -0.7425422772148962,
        -0.6814186637844317,
        -0.7205001034085541,
        -0.7546302809348182,
        -0.7299388956924522,
        -0.7388435788770882,
        -0.7435405319986659,
        -0.6763672152611566,
        -0.41278766837205205,
        -0.7482076082855655,
        1.0,
        -0.1755099358330034,
        -0.7557020325716893,
        -0.7411100177687631,
        -0.6250935379329026
      ],
      [
        -0.7199181224421815,
        -0.7296750856320329,
        -0.751397232276539,
        -0.318758302474919,
        -0.7258572048779717,
        -0.436487110036742,
        -0.7340451038775715,
        -0.7250076275428945,
        -0.7373441931347195,
        -0.7538091943724854,
        -0.7245983013515178,
        -0.755841865058503,
        -0.7073158171600321,
        -0.7317399436408238,
        -0.7452632633528262,
        -0.7245618540283278,
        -0.4693508582631445,
        -0.7476906436351897,
        -0.6791277631721421,
        -0.7008858250117452,
        -0.757933937112158,
        -0.740234974760165,
        -0.7415880778310872,
        -0.7387587379244733,
        -0.7297873998149935,
        -0.513898876426985,
        -0.7115317432957635,
        -0.1755099358330034,
        1.0,
        -0.7633690945079314,
        -0.7322123207944378,
        -0.5814314960917238
      ],
      [
        -0.7499879240874285,
        -0.7443012379170636,
        -0.7644454460573719,
        -0.7628654217033399,
        -0.7421786462640743,
        -0.760283104100399,
        -0.7609182431994668,
        -0.7552149923040501,
        -0.699530501787948,
        -0.7576021117425951,
        -0.7579276594278777,
        -0.7653481490318834,
        -0.7595410690893162,
        -0.7641435796792149,
        -0.755398715528417,
        -0.7504902484352329,
        -0.7641157198681194,
        -0.7655764556207769,
        -0.7590887754284998,
        -0.7544164291426307,
        -0.7600466165036454,
        -0.7591971864137743,
        -0.7635843571210765,
        -0.7609603509445911,
        -0.7616493981252177,
        -0.7579945969277699,
        -0.7567631399727184,
        -0.7557020325716893,
        -0.7633690945079314,
        1.0,
        -0.7612968980964221,
        -0.7558216661919849
      ],
      [
        -0.7483611829375636,
        -0.753647983728097,
        -0.7445124923421471,
        -0.7421214887482035,
        -0.7356947368633557,
        -0.7438786020178053,
        -0.7521606876348449,
        -0.74840061479968,
        -0.7394710557719512,
        -0.7583042214499498,
        -0.7376464613500984,
        -0.7569032813201484,
        -0.7433629558905235,
        -0.6115507376050235,
        -0.7503885571744247,
        -0.7287337697305705,
        -0.7428919139720043,
        -0.7501068319286237,
        -0.7508973730179371,
        -0.724861720149247,
        -0.7399374215767931,
        -0.7588290233036921,
        -0.7480529711337296,
        -0.745088598483046,
        -0.7522190757700022,
        -0.7436210396877456,
        -0.7450905265509575,
        -0.7411100177687631,
        -0.7322123207944378,
        -0.7612968980964221,
        1.0,
        -0.7483175751251872
      ],
      [
        -0.7399872535099777,
        -0.7506887437251791,
        -0.7594638984900628,
        -0.6191854061142172,
        -0.7509561650009442,
        -0.6639005666375428,
        -0.752405934078039,
        -0.7528006934186513,
        -0.7513920939642877,
        -0.7582001512589135,
        -0.7431817093082596,
        -0.7459600656456736,
        -0.7479054322461721,
        -0.7459075216016637,
        -0.7596495685431661,
        -0.7477609358102779,
        -0.7343960500533095,
        -0.7552304826939866,
        -0.7383028153306523,
        -0.7423111878371945,
        -0.7546710785449795,
        -0.7522002070096397,
        -0.7517273279520882,
        -0.7548945022662543,
        -0.7511890747391872,
        -0.7277622747219615,
        -0.711448660289901,
        -0.6250935379329026,
        -0.5814314960917238,
        -0.7558216661919849,
        -0.7483175751251872,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        61,
        1,
        7,
        17,
        3,
        2,
        15,
        10,
        5,
        5,
        10,
        0,
        5,
        7,
        6,
        2,
        6,
        1,
        1,
        17,
        1,
        4,
        7,
        2,
        2,
        13,
        4,
        11,
        8,
        2,
        7,
        0
      ],
      "2020-02": [
        61,
        0,
        16,
        22,
        2,
        8,
        37,
        11,
        11,
        11,
        9,
        1,
        22,
        4,
        1,
        1,
        8,
        1,
        2,
        15,
        0,
        7,
        6,
        0,
        5,
        12,
        5,
        5,
        6,
        4,
        5,
        2
      ],
      "2020-03": [
        65,
        0,
        14,
        16,
        3,
        11,
        28,
        12,
        8,
        6,
        8,
        0,
        13,
        5,
        2,
        2,
        5,
        3,
        0,
        16,
        0,
        5,
        6,
        4,
        4,
        10,
        3,
        18,
        6,
        3,
        5,
        5
      ],
      "2020-04": [
        61,
        0,
        10,
        15,
        0,
        13,
        24,
        10,
        9,
        13,
        18,
        2,
        16,
        4,
        4,
        3,
        7,
        3,
        4,
        20,
        5,
        7,
        7,
        3,
        8,
        6,
        3,
        11,
        3,
        1,
        6,
        15
      ],
      "2020-05": [
        82,
        0,
        19,
        13,
        3,
        6,
        22,
        6,
        13,
        10,
        18,
        2,
        17,
        6,
        0,
        2,
        6,
        4,
        1,
        21,
        4,
        11,
        7,
        3,
        6,
        17,
        7,
        4,
        6,
        4,
        3,
        28
      ],
      "2020-06": [
        51,
        0,
        11,
        26,
        5,
        7,
        27,
        7,
        19,
        8,
        7,
        1,
        20,
        7,
        4,
        3,
        8,
        1,
        3,
        23,
        5,
        11,
        7,
        5,
        6,
        11,
        10,
        13,
        6,
        3,
        10,
        18
      ],
      "2020-07": [
        70,
        0,
        9,
        25,
        4,
        16,
        32,
        6,
        17,
        4,
        11,
        1,
        21,
        7,
        3,
        5,
        4,
        7,
        4,
        27,
        5,
        5,
        4,
        4,
        4,
        10,
        8,
        11,
        10,
        0,
        7,
        21
      ],
      "2020-08": [
        45,
        1,
        10,
        22,
        1,
        4,
        8,
        11,
        9,
        7,
        6,
        4,
        8,
        2,
        4,
        1,
        5,
        3,
        3,
        14,
        0,
        9,
        5,
        4,
        4,
        8,
        15,
        13,
        8,
        1,
        7,
        9
      ],
      "2020-09": [
        63,
        0,
        9,
        22,
        1,
        12,
        25,
        13,
        8,
        8,
        13,
        3,
        9,
        5,
        1,
        1,
        5,
        3,
        1,
        10,
        1,
        12,
        12,
        3,
        9,
        18,
        3,
        9,
        6,
        4,
        7,
        7
      ],
      "2020-10": [
        57,
        0,
        11,
        34,
        2,
        10,
        32,
        9,
        11,
        12,
        11,
        5,
        20,
        3,
        5,
        6,
        10,
        4,
        3,
        22,
        2,
        6,
        4,
        7,
        6,
        15,
        4,
        13,
        8,
        4,
        7,
        13
      ],
      "2020-11": [
        47,
        0,
        16,
        21,
        2,
        15,
        27,
        7,
        10,
        7,
        11,
        2,
        17,
        3,
        4,
        1,
        6,
        3,
        6,
        19,
        2,
        4,
        6,
        3,
        3,
        10,
        4,
        10,
        3,
        1,
        7,
        11
      ],
      "2020-12": [
        43,
        1,
        10,
        21,
        2,
        13,
        26,
        15,
        15,
        10,
        7,
        2,
        13,
        7,
        3,
        2,
        7,
        3,
        1,
        20,
        2,
        3,
        6,
        6,
        5,
        17,
        4,
        13,
        7,
        3,
        9,
        12
      ],
      "2021-01": [
        61,
        0,
        8,
        16,
        1,
        6,
        17,
        5,
        6,
        13,
        10,
        0,
        8,
        4,
        4,
        3,
        5,
        6,
        3,
        27,
        2,
        7,
        4,
        1,
        5,
        14,
        7,
        12,
        7,
        3,
        7,
        9
      ],
      "2021-02": [
        61,
        0,
        6,
        21,
        2,
        13,
        31,
        7,
        12,
        13,
        15,
        2,
        21,
        2,
        4,
        1,
        5,
        7,
        1,
        23,
        0,
        7,
        7,
        3,
        4,
        9,
        10,
        11,
        6,
        5,
        4,
        6
      ],
      "2021-03": [
        81,
        0,
        14,
        26,
        5,
        9,
        27,
        15,
        12,
        20,
        17,
        6,
        22,
        9,
        1,
        1,
        11,
        5,
        4,
        27,
        3,
        4,
        5,
        1,
        3,
        11,
        5,
        13,
        9,
        3,
        5,
        11
      ],
      "2021-04": [
        71,
        0,
        9,
        11,
        1,
        15,
        22,
        14,
        9,
        10,
        17,
        3,
        14,
        3,
        5,
        3,
        5,
        6,
        3,
        19,
        4,
        6,
        7,
        1,
        5,
        12,
        6,
        6,
        6,
        1,
        8,
        9
      ],
      "2021-05": [
        76,
        0,
        21,
        25,
        6,
        15,
        25,
        11,
        16,
        7,
        18,
        3,
        18,
        4,
        5,
        4,
        6,
        0,
        2,
        27,
        1,
        9,
        7,
        3,
        5,
        10,
        11,
        10,
        3,
        4,
        8,
        5
      ],
      "2021-06": [
        73,
        0,
        12,
        37,
        2,
        13,
        36,
        12,
        11,
        8,
        20,
        3,
        10,
        6,
        4,
        5,
        5,
        1,
        6,
        14,
        3,
        9,
        6,
        2,
        11,
        14,
        8,
        12,
        5,
        5,
        5,
        5
      ],
      "2021-07": [
        66,
        0,
        11,
        16,
        3,
        9,
        23,
        11,
        12,
        14,
        14,
        2,
        9,
        5,
        3,
        4,
        2,
        4,
        4,
        20,
        0,
        12,
        5,
        1,
        2,
        15,
        3,
        9,
        7,
        0,
        3,
        4
      ],
      "2021-08": [
        50,
        1,
        8,
        16,
        2,
        18,
        25,
        14,
        12,
        9,
        18,
        4,
        11,
        1,
        5,
        7,
        4,
        1,
        1,
        16,
        6,
        6,
        4,
        1,
        4,
        8,
        3,
        8,
        5,
        2,
        5,
        7
      ],
      "2021-09": [
        79,
        1,
        20,
        17,
        2,
        15,
        19,
        5,
        7,
        18,
        12,
        2,
        12,
        6,
        3,
        0,
        6,
        10,
        5,
        38,
        3,
        8,
        4,
        3,
        9,
        13,
        7,
        9,
        4,
        9,
        4,
        2
      ],
      "2021-10": [
        55,
        1,
        17,
        38,
        3,
        18,
        35,
        8,
        14,
        8,
        13,
        4,
        19,
        7,
        1,
        3,
        6,
        9,
        5,
        19,
        4,
        9,
        9,
        1,
        11,
        16,
        3,
        10,
        6,
        2,
        7,
        4
      ],
      "2021-11": [
        56,
        0,
        12,
        26,
        4,
        18,
        18,
        14,
        11,
        17,
        12,
        0,
        12,
        8,
        5,
        2,
        8,
        4,
        2,
        16,
        4,
        7,
        5,
        2,
        5,
        15,
        6,
        6,
        9,
        6,
        2,
        5
      ],
      "2021-12": [
        57,
        1,
        19,
        24,
        5,
        12,
        26,
        11,
        20,
        12,
        16,
        4,
        11,
        4,
        3,
        7,
        8,
        7,
        3,
        12,
        2,
        14,
        2,
        1,
        5,
        12,
        8,
        6,
        4,
        8,
        7,
        8
      ],
      "2022-01": [
        66,
        0,
        10,
        28,
        3,
        18,
        19,
        7,
        11,
        9,
        9,
        3,
        23,
        5,
        4,
        8,
        3,
        7,
        3,
        23,
        2,
        6,
        1,
        4,
        8,
        12,
        6,
        18,
        7,
        5,
        4,
        6
      ],
      "2022-02": [
        74,
        2,
        16,
        27,
        2,
        19,
        21,
        9,
        14,
        8,
        13,
        2,
        17,
        6,
        4,
        2,
        8,
        13,
        5,
        18,
        4,
        15,
        6,
        4,
        3,
        20,
        4,
        9,
        6,
        5,
        7,
        7
      ],
      "2022-03": [
        83,
        0,
        10,
        20,
        3,
        20,
        36,
        11,
        17,
        10,
        11,
        4,
        18,
        8,
        3,
        2,
        3,
        9,
        1,
        26,
        5,
        9,
        7,
        6,
        5,
        13,
        8,
        10,
        11,
        2,
        2,
        5
      ],
      "2022-04": [
        59,
        3,
        15,
        17,
        1,
        21,
        31,
        8,
        15,
        9,
        15,
        5,
        11,
        7,
        3,
        2,
        7,
        8,
        1,
        11,
        5,
        11,
        4,
        2,
        4,
        12,
        11,
        12,
        2,
        3,
        3,
        2
      ],
      "2022-05": [
        63,
        1,
        17,
        28,
        1,
        25,
        30,
        15,
        14,
        15,
        16,
        3,
        24,
        6,
        7,
        4,
        6,
        3,
        4,
        16,
        3,
        11,
        6,
        7,
        9,
        14,
        6,
        14,
        3,
        2,
        12,
        4
      ],
      "2022-06": [
        58,
        1,
        15,
        34,
        3,
        29,
        47,
        7,
        9,
        13,
        12,
        2,
        17,
        5,
        2,
        2,
        4,
        5,
        3,
        21,
        1,
        20,
        8,
        2,
        10,
        19,
        5,
        16,
        9,
        1,
        7,
        3
      ],
      "2022-07": [
        68,
        0,
        12,
        29,
        4,
        22,
        25,
        10,
        13,
        6,
        18,
        2,
        8,
        6,
        5,
        6,
        9,
        5,
        3,
        14,
        4,
        7,
        6,
        1,
        3,
        17,
        6,
        13,
        6,
        2,
        7,
        1
      ],
      "2022-08": [
        82,
        1,
        10,
        27,
        2,
        30,
        27,
        13,
        15,
        17,
        13,
        8,
        26,
        5,
        8,
        3,
        10,
        4,
        5,
        25,
        2,
        12,
        5,
        9,
        7,
        21,
        6,
        14,
        5,
        4,
        6,
        3
      ],
      "2022-09": [
        62,
        1,
        14,
        30,
        4,
        19,
        23,
        13,
        18,
        12,
        11,
        6,
        17,
        4,
        2,
        2,
        4,
        3,
        4,
        17,
        4,
        8,
        9,
        2,
        8,
        24,
        10,
        12,
        5,
        7,
        8,
        2
      ],
      "2022-10": [
        66,
        2,
        18,
        41,
        2,
        28,
        39,
        9,
        19,
        12,
        22,
        5,
        26,
        4,
        4,
        7,
        5,
        9,
        7,
        21,
        2,
        10,
        7,
        2,
        10,
        20,
        5,
        19,
        3,
        5,
        5,
        2
      ],
      "2022-11": [
        66,
        0,
        18,
        33,
        4,
        22,
        29,
        9,
        23,
        12,
        9,
        3,
        28,
        4,
        4,
        3,
        7,
        8,
        2,
        14,
        0,
        11,
        6,
        3,
        10,
        16,
        5,
        21,
        5,
        4,
        8,
        2
      ],
      "2022-12": [
        79,
        0,
        17,
        26,
        3,
        22,
        17,
        10,
        13,
        17,
        13,
        3,
        20,
        3,
        5,
        8,
        5,
        2,
        1,
        18,
        6,
        8,
        2,
        2,
        6,
        15,
        6,
        17,
        12,
        7,
        8,
        3
      ],
      "2023-01": [
        61,
        0,
        12,
        27,
        0,
        14,
        18,
        5,
        19,
        7,
        6,
        3,
        22,
        7,
        4,
        3,
        7,
        6,
        5,
        21,
        2,
        12,
        2,
        4,
        7,
        7,
        7,
        7,
        3,
        6,
        10,
        2
      ],
      "2023-02": [
        62,
        6,
        22,
        31,
        2,
        19,
        19,
        8,
        14,
        12,
        17,
        2,
        28,
        4,
        1,
        2,
        11,
        3,
        1,
        14,
        0,
        9,
        12,
        4,
        11,
        9,
        9,
        13,
        8,
        2,
        9,
        3
      ],
      "2023-03": [
        73,
        6,
        23,
        22,
        3,
        26,
        36,
        8,
        18,
        10,
        14,
        7,
        22,
        5,
        5,
        4,
        11,
        4,
        2,
        26,
        2,
        11,
        4,
        1,
        8,
        10,
        3,
        11,
        10,
        6,
        8,
        2
      ],
      "2023-04": [
        72,
        3,
        22,
        17,
        5,
        22,
        29,
        12,
        17,
        8,
        14,
        4,
        19,
        5,
        1,
        6,
        9,
        6,
        1,
        28,
        1,
        18,
        6,
        3,
        9,
        14,
        2,
        15,
        10,
        7,
        4,
        4
      ],
      "2023-05": [
        77,
        16,
        11,
        36,
        7,
        33,
        39,
        5,
        21,
        13,
        16,
        10,
        19,
        5,
        5,
        7,
        6,
        6,
        2,
        28,
        5,
        14,
        9,
        3,
        10,
        22,
        4,
        17,
        13,
        6,
        10,
        0
      ],
      "2023-06": [
        91,
        14,
        18,
        46,
        5,
        32,
        37,
        18,
        15,
        13,
        14,
        6,
        26,
        2,
        4,
        1,
        14,
        4,
        7,
        23,
        2,
        10,
        11,
        1,
        13,
        20,
        10,
        18,
        15,
        4,
        11,
        1
      ],
      "2023-07": [
        71,
        15,
        12,
        34,
        1,
        16,
        29,
        10,
        12,
        12,
        24,
        4,
        20,
        7,
        5,
        4,
        11,
        6,
        3,
        11,
        2,
        8,
        10,
        4,
        10,
        20,
        3,
        15,
        9,
        8,
        11,
        4
      ],
      "2023-08": [
        84,
        13,
        16,
        28,
        8,
        34,
        29,
        10,
        24,
        17,
        23,
        5,
        22,
        4,
        4,
        4,
        15,
        7,
        5,
        16,
        8,
        15,
        5,
        1,
        13,
        17,
        6,
        9,
        5,
        3,
        9,
        1
      ],
      "2023-09": [
        92,
        13,
        26,
        27,
        7,
        21,
        31,
        13,
        14,
        12,
        20,
        8,
        19,
        4,
        1,
        4,
        11,
        4,
        5,
        20,
        5,
        15,
        5,
        1,
        14,
        22,
        3,
        9,
        10,
        3,
        13,
        4
      ],
      "2023-10": [
        66,
        36,
        17,
        23,
        4,
        24,
        22,
        10,
        24,
        9,
        15,
        8,
        25,
        7,
        2,
        6,
        9,
        2,
        3,
        22,
        9,
        16,
        4,
        5,
        9,
        24,
        4,
        19,
        5,
        6,
        3,
        2
      ],
      "2023-11": [
        78,
        24,
        27,
        20,
        3,
        23,
        37,
        13,
        22,
        7,
        16,
        7,
        17,
        9,
        4,
        4,
        4,
        2,
        3,
        22,
        3,
        10,
        5,
        3,
        12,
        18,
        3,
        22,
        6,
        3,
        5,
        1
      ],
      "2023-12": [
        67,
        32,
        20,
        34,
        5,
        32,
        40,
        10,
        20,
        11,
        21,
        11,
        22,
        7,
        4,
        5,
        11,
        4,
        4,
        28,
        1,
        19,
        10,
        5,
        11,
        17,
        1,
        14,
        8,
        9,
        10,
        2
      ],
      "2024-01": [
        75,
        28,
        24,
        17,
        2,
        30,
        22,
        10,
        20,
        7,
        13,
        6,
        16,
        4,
        10,
        4,
        12,
        12,
        4,
        25,
        7,
        14,
        6,
        3,
        8,
        18,
        2,
        13,
        7,
        8,
        3,
        2
      ],
      "2024-02": [
        102,
        64,
        21,
        42,
        6,
        27,
        40,
        15,
        17,
        9,
        22,
        14,
        31,
        6,
        4,
        6,
        7,
        2,
        6,
        22,
        11,
        16,
        6,
        1,
        15,
        28,
        11,
        18,
        8,
        6,
        6,
        5
      ],
      "2024-03": [
        84,
        43,
        15,
        27,
        3,
        28,
        34,
        18,
        23,
        13,
        14,
        13,
        24,
        6,
        3,
        5,
        10,
        3,
        4,
        30,
        4,
        18,
        8,
        5,
        17,
        35,
        6,
        18,
        9,
        5,
        12,
        2
      ],
      "2024-04": [
        78,
        43,
        23,
        24,
        3,
        31,
        26,
        9,
        23,
        16,
        19,
        9,
        24,
        12,
        1,
        3,
        3,
        11,
        1,
        21,
        3,
        10,
        6,
        1,
        12,
        13,
        4,
        15,
        13,
        2,
        3,
        3
      ],
      "2024-05": [
        112,
        57,
        22,
        35,
        1,
        38,
        29,
        12,
        24,
        22,
        17,
        17,
        33,
        5,
        4,
        4,
        7,
        3,
        5,
        32,
        7,
        24,
        9,
        5,
        17,
        20,
        6,
        18,
        8,
        7,
        7,
        1
      ],
      "2024-06": [
        81,
        57,
        19,
        43,
        4,
        35,
        23,
        12,
        24,
        7,
        13,
        8,
        30,
        3,
        8,
        7,
        8,
        4,
        1,
        27,
        5,
        16,
        6,
        6,
        25,
        29,
        1,
        19,
        4,
        8,
        4,
        1
      ],
      "2024-07": [
        75,
        53,
        12,
        38,
        6,
        32,
        27,
        17,
        37,
        7,
        6,
        11,
        29,
        1,
        1,
        6,
        9,
        3,
        2,
        27,
        4,
        8,
        8,
        4,
        17,
        22,
        8,
        19,
        10,
        4,
        13,
        6
      ],
      "2024-08": [
        75,
        56,
        16,
        26,
        3,
        19,
        30,
        18,
        33,
        14,
        16,
        5,
        19,
        6,
        6,
        1,
        10,
        7,
        2,
        18,
        11,
        9,
        4,
        3,
        16,
        14,
        6,
        16,
        3,
        3,
        6,
        5
      ],
      "2024-09": [
        100,
        61,
        20,
        23,
        5,
        31,
        24,
        14,
        29,
        17,
        17,
        11,
        33,
        14,
        8,
        7,
        7,
        11,
        3,
        20,
        9,
        11,
        6,
        3,
        19,
        13,
        9,
        22,
        6,
        11,
        6,
        4
      ],
      "2024-10": [
        94,
        102,
        34,
        37,
        2,
        36,
        37,
        15,
        22,
        12,
        23,
        23,
        30,
        4,
        4,
        1,
        16,
        6,
        6,
        19,
        7,
        11,
        5,
        2,
        14,
        25,
        8,
        21,
        16,
        8,
        2,
        1
      ],
      "2024-11": [
        78,
        54,
        29,
        23,
        5,
        33,
        24,
        12,
        20,
        11,
        10,
        21,
        28,
        7,
        3,
        6,
        8,
        8,
        3,
        18,
        8,
        15,
        8,
        4,
        12,
        25,
        4,
        18,
        12,
        3,
        4,
        6
      ],
      "2024-12": [
        91,
        68,
        20,
        29,
        2,
        22,
        31,
        18,
        19,
        11,
        17,
        7,
        25,
        9,
        2,
        4,
        13,
        2,
        2,
        19,
        13,
        15,
        9,
        6,
        21,
        19,
        4,
        13,
        8,
        5,
        5,
        6
      ],
      "2025-01": [
        83,
        67,
        30,
        17,
        6,
        23,
        28,
        21,
        20,
        12,
        21,
        9,
        23,
        8,
        5,
        4,
        11,
        4,
        4,
        22,
        3,
        24,
        10,
        4,
        9,
        22,
        5,
        14,
        10,
        5,
        10,
        2
      ],
      "2025-02": [
        101,
        115,
        23,
        29,
        1,
        28,
        24,
        19,
        32,
        14,
        20,
        18,
        28,
        7,
        2,
        4,
        9,
        8,
        3,
        17,
        11,
        16,
        3,
        3,
        16,
        30,
        5,
        23,
        5,
        8,
        6,
        1
      ],
      "2025-03": [
        96,
        70,
        30,
        22,
        3,
        32,
        25,
        11,
        26,
        13,
        27,
        13,
        23,
        7,
        9,
        1,
        14,
        5,
        1,
        20,
        11,
        10,
        5,
        4,
        14,
        24,
        7,
        16,
        3,
        4,
        9,
        6
      ],
      "2025-04": [
        130,
        95,
        29,
        22,
        1,
        18,
        17,
        21,
        30,
        17,
        18,
        10,
        28,
        3,
        5,
        1,
        18,
        12,
        2,
        24,
        5,
        12,
        6,
        3,
        22,
        20,
        6,
        17,
        3,
        8,
        11,
        3
      ],
      "2025-05": [
        126,
        160,
        29,
        32,
        4,
        31,
        32,
        21,
        31,
        23,
        23,
        25,
        29,
        2,
        12,
        7,
        10,
        15,
        4,
        24,
        10,
        20,
        4,
        4,
        19,
        21,
        5,
        14,
        14,
        11,
        10,
        3
      ],
      "2025-06": [
        120,
        111,
        26,
        43,
        2,
        27,
        37,
        21,
        26,
        21,
        17,
        15,
        20,
        6,
        7,
        6,
        10,
        4,
        5,
        18,
        10,
        17,
        2,
        4,
        14,
        35,
        8,
        27,
        8,
        5,
        6,
        6
      ],
      "2025-07": [
        110,
        83,
        42,
        21,
        4,
        21,
        17,
        10,
        29,
        24,
        15,
        13,
        22,
        6,
        8,
        7,
        5,
        11,
        2,
        18,
        12,
        15,
        7,
        5,
        21,
        17,
        6,
        16,
        14,
        9,
        6,
        2
      ],
      "2025-08": [
        100,
        117,
        22,
        17,
        4,
        26,
        27,
        14,
        32,
        15,
        20,
        12,
        27,
        10,
        5,
        4,
        9,
        6,
        6,
        28,
        7,
        22,
        12,
        1,
        23,
        16,
        11,
        11,
        6,
        4,
        6,
        3
      ],
      "2025-09": [
        56,
        53,
        23,
        18,
        2,
        12,
        8,
        8,
        19,
        11,
        17,
        10,
        12,
        4,
        2,
        6,
        7,
        1,
        2,
        16,
        9,
        7,
        5,
        3,
        6,
        11,
        5,
        10,
        8,
        8,
        4,
        0
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Blockchain as an IoT intermediary",
          "year": "2021-12",
          "abstract": "Blockchain technology provides a private, secure, transparent decentralized\nexchange of data. Also, blockchain is not limited to a particular area, but it\nhas a wide range of applications and can be integrated into a variety of\nInternet interactive systems. For example, the Internet of Things (IoT), supply\nchain tracking, Electronic Health Records (EHR), digital forensics, identity\nmanagement, trustless payments, and other key business elements will all\nbenefit from its implementation. Next layer solutions such as Ethereum 2.0,\nPolkadot, Cardano, and other Web 3.0 technologies provide developers\nversatility. Moreover, these platforms utilize smart contracts which are\nsimilar to standard, traditionalized software during development but offer key\nutilities to end-users such as online wallets, secure data with transparent\nrules. Blockchain is receiving a lot of attention in educational technology\n(EduTech) as it aims to achieve a more transparent and multipurpose educational\nsystem. In addition to smart contract technology which defines how data should\nbe registered, gathered and processed, blockchain can be used as an IoT\nintermediary for mobile usage. Therefore, we implemented an educational\nlearning platform powered by blockchain technology to examine feasibility in\nindustry and academic environment. In essence, this is a web application which\nis adapted to mobile platform and connected to blockchain for crucial data\nexchanges. In this paper we want to emphasize the potential of blockchain\ntechnology in multiple sectors as well as the need to really understand the\nunderlying principles which are allowing disruptability of traditional\ncentralized software solutions.",
          "arxiv_id": "2112.08371v1"
        },
        {
          "title": "Comparative Analysis of Blockchain Systems",
          "year": "2025-05",
          "abstract": "Blockchain is a type of decentralized distributed database. Unlike\ntraditional relational database management systems, it does not require\nmanagement or maintenance by a third party. All data management and update\nprocesses are open and transparent, solving the trust issues of centralized\ndatabase management systems. Blockchain ensures network-wide consistency,\nconsensus, traceability, and immutability. Under the premise of mutual distrust\nbetween nodes, blockchain technology integrates various technologies, such as\nP2P protocols, asymmetric encryption, consensus mechanisms, and chain\nstructures. Data is distributed and stored across multiple nodes, maintained by\nall nodes, ensuring transaction data integrity, undeniability, and security.\nThis facilitates trusted information sharing and supervision. The basic\nprinciples of blockchain form the foundation for all related research.\nUnderstanding the working principles is essential for further study of\nblockchain technology. There are many platforms based on blockchain technology,\nand they differ from one another. This paper will analyze the architecture of\nblockchain systems at each layer, focusing on the principles and technologies\nof blockchain platforms such as Bitcoin, Ethereum, and Hyperledger Fabric. The\nanalysis will cover their scalability and security and highlight their\nsimilarities, differences, advantages, and disadvantages.",
          "arxiv_id": "2505.08652v1"
        },
        {
          "title": "Towards Cross-Blockchain Smart Contracts",
          "year": "2020-10",
          "abstract": "In recent years, manifold blockchain protocols have been proposed by\nresearchers and industrial companies alike. This has led to a very\nheterogeneous blockchain landscape. Accordingly, it would be desirable if\nblockchains could interact with each other. However, current blockchain\ntechnologies offer only limited support for interoperability, thus preventing\ntokens or smart contracts from leaving the scope of a particular blockchain.\n  As a first step towards a solution for cross-chain smart contract\ninteractions, we introduce a framework which allows to invoke a smart contract\nfrom another blockchain. We offer support for continuing a smart contract after\nreceiving a result from a different blockchain, and for calling smart contracts\nrecursively across blockchains. We provide a reference implementation for\nEthereum-based blockchains using Solidity and evaluate the performance\nregarding time and cost overheads.",
          "arxiv_id": "2010.07352v2"
        }
      ],
      "1": [
        {
          "title": "Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs",
          "year": "2025-08",
          "abstract": "While Large Language Models (LLMs) have shown significant advancements in\nperformance, various jailbreak attacks have posed growing safety and ethical\nrisks. Malicious users often exploit adversarial context to deceive LLMs,\nprompting them to generate responses to harmful queries. In this study, we\npropose a new defense mechanism called Context Filtering model, an input\npre-processing method designed to filter out untrustworthy and unreliable\ncontext while identifying the primary prompts containing the real user intent\nto uncover concealed malicious intent. Given that enhancing the safety of LLMs\noften compromises their helpfulness, potentially affecting the experience of\nbenign users, our method aims to improve the safety of the LLMs while\npreserving their original performance. We evaluate the effectiveness of our\nmodel in defending against jailbreak attacks through comparative analysis,\ncomparing our approach with state-of-the-art defense mechanisms against six\ndifferent attacks and assessing the helpfulness of LLMs under these defenses.\nOur model demonstrates its ability to reduce the Attack Success Rates of\njailbreak attacks by up to 88% while maintaining the original LLMs'\nperformance, achieving state-of-the-art Safety and Helpfulness Product results.\nNotably, our model is a plug-and-play method that can be applied to all LLMs,\nincluding both white-box and black-box models, to enhance their safety without\nrequiring any fine-tuning of the models themselves. We will make our model\npublicly available for research purposes.",
          "arxiv_id": "2508.10031v1"
        },
        {
          "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes",
          "year": "2024-04",
          "abstract": "Large Language Models (LLMs) have gained widespread adoption across various\ndomains, including chatbots and auto-task completion agents. However, these\nmodels are susceptible to safety vulnerabilities such as jailbreaking, prompt\ninjection, and privacy leakage attacks. These vulnerabilities can lead to the\ngeneration of malicious content, unauthorized actions, or the disclosure of\nconfidential information. While foundational LLMs undergo alignment training\nand incorporate safety measures, they are often subject to fine-tuning, or\ndoing quantization resource-constrained environments. This study investigates\nthe impact of these modifications on LLM safety, a critical consideration for\nbuilding reliable and secure AI systems. We evaluate foundational models\nincluding Mistral, Llama series, Qwen, and MosaicML, along with their\nfine-tuned variants. Our comprehensive analysis reveals that fine-tuning\ngenerally increases the success rates of jailbreak attacks, while quantization\nhas variable effects on attack success rates. Importantly, we find that\nproperly implemented guardrails significantly enhance resistance to jailbreak\nattempts. These findings contribute to our understanding of LLM vulnerabilities\nand provide insights for developing more robust safety strategies in the\ndeployment of language models.",
          "arxiv_id": "2404.04392v3"
        },
        {
          "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
          "year": "2023-10",
          "abstract": "Optimizing large language models (LLMs) for downstream use cases often\ninvolves the customization of pre-trained LLMs through further fine-tuning.\nMeta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5\nTurbo on custom datasets also encourage this practice. But, what are the safety\ncosts associated with such custom fine-tuning? We note that while existing\nsafety alignment infrastructures can restrict harmful behaviors of LLMs at\ninference time, they do not cover safety risks when fine-tuning privileges are\nextended to end-users. Our red teaming studies find that the safety alignment\nof LLMs can be compromised by fine-tuning with only a few adversarially\ndesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safety\nguardrails by fine-tuning it on only 10 such examples at a cost of less than\n$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful\ninstructions. Disconcertingly, our research also reveals that, even without\nmalicious intent, simply fine-tuning with benign and commonly used datasets can\nalso inadvertently degrade the safety alignment of LLMs, though to a lesser\nextent. These findings suggest that fine-tuning aligned LLMs introduces new\nsafety risks that current safety infrastructures fall short of addressing --\neven if a model's initial safety alignment is impeccable, it is not necessarily\nto be maintained after custom fine-tuning. We outline and critically analyze\npotential mitigations and advocate for further research efforts toward\nreinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
          "arxiv_id": "2310.03693v1"
        }
      ],
      "2": [
        {
          "title": "Founding Quantum Cryptography on Quantum Advantage, or, Towards Cryptography from $\\mathsf{\\#P}$-Hardness",
          "year": "2024-09",
          "abstract": "Recent oracle separations [Kretschmer, TQC'21, Kretschmer et. al., STOC'23]\nhave raised the tantalizing possibility of building quantum cryptography from\nsources of hardness that persist even if the polynomial hierarchy collapses. We\nrealize this possibility by building quantum bit commitments and secure\ncomputation from unrelativized, well-studied mathematical problems that are\nconjectured to be hard for $\\mathsf{P^{\\#P}}$ -- such as approximating the\npermanents of complex Gaussian matrices, or approximating the output\nprobabilities of random quantum circuits. Indeed, we show that as long as any\none of the conjectures underlying sampling-based quantum advantage (e.g.,\nBosonSampling, Random Circuit Sampling, IQP, etc.) is true, quantum\ncryptography can be based on the extremely mild assumption that\n$\\mathsf{P^{\\#P}} \\not\\subseteq \\mathsf{(io)BQP/qpoly}$. We prove that the\nfollowing hardness assumptions are equivalent. (1) The hardness of\napproximating the probability assigned to a randomly chosen string in the\nsupport of certain efficiently sampleable distributions (upto inverse\npolynomial multiplicative error).(2) The existence of one-way puzzles, where a\nquantum sampler outputs a pair of classical strings -- a puzzle and its key --\nand where the hardness lies in finding the key corresponding to a random\npuzzle. These are known to imply quantum bit commitments [Khurana and Tomer,\nSTOC'24]. (3) The existence of state puzzles, or one-way state synthesis, where\nit is hard to synthesize a secret quantum state given a public classical\nidentifier. These capture the hardness of search problems with quantum inputs\n(secrets) and classical outputs (challenges). These are the first constructions\nof quantum cryptographic primitives (one-way puzzles, quantum bit commitments,\nstate puzzles) from concrete, well-founded mathematical assumptions that do not\nimply the existence of classical cryptography.",
          "arxiv_id": "2409.15248v2"
        },
        {
          "title": "Protecting Quantum Procrastinators with Signature Lifting: A Case Study in Cryptocurrencies",
          "year": "2023-03",
          "abstract": "Current solutions to quantum vulnerabilities of widely used cryptographic\nschemes involve migrating users to post-quantum schemes before quantum attacks\nbecome feasible. This work deals with protecting quantum procrastinators: users\nthat failed to migrate to post-quantum cryptography in time.\n  To address this problem in the context of digital signatures, we introduce a\ntechnique called signature lifting, that allows us to lift a deployed\npre-quantum signature scheme satisfying a certain property to a post-quantum\nsignature scheme that uses the same keys. Informally, the said property is that\na post-quantum one-way function is used \"somewhere along the way\" to derive the\npublic-key from the secret-key. Our constructions of signature lifting relies\nheavily on the post-quantum digital signature scheme Picnic (Chase et al.,\nCCS'17).\n  Our main case-study is cryptocurrencies, where this property holds in two\nscenarios: when the public-key is generated via a key-derivation function or\nwhen the public-key hash is posted instead of the public-key itself. We propose\na modification, based on signature lifting, that can be applied in many\ncryptocurrencies for securely spending pre-quantum coins in presence of quantum\nadversaries. Our construction improves upon existing constructions in two major\nways: it is not limited to pre-quantum coins whose ECDSA public-key has been\nkept secret (and in particular, it handles all coins that are stored in\naddresses generated by HD wallets), and it does not require access to\npost-quantum coins or using side payments to pay for posting the transaction.",
          "arxiv_id": "2303.06754v2"
        },
        {
          "title": "Efficient Quantum Pseudorandomness from Hamiltonian Phase States",
          "year": "2024-10",
          "abstract": "Quantum pseudorandomness has found applications in many areas of quantum\ninformation, ranging from entanglement theory, to models of scrambling\nphenomena in chaotic quantum systems, and, more recently, in the foundations of\nquantum cryptography. Kretschmer (TQC '21) showed that both pseudorandom states\nand pseudorandom unitaries exist even in a world without classical one-way\nfunctions. To this day, however, all known constructions require classical\ncryptographic building blocks which are themselves synonymous with the\nexistence of one-way functions, and which are also challenging to realize on\nrealistic quantum hardware.\n  In this work, we seek to make progress on both of these fronts simultaneously\n-- by decoupling quantum pseudorandomness from classical cryptography\naltogether. We introduce a quantum hardness assumption called the Hamiltonian\nPhase State (HPS) problem, which is the task of decoding output states of a\nrandom instantaneous quantum polynomial-time (IQP) circuit. Hamiltonian phase\nstates can be generated very efficiently using only Hadamard gates,\nsingle-qubit Z-rotations and CNOT circuits. We show that the hardness of our\nproblem reduces to a worst-case version of the problem, and we provide evidence\nthat our assumption is plausibly fully quantum; meaning, it cannot be used to\nconstruct one-way functions. We also show information-theoretic hardness when\nonly few copies of HPS are available by proving an approximate $t$-design\nproperty of our ensemble. Finally, we show that our HPS assumption and its\nvariants allow us to efficiently construct many pseudorandom quantum\nprimitives, ranging from pseudorandom states, to quantum pseudoentanglement, to\npseudorandom unitaries, and even primitives such as public-key encryption with\nquantum keys.",
          "arxiv_id": "2410.08073v3"
        }
      ],
      "3": [
        {
          "title": "Advances in Differential Privacy and Differentially Private Machine Learning",
          "year": "2024-04",
          "abstract": "There has been an explosion of research on differential privacy (DP) and its\nvarious applications in recent years, ranging from novel variants and\naccounting techniques in differential privacy to the thriving field of\ndifferentially private machine learning (DPML) to newer implementations in\npractice, like those by various companies and organisations such as census\nbureaus. Most recent surveys focus on the applications of differential privacy\nin particular contexts like data publishing, specific machine learning tasks,\nanalysis of unstructured data, location privacy, etc. This work thus seeks to\nfill the gap for a survey that primarily discusses recent developments in the\ntheory of differential privacy along with newer DP variants, viz. Renyi DP and\nConcentrated DP, novel mechanisms and techniques, and the theoretical\ndevelopments in differentially private machine learning in proper detail. In\naddition, this survey discusses its applications to privacy-preserving machine\nlearning in practice and a few practical implementations of DP.",
          "arxiv_id": "2404.04706v1"
        },
        {
          "title": "Reconstruction Attacks on Aggressive Relaxations of Differential Privacy",
          "year": "2022-09",
          "abstract": "Differential privacy is a widely accepted formal privacy definition that\nallows aggregate information about a dataset to be released while controlling\nprivacy leakage for individuals whose records appear in the data. Due to the\nunavoidable tension between privacy and utility, there have been many works\ntrying to relax the requirements of differential privacy to achieve greater\nutility. One class of relaxation, which is starting to gain support outside the\nprivacy community is embodied by the definitions of individual differential\nprivacy (IDP) and bootstrap differential privacy (BDP). The original version of\ndifferential privacy defines a set of neighboring database pairs and achieves\nits privacy guarantees by requiring that each pair of neighbors should be\nnearly indistinguishable to an attacker. The privacy definitions we study,\nhowever, aggressively reduce the set of neighboring pairs that are protected.\nBoth IDP and BDP define a measure of \"privacy loss\" that satisfies formal\nprivacy properties such as postprocessing invariance and composition, and\nachieve dramatically better utility than the traditional variants of\ndifferential privacy. However, there is a significant downside - we show that\nthey allow a significant portion of the dataset to be reconstructed using\nalgorithms that have arbitrarily low privacy loss under their privacy\naccounting rules. We demonstrate these attacks using the preferred mechanisms\nof these privacy definitions. In particular, we design a set of queries that,\nwhen protected by these mechanisms with high noise settings (i.e., with claims\nof very low privacy loss), yield more precise information about the dataset\nthan if they were not protected at all.",
          "arxiv_id": "2209.03905v1"
        },
        {
          "title": "Conciliating Privacy and Utility in Data Releases via Individual Differential Privacy and Microaggregation",
          "year": "2023-12",
          "abstract": "$\\epsilon$-Differential privacy (DP) is a well-known privacy model that\noffers strong privacy guarantees. However, when applied to data releases, DP\nsignificantly deteriorates the analytical utility of the protected outcomes. To\nkeep data utility at reasonable levels, practical applications of DP to data\nreleases have used weak privacy parameters (large $\\epsilon$), which dilute the\nprivacy guarantees of DP. In this work, we tackle this issue by using an\nalternative formulation of the DP privacy guarantees, named\n$\\epsilon$-individual differential privacy (iDP), which causes less data\ndistortion while providing the same protection as DP to subjects. We enforce\niDP in data releases by relying on attribute masking plus a pre-processing step\nbased on data microaggregation. The goal of this step is to reduce the\nsensitivity to record changes, which determines the amount of noise required to\nenforce iDP (and DP). Specifically, we propose data microaggregation strategies\ndesigned for iDP whose sensitivities are significantly lower than those used in\nDP. As a result, we obtain iDP-protected data with significantly better utility\nthan with DP. We report on experiments that show how our approach can provide\nstrong privacy (small $\\epsilon$) while yielding protected data that do not\nsignificantly degrade the accuracy of secondary data analysis.",
          "arxiv_id": "2312.13712v1"
        }
      ],
      "4": [
        {
          "title": "Leaking Control Flow Information via the Hardware Prefetcher",
          "year": "2021-09",
          "abstract": "Modern processor designs use a variety of microarchitectural methods to\nachieve high performance. Unfortunately, new side-channels have often been\nuncovered that exploit these enhanced designs. One area that has received\nlittle attention from a security perspective is the processor's hard-ware\nprefetcher, a critical component used to mitigate DRAM latency in today's\nsystems. Prefetchers, like branch predictors, hold critical state related to\nthe execution of the application, and have the potential to leak secret\ninformation. But up to now, there has not been a demonstration of a generic\nprefetcher side-channel that could be actively exploited in today's hardware.\n  In this paper, we present AfterImage, a new side-channel that exploits the\nIntel Instruction Pointer-based stride prefetcher. We observe that, when the\nexecution of the processor switches between different private domains, the\nprefetcher trained by one domain can be triggered in another. To the best of\nour knowledge, this work is the first to publicly demonstrate a methodology\nthat is both algorithm-agnostic and also able to leak kernel data into\nuserspace. AfterImage is different from previous works, as it leaks data on the\nnon-speculative path of execution. Because of this, a large class of work that\nhas focused on protecting transient, branch-outcome-based data will be unable\nto block this side-channel. By reverse-engineering the IP-stride prefetcher in\nmodern Intel processors, we have successfully developed three variants of\nAfterImage to leak control flow information across code regions, processes and\nthe user-kernel boundary. We find a high level of accuracy in leaking\ninformation with our methodology (from 91%, up to 99%), and propose two\nmitigation techniques to block this side-channel, one of which can be used on\nhardware systems today.",
          "arxiv_id": "2109.00474v1"
        },
        {
          "title": "CrypTag: Thwarting Physical and Logical Memory Vulnerabilities using Cryptographically Colored Memory",
          "year": "2020-12",
          "abstract": "Memory vulnerabilities are a major threat to many computing systems. To\neffectively thwart spatial and temporal memory vulnerabilities, full logical\nmemory safety is required. However, current mitigation techniques for memory\nsafety are either too expensive or trade security against efficiency. One\npromising attempt to detect memory safety vulnerabilities in hardware is memory\ncoloring, a security policy deployed on top of tagged memory architectures.\nHowever, due to the memory storage and bandwidth overhead of large tags,\ncommodity tagged memory architectures usually only provide small tag sizes,\nthus limiting their use for security applications. Irrespective of logical\nmemory safety, physical memory safety is a necessity in hostile environments\nprevalent for modern cloud computing and IoT devices. Architectures from Intel\nand AMD already implement transparent memory encryption to maintain\nconfidentiality and integrity of all off-chip data. Surprisingly, the\ncombination of both, logical and physical memory safety, has not yet been\nextensively studied in previous research, and a naive combination of both\nsecurity strategies would accumulate both overheads. In this paper, we propose\nCrypTag, an efficient hardware/software co-design mitigating a large class of\nlogical memory safety issues and providing full physical memory safety. At its\ncore, CrypTag utilizes a transparent memory encryption engine not only for\nphysical memory safety, but also for memory coloring at hardly any additional\ncosts. The design avoids any overhead for tag storage by embedding memory\ncolors in the upper bits of a pointer and using these bits as an additional\ninput for the memory encryption. A custom compiler extension automatically\nleverages CrypTag to detect logical memory safety issues for commodity programs\nand is fully backward compatible.",
          "arxiv_id": "2012.06761v2"
        },
        {
          "title": "Stockade: Hardware Hardening for Distributed Trusted Sandboxes",
          "year": "2021-08",
          "abstract": "The widening availability of hardware-based trusted execution environments\n(TEEs) has been accelerating the adaptation of new applications using TEEs.\nRecent studies showed that a cloud application consists of multiple distributed\nsoftware modules provided by mutually distrustful parties. The applications use\nmultiple TEEs (enclaves) communicating through software-encrypted memory\nchannels. Such execution model requires bi-directional protection: protecting\nthe rest of the system from the enclave module with sandboxing and protecting\nthe enclave module from a third-part module and operating systems. However, the\ncurrent TEE model, such as Intel SGX, cannot efficiently represent such\ndistributed sandbox applications. To overcome the lack of hardware supports for\nsandboxed TEEs, this paper proposes an extended enclave model called Stockade,\nwhich supports distributed sandboxes hardened by hardware. Stockade proposes\nnew three key techniques. First, it extends the hardware-based memory isolation\nin SGX to confine a user software module only within its enclave. Second, it\nproposes a trusted monitor enclave that filters and validates systems calls\nfrom enclaves. Finally, it allows hardware-protected memory sharing between a\npair of enclaves for efficient protected communication without software-based\nencryption. Using an emulated SGX platform with the proposed extensions, this\npaper shows that distributed sandbox applications can be effectively supported\nwith small changes of SGX hardware.",
          "arxiv_id": "2108.13922v2"
        }
      ],
      "5": [
        {
          "title": "FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information",
          "year": "2022-10",
          "abstract": "Federated learning is vulnerable to poisoning attacks in which malicious\nclients poison the global model via sending malicious model updates to the\nserver. Existing defenses focus on preventing a small number of malicious\nclients from poisoning the global model via robust federated learning methods\nand detecting malicious clients when there are a large number of them. However,\nit is still an open challenge how to recover the global model from poisoning\nattacks after the malicious clients are detected. A naive solution is to remove\nthe detected malicious clients and train a new global model from scratch, which\nincurs large cost that may be intolerable for resource-constrained clients such\nas smartphones and IoT devices.\n  In this work, we propose FedRecover, which can recover an accurate global\nmodel from poisoning attacks with small cost for the clients. Our key idea is\nthat the server estimates the clients' model updates instead of asking the\nclients to compute and communicate them during the recovery process. In\nparticular, the server stores the global models and clients' model updates in\neach round, when training the poisoned global model. During the recovery\nprocess, the server estimates a client's model update in each round using its\nstored historical information. Moreover, we further optimize FedRecover to\nrecover a more accurate global model using warm-up, periodic correction,\nabnormality fixing, and final tuning strategies, in which the server asks the\nclients to compute and communicate their exact model updates. Theoretically, we\nshow that the global model recovered by FedRecover is close to or the same as\nthat recovered by train-from-scratch under some assumptions. Empirically, our\nevaluation on four datasets, three federated learning methods, as well as\nuntargeted and targeted poisoning attacks (e.g., backdoor attacks) shows that\nFedRecover is both accurate and efficient.",
          "arxiv_id": "2210.10936v1"
        },
        {
          "title": "Byzantine-Robust Decentralized Federated Learning",
          "year": "2024-06",
          "abstract": "Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models without revealing their private training data. In\nconventional FL, the system follows the server-assisted architecture\n(server-assisted FL), where the training process is coordinated by a central\nserver. However, the server-assisted FL framework suffers from poor scalability\ndue to a communication bottleneck at the server, and trust dependency issues.\nTo address challenges, decentralized federated learning (DFL) architecture has\nbeen proposed to allow clients to train models collaboratively in a serverless\nand peer-to-peer manner. However, due to its fully decentralized nature, DFL is\nhighly vulnerable to poisoning attacks, where malicious clients could\nmanipulate the system by sending carefully-crafted local models to their\nneighboring clients. To date, only a limited number of Byzantine-robust DFL\nmethods have been proposed, most of which are either communication-inefficient\nor remain vulnerable to advanced poisoning attacks. In this paper, we propose a\nnew algorithm called BALANCE (Byzantine-robust averaging through local\nsimilarity in decentralization) to defend against poisoning attacks in DFL. In\nBALANCE, each client leverages its own local model as a similarity reference to\ndetermine if the received model is malicious or benign. We establish the\ntheoretical convergence guarantee for BALANCE under poisoning attacks in both\nstrongly convex and non-convex settings. Furthermore, the convergence rate of\nBALANCE under poisoning attacks matches those of the state-of-the-art\ncounterparts in Byzantine-free settings. Extensive experiments also demonstrate\nthat BALANCE outperforms existing DFL methods and effectively defends against\npoisoning attacks.",
          "arxiv_id": "2406.10416v4"
        },
        {
          "title": "BlindFL: Segmented Federated Learning with Fully Homomorphic Encryption",
          "year": "2025-01",
          "abstract": "Federated learning (FL) is a popular privacy-preserving edge-to-cloud\ntechnique used for training and deploying artificial intelligence (AI) models\non edge devices. FL aims to secure local client data while also collaboratively\ntraining a global model. Under standard FL, clients within the federation send\nmodel updates, derived from local data, to a central server for aggregation\ninto a global model. However, extensive research has demonstrated that private\ndata can be reliably reconstructed from these model updates using gradient\ninversion attacks (GIAs). To protect client data from server-side GIAs,\nprevious FL schemes have employed fully homomorphic encryption (FHE) to secure\nmodel updates while still enabling popular aggregation methods. However,\ncurrent FHE-based FL schemes either incur substantial computational overhead or\ntrade security and/or model accuracy for efficiency. We introduce BlindFL, a\nframework for global model aggregation in which clients encrypt and send a\nsubset of their local model update. With choice over the subset size, BlindFL\noffers flexible efficiency gains while preserving full encryption of aggregated\nupdates. Moreover, we demonstrate that implementing BlindFL can substantially\nlower space and time transmission costs per client, compared with plain FL with\nFHE, while maintaining global model accuracy. BlindFL also offers additional\ndepth of security. While current single-key, FHE-based FL schemes explicitly\ndefend against server-side adversaries, they do not address the realistic\nthreat of malicious clients within the federation. By contrast, we\ntheoretically and experimentally demonstrate that BlindFL significantly impedes\nclient-side model poisoning attacks, a first for single-key, FHE-based FL\nschemes.",
          "arxiv_id": "2501.11659v1"
        }
      ],
      "6": [
        {
          "title": "Nowhere to Hide: A Lightweight Unsupervised Detector against Adversarial Examples",
          "year": "2022-10",
          "abstract": "Although deep neural networks (DNNs) have shown impressive performance on\nmany perceptual tasks, they are vulnerable to adversarial examples that are\ngenerated by adding slight but maliciously crafted perturbations to benign\nimages. Adversarial detection is an important technique for identifying\nadversarial examples before they are entered into target DNNs. Previous studies\nto detect adversarial examples either targeted specific attacks or required\nexpensive computation. How design a lightweight unsupervised detector is still\na challenging problem. In this paper, we propose an AutoEncoder-based\nAdversarial Examples (AEAE) detector, that can guard DNN models by detecting\nadversarial examples with low computation in an unsupervised manner. The AEAE\nincludes only a shallow autoencoder but plays two roles. First, a well-trained\nautoencoder has learned the manifold of benign examples. This autoencoder can\nproduce a large reconstruction error for adversarial images with large\nperturbations, so we can detect significantly perturbed adversarial examples\nbased on the reconstruction error. Second, the autoencoder can filter out the\nsmall noise and change the DNN's prediction on adversarial examples with small\nperturbations. It helps to detect slightly perturbed adversarial examples based\non the prediction distance. To cover these two cases, we utilize the\nreconstruction error and prediction distance from benign images to construct a\ntwo-tuple feature set and train an adversarial detector using the isolation\nforest algorithm. We show empirically that the AEAE is unsupervised and\ninexpensive against the most state-of-the-art attacks. Through the detection in\nthese two cases, there is nowhere to hide adversarial examples.",
          "arxiv_id": "2210.08579v1"
        },
        {
          "title": "Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model",
          "year": "2023-04",
          "abstract": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarial\nexamples. Moreover, the transferability of the adversarial examples has\nreceived broad attention in recent years, which means that adversarial examples\ncrafted by a surrogate model can also attack unknown models. This phenomenon\ngave birth to the transfer-based adversarial attacks, which aim to improve the\ntransferability of the generated adversarial examples. In this paper, we\npropose to improve the transferability of adversarial examples in the\ntransfer-based attack via masking unimportant parameters (MUP). The key idea in\nMUP is to refine the pretrained surrogate models to boost the transfer-based\nattack. Based on this idea, a Taylor expansion-based metric is used to evaluate\nthe parameter importance score and the unimportant parameters are masked during\nthe generation of adversarial examples. This process is simple, yet can be\nnaturally combined with various existing gradient-based optimizers for\ngenerating adversarial examples, thus further improving the transferability of\nthe generated adversarial examples. Extensive experiments are conducted to\nvalidate the effectiveness of the proposed MUP-based methods.",
          "arxiv_id": "2304.06908v1"
        },
        {
          "title": "Generalizing Adversarial Examples by AdaBelief Optimizer",
          "year": "2021-01",
          "abstract": "Recent research has proved that deep neural networks (DNNs) are vulnerable to\nadversarial examples, the legitimate input added with imperceptible and\nwell-designed perturbations can fool DNNs easily in the testing stage. However,\nmost of the existing adversarial attacks are difficult to fool adversarially\ntrained models. To solve this issue, we propose an AdaBelief iterative Fast\nGradient Sign Method (AB-FGSM) to generalize adversarial examples. By\nintegrating AdaBelief optimization algorithm to I-FGSM, we believe that the\ngeneralization of adversarial examples will be improved, relying on the strong\ngeneralization of AdaBelief optimizer. To validate the effectiveness and\ntransferability of adversarial examples generated by our proposed AB-FGSM, we\nconduct the white-box and black-box attacks on various single models and\nensemble models. Compared with state-of-the-art attack methods, our proposed\nmethod can generate adversarial examples effectively in the white-box setting,\nand the transfer rate is 7%-21% higher than latest attack methods.",
          "arxiv_id": "2101.09930v1"
        }
      ],
      "7": [
        {
          "title": "CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data",
          "year": "2025-05",
          "abstract": "As computer networks proliferate, the gravity of network intrusions has\nescalated, emphasizing the criticality of network intrusion detection systems\nfor safeguarding security. While deep learning models have exhibited promising\nresults in intrusion detection, they face challenges in managing\nhigh-dimensional, complex traffic patterns and imbalanced data categories. This\npaper presents CSAGC-IDS, a network intrusion detection model based on deep\nlearning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced\nconvolutional conditional generative adversarial network that generates\nhigh-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS\nintegrates CSCA-CNN, a convolutional neural network enhanced through cost\nsensitive learning and channel attention mechanism, to extract features from\ncomplex traffic data for precise detection. Experiments conducted on the\nNSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of\n84.52% in five-class classification task, and an accuracy of 91.09% and an F1\nscore of 92.04% in binary classification task.Furthermore, this paper provides\nan interpretability analysis of the proposed model, using SHAP and LIME to\nexplain the decision-making mechanisms of the model.",
          "arxiv_id": "2505.14027v1"
        },
        {
          "title": "Host-Based Network Intrusion Detection via Feature Flattening and Two-stage Collaborative Classifier",
          "year": "2023-06",
          "abstract": "Network Intrusion Detection Systems (NIDS) have been extensively investigated\nby monitoring real network traffic and analyzing suspicious activities.\nHowever, there are limitations in detecting specific types of attacks with\nNIDS, such as Advanced Persistent Threats (APT). Additionally, NIDS is\nrestricted in observing complete traffic information due to encrypted traffic\nor a lack of authority. To address these limitations, a Host-based Intrusion\nDetection system (HIDS) evaluates resources in the host, including logs, files,\nand folders, to identify APT attacks that routinely inject malicious files into\nvictimized nodes. In this study, a hybrid network intrusion detection system\nthat combines NIDS and HIDS is proposed to improve intrusion detection\nperformance. The feature flattening technique is applied to flatten\ntwo-dimensional host-based features into one-dimensional vectors, which can be\ndirectly used by traditional Machine Learning (ML) models. A two-stage\ncollaborative classifier is introduced that deploys two levels of ML algorithms\nto identify network intrusions. In the first stage, a binary classifier is used\nto detect benign samples. All detected attack types undergo a multi-class\nclassifier to reduce the complexity of the original problem and improve the\noverall detection performance. The proposed method is shown to generalize\nacross two well-known datasets, CICIDS 2018 and NDSec-1. Performance of\nXGBoost, which represents conventional ML, is evaluated. Combining host and\nnetwork features enhances attack detection performance (macro average F1 score)\nby 8.1% under the CICIDS 2018 dataset and 3.7% under the NDSec-1 dataset.\nMeanwhile, the two-stage collaborative classifier improves detection\nperformance for most single classes, especially for DoS-LOIC-UDP and\nDoS-SlowHTTPTest, with improvements of 30.7% and 84.3%, respectively, when\ncompared with the traditional ML XGBoost.",
          "arxiv_id": "2306.09451v1"
        },
        {
          "title": "Meta-Analysis and Systematic Review for Anomaly Network Intrusion Detection Systems: Detection Methods, Dataset, Validation Methodology, and Challenges",
          "year": "2023-08",
          "abstract": "Intrusion detection systems (IDSs) built on artificial intelligence (AI) are\npresented as latent mechanisms for actively detecting fresh attacks over a\ncomplex network. Although review papers are used the systematic review or\nsimple methods to analyse and criticize the anomaly NIDS works, the current\nreview uses a traditional way as a quantitative description to find current\ngaps by synthesizing and summarizing the data comparison without considering\nalgorithms performance. This paper presents a systematic and meta-analysis\nstudy of AI for network intrusion detection systems (NIDS) focusing on deep\nlearning (DL) and machine learning (ML) approaches in network security. Deep\nlearning algorithms are explained in their structure, and data intrusion\nnetwork is justified based on an infrastructure of networks and attack types.\nBy conducting a meta-analysis and debating the validation of the DL and ML\napproach by effectiveness, used dataset, detected attacks, classification task,\nand time complexity, we offer a thorough benchmarking assessment of the current\nNIDS-based publications-based systematic approach. The proposed method is\nconsidered reviewing works for the anomaly-based network intrusion detection\nsystem (anomaly-NIDS) models. Furthermore, the effectiveness of proposed\nalgorithms and selected datasets are discussed for the recent direction and\nimprovements of ML and DL to the NIDS. The future trends for improving an\nanomaly-IDS for continuing detection in the evolution of cyberattacks are\nhighlighted in several research studies.",
          "arxiv_id": "2308.02805v2"
        }
      ],
      "8": [
        {
          "title": "How Far Have We Gone in Binary Code Understanding Using Large Language Models",
          "year": "2024-04",
          "abstract": "Binary code analysis plays a pivotal role in various software security\napplications, such as software maintenance, malware detection, software\nvulnerability discovery, patch analysis, etc. However, unlike source code,\nunderstanding binary code is challenging for reverse engineers due to the\nabsence of semantic information. Therefore, automated tools are needed to\nassist human players in interpreting binary code. In recent years, two groups\nof technologies have shown promising prospects: (1) Deep learning-based\ntechnologies have demonstrated competitive results in tasks related to binary\ncode understanding, furthermore, (2) Large Language Models (LLMs) have been\nextensively pre-trained at the source-code level for tasks such as code\nunderstanding and generation. This makes participants wonder about the ability\nof LLMs in binary code understanding.\n  In this work, we propose a benchmark to evaluate the effectiveness of LLMs in\nreal-world reverse engineering scenarios. The benchmark covers two key binary\ncode understanding tasks, including function name recovery and binary code\nsummarization. We gain valuable insights into their capabilities and\nlimitations through extensive evaluations of popular LLMs using our benchmark.\nOur evaluations reveal that existing LLMs can understand binary code to a\ncertain extent, thereby improving the efficiency of binary code analysis. Our\nresults highlight the great potential of the LLMs in advancing the field of\nbinary code understanding.",
          "arxiv_id": "2404.09836v3"
        },
        {
          "title": "Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?",
          "year": "2023-05",
          "abstract": "Software vulnerabilities bear enterprises significant costs. Despite\nextensive efforts in research and development of software vulnerability\ndetection methods, uncaught vulnerabilities continue to put software owners and\nusers at risk. Many current vulnerability detection methods require that code\nsnippets can compile and build before attempting detection. This,\nunfortunately, introduces a long latency between the time a vulnerability is\ninjected to the time it is removed, which can substantially increases the cost\nof fixing a vulnerability. We recognize that the current advances in machine\nlearning can be used to detect vulnerable code patterns on syntactically\nincomplete code snippets as the developer is writing the code at EditTime. In\nthis paper we present a practical system that leverages deep learning on a\nlarge-scale data set of vulnerable code patterns to learn complex\nmanifestations of more than 250 vulnerability types and detect vulnerable code\npatterns at EditTime. We discuss zero-shot, few-shot, and fine-tuning\napproaches on state of the art pre-trained Large Language Models (LLMs). We\nshow that in comparison with state of the art vulnerability detection models\nour approach improves the state of the art by 10%. We also evaluate our\napproach to detect vulnerability in auto-generated code by code LLMs.\nEvaluation on a benchmark of high-risk code scenarios shows a reduction of up\nto 90% vulnerability reduction.",
          "arxiv_id": "2306.01754v1"
        },
        {
          "title": "An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding",
          "year": "2025-04",
          "abstract": "Binary code analysis plays a pivotal role in the field of software security\nand is widely used in tasks such as software maintenance, malware detection,\nsoftware vulnerability discovery, patch analysis, etc. However, unlike source\ncode, reverse engineers face significant challenges in understanding binary\ncode due to the lack of intuitive semantic information. Although traditional\nreverse tools can convert binary code into C-like pseudo code, the lack of code\ncomments and symbolic information such as function names still makes code\nunderstanding difficult. In recent years, two groups of techniques have shown\npromising prospects: (1) Deep learning-based techniques have demonstrated\ncompetitive results in tasks related to binary code understanding, furthermore,\n(2) Large Language Models (LLMs) have been extensively pre-trained at the\nsource-code level for tasks such as code understanding and generation. This has\nleft participants wondering about the capabilities of LLMs in binary code\nunderstanding. To this end, this work proposes a benchmark to evaluate the\neffectiveness of LLMs in real-world reverse engineering scenarios, which covers\ntwo key binary code understanding tasks, i.e., function name recovery and\nbinary code summarization. To more comprehensively evaluate, we include\nbinaries with multiple target architectures as well as different optimization\noptions. We gain valuable insights into the capabilities and limitations\nthrough extensive empirical studies of popular LLMs using our benchmark. Our\nevaluations reveal that existing LLMs can understand binary code to a certain\nextent, thereby improving the efficiency of binary code analysis. Our results\nhighlight the great potential of the LLMs in advancing the field of binary code\nunderstanding, and provide new directions for binary code analysis techniques.",
          "arxiv_id": "2504.21803v1"
        }
      ],
      "9": [
        {
          "title": "PromptSAM+: Malware Detection based on Prompt Segment Anything Model",
          "year": "2024-08",
          "abstract": "Machine learning and deep learning (ML/DL) have been extensively applied in\nmalware detection, and some existing methods demonstrate robust performance.\nHowever, several issues persist in the field of malware detection: (1) Existing\nwork often overemphasizes accuracy at the expense of practicality, rarely\nconsidering false positive and false negative rates as important metrics. (2)\nConsidering the evolution of malware, the performance of classifiers\nsignificantly declines over time, greatly reducing the practicality of malware\ndetectors. (3) Prior ML/DL-based efforts heavily rely on ample labeled data for\nmodel training, largely dependent on feature engineering or domain knowledge to\nbuild feature databases, making them vulnerable if correct labels are scarce.\nWith the development of computer vision, vision-based malware detection\ntechnology has also rapidly evolved. In this paper, we propose a visual malware\ngeneral enhancement classification framework, `PromptSAM+', based on a large\nvisual network segmentation model, the Prompt Segment Anything Model(named\nPromptSAM+). Our experimental results indicate that 'PromptSAM+' is effective\nand efficient in malware detection and classification, achieving high accuracy\nand low rates of false positives and negatives. The proposed method outperforms\nthe most advanced image-based malware detection technologies on several\ndatasets. 'PromptSAM+' can mitigate aging in existing image-based malware\nclassifiers, reducing the considerable manpower needed for labeling new malware\nsamples through active learning. We conducted experiments on datasets for both\nWindows and Android platforms, achieving favorable outcomes. Additionally, our\nablation experiments on several datasets demonstrate that our model identifies\neffective modules within the large visual network.",
          "arxiv_id": "2408.02066v1"
        },
        {
          "title": "EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers",
          "year": "2025-06",
          "abstract": "A lack of accessible data has historically restricted malware analysis\nresearch, and practitioners have relied heavily on datasets provided by\nindustry sources to advance. Existing public datasets are limited by narrow\nscope - most include files targeting a single platform, have labels supporting\njust one type of malware classification task, and make no effort to capture the\nevasive files that make malware detection difficult in practice. We present\nEMBER2024, a new dataset that enables holistic evaluation of malware\nclassifiers. Created in collaboration with the authors of EMBER2017 and\nEMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,\nand labels for more than 3.2 million files from six file formats. Our dataset\nsupports the training and evaluation of machine learning models on seven\nmalware classification tasks, including malware detection, malware family\nclassification, and malware behavior identification. EMBER2024 is the first to\ninclude a collection of malicious files that initially went undetected by a set\nof antivirus products, creating a \"challenge\" set to assess classifier\nperformance against evasive malware. This work also introduces EMBER feature\nversion 3, with added support for several new feature types. We are releasing\nthe EMBER2024 dataset to promote reproducibility and empower researchers in the\npursuit of new malware research topics.",
          "arxiv_id": "2506.05074v1"
        },
        {
          "title": "Advances In Malware Detection- An Overview",
          "year": "2021-04",
          "abstract": "Malware has become a widely used means in cyber attacks in recent decades\nbecause of various new obfuscation techniques used by malwares. In order to\nprotect the systems, data and information, detection of malware is needed as\nearly as possible. There are various studies on malware detection techniques\nthat have been done but there is no method which can detect the malware\ncompletely and make malware detection problematic. Static Malware analysis is\nvery effective for known malwares but it does not work for zero day malware\nwhich leads to the need of dynamic malware detection and the behaviour based\nmalware detection is comparatively good among all detection techniques like\nsignature based, deep learning based, mobile/IOT and cloud based detection but\nstill it is not able to detect all zero day malware which shows the malware\ndetection is very challenging task and need more techniques for malware\ndetection. This paper describes a literature review of various methods of\nmalware detection. A short description of each method is provided and discusses\nvarious studies already done in the advanced malware detection field and their\ncomparison based on the detection method used, accuracy and other parameters.\nApart from this we will discuss various malware detection tools, dataset and\ntheir sources which can be used in further study. This paper gives you the\ndetailed knowledge of advanced malwares, its detection methods, how you can\nprotect your devices and data from malware attacks and it gives the comparison\nof different studies on malware detection.",
          "arxiv_id": "2104.01835v2"
        }
      ],
      "10": [
        {
          "title": "We need to aim at the top: Factors associated with cybersecurity awareness of cyber and information security decision-makers",
          "year": "2024-04",
          "abstract": "Cyberattacks pose a significant business risk to organizations. Although\nthere is ample literature focusing on why people pose a major risk to\norganizational cybersecurity and how to deal with it, there is surprisingly\nlittle we know about cyber and information security decision-makers who are\nessentially the people in charge of setting up and maintaining organizational\ncybersecurity. In this paper, we study cybersecurity awareness of cyber and\ninformation security decision-makers, and investigate factors associated with\nit. We conducted an online survey among Slovenian cyber and information\nsecurity decision-makers (N=283) to (1) determine whether their cybersecurity\nawareness is associated with adoption of antimalware solutions in their\norganizations, and (2) explore which organizational factors and personal\ncharacteristics are associated with their cybersecurity awareness. Our findings\nindicate that awareness of well-known threats and solutions seems to be quite\nlow for individuals in decision-making roles. They also provide insights into\nwhich threats and solutions are cyber and information security decision-makers\nthe least aware of. We uncovered that awareness of certain threats and\nsolutions is positively associated with either adoption of advanced antimalware\nsolutions with EDR/XDR capabilities or adoption of SOC. Additionally, we\nidentified significant organizational factors (organizational role type) and\npersonal characteristics (gender, age, experience with information security and\nexperience with IT) related to cybersecurity awareness of cyber and information\nsecurity decision-makers. Organization size and formal education were not\nsignificant. These results offer insights that can be leveraged in targeted\ncybersecurity training tailored to the needs of groups of cyber and information\nsecurity decision-makers based on these key factors.",
          "arxiv_id": "2404.04725v1"
        },
        {
          "title": "Foundations of Cyber Resilience: The Confluence of Game, Control, and Learning Theories",
          "year": "2024-04",
          "abstract": "Cyber resilience is a complementary concept to cybersecurity, focusing on the\npreparation, response, and recovery from cyber threats that are challenging to\nprevent. Organizations increasingly face such threats in an evolving cyber\nthreat landscape. Understanding and establishing foundations for cyber\nresilience provide a quantitative and systematic approach to cyber risk\nassessment, mitigation policy evaluation, and risk-informed defense design. A\nsystems-scientific view toward cyber risks provides holistic and system-level\nsolutions. This chapter starts with a systemic view toward cyber risks and\npresents the confluence of game theory, control theory, and learning theories,\nwhich are three major pillars for the design of cyber resilience mechanisms to\ncounteract increasingly sophisticated and evolving threats in our networks and\norganizations. Game and control theoretic methods provide a set of modeling\nframeworks to capture the strategic and dynamic interactions between defenders\nand attackers. Control and learning frameworks together provide a\nfeedback-driven mechanism that enables autonomous and adaptive responses to\nthreats. Game and learning frameworks offer a data-driven approach to\nproactively reason about adversarial behaviors and resilient strategies. The\nconfluence of the three lays the theoretical foundations for the analysis and\ndesign of cyber resilience. This chapter presents various theoretical\nparadigms, including dynamic asymmetric games, moving horizon control,\nconjectural learning, and meta-learning, as recent advances at the\nintersection. This chapter concludes with future directions and discussions of\nthe role of neurosymbolic learning and the synergy between foundation models\nand game models in cyber resilience.",
          "arxiv_id": "2404.01205v2"
        },
        {
          "title": "knowCC: Knowledge, awareness of computer & cyber ethics between CS/non-CS university students",
          "year": "2023-10",
          "abstract": "Technology has advanced dramatically in the previous several years. There are\nalso cyber assaults. Cyberattacks pose a possible danger to information\nsecurity and the general public. Since data practice and internet consumption\nrates continue to upswing, cyber awareness has become progressively important.\nFurthermore, as businesses pace their digital transformation with mobile\ndevices, cloud services, communal media, and Internet of Things services,\ncybersecurity has appeared as a critical issue in corporate risk management.\nThis research focuses on the relations between cybersecurity awareness, cyber\nknowledge, computer ethics, cyber ethics, and cyber behavior, as well as\nprotective tools, across university students in general. The findings express\nthat while internet users are alert of cyber threats, they only take the most\nelementary and easy-to-implement precautions. Several knowledge and awareness\nhave been proposed to knob the issue of cyber security. It also grants the\nprinciples of cybersecurity in terms of its structure, workforces, and evidence\npertaining to the shield of personal information in the cyber world. The first\nstep is for people to educate themselves about the negative aspects of the\ninternet and to learn more about cyber threats so that they can notice when an\nattack is taking place. To validate the efficiency of the suggested analysis\nbetween CS and non-CS university students, case study along with several\ncomparisons are provided.",
          "arxiv_id": "2310.12684v1"
        }
      ],
      "11": [
        {
          "title": "SEAL: Semantic Aware Image Watermarking",
          "year": "2025-03",
          "abstract": "Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels.",
          "arxiv_id": "2503.12172v3"
        },
        {
          "title": "DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models",
          "year": "2025-02",
          "abstract": "As large language models (LLMs) grow more powerful, concerns over copyright\ninfringement of LLM-generated texts have intensified. LLM watermarking has been\nproposed to trace unauthorized redistribution or resale of generated content by\nembedding identifiers within the text. Existing approaches primarily rely on\none-bit watermarking, which only verifies whether a text was generated by a\nspecific LLM. In contrast, multi-bit watermarking encodes richer information,\nenabling the identification of the specific LLM and user involved in generated\nor distributed content. However, current multi-bit methods directly embed the\nwatermark into the text without considering its watermark capacity, which can\nresult in failures, especially in low-entropy texts. In this paper, we analyze\nthat the watermark embedding follows a normal distribution. We then derive a\nformal inequality to optimally segment the text for watermark embedding.\nBuilding upon this, we propose DERMARK, a dynamic, efficient, and robust\nmulti-bit watermarking method that divides the text into variable-length\nsegments for each watermark bit during the inference. Moreover, DERMARK incurs\nnegligible overhead since no additional intermediate matrices are generated and\nachieves robustness against text editing by minimizing watermark extraction\nloss. Experiments demonstrate that, compared to SOTA, on average, our method\nreduces the number of tokens required per embedded bit by 25\\%, reduces\nwatermark embedding time by 50\\%, and maintains high robustness against text\nmodifications and watermark erasure attacks.",
          "arxiv_id": "2502.05213v2"
        },
        {
          "title": "ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization",
          "year": "2024-11",
          "abstract": "Watermarking generative content serves as a vital tool for authentication,\nownership protection, and mitigation of potential misuse. Existing watermarking\nmethods face the challenge of balancing robustness and concealment. They\nempirically inject a watermark that is both invisible and robust and passively\nachieve concealment by limiting the strength of the watermark, thus reducing\nthe robustness. In this paper, we propose to explicitly introduce a watermark\nhiding process to actively achieve concealment, thus allowing the embedding of\nstronger watermarks. To be specific, we implant a robust watermark in an\nintermediate diffusion state and then guide the model to hide the watermark in\nthe final generated image. We employ an adversarial optimization algorithm to\nproduce the optimal hiding prompt guiding signal for each watermark. The prompt\nembedding is optimized to minimize artifacts in the generated image, while the\nwatermark is optimized to achieve maximum strength. The watermark can be\nverified by reversing the generation process. Experiments on various diffusion\nmodels demonstrate the watermark remains verifiable even under significant\nimage tampering and shows superior invisibility compared to other\nstate-of-the-art robust watermarking methods. Code is available at\nhttps://github.com/Hannah1102/ROBIN.",
          "arxiv_id": "2411.03862v2"
        }
      ],
      "12": [
        {
          "title": "Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor",
          "year": "2024-05",
          "abstract": "Data-poisoning backdoor attacks are serious security threats to machine\nlearning models, where an adversary can manipulate the training dataset to\ninject backdoors into models. In this paper, we focus on in-training backdoor\ndefense, aiming to train a clean model even when the dataset may be potentially\npoisoned. Unlike most existing methods that primarily detect and remove/unlearn\nsuspicious samples to mitigate malicious backdoor attacks, we propose a novel\ndefense approach called PDB (Proactive Defensive Backdoor). Specifically, PDB\nleverages the home-field advantage of defenders by proactively injecting a\ndefensive backdoor into the model during training. Taking advantage of\ncontrolling the training process, the defensive backdoor is designed to\nsuppress the malicious backdoor effectively while remaining secret to\nattackers. In addition, we introduce a reversible mapping to determine the\ndefensive target label. During inference, PDB embeds a defensive trigger in the\ninputs and reverses the model's prediction, suppressing malicious backdoor and\nensuring the model's utility on the original task. Experimental results across\nvarious datasets and models demonstrate that our approach achieves\nstate-of-the-art defense performance against a wide range of backdoor attacks.\nThe code is available at\nhttps://github.com/shawkui/Proactive_Defensive_Backdoor.",
          "arxiv_id": "2405.16112v2"
        },
        {
          "title": "Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks",
          "year": "2021-12",
          "abstract": "Backdoor (Trojan) attacks are emerging threats against deep neural networks\n(DNN). A DNN being attacked will predict to an attacker-desired target class\nwhenever a test sample from any source class is embedded with a backdoor\npattern; while correctly classifying clean (attack-free) test samples. Existing\nbackdoor defenses have shown success in detecting whether a DNN is attacked and\nin reverse-engineering the backdoor pattern in a \"post-training\" regime: the\ndefender has access to the DNN to be inspected and a small, clean dataset\ncollected independently, but has no access to the (possibly poisoned) training\nset of the DNN. However, these defenses neither catch culprits in the act of\ntriggering the backdoor mapping, nor mitigate the backdoor attack at test-time.\nIn this paper, we propose an \"in-flight\" defense against backdoor attacks on\nimage classification that 1) detects use of a backdoor trigger at test-time;\nand 2) infers the class of origin (source class) for a detected trigger\nexample. The effectiveness of our defense is demonstrated experimentally\nagainst different strong backdoor attacks.",
          "arxiv_id": "2112.03350v1"
        },
        {
          "title": "Robust Backdoor Attacks against Deep Neural Networks in Real Physical World",
          "year": "2021-04",
          "abstract": "Deep neural networks (DNN) have been widely deployed in various applications.\nHowever, many researches indicated that DNN is vulnerable to backdoor attacks.\nThe attacker can create a hidden backdoor in target DNN model, and trigger the\nmalicious behaviors by submitting specific backdoor instance. However, almost\nall the existing backdoor works focused on the digital domain, while few\nstudies investigate the backdoor attacks in real physical world. Restricted to\na variety of physical constraints, the performance of backdoor attacks in the\nreal physical world will be severely degraded. In this paper, we propose a\nrobust physical backdoor attack method, PTB (physical transformations for\nbackdoors), to implement the backdoor attacks against deep learning models in\nthe real physical world. Specifically, in the training phase, we perform a\nseries of physical transformations on these injected backdoor instances at each\nround of model training, so as to simulate various transformations that a\nbackdoor may experience in real world, thus improves its physical robustness.\nExperimental results on the state-of-the-art face recognition model show that,\ncompared with the backdoor methods that without PTB, the proposed attack method\ncan significantly improve the performance of backdoor attacks in real physical\nworld. Under various complex physical conditions, by injecting only a very\nsmall ratio (0.5%) of backdoor instances, the attack success rate of physical\nbackdoor attacks with the PTB method on VGGFace is 82%, while the attack\nsuccess rate of backdoor attacks without the proposed PTB method is lower than\n11%. Meanwhile, the normal performance of the target DNN model has not been\naffected.",
          "arxiv_id": "2104.07395v2"
        }
      ],
      "13": [
        {
          "title": "Multimodal Personal Ear Authentication Using Smartphones",
          "year": "2021-03",
          "abstract": "In recent years, biometric authentication technology for smartphones has\nbecome widespread, with the mainstream methods being fingerprint authentication\nand face recognition. However, fingerprint authentication cannot be used when\nhands are wet, and face recognition cannot be used when a person is wearing a\nmask. Therefore, we examine a personal authentication system using the pinna as\na new approach for biometric authentication on smartphones. Authentication\nsystems based on the acoustic transfer function of the pinna (PRTF: Pinna\nRelated Transfer Function) have been investigated. However, the authentication\naccuracy decreases due to the positional fluctuation across each measurement.\nIn this paper, we propose multimodal personal authentication on smartphones\nusing PRTF. The pinna image and positional sensor information are used with the\nPRTF, and the effectiveness of the authentication method is examined. We\ndemonstrate that the proposed authentication system can compensate for the\npositional changes in each measurement and improve robustness.",
          "arxiv_id": "2103.12575v1"
        },
        {
          "title": "CryptoFace: End-to-End Encrypted Face Recognition",
          "year": "2025-08",
          "abstract": "Face recognition is central to many authentication, security, and\npersonalized applications. Yet, it suffers from significant privacy risks,\nparticularly arising from unauthorized access to sensitive biometric data. This\npaper introduces CryptoFace, the first end-to-end encrypted face recognition\nsystem with fully homomorphic encryption (FHE). It enables secure processing of\nfacial data across all stages of a face-recognition process--feature\nextraction, storage, and matching--without exposing raw images or features. We\nintroduce a mixture of shallow patch convolutional networks to support\nhigher-dimensional tensors via patch-based processing while reducing the\nmultiplicative depth and, thus, inference latency. Parallel FHE evaluation of\nthese networks ensures near-resolution-independent latency. On standard face\nrecognition benchmarks, CryptoFace significantly accelerates inference and\nincreases verification accuracy compared to the state-of-the-art FHE neural\nnetworks adapted for face recognition. CryptoFace will facilitate secure face\nrecognition systems requiring robust and provable security. The code is\navailable at https://github.com/human-analysis/CryptoFace.",
          "arxiv_id": "2509.00332v1"
        },
        {
          "title": "Face Encryption via Frequency-Restricted Identity-Agnostic Attacks",
          "year": "2023-08",
          "abstract": "Billions of people are sharing their daily live images on social media\neveryday. However, malicious collectors use deep face recognition systems to\neasily steal their biometric information (e.g., faces) from these images. Some\nstudies are being conducted to generate encrypted face photos using adversarial\nattacks by introducing imperceptible perturbations to reduce face information\nleakage. However, existing studies need stronger black-box scenario feasibility\nand more natural visual appearances, which challenge the feasibility of privacy\nprotection. To address these problems, we propose a frequency-restricted\nidentity-agnostic (FRIA) framework to encrypt face images from unauthorized\nface recognition without access to personal information. As for the weak\nblack-box scenario feasibility, we obverse that representations of the average\nfeature in multiple face recognition models are similar, thus we propose to\nutilize the average feature via the crawled dataset from the Internet as the\ntarget to guide the generation, which is also agnostic to identities of unknown\nface recognition systems; in nature, the low-frequency perturbations are more\nvisually perceptible by the human vision system. Inspired by this, we restrict\nthe perturbation in the low-frequency facial regions by discrete cosine\ntransform to achieve the visual naturalness guarantee. Extensive experiments on\nseveral face recognition models demonstrate that our FRIA outperforms other\nstate-of-the-art methods in generating more natural encrypted faces while\nattaining high black-box attack success rates of 96%. In addition, we validate\nthe efficacy of FRIA using real-world black-box commercial API, which reveals\nthe potential of FRIA in practice. Our codes can be found in\nhttps://github.com/XinDong10/FRIA.",
          "arxiv_id": "2308.05983v3"
        }
      ],
      "14": [
        {
          "title": "CAN-BERT do it? Controller Area Network Intrusion Detection System based on BERT Language Model",
          "year": "2022-10",
          "abstract": "Due to the rising number of sophisticated customer functionalities,\nelectronic control units (ECUs) are increasingly integrated into modern\nautomotive systems. However, the high connectivity between the in-vehicle and\nthe external networks paves the way for hackers who could exploit in-vehicle\nnetwork protocols' vulnerabilities. Among these protocols, the Controller Area\nNetwork (CAN), known as the most widely used in-vehicle networking technology,\nlacks encryption and authentication mechanisms, making the communications\ndelivered by distributed ECUs insecure. Inspired by the outstanding performance\nof bidirectional encoder representations from transformers (BERT) for improving\nmany natural language processing tasks, we propose in this paper ``CAN-BERT\", a\ndeep learning based network intrusion detection system, to detect cyber attacks\non CAN bus protocol. We show that the BERT model can learn the sequence of\narbitration identifiers (IDs) in the CAN bus for anomaly detection using the\n``masked language model\" unsupervised training objective. The experimental\nresults on the ``Car Hacking: Attack \\& Defense Challenge 2020\" dataset show\nthat ``CAN-BERT\" outperforms state-of-the-art approaches. In addition to being\nable to identify in-vehicle intrusions in real-time within 0.8 ms to 3 ms w.r.t\nCAN ID sequence length, it can also detect a wide variety of cyberattacks with\nan F1-score of between 0.81 and 0.99.",
          "arxiv_id": "2210.09439v1"
        },
        {
          "title": "Anomaly Detection in Intra-Vehicle Networks",
          "year": "2022-05",
          "abstract": "The progression of innovation and technology and ease of inter-connectivity\namong networks has allowed us to evolve towards one of the promising areas, the\nInternet of Vehicles. Nowadays, modern vehicles are connected to a range of\nnetworks, including intra-vehicle networks and external networks. However, a\nprimary challenge in the automotive industry is to make the vehicle safe and\nreliable; particularly with the loopholes in the existing traditional\nprotocols, cyber-attacks on the vehicle network are rising drastically.\nPractically every vehicle uses the universal Controller Area Network (CAN) bus\nprotocol for the communication between electronic control units to transmit key\nvehicle functionality and messages related to driver safety. The CAN bus\nsystem, although its critical significance, lacks the key feature of any\nprotocol authentication and authorization. Resulting in compromises of CAN bus\nsecurity leads to serious issues to both car and driver safety. This paper\ndiscusses the security issues of the CAN bus protocol and proposes an Intrusion\nDetection System (IDS) that detects known attacks on in-vehicle networks.\nMultiple Artificial Intelligence (AI) algorithms are employed to provide\nrecognition of known potential cyber-attacks based on messages, timestamps, and\ndata packets traveling through the CAN. The main objective of this paper is to\naccurately detect cyberattacks by considering time-series features and attack\nfrequency. The majority of the evaluated AI algorithms, when considering attack\nfrequency, correctly identify known attacks with remarkable accuracy of more\nthan 99%. However, these models achieve approximately 92% to 97% accuracy when\ntimestamps are not taken into account. Long Short Term Memory (LSTM), Xgboost,\nand SVC have proved to the well-performing classifiers.",
          "arxiv_id": "2205.03537v1"
        },
        {
          "title": "Cyberattacks and Countermeasures For In-Vehicle Networks",
          "year": "2020-04",
          "abstract": "As connectivity between and within vehicles increases, so does concern about\nsafety and security. Various automotive serial protocols are used inside\nvehicles such as Controller Area Network (CAN), Local Interconnect Network\n(LIN) and FlexRay. CAN bus is the most used in-vehicle network protocol to\nsupport exchange of vehicle parameters between Electronic Control Units (ECUs).\nThis protocol lacks security mechanisms by design and is therefore vulnerable\nto various attacks. Furthermore, connectivity of vehicles has made the CAN bus\nnot only vulnerable from within the vehicle but also from outside. With the\nrise of connected cars, more entry points and interfaces have been introduced\non board vehicles, thereby also leading to a wider potential attack surface.\nExisting security mechanisms focus on the use of encryption, authentication and\nvehicle Intrusion Detection Systems (IDS), which operate under various\nconstrains such as low bandwidth, small frame size (e.g. in the CAN protocol),\nlimited availability of computational resources and real-time sensitivity. We\nsurvey In-Vehicle Network (IVN) attacks which have been grouped under: direct\ninterfaces-initiated attacks, telematics and infotainment-initiated attacks,\nand sensor-initiated attacks. We survey and classify current cryptographic and\nIDS approaches and compare these approaches based on criteria such as real time\nconstrains, types of hardware used, changes in CAN bus behaviour, types of\nattack mitigation and software/ hardware used to validate these approaches. We\nconclude with potential mitigation strategies and research challenges for the\nfuture.",
          "arxiv_id": "2004.10781v1"
        }
      ],
      "15": [
        {
          "title": "Advanced Penetration Testing for Enhancing 5G Security",
          "year": "2024-07",
          "abstract": "Advances in fifth-generation (5G) networks enable unprecedented reliability,\nspeed, and connectivity compared to previous mobile networks. These\nadvancements can revolutionize various sectors by supporting applications\nrequiring real-time data processing. However, the rapid deployment and\nintegration of 5G networks bring security concerns that must be addressed to\noperate these infrastructures safely. This paper reviews penetration testing\napproaches for identifying security vulnerabilities in 5G networks. Penetration\ntesting is an ethical hacking technique used to simulate a network's security\nposture in the event of cyberattacks. This review highlights the capabilities,\nadvantages, and limitations of recent 5G-targeting security tools for\npenetration testing. It examines ways adversaries exploit vulnerabilities in 5G\nnetworks, covering tactics and strategies targeted at 5G features. A key topic\nexplored is the comparison of penetration testing methods for 5G and earlier\ngenerations. The article delves into the unique characteristics of 5G,\nincluding massive MIMO, edge computing, and network slicing, and how these\naspects require new penetration testing methods. Understanding these\ndifferences helps develop more effective security solutions tailored to 5G\nnetworks. Our research also indicates that 5G penetration testing should use a\nmultithreaded approach for addressing current security challenges. Furthermore,\nthis paper includes case studies illustrating practical challenges and\nlimitations in real-world applications of penetration testing in 5G networks. A\ncomparative analysis of penetration testing tools for 5G networks highlights\ntheir effectiveness in mitigating vulnerabilities, emphasizing the need for\nadvanced security measures against evolving cyber threats in 5G deployment.",
          "arxiv_id": "2407.17269v1"
        },
        {
          "title": "Smart Jamming Attacks in 5G New Radio: A Review",
          "year": "2020-09",
          "abstract": "The fifth generation of wireless cellular networks (5G) is expected to be the\ninfrastructure for emergency services, natural disasters rescue, public safety,\nand military communications. 5G, as any previous wireless cellular network, is\nvulnerable to jamming attacks, which create deliberate interference to hinder\nthe communication of legitimate users. Therefore, jamming 5G networks can be a\nreal threat to public safety. Thus, there is a strong need to investigate to\nwhat extent these networks are vulnerable to jamming attacks. For this\ninvestigation, we consider the 3GPP standard released in 2017, which is widely\naccepted as the primary reference for the deployment of these networks. First,\nwe describe the key elements of 5G New Radio (NR) architecture, such as\ndifferent channels and signals exchanged between the base station and user\nequipment. Second, we conduct an in-depth review of the jamming attack models\nand we assess the 5G NR vulnerabilities to these jamming attacks. Then, we\npresent the state-of-the-art detection and mitigation techniques, and we\ndiscuss their suitability to defeat smart jammers in 5G wireless networks.\nFinally, we provide some recommendations and future research directions at the\nend of this paper.",
          "arxiv_id": "2009.05531v1"
        },
        {
          "title": "5G System Security Analysis",
          "year": "2021-08",
          "abstract": "Fifth generation mobile networks (5G) are currently being deployed by mobile\noperators around the globe. 5G acts as an enabler for various use cases and\nalso improves the security and privacy over 4G and previous network\ngenerations. However, as recent security research has revealed, the standard\nstill has security weaknesses that may be exploitable by attackers. In\naddition, the migration from 4G to 5G systems is taking place by first\ndeploying 5G solutions in a non-standalone (NSA) manner where the first step of\nthe 5G deployment is restricted to the new radio aspects of 5G, while the\ncontrol of the user equipment is still based on 4G protocols, i.e. the core\nnetwork is still the legacy 4G evolved packet core (EPC) network. As a result,\nmany security vulnerabilities of 4G networks are still present in current 5G\ndeployments. This paper presents a systematic risk analysis of standalone and\nnon-standalone 5G networks. We first describe an overview of the 5G system\nspecification and the new security features of 5G compared to 4G. Then, we\ndefine possible threats according to the STRIDE threat classification model and\nderive a risk matrix based on the likelihood and impact of 12 threat scenarios\nthat affect the radio access and the network core. Finally, we discuss possible\nmitigations and security controls. Our analysis is generic and does not account\nfor the specifics of particular 5G network vendors or operators. Further work\nis required to understand the security vulnerabilities and risks of specific 5G\nimplementations and deployments.",
          "arxiv_id": "2108.08700v2"
        }
      ],
      "16": [
        {
          "title": "Revisiting Fully Homomorphic Encryption Schemes",
          "year": "2023-05",
          "abstract": "Homomorphic encryption is a sophisticated encryption technique that allows\ncomputations on encrypted data to be done without the requirement for\ndecryption. This trait makes homomorphic encryption appropriate for safe\ncomputation in sensitive data scenarios, such as cloud computing, medical data\nexchange, and financial transactions. The data is encrypted using a public key\nin homomorphic encryption, and the calculation is conducted on the encrypted\ndata using an algorithm that retains the encryption. The computed result is\nthen decrypted with a private key to acquire the final output. This abstract\nnotion protects data while allowing complicated computations to be done on the\nencrypted data, resulting in a secure and efficient approach to analysing\nsensitive information. This article is intended to give a clear idea about the\nvarious fully Homomorphic Encryption Schemes present in the literature and\nanalyse and compare the results of each of these schemes. Further, we also\nprovide applications and open-source tools of homomorphic encryption schemes.",
          "arxiv_id": "2305.05904v1"
        },
        {
          "title": "The Beginner's Textbook for Fully Homomorphic Encryption",
          "year": "2025-03",
          "abstract": "Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables\ncomputations to be performed directly on encrypted data, as if the data were in\nplaintext. After all computations are performed on the encrypted data, it can\nbe decrypted to reveal the result. The decrypted value matches the result that\nwould have been obtained if the same computations were applied to the plaintext\ndata.\n  FHE supports basic operations such as addition and multiplication on\nencrypted numbers. Using these fundamental operations, more complex\ncomputations can be constructed, including subtraction, division, logic gates\n(e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such\nas ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions\ncan be implemented either as exact formulas or as approximations, depending on\nthe trade-off between computational efficiency and accuracy.\n  FHE enables privacy-preserving machine learning by allowing a server to\nprocess the client's data in its encrypted form through an ML model. With FHE,\nthe server learns neither the plaintext version of the input features nor the\ninference results. Only the client, using their secret key, can decrypt and\naccess the results at the end of the service protocol. FHE can also be applied\nto confidential blockchain services, ensuring that sensitive data in smart\ncontracts remains encrypted and confidential while maintaining the transparency\nand integrity of the execution process. Other applications of FHE include\nsecure outsourcing of data analytics, encrypted database queries,\nprivacy-preserving searches, efficient multi-party computation for digital\nsignatures, and more.\n  As this book is an open project (https://fhetextbook.github.io), we welcome\nFHE experts to join us as collaborators to help expand the draft.",
          "arxiv_id": "2503.05136v15"
        },
        {
          "title": "MemFHE: End-to-End Computing with Fully Homomorphic Encryption in Memory",
          "year": "2022-04",
          "abstract": "The increasing amount of data and the growing complexity of problems has\nresulted in an ever-growing reliance on cloud computing. However, many\napplications, most notably in healthcare, finance or defense, demand security\nand privacy which today's solutions cannot fully address. Fully homomorphic\nencryption (FHE) elevates the bar of today's solutions by adding\nconfidentiality of data during processing. It allows computation on fully\nencrypted data without the need for decryption, thus fully preserving privacy.\nTo enable processing encrypted data at usable levels of classic security, e.g.,\n128-bit, the encryption procedure introduces noticeable data size expansion -\nthe ciphertext is much bigger than the native aggregate of native data types.\nIn this paper, we present MemFHE which is the first accelerator of both client\nand server for the latest Ring-GSW (Gentry, Sahai, and Waters) based\nhomomorphic encryption schemes using Processing In Memory (PIM). PIM alleviates\nthe data movement issues with large FHE encrypted data, while providing in-situ\nexecution and extensive parallelism needed for FHE's polynomial operations.\nWhile the client-PIM can homomorphically encrypt and decrypt data, the\nserver-PIM can process homomorphically encrypted data without decryption.\nMemFHE's server-PIM is pipelined and is designed to provide flexible\nbootstrapping, allowing two encryption techniques and various FHE\nsecurity-levels based on the application requirements. We evaluate MemFHE for\nvarious security-levels and compare it with state-of-the-art CPU\nimplementations for Ring-GSW based FHE. MemFHE is up to 20kx (265x) faster than\nCPU (GPU) for FHE arithmetic operations and provides on average 2007x higher\nthroughput than the state-of-the-art while implementing neural networks with\nFHE.",
          "arxiv_id": "2204.12557v1"
        }
      ],
      "17": [
        {
          "title": "Are disentangled representations all you need to build speaker anonymization systems?",
          "year": "2022-08",
          "abstract": "Speech signals contain a lot of sensitive information, such as the speaker's\nidentity, which raises privacy concerns when speech data get collected. Speaker\nanonymization aims to transform a speech signal to remove the source speaker's\nidentity while leaving the spoken content unchanged. Current methods perform\nthe transformation by relying on content/speaker disentanglement and voice\nconversion. Usually, an acoustic model from an automatic speech recognition\nsystem extracts the content representation while an x-vector system extracts\nthe speaker representation. Prior work has shown that the extracted features\nare not perfectly disentangled. This paper tackles how to improve features\ndisentanglement, and thus the converted anonymized speech. We propose enhancing\nthe disentanglement by removing speaker information from the acoustic model\nusing vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit\nshowed that vector quantization helps conceal the original speaker identity\nwhile maintaining utility for speech recognition.",
          "arxiv_id": "2208.10497v3"
        },
        {
          "title": "FreeTalk:A plug-and-play and black-box defense against speech synthesis attacks",
          "year": "2025-08",
          "abstract": "Recently, speech assistant and speech verification have been used in many\nfields, which brings much benefit and convenience for us. However, when we\nenjoy these speech applications, our speech may be collected by attackers for\nspeech synthesis. For example, an attacker generates some inappropriate\npolitical opinions with the characteristic of the victim's voice by obtaining a\npiece of the victim's speech, which will greatly influence the victim's\nreputation. Specifically, with the appearance of some zero-shot voice\nconversion methods, the cost of speech synthesis attacks has been further\nreduced, which also brings greater challenges to user voice security and\nprivacy. Some researchers have proposed the corresponding privacy-preserving\nmethods. However, the existing approaches have some non-negligible drawbacks:\nlow transferability and robustness, high computational overhead. These\ndeficiencies seriously limit the existing method deployed in practical\nscenarios. Therefore, in this paper, we propose a lightweight, robust,\nplug-and-play privacy preservation method against speech synthesis attacks in a\nblack-box setting. Our method generates and adds a frequency-domain\nperturbation to the original speech to achieve privacy protection and high\nspeech quality. Then, we present a data augmentation strategy and noise\nsmoothing mechanism to improve the robustness of the proposed method. Besides,\nto reduce the user's defense overhead, we also propose a novel identity-wise\nprotection mechanism. It can generate a universal perturbation for one speaker\nand support privacy preservation for speech of any length. Finally, we conduct\nextensive experiments on 5 speech synthesis models, 5 speech verification\nmodels, 1 speech recognition model, and 2 datasets. The experimental results\ndemonstrate that our method has satisfying privacy-preserving performance, high\nspeech quality, and utility.",
          "arxiv_id": "2509.00561v1"
        },
        {
          "title": "ClearMask: Noise-Free and Naturalness-Preserving Protection Against Voice Deepfake Attacks",
          "year": "2025-08",
          "abstract": "Voice deepfake attacks, which artificially impersonate human speech for\nmalicious purposes, have emerged as a severe threat. Existing defenses\ntypically inject noise into human speech to compromise voice encoders in speech\nsynthesis models. However, these methods degrade audio quality and require\nprior knowledge of the attack approaches, limiting their effectiveness in\ndiverse scenarios. Moreover, real-time audios, such as speech in virtual\nmeetings and voice messages, are still exposed to voice deepfake threats. To\novercome these limitations, we propose ClearMask, a noise-free defense\nmechanism against voice deepfake attacks. Unlike traditional approaches,\nClearMask modifies the audio mel-spectrogram by selectively filtering certain\nfrequencies, inducing a transferable voice feature loss without injecting\nnoise. We then apply audio style transfer to further deceive voice decoders\nwhile preserving perceived sound quality. Finally, optimized reverberation is\nintroduced to disrupt the output of voice generation models without affecting\nthe naturalness of the speech. Additionally, we develop LiveMask to protect\nstreaming speech in real-time through a universal frequency filter and\nreverberation generator. Our experimental results show that ClearMask and\nLiveMask effectively prevent voice deepfake attacks from deceiving speaker\nverification models and human listeners, even for unseen voice synthesis models\nand black-box API services. Furthermore, ClearMask demonstrates resilience\nagainst adaptive attackers who attempt to recover the original audio signal\nfrom the protected speech samples.",
          "arxiv_id": "2508.17660v1"
        }
      ],
      "18": [
        {
          "title": "Fast and Private Inference of Deep Neural Networks by Co-designing Activation Functions",
          "year": "2023-06",
          "abstract": "Machine Learning as a Service (MLaaS) is an increasingly popular design where\na company with abundant computing resources trains a deep neural network and\noffers query access for tasks like image classification. The challenge with\nthis design is that MLaaS requires the client to reveal their potentially\nsensitive queries to the company hosting the model. Multi-party computation\n(MPC) protects the client's data by allowing encrypted inferences. However,\ncurrent approaches suffer from prohibitively large inference times. The\ninference time bottleneck in MPC is the evaluation of non-linear layers such as\nReLU activation functions. Motivated by the success of previous work\nco-designing machine learning and MPC, we develop an activation function\nco-design. We replace all ReLUs with a polynomial approximation and evaluate\nthem with single-round MPC protocols, which give state-of-the-art inference\ntimes in wide-area networks. Furthermore, to address the accuracy issues\npreviously encountered with polynomial activations, we propose a novel training\nalgorithm that gives accuracy competitive with plaintext models. Our evaluation\nshows between $3$ and $110\\times$ speedups in inference time on large models\nwith up to $23$ million parameters while maintaining competitive inference\naccuracy.",
          "arxiv_id": "2306.08538v2"
        },
        {
          "title": "MOFHEI: Model Optimizing Framework for Fast and Efficient Homomorphically Encrypted Neural Network Inference",
          "year": "2024-12",
          "abstract": "Due to the extensive application of machine learning (ML) in a wide range of\nfields and the necessity of data privacy, privacy-preserving machine learning\n(PPML) solutions have recently gained significant traction. One group of\napproaches relies on Homomorphic Encryption (HE), which enables us to perform\nML tasks over encrypted data. However, even with state-of-the-art HE schemes,\nHE operations are still significantly slower compared to their plaintext\ncounterparts and require a considerable amount of memory. Therefore, we propose\nMOFHEI, a framework that optimizes the model to make HE-based neural network\ninference, referred to as private inference (PI), fast and efficient. First,\nour proposed learning-based method automatically transforms a pre-trained ML\nmodel into its compatible version with HE operations, called the HE-friendly\nversion. Then, our iterative block pruning method prunes the model's parameters\nin configurable block shapes in alignment with the data packing method. This\nallows us to drop a significant number of costly HE operations, thereby\nreducing the latency and memory consumption while maintaining the model's\nperformance. We evaluate our framework through extensive experiments on\ndifferent models using various datasets. Our method achieves up to 98% pruning\nratio on LeNet, eliminating up to 93% of the required HE operations for\nperforming PI, reducing latency and the required memory by factors of 9.63 and\n4.04, respectively, with negligible accuracy loss.",
          "arxiv_id": "2412.07954v1"
        },
        {
          "title": "Hyperdimensional Computing as a Rescue for Efficient Privacy-Preserving Machine Learning-as-a-Service",
          "year": "2023-08",
          "abstract": "Machine learning models are often provisioned as a cloud-based service where\nthe clients send their data to the service provider to obtain the result. This\nsetting is commonplace due to the high value of the models, but it requires the\nclients to forfeit the privacy that the query data may contain. Homomorphic\nencryption (HE) is a promising technique to address this adversity. With HE,\nthe service provider can take encrypted data as a query and run the model\nwithout decrypting it. The result remains encrypted, and only the client can\ndecrypt it. All these benefits come at the cost of computational cost because\nHE turns simple floating-point arithmetic into the computation between long\n(degree over 1024) polynomials. Previous work has proposed to tailor deep\nneural networks for efficient computation over encrypted data, but already high\ncomputational cost is again amplified by HE, hindering performance improvement.\nIn this paper we show hyperdimensional computing can be a rescue for\nprivacy-preserving machine learning over encrypted data. We find that the\nadvantage of hyperdimensional computing in performance is amplified when\nworking with HE. This observation led us to design HE-HDC, a machine-learning\ninference system that uses hyperdimensional computing with HE. We carefully\nstructure the machine learning service so that the server will perform only the\nHE-friendly computation. Moreover, we adapt the computation and HE parameters\nto expedite computation while preserving accuracy and security. Our\nexperimental result based on real measurements shows that HE-HDC outperforms\nexisting systems by 26~3000 times with comparable classification accuracy.",
          "arxiv_id": "2310.06840v1"
        }
      ],
      "19": [
        {
          "title": "Unleashing IoT Security: Assessing the Effectiveness of Best Practices in Protecting Against Threats",
          "year": "2023-08",
          "abstract": "The Internet of Things (IoT) market is rapidly growing and is expected to\ndouble from 2020 to 2025. The increasing use of IoT devices, particularly in\nsmart homes, raises crucial concerns about user privacy and security as these\ndevices often handle sensitive and critical information. Inadequate security\ndesigns and implementations by IoT vendors can lead to significant\nvulnerabilities.\n  To address these IoT device vulnerabilities, institutions, and organizations\nhave published IoT security best practices (BPs) to guide manufacturers in\nensuring the security of their products. However, there is currently no\nstandardized approach for evaluating the effectiveness of individual BP\nrecommendations. This leads to manufacturers investing effort in implementing\nless effective BPs while potentially neglecting measures with greater impact.\n  In this paper, we propose a methodology for evaluating the security impact of\nIoT BPs and ranking them based on their effectiveness in protecting against\nsecurity threats. Our approach involves translating identified BPs into\nconcrete test cases that can be applied to real-world IoT devices to assess\ntheir effectiveness in mitigating vulnerabilities. We applied this methodology\nto evaluate the security impact of nine commodity IoT products, discovering 18\nvulnerabilities. By empirically assessing the actual impact of BPs on device\nsecurity, IoT designers and implementers can prioritize their security\ninvestments more effectively, improving security outcomes and optimizing\nlimited security budgets.",
          "arxiv_id": "2308.12072v1"
        },
        {
          "title": "Technical Report-IoT Devices Proximity Authentication In Ad Hoc Network Environment",
          "year": "2022-10",
          "abstract": "Internet of Things (IoT) is a distributed communication technology system\nthat offers the possibility for physical devices (e.g. vehicles home appliances\nsensors actuators etc.) known as Things to connect and exchange data more\nimportantly without human interaction. Since IoT plays a significant role in\nour daily lives we must secure the IoT environment to work effectively. Among\nthe various security requirements authentication to the IoT devices is\nessential as it is the first step in preventing any negative impact of possible\nattackers. Using the current IEEE 802.11 infrastructure this paper implements\nan IoT devices authentication scheme based on something that is in the IoT\ndevices environment (i.e. ambient access points). Data from the broadcast\nmessages (i.e. beacon frame characteristics) are utilized to implement the\nauthentication factor that confirms proximity between two devices in an ad hoc\nIoT network.",
          "arxiv_id": "2210.00175v1"
        },
        {
          "title": "Survey on Enterprise Internet-of-Things Systems (E-IoT): A Security Perspective",
          "year": "2021-02",
          "abstract": "As technology becomes more widely available, millions of users worldwide have\ninstalled some form of smart device in their homes or workplaces. These devices\nare often off-the-shelf commodity systems, such as Google Home or Samsung\nSmartThings, that are installed by end-users looking to automate a small\ndeployment. In contrast to these \"plug-and-play\" systems, purpose-built\nEnterprise Internet-of-Things (E-IoT) systems such as Crestron, Control4, RTI,\nSavant offer a smart solution for more sophisticated applications (e.g.,\ncomplete lighting control, A/V management, security). In contrast to commodity\nsystems, E-IoT systems are usually closed source, costly, require certified\ninstallers, and are overall more robust for their use cases. Due to this, E-IoT\nsystems are often found in expensive smart homes, government and academic\nconference rooms, yachts, and smart private offices. However, while there has\nbeen plenty of research on the topic of commodity systems, no current study\nexists that provides a complete picture of E-IoT systems, their components, and\nrelevant threats. As such, lack of knowledge of E-IoT system threats, coupled\nwith the cost of E-IoT systems has led many to assume that E-IoT systems are\nsecure. To address this research gap, raise awareness on E-IoT security, and\nmotivate further research, this work emphasizes E-IoT system components, E-IoT\nvulnerabilities, solutions, and their security implications. In order to\nsystematically analyze the security of E-IoT systems, we divide E-IoT systems\ninto four layers: E-IoT Devices Layer, Communications Layer, Monitoring and\nApplications Layer, and Business Layer. We survey attacks and defense\nmechanisms, considering the E-IoT components at each layer and the associated\nthreats. In addition, we present key observations in state-of-the-art E-IoT\nsecurity and provide a list of open research problems that need further\nresearch.",
          "arxiv_id": "2102.10695v1"
        }
      ],
      "20": [
        {
          "title": "ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection",
          "year": "2024-02",
          "abstract": "The proliferation of phishing sites and emails poses significant challenges\nto existing cybersecurity efforts. Despite advances in malicious email filters\nand email security protocols, problems with oversight and false positives\npersist. Users often struggle to understand why emails are flagged as\npotentially fraudulent, risking the possibility of missing important\ncommunications or mistakenly trusting deceptive phishing emails. This study\nintroduces ChatSpamDetector, a system that uses large language models (LLMs) to\ndetect phishing emails. By converting email data into a prompt suitable for LLM\nanalysis, the system provides a highly accurate determination of whether an\nemail is phishing or not. Importantly, it offers detailed reasoning for its\nphishing determinations, assisting users in making informed decisions about how\nto handle suspicious emails. We conducted an evaluation using a comprehensive\nphishing email dataset and compared our system to several LLMs and baseline\nsystems. We confirmed that our system using GPT-4 has superior detection\ncapabilities with an accuracy of 99.70%. Advanced contextual interpretation by\nLLMs enables the identification of various phishing tactics and impersonations,\nmaking them a potentially powerful tool in the fight against email-based\nphishing threats.",
          "arxiv_id": "2402.18093v2"
        },
        {
          "title": "Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset Collector",
          "year": "2025-09",
          "abstract": "To combat phishing attacks -- aimed at luring web users to divulge their\nsensitive information -- various phishing detection approaches have been\nproposed. As attackers focus on devising new tactics to bypass existing\ndetection solutions, researchers have adapted by integrating machine learning\nand deep learning into phishing detection. Phishing dataset collection is vital\nto developing effective phishing detection approaches, which highly depend on\nthe diversity of the gathered datasets. The lack of diversity in the dataset\nresults in a biased model. Since phishing websites are often short-lived,\ncollecting them is also a challenge. Consequently, very few phishing webpage\ndataset repositories exist to date. No single repository comprehensively\nconsolidates all phishing elements corresponding to a phishing webpage, namely,\nURL, webpage source code, screenshot, and related webpage resources. This paper\nintroduces a resource collection tool designed to gather various resources\nassociated with a URL, such as CSS, Javascript, favicons, webpage images, and\nscreenshots. Our tool leverages PhishTank as the primary source for obtaining\nactive phishing URLs. Our tool fetches several additional webpage resources\ncompared to PyWebCopy Python library, which provides webpage content for a\ngiven URL. Additionally, we share a sample dataset generated using our tool\ncomprising 4,056 legitimate and 5,666 phishing URLs along with their associated\nresources. We also remark on the top correlated phishing features with their\nassociated class label found in our dataset. Our tool offers a comprehensive\nresource set that can aid researchers in developing effective phishing\ndetection approaches.",
          "arxiv_id": "2509.09592v1"
        },
        {
          "title": "An Innovative Information Theory-based Approach to Tackle and Enhance The Transparency in Phishing Detection",
          "year": "2024-02",
          "abstract": "Phishing attacks have become a serious and challenging issue for detection,\nexplanation, and defense. Despite more than a decade of research on phishing,\nencompassing both technical and non-technical remedies, phishing continues to\nbe a serious problem. Nowadays, AI-based phishing detection stands out as one\nof the most effective solutions for defending against phishing attacks by\nproviding vulnerability (i.e., phishing or benign) predictions for the data.\nHowever, it lacks explainability in terms of providing comprehensive\ninterpretations for the predictions, such as identifying the specific\ninformation that causes the data to be classified as phishing. To this end, we\npropose an innovative deep learning-based approach for email (the most common\nphishing way) phishing attack localization. Our method can not only predict the\nvulnerability of the email data but also automatically learn and figure out the\nmost important and phishing-relevant information (i.e., sentences) in the\nphishing email data where the selected information indicates useful and concise\nexplanations for the vulnerability. The rigorous experiments on seven\nreal-world diverse email datasets show the effectiveness and advancement of our\nproposed method in selecting crucial information, offering concise explanations\n(by successfully figuring out the most important and phishing-relevant\ninformation) for the vulnerability of the phishing email data. Particularly,\nour method achieves a significantly higher performance, ranging from\napproximately 1.5% to 3.5%, compared to state-of-the-art baselines, as measured\nby the combined average performance of two main metrics Label-Accuracy and\nCognitive-True-Positive.",
          "arxiv_id": "2402.17092v2"
        }
      ],
      "21": [
        {
          "title": "Certified Robustness of Graph Neural Networks against Adversarial Structural Perturbation",
          "year": "2020-08",
          "abstract": "Graph neural networks (GNNs) have recently gained much attention for node and\ngraph classification tasks on graph-structured data. However, multiple recent\nworks showed that an attacker can easily make GNNs predict incorrectly via\nperturbing the graph structure, i.e., adding or deleting edges in the graph. We\naim to defend against such attacks via developing certifiably robust GNNs.\nSpecifically, we prove the certified robustness guarantee of any GNN for both\nnode and graph classifications against structural perturbation. Moreover, we\nshow that our certified robustness guarantee is tight. Our results are based on\na recently proposed technique called randomized smoothing, which we extend to\ngraph data. We also empirically evaluate our method for both node and graph\nclassifications on multiple GNNs and multiple benchmark datasets. For instance,\non the Cora dataset, Graph Convolutional Network with our randomized smoothing\ncan achieve a certified accuracy of 0.49 when the attacker can arbitrarily\nadd/delete at most 15 edges in the graph.",
          "arxiv_id": "2008.10715v3"
        },
        {
          "title": "Deterministic Certification of Graph Neural Networks against Graph Poisoning Attacks with Arbitrary Perturbations",
          "year": "2025-03",
          "abstract": "Graph neural networks (GNNs) are becoming the de facto method to learn on the\ngraph data and have achieved the state-of-the-art on node and graph\nclassification tasks. However, recent works show GNNs are vulnerable to\ntraining-time poisoning attacks -- marginally perturbing edges, nodes, or/and\nnode features of training graph(s) can largely degrade GNNs' testing\nperformance. Most previous defenses against graph poisoning attacks are\nempirical and are soon broken by adaptive / stronger ones. A few provable\ndefenses provide robustness guarantees, but have large gaps when applied in\npractice: 1) restrict the attacker on only one type of perturbation; 2) design\nfor a particular GNN architecture or task; and 3) robustness guarantees are not\n100\\% accurate.\n  In this work, we bridge all these gaps by developing PGNNCert, the first\ncertified defense of GNNs against poisoning attacks under arbitrary (edge,\nnode, and node feature) perturbations with deterministic robustness guarantees.\nExtensive evaluations on multiple node and graph classification datasets and\nGNNs demonstrate the effectiveness of PGNNCert to provably defend against\narbitrary poisoning perturbations. PGNNCert is also shown to significantly\noutperform the state-of-the-art certified defenses against edge perturbation or\nnode perturbation during GNN training.",
          "arxiv_id": "2503.18503v1"
        },
        {
          "title": "A Hard Label Black-box Adversarial Attack Against Graph Neural Networks",
          "year": "2021-08",
          "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph structure related tasks such as node classification and graph\nclassification. However, GNNs are vulnerable to adversarial attacks. Existing\nworks mainly focus on attacking GNNs for node classification; nevertheless, the\nattacks against GNNs for graph classification have not been well explored.\n  In this work, we conduct a systematic study on adversarial attacks against\nGNNs for graph classification via perturbing the graph structure. In\nparticular, we focus on the most challenging attack, i.e., hard label black-box\nattack, where an attacker has no knowledge about the target GNN model and can\nonly obtain predicted labels through querying the target model.To achieve this\ngoal, we formulate our attack as an optimization problem, whose objective is to\nminimize the number of edges to be perturbed in a graph while maintaining the\nhigh attack success rate. The original optimization problem is intractable to\nsolve, and we relax the optimization problem to be a tractable one, which is\nsolved with theoretical convergence guarantee. We also design a coarse-grained\nsearching algorithm and a query-efficient gradient computation algorithm to\ndecrease the number of queries to the target GNN model. Our experimental\nresults on three real-world datasets demonstrate that our attack can\neffectively attack representative GNNs for graph classification with less\nqueries and perturbations. We also evaluate the effectiveness of our attack\nunder two defenses: one is well-designed adversarial graph detector and the\nother is that the target GNN model itself is equipped with a defense to prevent\nadversarial graph generation. Our experimental results show that such defenses\nare not effective enough, which highlights more advanced defenses.",
          "arxiv_id": "2108.09513v2"
        }
      ],
      "22": [
        {
          "title": "Investigating the Cybersecurity of Smart Grids Based on Cyber-Physical Twin Approach",
          "year": "2022-11",
          "abstract": "While the increasing penetration of information and communication technology\ninto distribution grid brings numerous benefits, it also opens up a new threat\nlandscape, particularly through cyberattacks. To provide a basis for\ncountermeasures against such threats, this paper addresses the investigation of\nthe impact and manifestations of cyberattacks on smart grids by replicating the\npower grid in a secure, isolated, and controlled laboratory environment as a\ncyber-physical twin. Currently, detecting intrusions by unauthorized third\nparties into the central monitoring and control system of grid operators,\nespecially attacks within the grid perimeter, is a major challenge. The\ndevelopment and validation of methods to detect and prevent coordinated and\ntimed attacks on electric power systems depends not only on the availability\nand quality of data from such attack scenarios, but also on suitable realistic\ninvestigation environments. However, to create a comprehensive investigation\nenvironment, a realistic representation of the study object is required to\nthoroughly investigate critical cyberattacks on grid operations and evaluate\ntheir impact on the power grid using real data. In this paper, we demonstrate\nour cyber-physical twin approach using a microgrid in the context of a\ncyberattack case study.",
          "arxiv_id": "2211.10974v1"
        },
        {
          "title": "Smart Grid: Cyber Attacks, Critical Defense Approaches, and Digital Twin",
          "year": "2022-05",
          "abstract": "As a national critical infrastructure, the smart grid has attracted\nwidespread attention for its cybersecurity issues. The development towards an\nintelligent, digital, and Internet-connected smart grid has attracted external\nadversaries for malicious activities. It is necessary to enhance its\ncybersecurity by both improving the existing defense approaches and introducing\nnovel developed technologies to the smart grid context. As an emerging\ntechnology, digital twin (DT) is considered as an enabler for enhanced\nsecurity. However, the practical implementation is quite challenging. This is\ndue to the knowledge barriers among smart grid designers, security experts, and\nDT developers. Each single domain is a complicated system covering various\ncomponents and technologies. As a result, works are needed to sort out relevant\ncontents so that DT can be better embedded in the security architecture design\nof smart grid.\n  In order to meet this demand, our paper covers the above three domains, i.e.,\nsmart grid, cybersecurity, and DT. Specifically, the paper i) introduces the\nbackground of the smart grid; ii) reviews external cyber attacks from attack\nincidents and attack methods; iii) introduces critical defense approaches in\nindustrial cyber systems, which include device identification, vulnerability\ndiscovery, intrusion detection systems (IDSs), honeypots, attribution, and\nthreat intelligence (TI); iv) reviews the relevant content of DT, including its\nbasic concepts, applications in the smart grid, and how DT enhances the\nsecurity. In the end, the paper puts forward our security considerations on the\nfuture development of DT-based smart grid. The survey is expected to help\ndevelopers break knowledge barriers among smart grid, cybersecurity, and DT,\nand provide guidelines for future security design of DT-based smart grid.",
          "arxiv_id": "2205.11783v2"
        },
        {
          "title": "Towards Automated Generation of Smart Grid Cyber Range for Cybersecurity Experiments and Training",
          "year": "2024-04",
          "abstract": "Assurance of cybersecurity is crucial to ensure dependability and resilience\nof smart power grid systems. In order to evaluate the impact of potential cyber\nattacks, to assess deployability and effectiveness of cybersecurity measures,\nand to enable hands-on exercise and training of personals, an interactive,\nvirtual environment that emulates the behaviour of a smart grid system, namely\nsmart grid cyber range, has been demanded by industry players as well as\nacademia. A smart grid cyber range is typically implemented as a combination of\ncyber system emulation, which allows interactivity, and physical system (i.e.,\npower grid) simulation that are tightly coupled for consistent cyber and\nphysical behaviours. However, its design and implementation require intensive\nexpertise and efforts in cyber and physical aspects of smart power systems as\nwell as software/system engineering. While many industry players, including\npower grid operators, device vendors, research and education sectors are\ninterested, availability of the smart grid cyber range is limited to a small\nnumber of research labs. To address this challenge, we have developed a\nframework for modelling a smart grid cyber range using an XML-based language,\ncalled SG-ML, and for \"compiling\" the model into an operational cyber range\nwith minimal engineering efforts. The modelling language includes standardized\nschema from IEC 61850 and IEC 61131, which allows industry players to utilize\ntheir existing configurations. The SG-ML framework aims at making a smart grid\ncyber range available to broader user bases to facilitate cybersecurity R\\&D\nand hands-on exercises.",
          "arxiv_id": "2404.00869v1"
        }
      ],
      "23": [
        {
          "title": "Injection Attacks Reloaded: Tunnelling Malicious Payloads over DNS",
          "year": "2022-05",
          "abstract": "The traditional design principle for Internet protocols indicates: \"Be strict\nwhen sending and tolerant when receiving\" [RFC1958], and DNS is no exception to\nthis. The transparency of DNS in handling the DNS records, also standardised\nspecifically for DNS [RFC3597], is one of the key features that made it such a\npopular platform facilitating a constantly increasing number of new\napplications. An application simply creates a new DNS record and can instantly\nstart distributing it over DNS without requiring any changes to the DNS servers\nand platforms. Our Internet wide study confirms that more than 1.3M (96% of\ntested) open DNS resolvers are standard compliant and treat DNS records\ntransparently.\n  In this work we show that this `transparency' introduces a severe\nvulnerability in the Internet: we demonstrate a new method to launch string\ninjection attacks by encoding malicious payloads into DNS records. We show how\nto weaponise such DNS records to attack popular applications. For instance, we\napply string injection to launch a new type of DNS cache poisoning attack,\nwhich we evaluated against a population of open resolvers and found 105K to be\nvulnerable. Such cache poisoning cannot be prevented with common setups of\nDNSSEC. Our attacks apply to internal as well as to public services, for\ninstance, we reveal that all eduroam services are vulnerable to our injection\nattacks, allowing us to launch exploits ranging from unauthorised access to\neduroam networks to resource starvation. Depending on the application, our\nattacks cause system crashes, data corruption and leakage, degradation of\nsecurity, and can introduce remote code execution and arbitrary errors.\n  In our evaluation of the attacks in the Internet we find that all the\nstandard compliant open DNS resolvers we tested allow our injection attacks\nagainst applications and users on their networks.",
          "arxiv_id": "2205.05439v1"
        },
        {
          "title": "A Survey on DNS Encryption: Current Development, Malware Misuse, and Inference Techniques",
          "year": "2022-01",
          "abstract": "The domain name system (DNS) that maps alphabetic names to numeric Internet\nProtocol (IP) addresses plays a foundational role for Internet communications.\nBy default, DNS queries and responses are exchanged in unencrypted plaintext,\nand hence, can be read and/or hijacked by third parties. To protect user\nprivacy, the networking community has proposed standard encryption technologies\nsuch as DNS over TLS (DoT), DNS over HTTPS (DoH), and DNS over QUIC (DoQ) for\nDNS communications, enabling clients to perform secure and private domain name\nlookups. We survey the DNS encryption literature published since 2016, focusing\non its current landscape and how it is misused by malware, and highlighting the\nexisting techniques developed to make inferences from encrypted DNS traffic.\nFirst, we provide an overview of various standards developed in the space of\nDNS encryption and their adoption status, performance, benefits, and security\nissues. Second, we highlight ways that various malware families can exploit DNS\nencryption to their advantage for botnet communications and/or data\nexfiltration. Third, we discuss existing inference methods for profiling normal\npatterns and/or detecting malicious encrypted DNS traffic. Several directions\nare presented to motivate future research in enhancing the performance and\nsecurity of DNS encryption.",
          "arxiv_id": "2201.00900v2"
        },
        {
          "title": "Oblivious DNS over HTTPS (ODoH): A Practical Privacy Enhancement to DNS",
          "year": "2020-11",
          "abstract": "The Domain Name System (DNS) is the foundation of a human-usable Internet,\nresponding to client queries for host-names with corresponding IP addresses and\nrecords. Traditional DNS is also unencrypted, and leaks user information to\nnetwork operators. Recent efforts to secure DNS using DNS over TLS (DoT) and\nDNS over HTTPS (DoH) have been gaining traction, ostensibly protecting traffic\nand hiding content from on-lookers. However, one of the criticisms of DoT and\nDoH is brought to bear by the small number of large-scale deployments (e.g.,\nComcast, Google, Cloudflare): DNS resolvers can associate query contents with\nclient identities in the form of IP addresses. Oblivious DNS over HTTPS(ODoH)\nsafeguards against this problem. In this paper we ask what it would take to\nmake ODoH practical? We describe ODoH, a practical DNS protocol aimed at\nresolving this issue by both protecting the client's content and identity. We\nimplement and deploy the protocol, and perform measurements to show that ODoH\nhas comparable performance to protocols like DoH and DoT which are gaining\nwidespread adoption, while improving client privacy, making ODoH a practical\nprivacy enhancing replacement for the usage of DNS.",
          "arxiv_id": "2011.10121v1"
        }
      ],
      "24": [
        {
          "title": "BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models",
          "year": "2023-07",
          "abstract": "The rise in popularity of text-to-image generative artificial intelligence\n(AI) has attracted widespread public interest. We demonstrate that this\ntechnology can be attacked to generate content that subtly manipulates its\nusers. We propose a Backdoor Attack on text-to-image Generative Models (BAGM),\nwhich upon triggering, infuses the generated images with manipulative details\nthat are naturally blended in the content. Our attack is the first to target\nthree popular text-to-image generative models across three stages of the\ngenerative process by modifying the behaviour of the embedded tokenizer, the\nlanguage model or the image generative model. Based on the penetration level,\nBAGM takes the form of a suite of attacks that are referred to as surface,\nshallow and deep attacks in this article. Given the existing gap within this\ndomain, we also contribute a comprehensive set of quantitative metrics designed\nspecifically for assessing the effectiveness of backdoor attacks on\ntext-to-image models. The efficacy of BAGM is established by attacking\nstate-of-the-art generative models, using a marketing scenario as the target\ndomain. To that end, we contribute a dataset of branded product images. Our\nembedded backdoors increase the bias towards the target outputs by more than\nfive times the usual, without compromising the model robustness or the\ngenerated content utility. By exposing generative AI's vulnerabilities, we\nencourage researchers to tackle these challenges and practitioners to exercise\ncaution when using pre-trained models. Relevant code, input prompts and\nsupplementary material can be found at https://github.com/JJ-Vice/BAGM, and the\ndataset is available at:\nhttps://ieee-dataport.org/documents/marketable-foods-mf-dataset.\n  Keywords: Generative Artificial Intelligence, Generative Models,\nText-to-Image generation, Backdoor Attacks, Trojan, Stable Diffusion.",
          "arxiv_id": "2307.16489v2"
        },
        {
          "title": "DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models",
          "year": "2022-10",
          "abstract": "Text-to-image generation models that generate images based on prompt\ndescriptions have attracted an increasing amount of attention during the past\nfew months. Despite their encouraging performance, these models raise concerns\nabout the misuse of their generated fake images. To tackle this problem, we\npioneer a systematic study on the detection and attribution of fake images\ngenerated by text-to-image generation models. Concretely, we first build a\nmachine learning classifier to detect the fake images generated by various\ntext-to-image generation models. We then attribute these fake images to their\nsource models, such that model owners can be held responsible for their models'\nmisuse. We further investigate how prompts that generate fake images affect\ndetection and attribution. We conduct extensive experiments on four popular\ntext-to-image generation models, including DALL$\\cdot$E 2, Stable Diffusion,\nGLIDE, and Latent Diffusion, and two benchmark prompt-image datasets. Empirical\nresults show that (1) fake images generated by various models can be\ndistinguished from real ones, as there exists a common artifact shared by fake\nimages from different models; (2) fake images can be effectively attributed to\ntheir source models, as different models leave unique fingerprints in their\ngenerated images; (3) prompts with the ``person'' topic or a length between 25\nand 75 enable models to generate fake images with higher authenticity. All\nfindings contribute to the community's insight into the threats caused by\ntext-to-image generation models. We appeal to the community's consideration of\nthe counterpart solutions, like ours, against the rapidly-evolving fake image\ngeneration.",
          "arxiv_id": "2210.06998v2"
        },
        {
          "title": "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning",
          "year": "2023-05",
          "abstract": "With the help of conditioning mechanisms, the state-of-the-art diffusion\nmodels have achieved tremendous success in guided image generation,\nparticularly in text-to-image synthesis. To gain a better understanding of the\ntraining process and potential risks of text-to-image synthesis, we perform a\nsystematic investigation of backdoor attack on text-to-image diffusion models\nand propose BadT2I, a general multimodal backdoor attack framework that tampers\nwith image synthesis in diverse semantic levels. Specifically, we perform\nbackdoor attacks on three levels of the vision semantics: Pixel-Backdoor,\nObject-Backdoor and Style-Backdoor. By utilizing a regularization loss, our\nmethods efficiently inject backdoors into a large-scale text-to-image diffusion\nmodel while preserving its utility with benign inputs. We conduct empirical\nexperiments on Stable Diffusion, the widely-used text-to-image diffusion model,\ndemonstrating that the large-scale diffusion model can be easily backdoored\nwithin a few fine-tuning steps. We conduct additional experiments to explore\nthe impact of different types of textual triggers, as well as the backdoor\npersistence during further training, providing insights for the development of\nbackdoor defense methods. Besides, our investigation may contribute to the\ncopyright protection of text-to-image models in the future.",
          "arxiv_id": "2305.04175v2"
        }
      ],
      "25": [
        {
          "title": "Label-Only Membership Inference Attacks",
          "year": "2020-07",
          "abstract": "Membership inference attacks are one of the simplest forms of privacy leakage\nfor machine learning models: given a data point and model, determine whether\nthe point was used to train the model. Existing membership inference attacks\nexploit models' abnormal confidence when queried on their training data. These\nattacks do not apply if the adversary only gets access to models' predicted\nlabels, without a confidence measure. In this paper, we introduce label-only\nmembership inference attacks. Instead of relying on confidence scores, our\nattacks evaluate the robustness of a model's predicted labels under\nperturbations to obtain a fine-grained membership signal. These perturbations\ninclude common data augmentations or adversarial examples. We empirically show\nthat our label-only membership inference attacks perform on par with prior\nattacks that required access to model confidences. We further demonstrate that\nlabel-only attacks break multiple defenses against membership inference attacks\nthat (implicitly or explicitly) rely on a phenomenon we call confidence\nmasking. These defenses modify a model's confidence scores in order to thwart\nattacks, but leave the model's predicted labels unchanged. Our label-only\nattacks demonstrate that confidence-masking is not a viable defense strategy\nagainst membership inference. Finally, we investigate worst-case label-only\nattacks, that infer membership for a small number of outlier data points. We\nshow that label-only attacks also match confidence-based attacks in this\nsetting. We find that training models with differential privacy and (strong) L2\nregularization are the only known defense strategies that successfully prevents\nall attacks. This remains true even when the differential privacy budget is too\nhigh to offer meaningful provable guarantees.",
          "arxiv_id": "2007.14321v3"
        },
        {
          "title": "On the (In)Feasibility of Attribute Inference Attacks on Machine Learning Models",
          "year": "2021-03",
          "abstract": "With an increase in low-cost machine learning APIs, advanced machine learning\nmodels may be trained on private datasets and monetized by providing them as a\nservice. However, privacy researchers have demonstrated that these models may\nleak information about records in the training dataset via membership inference\nattacks. In this paper, we take a closer look at another inference attack\nreported in literature, called attribute inference, whereby an attacker tries\nto infer missing attributes of a partially known record used in the training\ndataset by accessing the machine learning model as an API. We show that even if\na classification model succumbs to membership inference attacks, it is unlikely\nto be susceptible to attribute inference attacks. We demonstrate that this is\nbecause membership inference attacks fail to distinguish a member from a nearby\nnon-member. We call the ability of an attacker to distinguish the two (similar)\nvectors as strong membership inference. We show that membership inference\nattacks cannot infer membership in this strong setting, and hence inferring\nattributes is infeasible. However, under a relaxed notion of attribute\ninference, called approximate attribute inference, we show that it is possible\nto infer attributes close to the true attributes. We verify our results on\nthree publicly available datasets, five membership, and three attribute\ninference attacks reported in literature.",
          "arxiv_id": "2103.07101v1"
        },
        {
          "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
          "year": "2020-09",
          "abstract": "While being deployed in many critical applications as core components,\nmachine learning (ML) models are vulnerable to various security and privacy\nattacks. One major privacy attack in this domain is membership inference, where\nan adversary aims to determine whether a target data sample is part of the\ntraining set of a target ML model. So far, most of the current membership\ninference attacks are evaluated against ML models trained from scratch.\nHowever, real-world ML models are typically trained following the transfer\nlearning paradigm, where a model owner takes a pretrained model learned from a\ndifferent dataset, namely teacher model, and trains her own student model by\nfine-tuning the teacher model with her own data.\n  In this paper, we perform the first systematic evaluation of membership\ninference attacks against transfer learning models. We adopt the strategy of\nshadow model training to derive the data for training our membership inference\nclassifier. Extensive experiments on four real-world image datasets show that\nmembership inference can achieve effective performance. For instance, on the\nCIFAR100 classifier transferred from ResNet20 (pretrained with Caltech101), our\nmembership inference achieves $95\\%$ attack AUC. Moreover, we show that\nmembership inference is still effective when the architecture of target model\nis unknown. Our results shed light on the severity of membership risks stemming\nfrom machine learning models in practice.",
          "arxiv_id": "2009.04872v1"
        }
      ],
      "26": [
        {
          "title": "Analysis of Longitudinal Changes in Privacy Behavior of Android Applications",
          "year": "2021-12",
          "abstract": "Privacy concerns have long been expressed around smart devices, and the\nconcerns around Android apps have been studied by many past works. Over the\npast 10 years, we have crawled and scraped data for almost 1.9 million apps,\nand also stored the APKs for 135,536 of them. In this paper, we examine the\ntrends in how Android apps have changed over time with respect to privacy and\nlook at it from two perspectives: (1) how privacy behavior in apps have changed\nas they are updated over time, (2) how these changes can be accounted for when\ncomparing third-party libraries and the app's own internals. To study this, we\nexamine the adoption of HTTPS, whether apps scan the device for other installed\napps, the use of permissions for privacy-sensitive data, and the use of unique\nidentifiers. We find that privacy-related behavior has improved with time as\napps continue to receive updates, and that the third-party libraries used by\napps are responsible for more issues with privacy. However, we observe that in\nthe current state of Android apps, there has not been enough of an improvement\nin terms of privacy and many issues still need to be addressed.",
          "arxiv_id": "2112.14205v1"
        },
        {
          "title": "ModZoo: A Large-Scale Study of Modded Android Apps and their Markets",
          "year": "2024-02",
          "abstract": "We present the results of the first large-scale study into Android markets\nthat offer modified or modded apps: apps whose features and functionality have\nbeen altered by a third-party. We analyse over 146k (thousand) apps obtained\nfrom 13 of the most popular modded app markets. Around 90% of apps we collect\nare altered in some way when compared to the official counterparts on Google\nPlay. Modifications include games cheats, such as infinite coins or lives;\nmainstream apps with premium features provided for free; and apps with modified\nadvertising identifiers or excluded ads. We find the original app developers\nlose significant potential revenue due to: the provision of paid for apps for\nfree (around 5% of the apps across all markets); the free availability of\npremium features that require payment in the official app; and modified\nadvertising identifiers. While some modded apps have all trackers and ads\nremoved (3%), in general, the installation of these apps is significantly more\nrisky for the user than the official version: modded apps are ten times more\nlikely to be marked as malicious and often request additional permissions.",
          "arxiv_id": "2402.19180v2"
        },
        {
          "title": "SeMA: Extending and Analyzing Storyboards to Develop Secure Android Apps",
          "year": "2020-01",
          "abstract": "Mobile apps provide various critical services, such as banking,\ncommunication, and healthcare. To this end, they have access to our personal\ninformation and have the ability to perform actions on our behalf. Hence,\nsecuring mobile apps is crucial to ensuring the privacy and safety of its\nusers.\n  Recent research efforts have focused on developing solutions to secure mobile\necosystems (i.e., app platforms, apps, and app stores), specifically in the\ncontext of detecting vulnerabilities in Android apps. Despite this attention,\nknown vulnerabilities are often found in mobile apps, which can be exploited by\nmalicious apps to harm the user. Further, fixing vulnerabilities after\ndeveloping an app has downsides in terms of time, resources, user\ninconvenience, and information loss.\n  In an attempt to address this concern, we have developed SeMA, a mobile app\ndevelopment methodology that builds on existing mobile app design artifacts\nsuch as storyboards. With SeMA, security is a first-class citizen in an app's\ndesign -- app designers and developers can collaborate to specify and reason\nabout the security properties of an app at an abstract level without being\ndistracted by implementation level details. Our realization of SeMA using\nAndroid Studio tooling demonstrates the methodology is complementary to\nexisting design and development practices. An evaluation of the effectiveness\nof SeMA shows the methodology can detect and help prevent 49 vulnerabilities\nknown to occur in Android apps. Further, a usability study of the methodology\ninvolving ten real-world developers shows the methodology is likely to reduce\nthe development time and help developers uncover and prevent known\nvulnerabilities while designing apps.",
          "arxiv_id": "2001.10052v4"
        }
      ],
      "27": [
        {
          "title": "Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation",
          "year": "2023-10",
          "abstract": "Synthetic data is emerging as a promising way to harness the value of data,\nwhile reducing privacy risks. The potential of synthetic data is not limited to\nprivacy-friendly data release, but also includes complementing real data in\nuse-cases such as training machine learning algorithms that are more fair and\nrobust to distribution shifts etc. There is a lot of interest in algorithmic\nadvances in synthetic data generation for providing better privacy and\nstatistical guarantees and for its better utilisation in machine learning\npipelines. However, for responsible and trustworthy synthetic data generation,\nit is not sufficient to focus only on these algorithmic aspects and instead, a\nholistic view of the synthetic data generation pipeline must be considered. We\nbuild a novel system that allows the contributors of real data to autonomously\nparticipate in differentially private synthetic data generation without relying\non a trusted centre. Our modular, general and scalable solution is based on\nthree building blocks namely: Solid (Social Linked Data), MPC (Secure\nMulti-Party Computation) and Trusted Execution Environments (TEEs). Solid is a\nspecification that lets people store their data securely in decentralised data\nstores called Pods and control access to their data. MPC refers to the set of\ncryptographic methods for different parties to jointly compute a function over\ntheir inputs while keeping those inputs private. TEEs such as Intel SGX rely on\nhardware based features for confidentiality and integrity of code and data. We\nshow how these three technologies can be effectively used to address various\nchallenges in responsible and trustworthy synthetic data generation by\nensuring: 1) contributor autonomy, 2) decentralisation, 3) privacy and 4)\nscalability. We support our claims with rigorous empirical results on simulated\nand real datasets and different synthetic data generation algorithms.",
          "arxiv_id": "2310.20062v1"
        },
        {
          "title": "Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data Generation and Evaluation in Learning Analytics",
          "year": "2024-01",
          "abstract": "Privacy poses a significant obstacle to the progress of learning analytics\n(LA), presenting challenges like inadequate anonymization and data misuse that\ncurrent solutions struggle to address. Synthetic data emerges as a potential\nremedy, offering robust privacy protection. However, prior LA research on\nsynthetic data lacks thorough evaluation, essential for assessing the delicate\nbalance between privacy and data utility. Synthetic data must not only enhance\nprivacy but also remain practical for data analytics. Moreover, diverse LA\nscenarios come with varying privacy and utility needs, making the selection of\nan appropriate synthetic data approach a pressing challenge. To address these\ngaps, we propose a comprehensive evaluation of synthetic data, which\nencompasses three dimensions of synthetic data quality, namely resemblance,\nutility, and privacy. We apply this evaluation to three distinct LA datasets,\nusing three different synthetic data generation methods. Our results show that\nsynthetic data can maintain similar utility (i.e., predictive performance) as\nreal data, while preserving privacy. Furthermore, considering different privacy\nand data utility requirements in different LA scenarios, we make customized\nrecommendations for synthetic data generation. This paper not only presents a\ncomprehensive evaluation of synthetic data but also illustrates its potential\nin mitigating privacy concerns within the field of LA, thus contributing to a\nwider application of synthetic data in LA and promoting a better practice for\nopen science.",
          "arxiv_id": "2401.06883v1"
        },
        {
          "title": "PreFair: Privately Generating Justifiably Fair Synthetic Data",
          "year": "2022-12",
          "abstract": "When a database is protected by Differential Privacy (DP), its usability is\nlimited in scope. In this scenario, generating a synthetic version of the data\nthat mimics the properties of the private data allows users to perform any\noperation on the synthetic data, while maintaining the privacy of the original\ndata. Therefore, multiple works have been devoted to devising systems for DP\nsynthetic data generation. However, such systems may preserve or even magnify\nproperties of the data that make it unfair, endering the synthetic data unfit\nfor use. In this work, we present PreFair, a system that allows for DP fair\nsynthetic data generation. PreFair extends the state-of-the-art DP data\ngeneration mechanisms by incorporating a causal fairness criterion that ensures\nfair synthetic data. We adapt the notion of justifiable fairness to fit the\nsynthetic data generation scenario. We further study the problem of generating\nDP fair synthetic data, showing its intractability and designing algorithms\nthat are optimal under certain assumptions. We also provide an extensive\nexperimental evaluation, showing that PreFair generates synthetic data that is\nsignificantly fairer than the data generated by leading DP data generation\nmechanisms, while remaining faithful to the private data.",
          "arxiv_id": "2212.10310v2"
        }
      ],
      "28": [
        {
          "title": "AI-enabled Automation for Completeness Checking of Privacy Policies",
          "year": "2021-06",
          "abstract": "Technological advances in information sharing have raised concerns about data\nprotection. Privacy policies contain privacy-related requirements about how the\npersonal data of individuals will be handled by an organization or a software\nsystem (e.g., a web service or an app). In Europe, privacy policies are subject\nto compliance with the General Data Protection Regulation (GDPR). A\nprerequisite for GDPR compliance checking is to verify whether the content of a\nprivacy policy is complete according to the provisions of GDPR. Incomplete\nprivacy policies might result in large fines on violating organization as well\nas incomplete privacy-related software specifications. Manual completeness\nchecking is both time-consuming and error-prone. In this paper, we propose\nAI-based automation for the completeness checking of privacy policies. Through\nsystematic qualitative methods, we first build two artifacts to characterize\nthe privacy-related provisions of GDPR, namely a conceptual model and a set of\ncompleteness criteria. Then, we develop an automated solution on top of these\nartifacts by leveraging a combination of natural language processing and\nsupervised machine learning. Specifically, we identify the GDPR-relevant\ninformation content in privacy policies and subsequently check them against the\ncompleteness criteria. To evaluate our approach, we collected 234 real privacy\npolicies from the fund industry. Over a set of 48 unseen privacy policies, our\napproach detected 300 of the total of 334 violations of some completeness\ncriteria correctly, while producing 23 false positives. The approach thus has a\nprecision of 92.9% and recall of 89.8%. Compared to a baseline that applies\nkeyword search only, our approach results in an improvement of 24.5% in\nprecision and 38% in recall.",
          "arxiv_id": "2106.05688v2"
        },
        {
          "title": "A BERT-based Empirical Study of Privacy Policies' Compliance with GDPR",
          "year": "2024-07",
          "abstract": "Since its implementation in May 2018, the General Data Protection Regulation\n(GDPR) has prompted businesses to revisit and revise their data handling\npractices to ensure compliance. The privacy policy, which serves as the primary\nmeans of informing users about their privacy rights and the data practices of\ncompanies, has been significantly updated by numerous businesses post-GDPR\nimplementation. However, many privacy policies remain packed with technical\njargon, lengthy explanations, and vague descriptions of data practices and user\nrights. This makes it a challenging task for users and regulatory authorities\nto manually verify the GDPR compliance of these privacy policies. In this\nstudy, we aim to address the challenge of compliance analysis between GDPR\n(Article 13) and privacy policies for 5G networks. We manually collected\nprivacy policies from almost 70 different 5G MNOs, and we utilized an automated\nBERT-based model for classification. We show that an encouraging 51$\\%$ of\ncompanies demonstrate a strong adherence to GDPR. In addition, we present the\nfirst study that provides current empirical evidence on the readability of\nprivacy policies for 5G network. we adopted readability analysis toolset that\nincorporates various established readability metrics. The findings empirically\nshow that the readability of the majority of current privacy policies remains a\nsignificant challenge. Hence, 5G providers need to invest considerable effort\ninto revising these documents to enhance both their utility and the overall\nuser experience.",
          "arxiv_id": "2407.06778v1"
        },
        {
          "title": "Detecting Compliance of Privacy Policies with Data Protection Laws",
          "year": "2021-02",
          "abstract": "Privacy Policies are the legal documents that describe the practices that an\norganization or company has adopted in the handling of the personal data of its\nusers. But as policies are a legal document, they are often written in\nextensive legal jargon that is difficult to understand. Though work has been\ndone on privacy policies but none that caters to the problem of verifying if a\ngiven privacy policy adheres to the data protection laws of a given country or\nstate. We aim to bridge that gap by providing a framework that analyzes privacy\npolicies in light of various data protection laws, such as the General Data\nProtection Regulation (GDPR). To achieve that, firstly we labeled both the\nprivacy policies and laws. Then a correlation scheme is developed to map the\ncontents of a privacy policy to the appropriate segments of law that a policy\nmust conform to. Then we check the compliance of privacy policy's text with the\ncorresponding text of the law using NLP techniques. By using such a tool, users\nwould be better equipped to understand how their personal data is managed. For\nnow, we have provided a mapping for the GDPR and PDPA, but other laws can\neasily be incorporated in the already built pipeline.",
          "arxiv_id": "2102.12362v1"
        }
      ],
      "29": [
        {
          "title": "Large Language Model assisted Hybrid Fuzzing",
          "year": "2024-12",
          "abstract": "Greybox fuzzing is one of the most popular methods for detecting software\nvulnerabilities, which conducts a biased random search within the program input\nspace. To enhance its effectiveness in achieving deep coverage of program\nbehaviors, greybox fuzzing is often combined with concolic execution, which\nperforms a path-sensitive search over the domain of program inputs. In hybrid\nfuzzing, conventional greybox fuzzing is followed by concolic execution in an\niterative loop, where reachability roadblocks encountered by greybox fuzzing\nare tackled by concolic execution. However, such hybrid fuzzing still suffers\nfrom difficulties conventionally faced by symbolic execution, such as the need\nfor environment modeling and system call support. In this work, we show how to\nachieve the effect of concolic execution without having to compute and solve\nsymbolic path constraints. When coverage-based greybox fuzzing reaches a\nroadblock in terms of reaching certain branches, we conduct a slicing on the\nexecution trace and suggest modifications of the input to reach the relevant\nbranches. A Large Language Model (LLM) is used as a solver to generate the\nmodified input for reaching the desired branches. Compared with both the\nvanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based\nhybrid fuzzer HyLLfuzz (pronounced \"hill fuzz\") demonstrates superior coverage.\nFurthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is\n4-19 times faster than the concolic execution running in existing hybrid\nfuzzing tools. This experience shows that LLMs can be effectively inserted into\nthe iterative loop of hybrid fuzzers, to efficiently expose more program\nbehaviors.",
          "arxiv_id": "2412.15931v1"
        },
        {
          "title": "DARWIN: Survival of the Fittest Fuzzing Mutators",
          "year": "2022-10",
          "abstract": "Fuzzing is an automated software testing technique broadly adopted by the\nindustry. A popular variant is mutation-based fuzzing, which discovers a large\nnumber of bugs in practice. While the research community has studied\nmutation-based fuzzing for years now, the algorithms' interactions within the\nfuzzer are highly complex and can, together with the randomness in every\ninstance of a fuzzer, lead to unpredictable effects. Most efforts to improve\nthis fragile interaction focused on optimizing seed scheduling. However,\nreal-world results like Google's FuzzBench highlight that these approaches do\nnot consistently show improvements in practice. Another approach to improve the\nfuzzing process algorithmically is optimizing mutation scheduling.\nUnfortunately, existing mutation scheduling approaches also failed to convince\nbecause of missing real-world improvements or too many user-controlled\nparameters whose configuration requires expert knowledge about the target\nprogram. This leaves the challenging problem of cleverly processing test cases\nand achieving a measurable improvement unsolved.\n  We present DARWIN, a novel mutation scheduler and the first to show fuzzing\nimprovements in a realistic scenario without the need to introduce additional\nuser-configurable parameters, opening this approach to the broad fuzzing\ncommunity. DARWIN uses an Evolution Strategy to systematically optimize and\nadapt the probability distribution of the mutation operators during fuzzing. We\nimplemented a prototype based on the popular general-purpose fuzzer AFL. DARWIN\nsignificantly outperforms the state-of-the-art mutation scheduler and the AFL\nbaseline in our own coverage experiment, in FuzzBench, and by finding 15 out of\n21 bugs the fastest in the MAGMA benchmark. Finally, DARWIN found 20 unique\nbugs (including one novel bug), 66% more than AFL, in widely-used real-world\napplications.",
          "arxiv_id": "2210.11783v1"
        },
        {
          "title": "UltraFuzz: Towards Resource-saving in Distributed Fuzzing",
          "year": "2020-09",
          "abstract": "Recent research has sought to improve fuzzing performance via parallel\ncomputing. However, researchers focus on improving efficiency while ignoring\nthe increasing cost of testing resources. Parallel fuzzing in the distributed\nenvironment amplifies the resource-wasting problem caused by the random nature\nof fuzzing. In the parallel mode, owing to the lack of an appropriate task\ndispatching scheme and timely fuzzing status synchronization among different\nfuzzing instances, task conflicts and workload imbalance occur, making the\nresource-wasting problem severe. In this paper, we design UltraFuzz, a fuzzer\nfor resource-saving in distributed fuzzing. Based on centralized dynamic\nscheduling, UltraFuzz can dispatch tasks and schedule power globally and\nreasonably to avoid resource-wasting. Besides, UltraFuzz can elastically\nallocate computing power for fuzzing and seed evaluation, thereby avoiding the\npotential bottleneck of seed evaluation that blocks the fuzzing process.\nUltraFuzz was evaluated using real-world programs, and the results show that\nwith the same testing resource, UltraFuzz outperforms state-of-the-art tools,\nsuch as AFL, AFL-P, PAFL, and EnFuzz. Most importantly, the experiment reveals\ncertain results that seem counter-intuitive, namely that parallel fuzzing can\nachieve ``super-linear acceleration'' when compared with single-core fuzzing.\nWe conduct additional experiments to reveal the deep reasons behind this\nphenomenon and dig deep into the inherent advantages of parallel fuzzing over\nserial fuzzing, including the global optimization of seed energy scheduling and\nthe escape of local optimal seed. Additionally, 24 real-world vulnerabilities\nwere discovered using UltraFuzz.",
          "arxiv_id": "2009.06124v2"
        }
      ],
      "30": [
        {
          "title": "Image Based Password Authentication System",
          "year": "2022-05",
          "abstract": "Preservation of information and computer security is broadly dependent on the\nsecured authentication system which is underpinned by password. Text based\npassword is a commonly used and available system for authentication. But it\nbears many limitations like shoulder surfing, dictionary attack, Phishing,\nguessing the password etc. In order to overwhelm these vulnerabilities of\nancient textual password, many graphical or image based password authentication\nsystem has been introduced form last few years. But none of this graphical\nsystem is considered as enough adventurous to keep pace with these issues. Here\nwe have proposed an image based password authentication system which is more\nmethodical and can cope up with every vulnerability of recent password\nauthentication system. To make our system hassle free and more reliable, we\nwill only take username from an user for registration purpose as our system\nwill generate a unique key number for that particular user and this key will be\nused as password for later login procedure. The user name and key both will be\nencrypted using a cryptography algorithm to prevent database hacking. There\nwill be a randomized clickable image grid in our system. By clicking on this\nimage grid, user will input the password key for login purpose. Here we have\ndeveloped another method namely shoulder surfing resistant password. To prevent\nthe attack of shoulder surfing, if any user wishes to change our system\nprovided password key then he or she is allowed to do so by using this method.\nBesides this method allows user to change the password every single time of\nlogin. A user doesn't need to enter any textual password for authentication in\nour recent module and hence combination of all these features improve the\nsecurity, usability and user friendliness of our system.",
          "arxiv_id": "2205.12352v1"
        },
        {
          "title": "Online Authentication Habits of Indian Users",
          "year": "2025-01",
          "abstract": "Passwords have been long used as the primary authentication method for web\nservices. Weak passwords used by the users have prompted the use of password\nmanagement tools and two-factor authentication to ensure better account\nsecurity. While prior studies have studied their adoption individually, none of\nthese studies focuses particularly on the Indian setting, which is culturally\nand economically different from the countries in which these studies have been\ndone in the past. To this end, we conducted a survey with 90 participants\nresiding in India to better understand the mindset of people on using password\nmanagers and two-factor authentication (2FA).\n  Our findings suggest that a majority of the participants have used 2FA and\npassword managers in some form, although they are sometimes unaware of their\nformal names. While many participants used some form of 2FA across all their\naccounts, browser-integrated and device-default password managers are\npredominantly utilized for less sensitive platforms such as e-commerce and\nsocial media rather than for more critical accounts like banking. The primary\nmotivation for using password managers is the convenience of auto-filling.\nHowever, some participants avoid using password managers due to a lack of trust\nin these tools. Notably, dedicated third-party applications show low adoption\nfor both password manager and 2FA.\n  Despite acknowledging the importance of secure password practices, many\nparticipants still reuse passwords across multiple accounts, prefer shorter\npasswords, and use commonly predictable password patterns. Overall, the study\nsuggests that Indians are more inclined to choose default settings,\nunderscoring the need for tailored strategies to improve user awareness and\nstrengthen password security practices.",
          "arxiv_id": "2501.14330v1"
        },
        {
          "title": "Skeptic: Automatic, Justified and Privacy-Preserving Password Composition Policy Selection",
          "year": "2020-07",
          "abstract": "The choice of password composition policy to enforce on a password-protected\nsystem represents a critical security decision, and has been shown to\nsignificantly affect the vulnerability of user-chosen passwords to guessing\nattacks. In practice, however, this choice is not usually rigorous or\njustifiable, with a tendency for system administrators to choose password\ncomposition policies based on intuition alone. In this work, we propose a novel\nmethodology that draws on password probability distributions constructed from\nlarge sets of real-world password data which have been filtered according to\nvarious password composition policies. Password probabilities are then\nredistributed to simulate different user password reselection behaviours in\norder to automatically determine the password composition policy that will\ninduce the distribution of user-chosen passwords with the greatest uniformity,\na metric which we show to be a useful proxy to measure overall resistance to\npassword guessing attacks. Further, we show that by fitting power-law equations\nto the password probability distributions we generate, we can justify our\nchoice of password composition policy without any direct access to user\npassword data. Finally, we present Skeptic -- a software toolkit that\nimplements this methodology, including a DSL to enable system administrators\nwith no background in password security to compare and rank password\ncomposition policies without resorting to expensive and time-consuming user\nstudies. Drawing on 205,176,321 pass words across 3 datasets, we lend validity\nto our approach by demonstrating that the results we obtain align closely with\nfindings from a previous empirical study into password composition policy\neffectiveness.",
          "arxiv_id": "2007.03809v2"
        }
      ],
      "31": [
        {
          "title": "The evolving ecosystem of COVID-19 contact tracing applications",
          "year": "2021-03",
          "abstract": "Since the outbreak of the novel coronavirus, COVID-19, there has been\nincreased interest in the use of digital contact tracing as a means of stopping\nchains of viral transmission, provoking alarm from privacy advocates.\nConcerning the ethics of this technology, recent studies have predominantly\nfocused on (1) the formation of guidelines for ethical contact tracing, (2) the\nanalysis of specific implementations, or (3) the review of a select number of\ncontact tracing applications and their relevant privacy or ethical\nimplications. In this study, we provide a comprehensive survey of the evolving\necosystem of COVID-19 tracing applications, examining 152 contact tracing\napplications and assessing the extent to which they comply with existing\nguidelines for ethical contact tracing. The assessed criteria cover areas\nincluding data collection and storage, transparency and consent, and whether\nthe implementation is open source. We find that although many apps released\nearly in the pandemic fell short of best practices, apps released more\nrecently, following the publication of the Apple/Google exposure notification\nprotocol, have tended to be more closely aligned with ethical contact tracing\nprinciples. This dataset will be publicly available and may be updated as the\npandemic continues.",
          "arxiv_id": "2103.10585v1"
        },
        {
          "title": "Privacy Risk and Preservation For COVID-19 Contact Tracing Apps",
          "year": "2020-06",
          "abstract": "Contact tracing in the COVID-19 pandemic is key to prevent the further spread\nof COVID-19. Countries and regions around the world have developed and deployed\nor are considering adopting contact-tracing software or mobile apps. While\ncontact tracing apps and software play an important role in the pandemic, red\nflags have been raised regarding the privacy risk associated with contact\ntracing. In this short paper, we provide an overview on the GPS and Bluetooth\nbased contact-tracing apps in the framework of both centralized and\ndecentralized models, examine the associated privacy risk and the effectiveness\nof the privacy-preserving measures adopted in different apps.",
          "arxiv_id": "2006.15433v1"
        },
        {
          "title": "A Note on Cryptographic Algorithms for Private Data Analysis in Contact Tracing Applications",
          "year": "2020-05",
          "abstract": "Contact tracing is an important measure to counter the COVID-19 pandemic. In\nthe early phase, many countries employed manual contact tracing to contain the\nrate of disease spread, however it has many issues. The manual approach is\ncumbersome, time consuming and also requires active participation of a large\nnumber of people to realize it. In order to overcome these drawbacks, digital\ncontact tracing has been proposed that typically involves deploying a contact\ntracing application on people's mobile devices which can track their movements\nand close social interactions. While studies suggest that digital contact\ntracing is more effective than manual contact tracing, it has been observed\nthat higher adoption rates of the contact tracing app may result in a better\ncontrolled epidemic. This also increases the confidence in the accuracy of the\ncollected data and the subsequent analytics. One key reason for low adoption\nrate of contact tracing applications is the concern about individual privacy.\nIn fact, several studies report that contact tracing applications deployed in\nmultiple countries are not privacy friendly and have potential to be used for\nmass surveillance by the concerned governments. Hence, privacy respecting\ncontact tracing application is the need of the hour that can lead to highly\neffective, efficient contact tracing. As part of this study, we focus on\nvarious cryptographic techniques that can help in addressing the Private Set\nIntersection problem which lies at the heart of privacy respecting contact\ntracing. We analyze the computation and communication complexities of these\ntechniques under the typical client-server architecture utilized by contact\ntracing applications. Further we evaluate those computation and communication\ncomplexity expressions for India scenario and thus identify cryptographic\ntechniques that can be more suitably deployed there.",
          "arxiv_id": "2005.10634v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:24:31Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}