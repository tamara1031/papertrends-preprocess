{
  "topics": {
    "data": {
      "0": {
        "name": "0_rendering_scene_NeRF_view",
        "keywords": [
          [
            "rendering",
            0.02515838062165218
          ],
          [
            "scene",
            0.018467146600650013
          ],
          [
            "NeRF",
            0.01755287906720462
          ],
          [
            "view",
            0.01746201629659234
          ],
          [
            "Gaussian",
            0.016923446941643716
          ],
          [
            "3D",
            0.014783927306140478
          ],
          [
            "scenes",
            0.014341919233054175
          ],
          [
            "novel",
            0.01406055932842444
          ],
          [
            "neural",
            0.012849153182510065
          ],
          [
            "method",
            0.012673885441626178
          ]
        ],
        "count": 771
      },
      "1": {
        "name": "1_3D_human_facial_face",
        "keywords": [
          [
            "3D",
            0.021787064553990123
          ],
          [
            "human",
            0.02080271937192134
          ],
          [
            "facial",
            0.018944329576436342
          ],
          [
            "face",
            0.01742478332584172
          ],
          [
            "body",
            0.017199704465170752
          ],
          [
            "model",
            0.014583549251739398
          ],
          [
            "pose",
            0.014001079913185976
          ],
          [
            "method",
            0.0128061532144773
          ],
          [
            "head",
            0.011953446648092548
          ],
          [
            "high",
            0.011466125148204151
          ]
        ],
        "count": 659
      },
      "2": {
        "name": "2_3D_generation_models_model",
        "keywords": [
          [
            "3D",
            0.04319798802748176
          ],
          [
            "generation",
            0.02361662043421835
          ],
          [
            "models",
            0.016370561864802746
          ],
          [
            "model",
            0.015898609088837064
          ],
          [
            "diffusion",
            0.014905028075243661
          ],
          [
            "generative",
            0.014583133134340268
          ],
          [
            "text",
            0.013318120855478706
          ],
          [
            "texture",
            0.012932563459926303
          ],
          [
            "image",
            0.012319488947584266
          ],
          [
            "shape",
            0.012121094820147402
          ]
        ],
        "count": 417
      },
      "3": {
        "name": "3_image_text_models_images",
        "keywords": [
          [
            "image",
            0.03774695409204441
          ],
          [
            "text",
            0.023371971504734905
          ],
          [
            "models",
            0.02193089186678519
          ],
          [
            "images",
            0.02130694424014541
          ],
          [
            "diffusion",
            0.018325637218019965
          ],
          [
            "model",
            0.01712700787455548
          ],
          [
            "generation",
            0.015641966747935096
          ],
          [
            "editing",
            0.01415694453822274
          ],
          [
            "Image",
            0.013052199458337055
          ],
          [
            "Diffusion",
            0.012811335828445026
          ]
        ],
        "count": 370
      },
      "4": {
        "name": "4_data_visualization_visual_analysis",
        "keywords": [
          [
            "data",
            0.045183346285774625
          ],
          [
            "visualization",
            0.04444800832365036
          ],
          [
            "visual",
            0.02264416985598528
          ],
          [
            "analysis",
            0.02011493525250749
          ],
          [
            "visualizations",
            0.018108370474304657
          ],
          [
            "study",
            0.013017916782931823
          ],
          [
            "design",
            0.012805325252271356
          ],
          [
            "Visualization",
            0.011569410969894308
          ],
          [
            "dimensional",
            0.01112969889333025
          ],
          [
            "user",
            0.010942144873452212
          ]
        ],
        "count": 308
      },
      "5": {
        "name": "5_point_shape_surface_3D",
        "keywords": [
          [
            "point",
            0.026566142780502417
          ],
          [
            "shape",
            0.02598618219624002
          ],
          [
            "surface",
            0.0245264773983989
          ],
          [
            "3D",
            0.020019240119858898
          ],
          [
            "neural",
            0.019498173895191415
          ],
          [
            "implicit",
            0.015910974676132228
          ],
          [
            "learning",
            0.015580908438823626
          ],
          [
            "shapes",
            0.015461852587953127
          ],
          [
            "reconstruction",
            0.013929176167947583
          ],
          [
            "mesh",
            0.013829487276312346
          ]
        ],
        "count": 241
      },
      "6": {
        "name": "6_VR_virtual_user_Reality",
        "keywords": [
          [
            "VR",
            0.03129299659076498
          ],
          [
            "virtual",
            0.02627875344006922
          ],
          [
            "user",
            0.021987380767461887
          ],
          [
            "Reality",
            0.02051275916780251
          ],
          [
            "reality",
            0.01972757471129066
          ],
          [
            "AR",
            0.018331195739458322
          ],
          [
            "Virtual",
            0.015881509114102008
          ],
          [
            "users",
            0.014089427191301301
          ],
          [
            "study",
            0.01367484448985619
          ],
          [
            "data",
            0.012494858698871148
          ]
        ],
        "count": 209
      },
      "7": {
        "name": "7_mesh_surface_meshes_algorithm",
        "keywords": [
          [
            "mesh",
            0.03516445991295034
          ],
          [
            "surface",
            0.02350862128006203
          ],
          [
            "meshes",
            0.022608105086265616
          ],
          [
            "algorithm",
            0.019635950891735705
          ],
          [
            "method",
            0.01792840473715667
          ],
          [
            "medial",
            0.017584677283864803
          ],
          [
            "surfaces",
            0.014384622528916254
          ],
          [
            "triangle",
            0.013092224260454933
          ],
          [
            "points",
            0.01242932373760789
          ],
          [
            "geometric",
            0.011517365490869883
          ]
        ],
        "count": 170
      },
      "8": {
        "name": "8_motion_motions_Motion_human",
        "keywords": [
          [
            "motion",
            0.08976226781417461
          ],
          [
            "motions",
            0.030591017295100196
          ],
          [
            "Motion",
            0.029710502624254946
          ],
          [
            "human",
            0.024772883428212142
          ],
          [
            "human motion",
            0.01843395116135872
          ],
          [
            "generation",
            0.017082921345414722
          ],
          [
            "action",
            0.016681778656260787
          ],
          [
            "motion generation",
            0.016478163212332815
          ],
          [
            "text",
            0.014917367211151762
          ],
          [
            "style",
            0.014565304907498891
          ]
        ],
        "count": 146
      },
      "9": {
        "name": "9_simulation_method_fluid_time",
        "keywords": [
          [
            "simulation",
            0.026407816599918366
          ],
          [
            "method",
            0.022621831727378628
          ],
          [
            "fluid",
            0.017443299661789518
          ],
          [
            "time",
            0.016071262137452882
          ],
          [
            "simulations",
            0.015029469474825027
          ],
          [
            "contact",
            0.013301806804222067
          ],
          [
            "model",
            0.01298008122947851
          ],
          [
            "Newton",
            0.012589243747158566
          ],
          [
            "approach",
            0.01247216524163874
          ],
          [
            "order",
            0.011521348232959439
          ]
        ],
        "count": 145
      },
      "10": {
        "name": "10_motion_learning_character_policy",
        "keywords": [
          [
            "motion",
            0.042878502144913976
          ],
          [
            "learning",
            0.026118312727780455
          ],
          [
            "character",
            0.024871795834600138
          ],
          [
            "policy",
            0.02460220296586192
          ],
          [
            "control",
            0.023053328980908526
          ],
          [
            "motions",
            0.022005513642895345
          ],
          [
            "skills",
            0.019706472607238016
          ],
          [
            "human",
            0.01795699302795002
          ],
          [
            "physics",
            0.017298600811753446
          ],
          [
            "tasks",
            0.016272281846432306
          ]
        ],
        "count": 116
      },
      "11": {
        "name": "11_design_structures_printing_fabrication",
        "keywords": [
          [
            "design",
            0.03895927142719287
          ],
          [
            "structures",
            0.02542778735596957
          ],
          [
            "printing",
            0.024747142888972524
          ],
          [
            "fabrication",
            0.019782429267889774
          ],
          [
            "method",
            0.01916190683140373
          ],
          [
            "3D",
            0.017312646862608232
          ],
          [
            "surfaces",
            0.016785003001157546
          ],
          [
            "material",
            0.016645702475277326
          ],
          [
            "manufacturing",
            0.01664549605587338
          ],
          [
            "optimization",
            0.016622613384665603
          ]
        ],
        "count": 106
      },
      "12": {
        "name": "12_ray_tracing_GPU_rendering",
        "keywords": [
          [
            "ray",
            0.03723924745848248
          ],
          [
            "tracing",
            0.03175986361206845
          ],
          [
            "GPU",
            0.02840256423795348
          ],
          [
            "rendering",
            0.02637481269699109
          ],
          [
            "ray tracing",
            0.025207964621949637
          ],
          [
            "data",
            0.020515134823513463
          ],
          [
            "hardware",
            0.01888018211110893
          ],
          [
            "time",
            0.01796466340762899
          ],
          [
            "performance",
            0.017692201316536236
          ],
          [
            "volume",
            0.01562251667803541
          ]
        ],
        "count": 97
      }
    },
    "correlations": [
      [
        1.0,
        -0.6922915918974059,
        -0.6224239794636157,
        -0.6687579679007211,
        -0.6976955941810705,
        -0.6547482969831948,
        -0.6917717404201748,
        -0.6908699876748949,
        -0.708820341871109,
        -0.6403458286501127,
        -0.7161493855482799,
        -0.7358949278867307,
        -0.43201830005498554
      ],
      [
        -0.6922915918974059,
        1.0,
        -0.5905378891018406,
        -0.6705382820784862,
        -0.7206349541858479,
        -0.6803923314940281,
        -0.7079266040205752,
        -0.7036664142600194,
        -0.6506450251895827,
        -0.673557697636358,
        -0.6946281906738108,
        -0.7378783823319528,
        -0.7325156361188372
      ],
      [
        -0.6224239794636157,
        -0.5905378891018406,
        1.0,
        -0.506451282826745,
        -0.6944160735524343,
        -0.541061913844826,
        -0.7024442689364276,
        -0.6557523452624544,
        -0.7099593364192546,
        -0.6388014109312772,
        -0.7003203803510356,
        -0.7030665286547666,
        -0.71740736148643
      ],
      [
        -0.6687579679007211,
        -0.6705382820784862,
        -0.506451282826745,
        1.0,
        -0.6839680316931072,
        -0.7085883177200838,
        -0.722488418813289,
        -0.6928297443754632,
        -0.7084776898108841,
        -0.6176049574359979,
        -0.7123023659349393,
        -0.7261412120503621,
        -0.719203948584085
      ],
      [
        -0.6976955941810705,
        -0.7206349541858479,
        -0.6944160735524343,
        -0.6839680316931072,
        1.0,
        -0.6941426320240118,
        -0.6795862113253835,
        -0.564620747450101,
        -0.7216822528362994,
        -0.5644160624306584,
        -0.7198598612648732,
        -0.6699688836097539,
        -0.6883440740886806
      ],
      [
        -0.6547482969831948,
        -0.6803923314940281,
        -0.541061913844826,
        -0.7085883177200838,
        -0.6941426320240118,
        1.0,
        -0.7328483357683248,
        -0.5689947678627814,
        -0.7295180029165136,
        -0.6318106303108986,
        -0.7309175655438868,
        -0.7067132002266342,
        -0.7087018939154055
      ],
      [
        -0.6917717404201748,
        -0.7079266040205752,
        -0.7024442689364276,
        -0.722488418813289,
        -0.6795862113253835,
        -0.7328483357683248,
        1.0,
        -0.7252980975186969,
        -0.7120872337177402,
        -0.7057270036195058,
        -0.7271276526310204,
        -0.7019257642805401,
        -0.71019016196426
      ],
      [
        -0.6908699876748949,
        -0.7036664142600194,
        -0.6557523452624544,
        -0.6928297443754632,
        -0.564620747450101,
        -0.5689947678627814,
        -0.7252980975186969,
        1.0,
        -0.7238203821191525,
        -0.4727894592499755,
        -0.7204953261401376,
        -0.7093272276764517,
        -0.68952147011087
      ],
      [
        -0.708820341871109,
        -0.6506450251895827,
        -0.7099593364192546,
        -0.7084776898108841,
        -0.7216822528362994,
        -0.7295180029165136,
        -0.7120872337177402,
        -0.7238203821191525,
        1.0,
        -0.6914213002576268,
        -0.39795297616690556,
        -0.7351140087439414,
        -0.7377463224705338
      ],
      [
        -0.6403458286501127,
        -0.673557697636358,
        -0.6388014109312772,
        -0.6176049574359979,
        -0.5644160624306584,
        -0.6318106303108986,
        -0.7057270036195058,
        -0.4727894592499755,
        -0.6914213002576268,
        1.0,
        -0.6777551843456142,
        -0.6896540541506755,
        -0.6782784937785231
      ],
      [
        -0.7161493855482799,
        -0.6946281906738108,
        -0.7003203803510356,
        -0.7123023659349393,
        -0.7198598612648732,
        -0.7309175655438868,
        -0.7271276526310204,
        -0.7204953261401376,
        -0.39795297616690556,
        -0.6777551843456142,
        1.0,
        -0.7228215467681325,
        -0.7373929408789747
      ],
      [
        -0.7358949278867307,
        -0.7378783823319528,
        -0.7030665286547666,
        -0.7261412120503621,
        -0.6699688836097539,
        -0.7067132002266342,
        -0.7019257642805401,
        -0.7093272276764517,
        -0.7351140087439414,
        -0.6896540541506755,
        -0.7228215467681325,
        1.0,
        -0.7362295080564288
      ],
      [
        -0.43201830005498554,
        -0.7325156361188372,
        -0.71740736148643,
        -0.719203948584085,
        -0.6883440740886806,
        -0.7087018939154055,
        -0.71019016196426,
        -0.68952147011087,
        -0.7377463224705338,
        -0.6782784937785231,
        -0.7373929408789747,
        -0.7362295080564288,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        3,
        0,
        1,
        2,
        1,
        1,
        1,
        0,
        0,
        3,
        0,
        1,
        3
      ],
      "2020-02": [
        12,
        1,
        6,
        1,
        9,
        5,
        3,
        1,
        3,
        0,
        0,
        1,
        1
      ],
      "2020-03": [
        7,
        3,
        5,
        6,
        3,
        11,
        0,
        1,
        2,
        1,
        4,
        4,
        1
      ],
      "2020-04": [
        11,
        2,
        4,
        6,
        7,
        8,
        1,
        0,
        4,
        6,
        2,
        3,
        3
      ],
      "2020-05": [
        10,
        1,
        3,
        5,
        10,
        4,
        3,
        1,
        6,
        6,
        2,
        4,
        2
      ],
      "2020-06": [
        13,
        0,
        5,
        6,
        3,
        7,
        4,
        3,
        6,
        4,
        1,
        7,
        2
      ],
      "2020-07": [
        16,
        1,
        2,
        6,
        12,
        6,
        3,
        1,
        4,
        9,
        2,
        1,
        2
      ],
      "2020-08": [
        13,
        4,
        2,
        8,
        14,
        12,
        3,
        1,
        8,
        10,
        1,
        3,
        2
      ],
      "2020-09": [
        15,
        3,
        7,
        3,
        20,
        5,
        3,
        2,
        3,
        12,
        1,
        3,
        4
      ],
      "2020-10": [
        6,
        5,
        2,
        4,
        6,
        6,
        6,
        3,
        3,
        4,
        0,
        3,
        2
      ],
      "2020-11": [
        12,
        1,
        4,
        4,
        11,
        5,
        8,
        1,
        6,
        7,
        3,
        3,
        2
      ],
      "2020-12": [
        12,
        2,
        2,
        9,
        4,
        4,
        2,
        2,
        7,
        7,
        1,
        4,
        3
      ],
      "2021-01": [
        7,
        1,
        2,
        3,
        3,
        0,
        3,
        2,
        4,
        2,
        1,
        3,
        3
      ],
      "2021-02": [
        9,
        0,
        3,
        1,
        3,
        3,
        4,
        2,
        5,
        4,
        1,
        3,
        1
      ],
      "2021-03": [
        11,
        2,
        7,
        6,
        12,
        11,
        3,
        4,
        7,
        7,
        3,
        8,
        2
      ],
      "2021-04": [
        10,
        4,
        6,
        8,
        6,
        13,
        0,
        2,
        4,
        8,
        3,
        3,
        2
      ],
      "2021-05": [
        9,
        2,
        5,
        8,
        2,
        7,
        2,
        1,
        8,
        2,
        1,
        4,
        2
      ],
      "2021-06": [
        18,
        4,
        2,
        6,
        8,
        11,
        2,
        3,
        6,
        4,
        0,
        7,
        3
      ],
      "2021-07": [
        9,
        4,
        2,
        7,
        10,
        8,
        3,
        3,
        4,
        4,
        0,
        6,
        3
      ],
      "2021-08": [
        10,
        2,
        3,
        5,
        14,
        7,
        2,
        1,
        4,
        9,
        0,
        4,
        3
      ],
      "2021-09": [
        11,
        2,
        7,
        7,
        7,
        5,
        6,
        2,
        3,
        3,
        2,
        5,
        1
      ],
      "2021-10": [
        11,
        2,
        5,
        9,
        7,
        8,
        2,
        1,
        5,
        9,
        0,
        3,
        3
      ],
      "2021-11": [
        20,
        8,
        5,
        8,
        3,
        13,
        3,
        2,
        5,
        7,
        0,
        1,
        5
      ],
      "2021-12": [
        21,
        7,
        5,
        18,
        4,
        7,
        0,
        1,
        6,
        8,
        0,
        4,
        4
      ],
      "2022-01": [
        13,
        4,
        4,
        3,
        5,
        11,
        5,
        2,
        4,
        4,
        2,
        3,
        5
      ],
      "2022-02": [
        16,
        1,
        0,
        8,
        2,
        4,
        4,
        3,
        9,
        11,
        0,
        3,
        5
      ],
      "2022-03": [
        14,
        2,
        9,
        12,
        9,
        2,
        2,
        1,
        10,
        9,
        4,
        2,
        5
      ],
      "2022-04": [
        9,
        3,
        2,
        8,
        9,
        4,
        3,
        2,
        5,
        6,
        2,
        5,
        3
      ],
      "2022-05": [
        23,
        1,
        2,
        11,
        4,
        14,
        5,
        3,
        11,
        12,
        4,
        3,
        3
      ],
      "2022-06": [
        12,
        1,
        3,
        5,
        8,
        6,
        4,
        4,
        10,
        10,
        3,
        2,
        4
      ],
      "2022-07": [
        20,
        2,
        4,
        6,
        12,
        7,
        2,
        2,
        5,
        9,
        2,
        2,
        4
      ],
      "2022-08": [
        24,
        2,
        5,
        6,
        15,
        3,
        1,
        5,
        3,
        4,
        2,
        5,
        5
      ],
      "2022-09": [
        10,
        2,
        6,
        5,
        17,
        7,
        6,
        0,
        10,
        7,
        1,
        2,
        1
      ],
      "2022-10": [
        11,
        4,
        3,
        14,
        7,
        8,
        5,
        3,
        5,
        12,
        2,
        1,
        11
      ],
      "2022-11": [
        15,
        2,
        8,
        13,
        8,
        9,
        4,
        1,
        6,
        9,
        1,
        2,
        4
      ],
      "2022-12": [
        16,
        4,
        4,
        11,
        3,
        9,
        4,
        3,
        11,
        10,
        0,
        1,
        2
      ],
      "2023-01": [
        21,
        2,
        1,
        3,
        6,
        5,
        5,
        0,
        8,
        7,
        1,
        2,
        1
      ],
      "2023-02": [
        12,
        4,
        5,
        14,
        6,
        5,
        4,
        2,
        9,
        6,
        0,
        3,
        1
      ],
      "2023-03": [
        26,
        5,
        8,
        19,
        11,
        7,
        5,
        2,
        26,
        12,
        2,
        4,
        6
      ],
      "2023-04": [
        26,
        5,
        7,
        13,
        5,
        6,
        1,
        0,
        8,
        6,
        4,
        4,
        4
      ],
      "2023-05": [
        18,
        6,
        7,
        10,
        7,
        6,
        3,
        5,
        12,
        13,
        3,
        4,
        13
      ],
      "2023-06": [
        15,
        2,
        5,
        18,
        12,
        8,
        3,
        2,
        9,
        10,
        1,
        6,
        4
      ],
      "2023-07": [
        11,
        3,
        4,
        10,
        13,
        4,
        5,
        3,
        7,
        8,
        0,
        3,
        1
      ],
      "2023-08": [
        27,
        3,
        9,
        7,
        22,
        8,
        5,
        4,
        17,
        4,
        2,
        1,
        5
      ],
      "2023-09": [
        20,
        4,
        7,
        15,
        12,
        6,
        3,
        2,
        13,
        11,
        2,
        7,
        6
      ],
      "2023-10": [
        27,
        5,
        12,
        13,
        8,
        6,
        3,
        3,
        24,
        5,
        3,
        4,
        7
      ],
      "2023-11": [
        36,
        2,
        13,
        23,
        4,
        6,
        7,
        4,
        13,
        9,
        1,
        0,
        6
      ],
      "2023-12": [
        30,
        6,
        13,
        16,
        9,
        7,
        1,
        4,
        21,
        10,
        1,
        4,
        11
      ],
      "2024-01": [
        26,
        3,
        14,
        14,
        7,
        7,
        8,
        1,
        7,
        6,
        1,
        3,
        2
      ],
      "2024-02": [
        23,
        5,
        7,
        16,
        6,
        3,
        6,
        1,
        5,
        9,
        0,
        6,
        2
      ],
      "2024-03": [
        31,
        3,
        13,
        18,
        16,
        7,
        4,
        8,
        12,
        7,
        1,
        3,
        1
      ],
      "2024-04": [
        21,
        5,
        6,
        14,
        14,
        9,
        4,
        1,
        11,
        11,
        2,
        1,
        3
      ],
      "2024-05": [
        28,
        8,
        20,
        16,
        2,
        6,
        7,
        5,
        10,
        18,
        2,
        6,
        9
      ],
      "2024-06": [
        49,
        2,
        11,
        18,
        12,
        8,
        4,
        3,
        13,
        10,
        2,
        6,
        3
      ],
      "2024-07": [
        32,
        4,
        8,
        11,
        20,
        8,
        4,
        2,
        10,
        3,
        1,
        3,
        3
      ],
      "2024-08": [
        16,
        3,
        10,
        8,
        18,
        5,
        2,
        9,
        7,
        7,
        5,
        4,
        1
      ],
      "2024-09": [
        34,
        4,
        17,
        12,
        9,
        14,
        6,
        6,
        17,
        13,
        3,
        0,
        6
      ],
      "2024-10": [
        39,
        4,
        10,
        12,
        11,
        5,
        4,
        4,
        13,
        13,
        0,
        5,
        9
      ],
      "2024-11": [
        26,
        3,
        19,
        13,
        4,
        7,
        2,
        4,
        17,
        5,
        1,
        2,
        2
      ],
      "2024-12": [
        38,
        2,
        14,
        19,
        7,
        7,
        4,
        4,
        21,
        17,
        1,
        2,
        7
      ],
      "2025-01": [
        17,
        2,
        7,
        14,
        10,
        4,
        4,
        3,
        5,
        10,
        1,
        3,
        3
      ],
      "2025-02": [
        25,
        7,
        14,
        25,
        13,
        7,
        3,
        3,
        15,
        11,
        1,
        4,
        11
      ],
      "2025-03": [
        33,
        6,
        11,
        29,
        11,
        8,
        4,
        8,
        23,
        12,
        2,
        5,
        3
      ],
      "2025-04": [
        39,
        3,
        12,
        15,
        12,
        7,
        9,
        4,
        17,
        7,
        2,
        7,
        6
      ],
      "2025-05": [
        38,
        6,
        16,
        15,
        6,
        10,
        4,
        4,
        16,
        16,
        6,
        2,
        3
      ],
      "2025-06": [
        48,
        3,
        9,
        21,
        10,
        9,
        5,
        3,
        17,
        16,
        0,
        5,
        13
      ],
      "2025-07": [
        38,
        1,
        13,
        13,
        27,
        6,
        3,
        3,
        10,
        7,
        0,
        1,
        3
      ],
      "2025-08": [
        38,
        4,
        10,
        24,
        9,
        9,
        4,
        1,
        13,
        16,
        1,
        6,
        6
      ],
      "2025-09": [
        11,
        2,
        4,
        7,
        4,
        1,
        1,
        4,
        6,
        1,
        0,
        3,
        1
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Virtual Memory for 3D Gaussian Splatting",
          "year": "2025-06",
          "abstract": "3D Gaussian Splatting represents a breakthrough in the field of novel view\nsynthesis. It establishes Gaussians as core rendering primitives for highly\naccurate real-world environment reconstruction. Recent advances have\ndrastically increased the size of scenes that can be created. In this work, we\npresent a method for rendering large and complex 3D Gaussian Splatting scenes\nusing virtual memory. By leveraging well-established virtual memory and virtual\ntexturing techniques, our approach efficiently identifies visible Gaussians and\ndynamically streams them to the GPU just in time for real-time rendering.\nSelecting only the necessary Gaussians for both storage and rendering results\nin reduced memory usage and effectively accelerates rendering, especially for\nhighly complex scenes. Furthermore, we demonstrate how level of detail can be\nintegrated into our proposed method to further enhance rendering speed for\nlarge-scale scenes. With an optimized implementation, we highlight key\npractical considerations and thoroughly evaluate the proposed technique and its\nimpact on desktop and mobile devices.",
          "arxiv_id": "2506.19415v1"
        },
        {
          "title": "Real-Time Scene Reconstruction using Light Field Probes",
          "year": "2025-07",
          "abstract": "Reconstructing photo-realistic large-scale scenes from images, for example at\ncity scale, is a long-standing problem in computer graphics. Neural rendering\nis an emerging technique that enables photo-realistic image synthesis from\npreviously unobserved viewpoints; however, state-of-the-art neural rendering\nmethods have difficulty efficiently rendering a high complex large-scale scene\nbecause these methods typically trade scene size, fidelity, and rendering speed\nfor quality. The other stream of techniques utilizes scene geometries for\nreconstruction. But the cost of building and maintaining a large set of\ngeometry data increases as scene size grows. Our work explores novel view\nsynthesis methods that efficiently reconstruct complex scenes without explicit\nuse of scene geometries. Specifically, given sparse images of the scene\n(captured from the real world), we reconstruct intermediate, multi-scale,\nimplicit representations of scene geometries. In this way, our method avoids\nexplicitly relying on scene geometry, significantly reducing the computational\ncost of maintaining large 3D data. Unlike current methods, we reconstruct the\nscene using a probe data structure. Probe data hold highly accurate depth\ninformation of dense data points, enabling the reconstruction of highly complex\nscenes. By reconstructing the scene using probe data, the rendering cost is\nindependent of the complexity of the scene. As such, our approach combines\ngeometry reconstruction and novel view synthesis. Moreover, when rendering\nlarge-scale scenes, compressing and streaming probe data is more efficient than\nusing explicit scene geometry. Therefore, our neural representation approach\ncan potentially be applied to virtual reality (VR) and augmented reality (AR)\napplications.",
          "arxiv_id": "2507.14624v1"
        },
        {
          "title": "Recent Advances in 3D Gaussian Splatting",
          "year": "2024-03",
          "abstract": "The emergence of 3D Gaussian Splatting (3DGS) has greatly accelerated the\nrendering speed of novel view synthesis. Unlike neural implicit representations\nlike Neural Radiance Fields (NeRF) that represent a 3D scene with position and\nviewpoint-conditioned neural networks, 3D Gaussian Splatting utilizes a set of\nGaussian ellipsoids to model the scene so that efficient rendering can be\naccomplished by rasterizing Gaussian ellipsoids into images. Apart from the\nfast rendering speed, the explicit representation of 3D Gaussian Splatting\nfacilitates editing tasks like dynamic reconstruction, geometry editing, and\nphysical simulation. Considering the rapid change and growing number of works\nin this field, we present a literature review of recent 3D Gaussian Splatting\nmethods, which can be roughly classified into 3D reconstruction, 3D editing,\nand other downstream applications by functionality. Traditional point-based\nrendering methods and the rendering formulation of 3D Gaussian Splatting are\nalso illustrated for a better understanding of this technique. This survey aims\nto help beginners get into this field quickly and provide experienced\nresearchers with a comprehensive overview, which can stimulate the future\ndevelopment of the 3D Gaussian Splatting representation.",
          "arxiv_id": "2403.11134v2"
        }
      ],
      "1": [
        {
          "title": "3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head",
          "year": "2021-04",
          "abstract": "Impressive progress has been made in audio-driven 3D facial animation\nrecently, but synthesizing 3D talking-head with rich emotion is still unsolved.\nThis is due to the lack of 3D generative models and available 3D emotional\ndataset with synchronized audios. To address this, we introduce 3D-TalkEmo, a\ndeep neural network that generates 3D talking head animation with various\nemotions. We also create a large 3D dataset with synchronized audios and\nvideos, rich corpus, as well as various emotion states of different persons\nwith the sophisticated 3D face reconstruction methods. In the emotion\ngeneration network, we propose a novel 3D face representation structure -\ngeometry map by classical multi-dimensional scaling analysis. It maps the\ncoordinates of vertices on a 3D face to a canonical image plane, while\npreserving the vertex-to-vertex geodesic distance metric in a least-square\nsense. This maintains the adjacency relationship of each vertex and holds the\neffective convolutional structure for the 3D facial surface. Taking a neutral\n3D mesh and a speech signal as inputs, the 3D-TalkEmo is able to generate vivid\nfacial animations. Moreover, it provides access to change the emotion state of\nthe animated speaker.\n  We present extensive quantitative and qualitative evaluation of our method,\nin addition to user studies, demonstrating the generated talking-heads of\nsignificantly higher quality compared to previous state-of-the-art methods.",
          "arxiv_id": "2104.12051v1"
        },
        {
          "title": "FaceScape: 3D Facial Dataset and Benchmark for Single-View 3D Face Reconstruction",
          "year": "2021-11",
          "abstract": "In this paper, we present a large-scale detailed 3D face dataset, FaceScape,\nand the corresponding benchmark to evaluate single-view facial 3D\nreconstruction. By training on FaceScape data, a novel algorithm is proposed to\npredict elaborate riggable 3D face models from a single image input. FaceScape\ndataset releases $16,940$ textured 3D faces, captured from $847$ subjects and\neach with $20$ specific expressions. The 3D models contain the pore-level\nfacial geometry that is also processed to be topologically uniform. These fine\n3D facial models can be represented as a 3D morphable model for coarse shapes\nand displacement maps for detailed geometry. Taking advantage of the\nlarge-scale and high-accuracy dataset, a novel algorithm is further proposed to\nlearn the expression-specific dynamic details using a deep neural network. The\nlearned relationship serves as the foundation of our 3D face prediction system\nfrom a single image input. Different from most previous methods, our predicted\n3D models are riggable with highly detailed geometry under different\nexpressions. We also use FaceScape data to generate the in-the-wild and\nin-the-lab benchmark to evaluate recent methods of single-view face\nreconstruction. The accuracy is reported and analyzed on the dimensions of\ncamera pose and focal length, which provides a faithful and comprehensive\nevaluation and reveals new challenges. The unprecedented dataset, benchmark,\nand code have been released at https://github.com/zhuhao-nju/facescape.",
          "arxiv_id": "2111.01082v2"
        },
        {
          "title": "DEGAS: Detailed Expressions on Full-Body Gaussian Avatars",
          "year": "2024-08",
          "abstract": "Although neural rendering has made significant advances in creating lifelike,\nanimatable full-body and head avatars, incorporating detailed expressions into\nfull-body avatars remains largely unexplored. We present DEGAS, the first 3D\nGaussian Splatting (3DGS)-based modeling method for full-body avatars with rich\nfacial expressions. Trained on multiview videos of a given subject, our method\nlearns a conditional variational autoencoder that takes both the body motion\nand facial expression as driving signals to generate Gaussian maps in the UV\nlayout. To drive the facial expressions, instead of the commonly used 3D\nMorphable Models (3DMMs) in 3D head avatars, we propose to adopt the expression\nlatent space trained solely on 2D portrait images, bridging the gap between 2D\ntalking faces and 3D avatars. Leveraging the rendering capability of 3DGS and\nthe rich expressiveness of the expression latent space, the learned avatars can\nbe reenacted to reproduce photorealistic rendering images with subtle and\naccurate facial expressions. Experiments on an existing dataset and our newly\nproposed dataset of full-body talking avatars demonstrate the efficacy of our\nmethod. We also propose an audio-driven extension of our method with the help\nof 2D talking faces, opening new possibilities for interactive AI agents.",
          "arxiv_id": "2408.10588v2"
        }
      ],
      "2": [
        {
          "title": "SketchDream: Sketch-based Text-to-3D Generation and Editing",
          "year": "2024-05",
          "abstract": "Existing text-based 3D generation methods generate attractive results but\nlack detailed geometry control. Sketches, known for their conciseness and\nexpressiveness, have contributed to intuitive 3D modeling but are confined to\nproducing texture-less mesh models within predefined categories. Integrating\nsketch and text simultaneously for 3D generation promises enhanced control over\ngeometry and appearance but faces challenges from 2D-to-3D translation\nambiguity and multi-modal condition integration. Moreover, further editing of\n3D models in arbitrary views will give users more freedom to customize their\nmodels. However, it is difficult to achieve high generation quality, preserve\nunedited regions, and manage proper interactions between shape components. To\nsolve the above issues, we propose a text-driven 3D content generation and\nediting method, SketchDream, which supports NeRF generation from given\nhand-drawn sketches and achieves free-view sketch-based local editing. To\ntackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view\nimage generation diffusion model, which leverages depth guidance to establish\nspatial correspondence. A 3D ControlNet with a 3D attention module is utilized\nto control multi-view images and ensure their 3D consistency. To support local\nediting, we further propose a coarse-to-fine editing approach: the coarse phase\nanalyzes component interactions and provides 3D masks to label edited regions,\nwhile the fine stage generates realistic results with refined details by local\nenhancement. Extensive experiments validate that our method generates\nhigher-quality results compared with a combination of 2D ControlNet and\nimage-to-3D generation techniques and achieves detailed control compared with\nexisting diffusion-based 3D editing approaches.",
          "arxiv_id": "2405.06461v2"
        },
        {
          "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
          "year": "2025-03",
          "abstract": "Diffusion models have achieved great success in generating 2D images.\nHowever, the quality and generalizability of 3D content generation remain\nlimited. State-of-the-art methods often require large-scale 3D assets for\ntraining, which are challenging to collect. In this work, we introduce\nKiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient\nframework for generating, editing, and enhancing 3D objects by repurposing a\nwell-trained 2D image diffusion model for 3D generation. Specifically, we\nfine-tune a diffusion model to generate ''3D Bundle Image'', a tiled\nrepresentation composed of multi-view images and their corresponding normal\nmaps. The normal maps are then used to reconstruct a 3D mesh, and the\nmulti-view images provide texture mapping, resulting in a complete 3D model.\nThis simple method effectively transforms the 3D generation problem into a 2D\nimage generation task, maximizing the utilization of knowledge in pretrained\ndiffusion models. Furthermore, we demonstrate that our Kiss3DGen model is\ncompatible with various diffusion model techniques, enabling advanced features\nsuch as 3D editing, mesh and texture enhancement, etc. Through extensive\nexperiments, we demonstrate the effectiveness of our approach, showcasing its\nability to produce high-quality 3D models efficiently.",
          "arxiv_id": "2503.01370v2"
        },
        {
          "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior",
          "year": "2023-12",
          "abstract": "Recently, 3D content creation from text prompts has demonstrated remarkable\nprogress by utilizing 2D and 3D diffusion models. While 3D diffusion models\nensure great multi-view consistency, their ability to generate high-quality and\ndiverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion\nmodels find a distillation approach that achieves excellent generalization and\nrich details without any 3D data. However, 2D lifting methods suffer from\ninherent view-agnostic ambiguity thereby leading to serious multi-face Janus\nissues, where text prompts fail to provide sufficient guidance to learn\ncoherent 3D results. Instead of retraining a costly viewpoint-aware model, we\nstudy how to fully exploit easily accessible coarse 3D knowledge to enhance the\nprompts and guide 2D lifting optimization for refinement. In this paper, we\npropose Sherpa3D, a new text-to-3D framework that achieves high-fidelity,\ngeneralizability, and geometric consistency simultaneously. Specifically, we\ndesign a pair of guiding strategies derived from the coarse 3D prior generated\nby the 3D diffusion model: a structural guidance for geometric fidelity and a\nsemantic guidance for 3D coherence. Employing the two types of guidance, the 2D\ndiffusion model enriches the 3D content with diversified and high-quality\nresults. Extensive experiments show the superiority of our Sherpa3D over the\nstate-of-the-art text-to-3D methods in terms of quality and 3D consistency.",
          "arxiv_id": "2312.06655v1"
        }
      ],
      "3": [
        {
          "title": "Zero-shot Image-to-Image Translation",
          "year": "2023-02",
          "abstract": "Large-scale text-to-image generative models have shown their remarkable\nability to synthesize diverse and high-quality images. However, it is still\nchallenging to directly apply these models for editing real images for two\nreasons. First, it is hard for users to come up with a perfect text prompt that\naccurately describes every visual detail in the input image. Second, while\nexisting models can introduce desirable changes in certain regions, they often\ndramatically alter the input content and introduce unexpected changes in\nunwanted regions. In this work, we propose pix2pix-zero, an image-to-image\ntranslation method that can preserve the content of the original image without\nmanual prompting. We first automatically discover editing directions that\nreflect desired edits in the text embedding space. To preserve the general\ncontent structure after editing, we further propose cross-attention guidance,\nwhich aims to retain the cross-attention maps of the input image throughout the\ndiffusion process. In addition, our method does not need additional training\nfor these edits and can directly use the existing pre-trained text-to-image\ndiffusion model. We conduct extensive experiments and show that our method\noutperforms existing and concurrent works for both real and synthetic image\nediting.",
          "arxiv_id": "2302.03027v1"
        },
        {
          "title": "Diffusion Self-Distillation for Zero-Shot Customized Image Generation",
          "year": "2024-11",
          "abstract": "Text-to-image diffusion models produce impressive results but are frustrating\ntools for artists who desire fine-grained control. For example, a common use\ncase is to create images of a specific instance in novel contexts, i.e.,\n\"identity-preserving generation\". This setting, along with many other tasks\n(e.g., relighting), is a natural fit for image+text-conditional generative\nmodels. However, there is insufficient high-quality paired data to train such a\nmodel directly. We propose Diffusion Self-Distillation, a method for using a\npre-trained text-to-image model to generate its own dataset for\ntext-conditioned image-to-image tasks. We first leverage a text-to-image\ndiffusion model's in-context generation ability to create grids of images and\ncurate a large paired dataset with the help of a Visual-Language Model. We then\nfine-tune the text-to-image model into a text+image-to-image model using the\ncurated paired dataset. We demonstrate that Diffusion Self-Distillation\noutperforms existing zero-shot methods and is competitive with per-instance\ntuning techniques on a wide range of identity-preservation generation tasks,\nwithout requiring test-time optimization.",
          "arxiv_id": "2411.18616v1"
        },
        {
          "title": "ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models",
          "year": "2023-05",
          "abstract": "Personalizing generative models offers a way to guide image generation with\nuser-provided references. Current personalization methods can invert an object\nor concept into the textual conditioning space and compose new natural\nsentences for text-to-image diffusion models. However, representing and editing\nspecific visual attributes such as material, style, and layout remains a\nchallenge, leading to a lack of disentanglement and editability. To address\nthis problem, we propose a novel approach that leverages the step-by-step\ngeneration process of diffusion models, which generate images from low to high\nfrequency information, providing a new perspective on representing, generating,\nand editing images. We develop the Prompt Spectrum Space P*, an expanded\ntextual conditioning space, and a new image representation method called\n\\sysname. ProSpect represents an image as a collection of inverted textual\ntoken embeddings encoded from per-stage prompts, where each prompt corresponds\nto a specific generation stage (i.e., a group of consecutive steps) of the\ndiffusion model. Experimental results demonstrate that P* and ProSpect offer\nbetter disentanglement and controllability compared to existing methods. We\napply ProSpect in various personalized attribute-aware image generation\napplications, such as image-guided or text-driven manipulations of materials,\nstyle, and layout, achieving previously unattainable results from a single\nimage input without fine-tuning the diffusion models. Our source code is\navailable athttps://github.com/zyxElsa/ProSpect.",
          "arxiv_id": "2305.16225v3"
        }
      ],
      "4": [
        {
          "title": "Immersive and Interactive Visualization of 3D Spatio-Temporal Data using a Space Time Hypercube",
          "year": "2022-06",
          "abstract": "We propose an extension of the well-known Space-Time Cube (STC) visualization\ntechnique in order to visualize time-varying 3D spatial data, taking advantage\nof the interaction capabilities of Virtual Reality (VR). The analysis of\nmultidimensional time-varying datasets, which size grows as recording and\nsimulating techniques advance, faces challenges on the representation and\nvisualization of dense data, as well as on the study of temporal variations.\nFirst, we propose the Space-Time Hypercube (STH) as an abstraction for 3D\ntemporal data, extended from the STC concept. Second, through the example of\nembryo development imaging dataset, we detail the construction and\nvisualization of a STC based on a user-driven projection of the spatial and\ntemporal information. This projection yields a 3D STC visualization, which can\nalso encode additional numerical and categorical data. Additionally, we propose\na set of tools allowing the user to filter and manipulate the 3D STC which\nbenefits from the visualization, exploration and interaction possibilities\noffered by VR. Finally, we evaluated the proposed visualization method in the\ncontext of the visualization of spatio-temporal biological data. Several\nbiology experts accompanied the application design to provide insight on how\nthe STC visualization could be used to explore such data. We report a user\nstudy (n=12) using non-expert users performing a set of exploration and query\ntasks to evaluate the system.",
          "arxiv_id": "2206.13213v1"
        },
        {
          "title": "To Measure What Isn't There -- Visual Exploration of Missingness Structures Using Quality Metrics",
          "year": "2025-05",
          "abstract": "This paper contributes a set of quality metrics for identification and visual\nanalysis of structured missingness in high-dimensional data. Missing values in\ndata are a frequent challenge in most data generating domains and may cause a\nrange of analysis issues. Structural missingness in data may indicate issues in\ndata collection and pre-processing, but may also highlight important data\ncharacteristics. While research into statistical methods for dealing with\nmissing data are mainly focusing on replacing missing values with plausible\nestimated values, visualization has great potential to support a more in-depth\nunderstanding of missingness structures in data. Nonetheless, while the\ninterest in missing data visualization has increased in the last decade, it is\nstill a relatively overlooked research topic with a comparably small number of\npublications, few of which address scalability issues. Efficient visual\nanalysis approaches are needed to enable exploration of missingness structures\nin large and high-dimensional data, and to support informed decision-making in\ncontext of potential data quality issues. This paper suggests a set of quality\nmetrics for identification of patterns of interest for understanding of\nstructural missingness in data. These quality metrics can be used as guidance\nin visual analysis, as demonstrated through a use case exploring structural\nmissingness in data from a real-life walking monitoring study. All supplemental\nmaterials for this paper are available at\nhttps://doi.org/10.25405/data.ncl.c.7741829.",
          "arxiv_id": "2505.23447v1"
        },
        {
          "title": "Time Series Information Visualization -- A Review of Approaches and Tools",
          "year": "2025-07",
          "abstract": "Time series data are prevalent across various domains and often encompass\nlarge datasets containing multiple time-dependent features in each sample.\nExploring time-varying data is critical for data science practitioners aiming\nto understand dynamic behaviors and discover periodic patterns and trends.\nHowever, the analysis of such data often requires sophisticated procedures and\ntools. Information visualization is a communication channel that leverages\nhuman perceptual abilities to transform abstract data into visual\nrepresentations. Visualization techniques have been successfully applied in the\ncontext of time series to enhance interpretability by graphically representing\nthe temporal evolution of data. The challenge for information visualization\ndevelopers lies in integrating a wide range of analytical tools into rich\nvisualization systems that can summarize complex datasets while clearly\ndescribing the impacts of the temporal component. Such systems enable data\nscientists to turn raw data into understandable and potentially useful\nknowledge. This review examines techniques and approaches designed for handling\ntime series data, guiding users through knowledge discovery processes based on\nvisual analysis. We also provide readers with theoretical insights and design\nguidelines for considering when developing comprehensive information\nvisualization approaches for time series, with a particular focus on time\nseries with multiple features. As a result, we highlight the challenges and\nfuture research directions to address open questions in the visualization of\ntime-dependent data.",
          "arxiv_id": "2507.14920v1"
        }
      ],
      "5": [
        {
          "title": "ANISE: Assembly-based Neural Implicit Surface rEconstruction",
          "year": "2022-05",
          "abstract": "We present ANISE, a method that reconstructs a 3D~shape from partial\nobservations (images or sparse point clouds) using a part-aware neural implicit\nshape representation. The shape is formulated as an assembly of neural implicit\nfunctions, each representing a different part instance. In contrast to previous\napproaches, the prediction of this representation proceeds in a coarse-to-fine\nmanner. Our model first reconstructs a structural arrangement of the shape in\nthe form of geometric transformations of its part instances. Conditioned on\nthem, the model predicts part latent codes encoding their surface geometry.\nReconstructions can be obtained in two ways: (i) by directly decoding the part\nlatent codes to part implicit functions, then combining them into the final\nshape; or (ii) by using part latents to retrieve similar part instances in a\npart database and assembling them in a single shape. We demonstrate that, when\nperforming reconstruction by decoding part representations into implicit\nfunctions, our method achieves state-of-the-art part-aware reconstruction\nresults from both images and sparse point clouds.When reconstructing shapes by\nassembling parts retrieved from a dataset, our approach significantly\noutperforms traditional shape retrieval methods even when significantly\nrestricting the database size. We present our results in well-known sparse\npoint cloud reconstruction and single-view reconstruction benchmarks.",
          "arxiv_id": "2205.13682v2"
        },
        {
          "title": "CarveNet: Carving Point-Block for Complex 3D Shape Completion",
          "year": "2021-07",
          "abstract": "3D point cloud completion is very challenging because it heavily relies on\nthe accurate understanding of the complex 3D shapes (e.g., high-curvature,\nconcave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns\nof the partially available point clouds. In this paper, we propose a novel\nsolution,i.e., Point-block Carving (PC), for completing the complex 3D point\ncloud completion. Given the partial point cloud as the guidance, we carve a3D\nblock that contains the uniformly distributed 3D points, yielding the entire\npoint cloud. To achieve PC, we propose a new network architecture, i.e.,\nCarveNet. This network conducts the exclusive convolution on each point of the\nblock, where the convolutional kernels are trained on the 3D shape data.\nCarveNet determines which point should be carved, for effectively recovering\nthe details of the complete shapes. Furthermore, we propose a sensor-aware\nmethod for data augmentation,i.e., SensorAug, for training CarveNet on richer\npatterns of partial point clouds, thus enhancing the completion power of the\nnetwork. The extensive evaluations on the ShapeNet and KITTI datasets\ndemonstrate the generality of our approach on the partial point clouds with\ndiverse patterns. On these datasets, CarveNet successfully outperforms the\nstate-of-the-art methods.",
          "arxiv_id": "2107.13452v1"
        },
        {
          "title": "Shape As Points: A Differentiable Poisson Solver",
          "year": "2021-06",
          "abstract": "In recent years, neural implicit representations gained popularity in 3D\nreconstruction due to their expressiveness and flexibility. However, the\nimplicit nature of neural implicit representations results in slow inference\ntime and requires careful initialization. In this paper, we revisit the classic\nyet ubiquitous point cloud representation and introduce a differentiable\npoint-to-mesh layer using a differentiable formulation of Poisson Surface\nReconstruction (PSR) that allows for a GPU-accelerated fast solution of the\nindicator function given an oriented point cloud. The differentiable PSR layer\nallows us to efficiently and differentiably bridge the explicit 3D point\nrepresentation with the 3D mesh via the implicit indicator field, enabling\nend-to-end optimization of surface reconstruction metrics such as Chamfer\ndistance. This duality between points and meshes hence allows us to represent\nshapes as oriented point clouds, which are explicit, lightweight and\nexpressive. Compared to neural implicit representations, our Shape-As-Points\n(SAP) model is more interpretable, lightweight, and accelerates inference time\nby one order of magnitude. Compared to other explicit representations such as\npoints, patches, and meshes, SAP produces topology-agnostic, watertight\nmanifold surfaces. We demonstrate the effectiveness of SAP on the task of\nsurface reconstruction from unoriented point clouds and learning-based\nreconstruction.",
          "arxiv_id": "2106.03452v2"
        }
      ],
      "6": [
        {
          "title": "Effects of Realism and Representation on Self-Embodied Avatars in Immersive Virtual Environments",
          "year": "2024-05",
          "abstract": "Virtual Reality (VR) has recently gained traction with many new and ever more\naffordable devices being released. The increase in popularity of this paradigm\nof interaction has given birth to new applications and has attracted casual\nconsumers to experience VR. Providing a self-embodied representation (avatar)\nof users' full bodies inside shared virtual spaces can improve the VR\nexperience and make it more engaging to both new and experienced users . This\nis especially important in fully immersive systems, where the equipment\ncompletely occludes the real world making self awareness problematic. Indeed,\nthe feeling of presence of the user is highly influenced by their virtual\nrepresentations, even though small flaws could lead to uncanny valley\nside-effects. Following previous research, we would like to assess whether\nusing a third-person perspective could also benefit the VR experience, via an\nimproved spatial awareness of the user's virtual surroundings. In this paper we\ninvestigate realism and perspective of self-embodied representation in VR\nsetups in natural tasks, such as walking and avoiding obstacles. We compare\nboth First and Third-Person perspectives with three different levels of realism\nin avatar representation. These range from a stylized abstract avatar, to a\n\"realistic\" mesh-based humanoid representation and a point-cloud rendering. The\nlatter uses data captured via depth-sensors and mapped into a virtual self\ninside the Virtual Environment. We present a throughout evaluation and\ncomparison of these different representations, describing a series of\nguidelines for self-embodied VR applications. The effects of the uncanny valley\nare also discussed in the context of navigation and reflex-based tasks.",
          "arxiv_id": "2405.02672v1"
        },
        {
          "title": "Evaluation of Virtual Reality Interaction Techniques: the case of 3D Graph",
          "year": "2023-02",
          "abstract": "The virtual reality (VR) and human-computer interaction (HCI) combination has\nradically changed the way users approach a virtual environment, increasing the\nfeeling of VR immersion, and improving the user experience and usability. The\nevolution of these two technologies led to the focus on VR locomotion and\ninteraction. Locomotion is generally controller-based, but today hand gesture\nrecognition methods were also used for this purpose. However, hand gestures can\nbe stressful for the user who has to keep the gesture activation for a long\ntime to ensure locomotion, especially continuously. Likewise, in Head Mounted\nDisplay (HMD)-based virtual environment or Spherical-based system, the use of\nclassic controllers for the 3D scene interaction could be unnatural for the\nuser compared to using hand gestures such \\eg pinching to grab 3D objects. To\naddress these issues, we propose a user study comparing the use of the classic\ncontrollers (six-degree-of-freedom (6-DOF) or trackballs) in HMD and\nspherical-based systems, and the hand tracking and gestures in both VR\nimmersive modes. In particular, we focused on the possible differences between\nspherical-based systems and HMD in terms of the level of immersion perceived by\nthe user, the mode of user interaction (controller and hands), on the reaction\nof users concerning usefulness, easiness, and behavioral intention to use.",
          "arxiv_id": "2302.05660v1"
        },
        {
          "title": "Using Virtual Reality as a Simulation Tool for Augmented Reality Virtual Windows: Effects on Cognitive Workload and Task Performance",
          "year": "2024-09",
          "abstract": "Virtual content in Augmented Reality (AR) applications can be constructed\naccording to the designer's requirements, but real environments, are difficult\nto be accurate control or completely reproduce. This makes it difficult to\nprototype AR applications for certain real environments. One way to address\nthis issue is to use Virtual Reality (VR) to simulate an AR system, enabling\nthe design of controlled experiments and conducting usability evaluations.\nHowever, the effectiveness of using VR to simulate AR has not been well\nstudied. In this paper, we report on a user study (N=20) conducted to\ninvestigate the impact of using an VR simulation of AR on participants' task\nperformance and cognitive workload (CWL). Participants performed several office\ntasks in an AR scene with virtual monitors and then again in the VR-simulated\nAR scene. While using the interfaces CWL was measured with\nElectroencephalography (EEG) data and a subjective questionnaire. Results\nshowed that frequent visual checks on the keyboard resulted in decreased task\nperformance and increased cognitive workload. This study found that using AR\ncentered on virtual monitor can be effectively simulated using VR. However,\nthere is more research that can be done, so we also report on the study\nlimitations and directions for future work.",
          "arxiv_id": "2409.16037v1"
        }
      ],
      "7": [
        {
          "title": "PaMO: Parallel Mesh Optimization for Intersection-Free Low-Poly Modeling on the GPU",
          "year": "2025-09",
          "abstract": "Reducing the triangle count in complex 3D models is a basic geometry\npreprocessing step in graphics pipelines such as efficient rendering and\ninteractive editing. However, most existing mesh simplification methods exhibit\na few issues. Firstly, they often lead to self-intersections during decimation,\na major issue for applications such as 3D printing and soft-body simulation.\nSecond, to perform simplification on a mesh in the wild, one would first need\nto perform re-meshing, which often suffers from surface shifts and losses of\nsharp features. Finally, existing re-meshing and simplification methods can\ntake minutes when processing large-scale meshes, limiting their applications in\npractice. To address the challenges, we introduce a novel GPU-based mesh\noptimization approach containing three key components: (1) a parallel\nre-meshing algorithm to turn meshes in the wild into watertight, manifold, and\nintersection-free ones, and reduce the prevalence of poorly shaped triangles;\n(2) a robust parallel simplification algorithm with intersection-free\nguarantees; (3) an optimization-based safe projection algorithm to realign the\nsimplified mesh with the input, eliminating the surface shift introduced by\nre-meshing and recovering the original sharp features. The algorithm\ndemonstrates remarkable efficiency, simplifying a 2-million-face mesh to 20k\ntriangles in 3 seconds on RTX4090. We evaluated the approach on the Thingi10K\ndataset and showcased its exceptional performance in geometry preservation and\nspeed.",
          "arxiv_id": "2509.05595v1"
        },
        {
          "title": "Towards Voronoi Diagrams of Surface Patches",
          "year": "2024-11",
          "abstract": "Extraction of a high-fidelity 3D medial axis is a crucial operation in CAD.\nWhen dealing with a polygonal model as input, ensuring accuracy and tidiness\nbecomes challenging due to discretization errors inherent in the mesh surface.\nCommonly, existing approaches yield medial-axis surfaces with various\nartifacts, including zigzag boundaries, bumpy surfaces, unwanted spikes, and\nnon-smooth stitching curves. Considering that the surface of a CAD model can be\neasily decomposed into a collection of surface patches, its 3D medial axis can\nbe extracted by computing the Voronoi diagram of these surface patches, where\neach surface patch serves as a generator. However, no solver currently exists\nfor accurately computing such an extended Voronoi diagram. Under the assumption\nthat each generator defines a linear distance field over a sufficiently small\nrange, our approach operates by tetrahedralizing the region of interest and\ncomputing the medial axis within each tetrahedral element. Just as\nSurfaceVoronoi computes surface-based Voronoi diagrams by cutting a 3D prism\nwith 3D planes (each plane encodes a linear field in a triangle), the key\noperation in this paper is to conduct the hyperplane cutting process in 4D,\nwhere each hyperplane encodes a linear field in a tetrahedron. In comparison\nwith the state-of-the-art, our algorithm produces better outcomes. Furthermore,\nit can also be used to compute the offset surface.",
          "arxiv_id": "2411.06471v1"
        },
        {
          "title": "Simplifying Triangle Meshes in the Wild",
          "year": "2024-09",
          "abstract": "This paper introduces a fast and robust method for simplifying surface\ntriangle meshes in the wild while maintaining high visual quality. While\nprevious methods achieve excellent results on manifold meshes by using the\nquadric error metric, they suffer from producing high-quality outputs for\nuser-created meshes, which often contain non-manifold elements and multiple\nconnected components. In this work, we begin by outlining the pitfalls of\nexisting mesh simplification techniques and highlighting the discrepancy in\ntheir formulations with existing mesh data. We then propose a method for\nsimplifying these (non-manifold) triangle meshes, while maintaining quality\ncomparable to the existing methods for manifold inputs. Our key idea is to\nreformulate mesh simplification as a problem of decimating simplicial\n2-complexes. This involves a novel construction to turn a triangle soup into a\nsimplicial 2-complex, followed by iteratively collapsing 1-simplices (vertex\npairs) with our modified quadric error metric tailored for topology changes.\nBesides, we also tackle textured mesh simplification. Instead of following\nexisting strategies to preserve mesh UVs, we propose a novel perspective that\nonly focuses on preserving texture colors defined on the surface, regardless of\nthe layout in the texture UV space. This leads to a more robust method for\ntextured mesh simplification that is free from the texture bleeding artifact.\nOur mesh simplification enables level-of-detail algorithms to operate on\narbitrary triangle meshes in the wild. We demonstrate improvements over prior\ntechniques through extensive qualitative and quantitative evaluations, along\nwith user studies.",
          "arxiv_id": "2409.15458v1"
        }
      ],
      "8": [
        {
          "title": "Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation",
          "year": "2024-12",
          "abstract": "Text-driven human motion synthesis is capturing significant attention for its\nability to effortlessly generate intricate movements from abstract text cues,\nshowcasing its potential for revolutionizing motion design not only in film\nnarratives but also in virtual reality experiences and computer game\ndevelopment. Existing methods often rely on 3D motion capture data, which\nrequire special setups resulting in higher costs for data acquisition,\nultimately limiting the diversity and scope of human motion. In contrast, 2D\nhuman videos offer a vast and accessible source of motion data, covering a\nwider range of styles and activities. In this paper, we explore leveraging 2D\nhuman motion extracted from videos as an alternative data source to improve\ntext-driven 3D motion generation. Our approach introduces a novel framework\nthat disentangles local joint motion from global movements, enabling efficient\nlearning of local motion priors from 2D data. We first train a single-view 2D\nlocal motion generator on a large dataset of text-motion pairs. To enhance this\nmodel to synthesize 3D motion, we fine-tune the generator with 3D data,\ntransforming it into a multi-view generator that predicts view-consistent local\njoint motion and root dynamics. Experiments on the HumanML3D dataset and novel\ntext prompts demonstrate that our method efficiently utilizes 2D data,\nsupporting realistic 3D human motion generation and broadening the range of\nmotion types it supports. Our code will be made publicly available at\nhttps://zju3dv.github.io/Motion-2-to-3/.",
          "arxiv_id": "2412.13111v1"
        },
        {
          "title": "MotionGPT: Human Motion as a Foreign Language",
          "year": "2023-06",
          "abstract": "Though the advancement of pre-trained large language models unfolds, the\nexploration of building a unified model for language and other multi-modal\ndata, such as motion, remains challenging and untouched so far. Fortunately,\nhuman motion displays a semantic coupling akin to human language, often\nperceived as a form of body language. By fusing language data with large-scale\nmotion models, motion-language pre-training that can enhance the performance of\nmotion-related tasks becomes feasible. Driven by this insight, we propose\nMotionGPT, a unified, versatile, and user-friendly motion-language model to\nhandle multiple motion-relevant tasks. Specifically, we employ the discrete\nvector quantization for human motion and transfer 3D motion into motion tokens,\nsimilar to the generation process of word tokens. Building upon this \"motion\nvocabulary\", we perform language modeling on both motion and text in a unified\nmanner, treating human motion as a specific language. Moreover, inspired by\nprompt learning, we pre-train MotionGPT with a mixture of motion-language data\nand fine-tune it on prompt-based question-and-answer tasks. Extensive\nexperiments demonstrate that MotionGPT achieves state-of-the-art performances\non multiple motion tasks including text-driven motion generation, motion\ncaptioning, motion prediction, and motion in-between.",
          "arxiv_id": "2306.14795v2"
        },
        {
          "title": "SMooGPT: Stylized Motion Generation using Large Language Models",
          "year": "2025-09",
          "abstract": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.",
          "arxiv_id": "2509.04058v1"
        }
      ],
      "9": [
        {
          "title": "Fast Subspace Fluid Simulation with Temporal-Aware Basis",
          "year": "2025-02",
          "abstract": "We present a novel reduced-order fluid simulation technique leveraging\nDynamic Mode Decomposition (DMD) to achieve fast, memory-efficient, and\nuser-controllable subspace simulation. We demonstrate that our approach\ncombines the strengths of both spatial reduced order models (ROMs) as well as\nspectral decompositions. By optimizing for the operator that evolves a system\nstate from one timestep to the next, rather than the system state itself, we\ngain both the compressive power of spatial ROMs as well as the intuitive\nphysical dynamics of spectral methods. The latter property is of particular\ninterest in graphics applications, where user control of fluid phenomena is of\nhigh demand. We demonstrate this in various applications including spatial and\ntemporal modulation tools and fluid upscaling with added turbulence.\n  We adapt DMD for graphics applications by reducing computational overhead,\nincorporating user-defined force inputs, and optimizing memory usage with\nrandomized SVD. The integration of OptDMD and DMD with Control (DMDc)\nfacilitates noise-robust reconstruction and real-time user interaction. We\ndemonstrate the technique's robustness across diverse simulation scenarios,\nincluding artistic editing, time-reversal, and super-resolution.\n  Through experimental validation on challenging scenarios, such as colliding\nvortex rings and boundary-interacting plumes, our method also exhibits superior\nperformance and fidelity with significantly fewer basis functions compared to\nexisting spatial ROMs. The inherent linearity of the DMD operator enables\nunique application modes, such as time-reversible fluid simulation. This work\nestablishes another avenue for developing real-time, high-quality fluid\nsimulations, enriching the space of fluid simulation techniques in interactive\ngraphics and animation.",
          "arxiv_id": "2502.05339v1"
        },
        {
          "title": "StiffGIPC: Advancing GPU IPC for stiff affine-deformable simulation",
          "year": "2024-11",
          "abstract": "Incremental Potential Contact (IPC) is a widely used, robust, and accurate\nmethod for simulating complex frictional contact behaviors. However, achieving\nhigh efficiency remains a major challenge, particularly as material stiffness\nincreases, which leads to slower Preconditioned Conjugate Gradient (PCG)\nconvergence, even with the state-of-the-art preconditioners. In this paper, we\npropose a fully GPU-optimized IPC simulation framework capable of handling\nmaterials across a wide range of stiffnesses, delivering consistent high\nperformance and scalability with up to 10x speedup over state-of-the-art GPU\nIPC methods. Our framework introduces three key innovations: 1) A novel\nconnectivity-enhanced Multilevel Additive Schwarz (MAS) preconditioner on the\nGPU, designed to efficiently capture both stiff and soft elastodynamics and\nimprove PCG convergence at a reduced preconditioning cost. 2) A C2-continuous\ncubic energy with an analytic eigensystem for strain limiting, enabling more\nparallel-friendly simulations of stiff membranes, such as cloth, without\nmembrane locking. 3) For extremely stiff behaviors where elastic waves are\nbarely visible, we employ affine body dynamics (ABD) with a hash-based\nmulti-layer reduction strategy for fast Hessian assembly and efficient\naffine-deformable coupling. We conduct extensive performance analyses and\nbenchmark studies to compare our framework against state-of-the-art methods and\nalternative design choices. Our system consistently delivers the fastest\nperformance across soft, stiff, and hybrid simulation scenarios, even in cases\nwith high resolution, large deformations, and high-speed impacts. Our framework\nwill be fully open-sourced upon acceptance.",
          "arxiv_id": "2411.06224v4"
        },
        {
          "title": "SymX: Energy-based Simulation from Symbolic Expressions",
          "year": "2023-02",
          "abstract": "Optimization time integrators have proven to be effective at solving complex\nmulti-physics problems, such as deformation of solids with non-linear material\nmodels, contact with friction, strain limiting, etc. For challenging problems\nwith high accuracy requirements, Newton-type optimizers are often used. This\nnecessitates first- and second-order derivatives of the global non-linear\nobjective function. Manually differentiating, implementing and optimizing the\nresulting code is extremely time-consuming, error-prone, and precludes quick\nchanges to the model.\n  We present SymX, a framework based on symbolic expressions that computes the\nfirst and second derivatives by symbolic differentiation, generates efficient\nvectorized source code, compiles it on-the-fly, and performs the global\nassembly of element contributions in parallel. The user only has to provide the\nsymbolic expression of an energy function for a single element in the\ndiscretization and our system will determine the assembled derivatives for the\nwhole model. SymX is designed to be an integral part of a simulation system and\ncan easily be integrated into existing ones. We demonstrate the versatility of\nour framework in various complex simulations showing different non-linear\nmaterials, higher-order finite elements, rigid body systems, adaptive cloth,\nfrictional contact, and coupling multiple interacting physical systems.\nMoreover, we compare our method with alternative approaches and show that SymX\nis significantly faster than a current state-or-the-art framework (up to two\norders of magnitude for a higher-order FEM simulation).",
          "arxiv_id": "2303.02156v1"
        }
      ],
      "10": [
        {
          "title": "Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis",
          "year": "2020-06",
          "abstract": "Reinforcement learning has shown great promise for synthesizing realistic\nhuman behaviors by learning humanoid control policies from motion capture data.\nHowever, it is still very challenging to reproduce sophisticated human skills\nlike ballet dance, or to stably imitate long-term human behaviors with complex\ntransitions. The main difficulty lies in the dynamics mismatch between the\nhumanoid model and real humans. That is, motions of real humans may not be\nphysically possible for the humanoid model. To overcome the dynamics mismatch,\nwe propose a novel approach, residual force control (RFC), that augments a\nhumanoid control policy by adding external residual forces into the action\nspace. During training, the RFC-based policy learns to apply residual forces to\nthe humanoid to compensate for the dynamics mismatch and better imitate the\nreference motion. Experiments on a wide range of dynamic motions demonstrate\nthat our approach outperforms state-of-the-art methods in terms of convergence\nspeed and the quality of learned motions. Notably, we showcase a physics-based\nvirtual character empowered by RFC that can perform highly agile ballet dance\nmoves such as pirouette, arabesque and jet\\'e. Furthermore, we propose a\ndual-policy control framework, where a kinematic policy and an RFC-based policy\nwork in tandem to synthesize multi-modal infinite-horizon human motions without\nany task guidance or user input. Our approach is the first humanoid control\nmethod that successfully learns from a large-scale human motion dataset\n(Human3.6M) and generates diverse long-term motions. Code and videos are\navailable at https://www.ye-yuan.com/rfc.",
          "arxiv_id": "2006.07364v2"
        },
        {
          "title": "ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters",
          "year": "2022-05",
          "abstract": "The incredible feats of athleticism demonstrated by humans are made possible\nin part by a vast repertoire of general-purpose motor skills, acquired through\nyears of practice and experience. These skills not only enable humans to\nperform complex tasks, but also provide powerful priors for guiding their\nbehaviors when learning new tasks. This is in stark contrast to what is common\npractice in physics-based character animation, where control policies are most\ntypically trained from scratch for each task. In this work, we present a\nlarge-scale data-driven framework for learning versatile and reusable skill\nembeddings for physically simulated characters. Our approach combines\ntechniques from adversarial imitation learning and unsupervised reinforcement\nlearning to develop skill embeddings that produce life-like behaviors, while\nalso providing an easy to control representation for use on new downstream\ntasks. Our models can be trained using large datasets of unstructured motion\nclips, without requiring any task-specific annotation or segmentation of the\nmotion data. By leveraging a massively parallel GPU-based simulator, we are\nable to train skill embeddings using over a decade of simulated experiences,\nenabling our model to learn a rich and versatile repertoire of skills. We show\nthat a single pre-trained model can be effectively applied to perform a diverse\nset of new tasks. Our system also allows users to specify tasks through simple\nreward functions, and the skill embedding then enables the character to\nautomatically synthesize complex and naturalistic strategies in order to\nachieve the task objectives.",
          "arxiv_id": "2205.01906v2"
        },
        {
          "title": "AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control",
          "year": "2021-04",
          "abstract": "Synthesizing graceful and life-like behaviors for physically simulated\ncharacters has been a fundamental challenge in computer animation. Data-driven\nmethods that leverage motion tracking are a prominent class of techniques for\nproducing high fidelity motions for a wide range of behaviors. However, the\neffectiveness of these tracking-based methods often hinges on carefully\ndesigned objective functions, and when applied to large and diverse motion\ndatasets, these methods require significant additional machinery to select the\nappropriate motion for the character to track in a given scenario. In this\nwork, we propose to obviate the need to manually design imitation objectives\nand mechanisms for motion selection by utilizing a fully automated approach\nbased on adversarial imitation learning. High-level task objectives that the\ncharacter should perform can be specified by relatively simple reward\nfunctions, while the low-level style of the character's behaviors can be\nspecified by a dataset of unstructured motion clips, without any explicit clip\nselection or sequencing. These motion clips are used to train an adversarial\nmotion prior, which specifies style-rewards for training the character through\nreinforcement learning (RL). The adversarial RL procedure automatically selects\nwhich motion to perform, dynamically interpolating and generalizing from the\ndataset. Our system produces high-quality motions that are comparable to those\nachieved by state-of-the-art tracking-based techniques, while also being able\nto easily accommodate large datasets of unstructured motion clips. Composition\nof disparate skills emerges automatically from the motion prior, without\nrequiring a high-level motion planner or other task-specific annotations of the\nmotion clips. We demonstrate the effectiveness of our framework on a diverse\ncast of complex simulated characters and a challenging suite of motor control\ntasks.",
          "arxiv_id": "2104.02180v2"
        }
      ],
      "11": [
        {
          "title": "A Computational Design and Evaluation Tool for 3D Structures with Planar Surfaces",
          "year": "2021-03",
          "abstract": "Three dimensional (3D) structures composed of planar surfaces can be build\nout of accessible materials using easier fabrication technique with shorter\nfabrication time. To better design 3D structures with planar surfaces,\nrealistic models are required to understand and evaluate mechanical behaviors.\nExisting design tools are either effort-consuming (e.g. finite element\nanalysis) or bounded by assumptions (e.g. numerical solutions). In this\nproject, We have built a computational design tool that is (1) capable of\nrapidly and inexpensively evaluating planar surfaces in 3D structures, with\nsufficient computational efficiency and accuracy; (2) applicable to complex\nboundary conditions and loading conditions, both isotropic materials and\northotropic materials; and (3) suitable for rapid accommodation when design\nparameters need to be adjusted. We demonstrate the efficiency and necessity of\nthis design tool by evaluating a glass table as well as a wood bookcase, and\niteratively designing an origami gripper to satisfy performance requirements.\nThis design tool gives non-expert users as well as engineers a simple and\neffective modus operandi in structural design.",
          "arxiv_id": "2103.02114v1"
        },
        {
          "title": "Direction-Oriented Stress-Constrained Topology Optimization of Orthotropic Materials",
          "year": "2021-12",
          "abstract": "Efficient optimization of topology and raster angle has shown unprecedented\nenhancements in the mechanical properties of 3D printed materials. Topology\noptimization helps reduce the waste of raw material in the fabrication of 3D\nprinted parts, thus decreasing production costs associated with manufacturing\nlighter structures. Fiber orientation plays an important role in increasing the\nstiffness of a structure. This paper develops and tests a new method for\nhandling stress constraints in topology and fiber orientation optimization of\n3D printed orthotropic structures. The stress constraints are coupled with an\nobjective function that maximizes stiffness. This is accomplished by using the\nmodified solid isotropic material with penalization method with the method of\nmoving asymptotes as the mathematical optimizer. Each element has a fictitious\ndensity and an angle as the main design variables. To reduce the number of\nstress constraints and thus the computational cost, a new clustering strategy\nis employed in which the highest stresses in the principal material coordinates\nare grouped separately into two clusters using an adjusted $P$-norm. A detailed\ndescription of the formulation and sensitivity analysis is discussed. While we\npresent an analysis of 2D structures in the numerical examples section, the\nmethod can also be used for 3D structures, as the formulation is generic. Our\nresults show that this method can produce efficient structures suitable for 3D\nprinting while thresholding the stresses.",
          "arxiv_id": "2112.02030v2"
        },
        {
          "title": "Encoding of direct 4D printing of isotropic single-material system for double-curvature and multimodal morphing",
          "year": "2022-05",
          "abstract": "The ability to morph flat sheets into complex 3D shapes is extremely useful\nfor fast manufacturing and saving materials while also allowing volumetrically\nefficient storage and shipment and a functional use. Direct 4D printing is a\ncompelling method to morph complex 3D shapes out of as-printed 2D plates.\nHowever, most direct 4D printing methods require multi-material systems\ninvolving costly machines. Moreover, most works have used an open-cell design\nfor shape shifting by encoding a collection of 1D rib deformations, which\ncannot remain structurally stable. Here, we demonstrate the direct 4D printing\nof an isotropic single-material system to morph 2D continuous bilayer plates\ninto doubly curved and multimodal 3D complex shapes whose geometry can also be\nlocked after deployment. We develop an inverse-design algorithm that integrates\nextrusion-based 3D printing of a single-material system to directly morph a raw\nprinted sheet into complex 3D geometries such as a doubly curved surface with\nshape locking. Furthermore, our inverse-design tool encodes the localized\nshape-memory anisotropy during the process, providing the processing conditions\nfor a target 3D morphed geometry. Our approach could be used for conventional\nextrusion-based 3D printing for various applications including biomedical\ndevices, deployable structures, smart textiles, and pop-up Kirigami structures.",
          "arxiv_id": "2205.02510v1"
        }
      ],
      "12": [
        {
          "title": "Minimizing Ray Tracing Memory Traffic through Quantized Structures and Ray Stream Tracing",
          "year": "2025-05",
          "abstract": "Memory bandwidth constraints continue to be a significant limiting factor in\nray tracing performance, particularly as scene complexity grows and\ncomputational capabilities outpace memory access speeds. This paper presents a\nmemory-efficient ray tracing methodology that integrates compressed data\nstructures with ray stream techniques to reduce memory traffic. The approach\nimplements compressed BVH and triangle representations to minimize acceleration\nstructure size in combination with ray stream tracing to reduce traversal stack\nmemory traffic. The technique employs fixed-point arithmetic for intersection\ntests for prospective hardware with tailored integer operations. Despite using\nreduced precision, geometric holes are avoided by leveraging fixed-point\narithmetic instead of encountering the floating-point rounding errors common in\ntraditional approaches. Quantitative analysis demonstrates significant memory\ntraffic reduction across various scene complexities and BVH configurations. The\npresented 8-wide BVH ray stream implementation reduces memory traffic to only\n18% of traditional approaches by using 8-bit quantization for box and triangle\ncoordinates and directly ray tracing these quantized structures. These\nreductions are especially beneficial for bandwidth-constrained hardware\nenvironments such as mobile devices. This integrated approach addresses both\nmemory bandwidth limitations and numerical precision challenges inherent to\nmodern ray tracing applications.",
          "arxiv_id": "2505.24653v1"
        },
        {
          "title": "Combining GPU Tracing Methods within a Single Ray Query",
          "year": "2023-05",
          "abstract": "A recent trend in real-time rendering is the utilization of the new hardware\nray tracing capabilities. Often, usage of a distance field representation is\nproposed as an alternative when hardware ray tracing is deemed too costly, and\nthe two are seen as competing approaches. In this work, we show that both\napproaches can work together effectively for a single ray query on modern\nhardware. We choose to use hardware ray tracing where precision is most\nimportant, while avoiding its heavy cost by using a distance field when\npossible. While a simple approach, in our experiments the resulting tracing\nalgorithm overcomes the associated overhead and allows a user-defined middle\nground between the performance of distance field traversal and the improved\nvisual quality of hardware ray tracing.",
          "arxiv_id": "2305.07253v1"
        },
        {
          "title": "Accelerating Java Ray Tracing Applications on Heterogeneous Hardware",
          "year": "2023-05",
          "abstract": "Ray tracing has been typically known as a graphics rendering method capable\nof producing highly realistic imagery and visual effects generated by\ncomputers. More recently the performance improvements in Graphics Processing\nUnits (GPUs) have enabled developers to exploit sufficient computing power to\nbuild a fair amount of ray tracing applications with the ability to run in\nreal-time. Typically, real-time ray tracing is achieved by utilizing high\nperformance kernels written in CUDA, OpenCL, and Vulkan which can be invoked by\nhigh-level languages via native bindings; a technique that fragments\napplication code bases as well as limits portability.\n  This paper presents a hardware-accelerated ray tracing rendering engine,\nfully written in Java, that can seamlessly harness the performance of\nunderlying GPUs via the TornadoVM framework. Through this paper, we show the\npotential of Java and acceleration frameworks to process in real time a compute\nintensive application. Our results indicate that it is possible to enable real\ntime ray tracing from Java by achieving up to 234, 152, 45 frames-per-second in\n720p, 1080p, and 4K resolutions, respectively.",
          "arxiv_id": "2305.07450v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:41:36Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}