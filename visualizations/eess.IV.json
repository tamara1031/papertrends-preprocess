{
  "topics": {
    "data": {
      "0": {
        "name": "0_compression_video_quality_coding",
        "keywords": [
          [
            "compression",
            0.025574146770301988
          ],
          [
            "video",
            0.022064048451081935
          ],
          [
            "quality",
            0.018401663612054964
          ],
          [
            "coding",
            0.01572109236432365
          ],
          [
            "rate",
            0.011792480987867076
          ],
          [
            "image",
            0.01179174601467203
          ],
          [
            "image compression",
            0.011285626448536087
          ],
          [
            "Video",
            0.009906535452135514
          ],
          [
            "distortion",
            0.009753773346865072
          ],
          [
            "Compression",
            0.009407381605828505
          ]
        ],
        "count": 1515
      },
      "1": {
        "name": "1_chest_lung_CT_ray",
        "keywords": [
          [
            "chest",
            0.01498828234203401
          ],
          [
            "lung",
            0.014872475418875417
          ],
          [
            "CT",
            0.013572025506142447
          ],
          [
            "ray",
            0.011046221900408701
          ],
          [
            "learning",
            0.009330959578144888
          ],
          [
            "classification",
            0.009019022708152108
          ],
          [
            "diagnosis",
            0.009000873947951227
          ],
          [
            "medical",
            0.008924460061073444
          ],
          [
            "model",
            0.00890108338032351
          ],
          [
            "images",
            0.008542675796693423
          ]
        ],
        "count": 1393
      },
      "2": {
        "name": "2_segmentation_medical_image segmentation_medical image",
        "keywords": [
          [
            "segmentation",
            0.029489526392525372
          ],
          [
            "medical",
            0.018030742378853677
          ],
          [
            "image segmentation",
            0.01634011024516743
          ],
          [
            "medical image",
            0.014189509148982444
          ],
          [
            "medical image segmentation",
            0.013208202423483643
          ],
          [
            "Segmentation",
            0.011154364627672077
          ],
          [
            "image",
            0.010644238302701696
          ],
          [
            "Net",
            0.009650615526215229
          ],
          [
            "Medical",
            0.009041867897258514
          ],
          [
            "3D",
            0.008087089436971149
          ]
        ],
        "count": 1030
      },
      "3": {
        "name": "3_imaging_phase_microscopy_resolution",
        "keywords": [
          [
            "imaging",
            0.02305557140333018
          ],
          [
            "phase",
            0.017633781494513955
          ],
          [
            "microscopy",
            0.015176353706409218
          ],
          [
            "resolution",
            0.012172564945628454
          ],
          [
            "optical",
            0.010696515770822063
          ],
          [
            "light",
            0.010193402976753623
          ],
          [
            "reconstruction",
            0.009997813689623752
          ],
          [
            "high",
            0.009902199529723545
          ],
          [
            "single",
            0.009623545098608365
          ],
          [
            "method",
            0.008838670660664029
          ]
        ],
        "count": 1023
      },
      "4": {
        "name": "4_MRI_reconstruction_MR_data",
        "keywords": [
          [
            "MRI",
            0.02776049470626268
          ],
          [
            "reconstruction",
            0.020530762438322966
          ],
          [
            "MR",
            0.012413048182004111
          ],
          [
            "data",
            0.011247096331406733
          ],
          [
            "motion",
            0.01043703780693749
          ],
          [
            "space",
            0.0103133684340208
          ],
          [
            "resolution",
            0.009707406952168596
          ],
          [
            "imaging",
            0.009459290337283602
          ],
          [
            "acquisition",
            0.00913464334883624
          ],
          [
            "image",
            0.008886200587980172
          ]
        ],
        "count": 959
      },
      "5": {
        "name": "5_medical_images_data_image",
        "keywords": [
          [
            "medical",
            0.014370011702179844
          ],
          [
            "images",
            0.013750577276531492
          ],
          [
            "data",
            0.012939753667864252
          ],
          [
            "image",
            0.01234355703585703
          ],
          [
            "domain",
            0.011899326930900204
          ],
          [
            "synthetic",
            0.010404632683379416
          ],
          [
            "MRI",
            0.009320408816012427
          ],
          [
            "models",
            0.00922527230657577
          ],
          [
            "CT",
            0.009043233876013327
          ],
          [
            "synthesis",
            0.008855604555144586
          ]
        ],
        "count": 873
      },
      "6": {
        "name": "6_SAR_data_imagery_satellite",
        "keywords": [
          [
            "SAR",
            0.021230275561444054
          ],
          [
            "data",
            0.013069795128806854
          ],
          [
            "imagery",
            0.012280771582930058
          ],
          [
            "satellite",
            0.011844400884610702
          ],
          [
            "remote sensing",
            0.011220770592027852
          ],
          [
            "remote",
            0.011109819874607098
          ],
          [
            "sensing",
            0.01056811628726456
          ],
          [
            "resolution",
            0.010197841182392546
          ],
          [
            "images",
            0.00984522856166284
          ],
          [
            "learning",
            0.008103579463208933
          ]
        ],
        "count": 835
      },
      "7": {
        "name": "7_pathology_cell_images_slide",
        "keywords": [
          [
            "pathology",
            0.01419776389634879
          ],
          [
            "cell",
            0.013491879020300943
          ],
          [
            "images",
            0.0124943273086738
          ],
          [
            "slide",
            0.011923407964737027
          ],
          [
            "cancer",
            0.011198124670977909
          ],
          [
            "tissue",
            0.0106751563787489
          ],
          [
            "learning",
            0.01042790254951215
          ],
          [
            "WSIs",
            0.009738491861686077
          ],
          [
            "histopathology",
            0.009436667249699751
          ],
          [
            "staining",
            0.009082471475366137
          ]
        ],
        "count": 779
      },
      "8": {
        "name": "8_retinal_OCT_fundus_images",
        "keywords": [
          [
            "retinal",
            0.029770818269122572
          ],
          [
            "OCT",
            0.01995455790586391
          ],
          [
            "fundus",
            0.017600392974640694
          ],
          [
            "images",
            0.011768205154413137
          ],
          [
            "segmentation",
            0.011628358390288975
          ],
          [
            "Retinal",
            0.009987295783151212
          ],
          [
            "fundus images",
            0.009709201313825565
          ],
          [
            "vessel",
            0.009577960717140371
          ],
          [
            "glaucoma",
            0.009043610460483196
          ],
          [
            "diseases",
            0.008272122944530074
          ]
        ],
        "count": 708
      },
      "9": {
        "name": "9_tumor_brain_segmentation_MRI",
        "keywords": [
          [
            "tumor",
            0.02609342191920966
          ],
          [
            "brain",
            0.02561639401068797
          ],
          [
            "segmentation",
            0.017925611158873107
          ],
          [
            "MRI",
            0.01593802107097749
          ],
          [
            "Brain",
            0.014068308145381249
          ],
          [
            "brain tumor",
            0.013147318038575355
          ],
          [
            "BraTS",
            0.009770637547861822
          ],
          [
            "tumors",
            0.009264970398738885
          ],
          [
            "model",
            0.008949764329401923
          ],
          [
            "Tumor",
            0.008943394963737826
          ]
        ],
        "count": 664
      },
      "10": {
        "name": "10_object_detection_segmentation_real",
        "keywords": [
          [
            "object",
            0.012474461241570593
          ],
          [
            "detection",
            0.011522068360269878
          ],
          [
            "segmentation",
            0.008993910404187497
          ],
          [
            "real",
            0.008788500919656465
          ],
          [
            "vehicle",
            0.0086927185383231
          ],
          [
            "semantic",
            0.00866955150927758
          ],
          [
            "driving",
            0.008626935531908651
          ],
          [
            "autonomous",
            0.008489065783511161
          ],
          [
            "3D",
            0.007811921425602105
          ],
          [
            "LiDAR",
            0.007486405852636024
          ]
        ],
        "count": 605
      },
      "11": {
        "name": "11_resolution_SR_super_super resolution",
        "keywords": [
          [
            "resolution",
            0.032189187378394366
          ],
          [
            "SR",
            0.02874414860493012
          ],
          [
            "super",
            0.02792020196673257
          ],
          [
            "super resolution",
            0.026647347264719998
          ],
          [
            "Super",
            0.022193638083291372
          ],
          [
            "Resolution",
            0.019938490957369766
          ],
          [
            "image super",
            0.01622008582303337
          ],
          [
            "image",
            0.015926552984090948
          ],
          [
            "SISR",
            0.013273941048864289
          ],
          [
            "Image",
            0.010086548040674619
          ]
        ],
        "count": 489
      },
      "12": {
        "name": "12_brain_AD_Alzheimer_fMRI",
        "keywords": [
          [
            "brain",
            0.02785269472276437
          ],
          [
            "AD",
            0.022207645500872807
          ],
          [
            "Alzheimer",
            0.021539850795332025
          ],
          [
            "fMRI",
            0.01663979654488412
          ],
          [
            "disease",
            0.01663253878877051
          ],
          [
            "functional",
            0.012544178821567784
          ],
          [
            "data",
            0.012064036598160738
          ],
          [
            "MRI",
            0.011843201378938512
          ],
          [
            "Disease",
            0.011322697969400672
          ],
          [
            "diagnosis",
            0.011304370608037635
          ]
        ],
        "count": 476
      },
      "13": {
        "name": "13_spectral_hyperspectral_HSI_Hyperspectral",
        "keywords": [
          [
            "spectral",
            0.038700089200715014
          ],
          [
            "hyperspectral",
            0.030523928453409805
          ],
          [
            "HSI",
            0.02890212760261684
          ],
          [
            "Hyperspectral",
            0.022571305818765215
          ],
          [
            "spatial",
            0.01895362841530911
          ],
          [
            "unmixing",
            0.01399760670259268
          ],
          [
            "hyperspectral image",
            0.012116510620213815
          ],
          [
            "Spectral",
            0.010748696570258837
          ],
          [
            "resolution",
            0.009998631917131948
          ],
          [
            "HSIs",
            0.009634272434964905
          ]
        ],
        "count": 459
      },
      "14": {
        "name": "14_hardware_energy_accuracy_neural",
        "keywords": [
          [
            "hardware",
            0.01311888911891463
          ],
          [
            "energy",
            0.011737310796074759
          ],
          [
            "accuracy",
            0.011691952098447249
          ],
          [
            "neural",
            0.011138267065058686
          ],
          [
            "memory",
            0.010600205453344114
          ],
          [
            "pruning",
            0.010017495622781073
          ],
          [
            "network",
            0.009846116907235776
          ],
          [
            "Neural",
            0.00915369849988301
          ],
          [
            "networks",
            0.00905319378480113
          ],
          [
            "event",
            0.008958261388290183
          ]
        ],
        "count": 354
      },
      "15": {
        "name": "15_CT_dose_reconstruction_LDCT",
        "keywords": [
          [
            "CT",
            0.03446965040741069
          ],
          [
            "dose",
            0.025503211352729763
          ],
          [
            "reconstruction",
            0.02274590806215738
          ],
          [
            "LDCT",
            0.013764116961207266
          ],
          [
            "denoising",
            0.0134029937929324
          ],
          [
            "low dose",
            0.01241424482726511
          ],
          [
            "view",
            0.011736664376228965
          ],
          [
            "image",
            0.011516942334724015
          ],
          [
            "noise",
            0.01139093610361737
          ],
          [
            "projection",
            0.01043102022257491
          ]
        ],
        "count": 340
      },
      "16": {
        "name": "16_GANs_GAN_image_Generative",
        "keywords": [
          [
            "GANs",
            0.024887240139415002
          ],
          [
            "GAN",
            0.024275720269872385
          ],
          [
            "image",
            0.020598311954231414
          ],
          [
            "Generative",
            0.015533010688602768
          ],
          [
            "Adversarial",
            0.013415994957887731
          ],
          [
            "images",
            0.013148246308198849
          ],
          [
            "generation",
            0.012919430578418017
          ],
          [
            "generator",
            0.012195575695165198
          ],
          [
            "style",
            0.012135821339348618
          ],
          [
            "translation",
            0.012086146713767032
          ]
        ],
        "count": 328
      },
      "17": {
        "name": "17_defect_defects_detection_inspection",
        "keywords": [
          [
            "defect",
            0.019348361532992284
          ],
          [
            "defects",
            0.013952825841595878
          ],
          [
            "detection",
            0.013927854477313156
          ],
          [
            "inspection",
            0.013093333770024824
          ],
          [
            "manufacturing",
            0.011035688614670762
          ],
          [
            "images",
            0.009158543593608565
          ],
          [
            "defect detection",
            0.009102259003547672
          ],
          [
            "data",
            0.008729568097369586
          ],
          [
            "learning",
            0.008361669793726068
          ],
          [
            "model",
            0.007496309125318035
          ]
        ],
        "count": 318
      },
      "18": {
        "name": "18_HDR_light_low light_low",
        "keywords": [
          [
            "HDR",
            0.037850367634606405
          ],
          [
            "light",
            0.029998923407511594
          ],
          [
            "low light",
            0.028064744935259435
          ],
          [
            "low",
            0.021404014256195716
          ],
          [
            "enhancement",
            0.021078274733720977
          ],
          [
            "image",
            0.016807062174920232
          ],
          [
            "exposure",
            0.015650130821287838
          ],
          [
            "dynamic range",
            0.015649281645467916
          ],
          [
            "images",
            0.014216606430048628
          ],
          [
            "LDR",
            0.014055627967205992
          ]
        ],
        "count": 304
      },
      "19": {
        "name": "19_cardiac_CMR_segmentation_myocardial",
        "keywords": [
          [
            "cardiac",
            0.03205781545650443
          ],
          [
            "CMR",
            0.01986272073794945
          ],
          [
            "segmentation",
            0.015607152884593647
          ],
          [
            "myocardial",
            0.01355194573051455
          ],
          [
            "Cardiac",
            0.013451245792181977
          ],
          [
            "heart",
            0.013244132394309955
          ],
          [
            "left",
            0.013092384517506628
          ],
          [
            "LGE",
            0.011415675996708936
          ],
          [
            "LV",
            0.011401455563380519
          ],
          [
            "LA",
            0.010533732375584477
          ]
        ],
        "count": 294
      },
      "20": {
        "name": "20_bone_knee_fracture_spine",
        "keywords": [
          [
            "bone",
            0.0181142448774952
          ],
          [
            "knee",
            0.014981603815630665
          ],
          [
            "fracture",
            0.012690710795978494
          ],
          [
            "spine",
            0.010823441172331156
          ],
          [
            "segmentation",
            0.009969922222716546
          ],
          [
            "vertebrae",
            0.0093226738746177
          ],
          [
            "radiographs",
            0.008833498452939455
          ],
          [
            "spinal",
            0.008516769310570837
          ],
          [
            "CT",
            0.008474678904304358
          ],
          [
            "learning",
            0.008198418783882234
          ]
        ],
        "count": 291
      },
      "21": {
        "name": "21_coronary_vessel_artery_segmentation",
        "keywords": [
          [
            "coronary",
            0.02527442129232554
          ],
          [
            "vessel",
            0.02379488909588621
          ],
          [
            "artery",
            0.01821956898749771
          ],
          [
            "segmentation",
            0.017234445893275083
          ],
          [
            "vascular",
            0.016378025135484076
          ],
          [
            "vessels",
            0.013139286263509862
          ],
          [
            "angiography",
            0.011985054755136977
          ],
          [
            "Coronary",
            0.01156854851537218
          ],
          [
            "3D",
            0.011336522539567658
          ],
          [
            "coronary artery",
            0.011167127326722611
          ]
        ],
        "count": 282
      },
      "22": {
        "name": "22_breast_cancer_breast cancer_Breast",
        "keywords": [
          [
            "breast",
            0.05336582888562409
          ],
          [
            "cancer",
            0.02818120923822035
          ],
          [
            "breast cancer",
            0.026534809709991173
          ],
          [
            "Breast",
            0.023865179336586703
          ],
          [
            "mammography",
            0.01230203723322215
          ],
          [
            "classification",
            0.00996891366297251
          ],
          [
            "detection",
            0.009701470515922125
          ],
          [
            "images",
            0.009487129470714877
          ],
          [
            "learning",
            0.009342976713560226
          ],
          [
            "model",
            0.009217682466223336
          ]
        ],
        "count": 281
      },
      "23": {
        "name": "23_skin_Skin_melanoma_lesion",
        "keywords": [
          [
            "skin",
            0.04851835823828867
          ],
          [
            "Skin",
            0.018562782807570437
          ],
          [
            "melanoma",
            0.018374840250912825
          ],
          [
            "lesion",
            0.017191425441804523
          ],
          [
            "skin lesion",
            0.015212873407455712
          ],
          [
            "skin cancer",
            0.012737526015966191
          ],
          [
            "lesions",
            0.011285176845576362
          ],
          [
            "cancer",
            0.01094988258479774
          ],
          [
            "skin lesions",
            0.010397475526038214
          ],
          [
            "diagnosis",
            0.010306475799010735
          ]
        ],
        "count": 258
      },
      "24": {
        "name": "24_problems_inverse_inverse problems_PnP",
        "keywords": [
          [
            "problems",
            0.02804874788725552
          ],
          [
            "inverse",
            0.02803707760812182
          ],
          [
            "inverse problems",
            0.026327192432233318
          ],
          [
            "PnP",
            0.02008150823265391
          ],
          [
            "image",
            0.01337470127102375
          ],
          [
            "regularization",
            0.012229340074383655
          ],
          [
            "reconstruction",
            0.011922399884165989
          ],
          [
            "denoisers",
            0.01163006570263949
          ],
          [
            "imaging",
            0.011164640032398223
          ],
          [
            "denoiser",
            0.011044353263524662
          ]
        ],
        "count": 247
      },
      "25": {
        "name": "25_registration_image registration_Registration_image",
        "keywords": [
          [
            "registration",
            0.07445270928132564
          ],
          [
            "image registration",
            0.029636881334424922
          ],
          [
            "Registration",
            0.02147513484824158
          ],
          [
            "image",
            0.01665389776053393
          ],
          [
            "deformation",
            0.01265597195498956
          ],
          [
            "deformable",
            0.011341745634888632
          ],
          [
            "learning",
            0.011182409011201273
          ],
          [
            "methods",
            0.009958436049118032
          ],
          [
            "medical image registration",
            0.009461729564652252
          ],
          [
            "medical",
            0.009425521992028884
          ]
        ],
        "count": 239
      },
      "26": {
        "name": "26_speech_audio_face_speaker",
        "keywords": [
          [
            "speech",
            0.036751618550611174
          ],
          [
            "audio",
            0.02876494786093093
          ],
          [
            "face",
            0.026878652209407904
          ],
          [
            "speaker",
            0.022033547797249472
          ],
          [
            "facial",
            0.01964876716704776
          ],
          [
            "recognition",
            0.019164003615301874
          ],
          [
            "visual",
            0.017082742470098663
          ],
          [
            "lip",
            0.016854975389207803
          ],
          [
            "Speech",
            0.011481849131072428
          ],
          [
            "video",
            0.011002006957524102
          ]
        ],
        "count": 231
      },
      "27": {
        "name": "27_ultrasound_imaging_Ultrasound_photoacoustic",
        "keywords": [
          [
            "ultrasound",
            0.02391815530113206
          ],
          [
            "imaging",
            0.023737475480323057
          ],
          [
            "Ultrasound",
            0.016239292428463156
          ],
          [
            "photoacoustic",
            0.015091412121937138
          ],
          [
            "data",
            0.01263374696723832
          ],
          [
            "vivo",
            0.011810869107784902
          ],
          [
            "resolution",
            0.011628566085161914
          ],
          [
            "beamforming",
            0.011259249743261306
          ],
          [
            "wave",
            0.01092603876876211
          ],
          [
            "image",
            0.010638289872390615
          ]
        ],
        "count": 227
      },
      "28": {
        "name": "28_fetal_brain_fetal brain_Fetal",
        "keywords": [
          [
            "fetal",
            0.05529869413851304
          ],
          [
            "brain",
            0.024178764330452084
          ],
          [
            "fetal brain",
            0.021916438428682673
          ],
          [
            "Fetal",
            0.015191555156456144
          ],
          [
            "MRI",
            0.013975687106874326
          ],
          [
            "segmentation",
            0.013010689570332556
          ],
          [
            "ultrasound",
            0.01296838485856508
          ],
          [
            "data",
            0.008326749641602784
          ],
          [
            "motion",
            0.008218074342733046
          ],
          [
            "clinical",
            0.007471005338683173
          ]
        ],
        "count": 203
      },
      "29": {
        "name": "29_FL_Federated_federated_privacy",
        "keywords": [
          [
            "FL",
            0.03551578884491567
          ],
          [
            "Federated",
            0.03345104443142477
          ],
          [
            "federated",
            0.032200689786771354
          ],
          [
            "privacy",
            0.03206411259103438
          ],
          [
            "data",
            0.025974514327725278
          ],
          [
            "federated learning",
            0.02231502489281394
          ],
          [
            "learning",
            0.020360041963466656
          ],
          [
            "medical",
            0.016345197391720717
          ],
          [
            "client",
            0.014972487227058772
          ],
          [
            "model",
            0.013665254121847586
          ]
        ],
        "count": 197
      },
      "30": {
        "name": "30_point_point cloud_cloud_clouds",
        "keywords": [
          [
            "point",
            0.07104756372515489
          ],
          [
            "point cloud",
            0.0537239157619989
          ],
          [
            "cloud",
            0.05092439517206602
          ],
          [
            "clouds",
            0.03205929724242639
          ],
          [
            "point clouds",
            0.03139171587636197
          ],
          [
            "compression",
            0.03060268159805246
          ],
          [
            "Point",
            0.028999807449605338
          ],
          [
            "geometry",
            0.022967139592108686
          ],
          [
            "PCC",
            0.021536157995252485
          ],
          [
            "Cloud",
            0.021257406152467375
          ]
        ],
        "count": 181
      },
      "31": {
        "name": "31_prostate_prostate cancer_cancer_Prostate",
        "keywords": [
          [
            "prostate",
            0.05776433337705573
          ],
          [
            "prostate cancer",
            0.02818244076000149
          ],
          [
            "cancer",
            0.02678783586675272
          ],
          [
            "Prostate",
            0.02314131458034969
          ],
          [
            "Gleason",
            0.014990582146354016
          ],
          [
            "MRI",
            0.013630468823258473
          ],
          [
            "segmentation",
            0.012575131117742595
          ],
          [
            "PCa",
            0.011943128451709495
          ],
          [
            "biopsy",
            0.010736545399141931
          ],
          [
            "prostate segmentation",
            0.009926756115706594
          ]
        ],
        "count": 171
      },
      "32": {
        "name": "32_denoising_noise_image denoising_image",
        "keywords": [
          [
            "denoising",
            0.052198640032960236
          ],
          [
            "noise",
            0.03276802292888646
          ],
          [
            "image denoising",
            0.026130934018132237
          ],
          [
            "image",
            0.020802918846064365
          ],
          [
            "noisy",
            0.01621213012884354
          ],
          [
            "Denoising",
            0.015635937481042042
          ],
          [
            "Image Denoising",
            0.013310745221887429
          ],
          [
            "network",
            0.013279452100758366
          ],
          [
            "networks",
            0.011993974764573073
          ],
          [
            "real",
            0.011316333709321466
          ]
        ],
        "count": 167
      },
      "33": {
        "name": "33_action_recognition_human_video",
        "keywords": [
          [
            "action",
            0.027818926160752287
          ],
          [
            "recognition",
            0.022496208448085225
          ],
          [
            "human",
            0.017124296698750457
          ],
          [
            "video",
            0.016706090191154797
          ],
          [
            "action recognition",
            0.01604410457592525
          ],
          [
            "actions",
            0.012046445477463065
          ],
          [
            "pose",
            0.011737964077191063
          ],
          [
            "skeleton",
            0.011498452408004348
          ],
          [
            "Action",
            0.010881010129799418
          ],
          [
            "temporal",
            0.01072793715178936
          ]
        ],
        "count": 166
      },
      "34": {
        "name": "34_forensic_attack_face_attacks",
        "keywords": [
          [
            "forensic",
            0.020872902081202384
          ],
          [
            "attack",
            0.018282563332934213
          ],
          [
            "face",
            0.01791256259803096
          ],
          [
            "attacks",
            0.016214950535141028
          ],
          [
            "fake",
            0.01575303998721534
          ],
          [
            "deepfake",
            0.01569471667391135
          ],
          [
            "detection",
            0.014899391158224962
          ],
          [
            "images",
            0.014549374245704865
          ],
          [
            "PRNU",
            0.014059320575915312
          ],
          [
            "camera",
            0.01357959653258707
          ]
        ],
        "count": 160
      },
      "35": {
        "name": "35_polyp_polyps_colonoscopy_polyp segmentation",
        "keywords": [
          [
            "polyp",
            0.03743702953436332
          ],
          [
            "polyps",
            0.026809552002019146
          ],
          [
            "colonoscopy",
            0.020822482065002833
          ],
          [
            "polyp segmentation",
            0.019590931875310188
          ],
          [
            "cancer",
            0.01740643367108986
          ],
          [
            "segmentation",
            0.017086885612874924
          ],
          [
            "Polyp",
            0.015144616766738315
          ],
          [
            "colorectal",
            0.014004446379454943
          ],
          [
            "detection",
            0.010753776027878659
          ],
          [
            "colorectal cancer",
            0.010136990546096307
          ]
        ],
        "count": 159
      }
    },
    "correlations": [
      [
        1.0,
        -0.7581916893377563,
        -0.74681869737966,
        -0.7466155011439026,
        -0.7399236166065545,
        -0.6985373440567935,
        -0.7400943238124074,
        -0.7257892119339611,
        -0.7579312137611932,
        -0.7564745683797749,
        -0.7433086952370729,
        -0.728214735717565,
        -0.7618907786832061,
        -0.7468366144982364,
        -0.7309193923414046,
        -0.7398492290188061,
        -0.7110726416412534,
        -0.7568229542666368,
        -0.7273130239819028,
        -0.7552486513444223,
        -0.7628957660820463,
        -0.759894025988646,
        -0.7615156679554083,
        -0.7633977292363744,
        -0.7467500938452694,
        -0.7500898023411936,
        -0.7396280030500537,
        -0.7509123541317118,
        -0.7610201533568663,
        -0.7386078068241891,
        -0.6997451024816287,
        -0.7634388369218692,
        -0.7159338673005805,
        -0.6813835399755819,
        -0.7454662580780127,
        -0.7623350168394754
      ],
      [
        -0.7581916893377563,
        1.0,
        -0.7320758315480083,
        -0.7562739472033225,
        -0.7622402271973017,
        -0.7174569293872171,
        -0.7538087801489817,
        -0.738765360371106,
        -0.7631631081538914,
        -0.7569599368535751,
        -0.7578437884801694,
        -0.7600083391702529,
        -0.7495224695170224,
        -0.7627572799384228,
        -0.7464660357069283,
        -0.6976962458140252,
        -0.7487854727752241,
        -0.7457293128223664,
        -0.7609360852957627,
        -0.7578163552025456,
        -0.7603337298080579,
        -0.7586210211639124,
        -0.7604373019386705,
        -0.7532178739342903,
        -0.7623057021991659,
        -0.7560087003863291,
        -0.7453800628730511,
        -0.7504746770966946,
        -0.7648380770541549,
        -0.7311266790554369,
        -0.757394611983619,
        -0.7612143672590108,
        -0.7560932404191401,
        -0.7543709087849677,
        -0.75985453325585,
        -0.7544583321816066
      ],
      [
        -0.74681869737966,
        -0.7320758315480083,
        1.0,
        -0.7447185280214246,
        -0.7237773758311378,
        -0.33288057246672437,
        -0.7300793900816039,
        -0.6970121228541101,
        -0.7291894950499651,
        -0.5766409863577668,
        -0.6791744939068287,
        -0.7471606157294692,
        -0.744450412745411,
        -0.7554911323997735,
        -0.717438446856175,
        -0.7074963505879397,
        -0.7132048589829139,
        -0.743251032635107,
        -0.756484947371165,
        -0.6081221557362902,
        -0.7493076899532425,
        -0.5938224103649787,
        -0.7341594326850132,
        -0.7251790253772359,
        -0.7484826584499773,
        -0.7259554228324373,
        -0.7578979146162788,
        -0.7343442194295411,
        -0.7441531175738565,
        -0.7023091596853401,
        -0.7446877919165509,
        -0.7220343029253182,
        -0.7313470092460164,
        -0.7533340635595467,
        -0.7576024684733309,
        -0.5873953217414847
      ],
      [
        -0.7466155011439026,
        -0.7562739472033225,
        -0.7447185280214246,
        1.0,
        -0.7276863214424711,
        -0.7197253274498767,
        -0.7337514841926548,
        -0.7281064191535951,
        -0.7539476563840734,
        -0.752589822304605,
        -0.7293461330009068,
        -0.6607959685436244,
        -0.7571378729002244,
        -0.7319661276423791,
        -0.7418967122197377,
        -0.7302715398929664,
        -0.7364264694558385,
        -0.7471352784581388,
        -0.7301182496600963,
        -0.7479618203400885,
        -0.7578884177323466,
        -0.7563614378160889,
        -0.7521297297220007,
        -0.7630403808305213,
        -0.7274626135950766,
        -0.7456622915988519,
        -0.7593074882281213,
        -0.43243451301893776,
        -0.7581491469354954,
        -0.7430942149076554,
        -0.741316869626365,
        -0.7600875451279522,
        -0.7302001225957305,
        -0.7567339560743724,
        -0.7627807141363598,
        -0.7600620978567236
      ],
      [
        -0.7399236166065545,
        -0.7622402271973017,
        -0.7237773758311378,
        -0.7276863214424711,
        1.0,
        -0.6884612602652453,
        -0.7347357040168174,
        -0.7246652901110917,
        -0.7628807011491288,
        -0.6124385764010614,
        -0.7557850222195857,
        -0.7294724323201236,
        -0.6916035605523752,
        -0.7490531417270634,
        -0.7327428946936119,
        -0.5589882290099515,
        -0.7194788681169408,
        -0.7508599479189905,
        -0.7544853901255253,
        -0.6954604995919065,
        -0.7369634746955422,
        -0.750902927420156,
        -0.7531928312177469,
        -0.7604244536361606,
        -0.7031512599178231,
        -0.7242670575763352,
        -0.7572461010192233,
        -0.7348256974584474,
        -0.7198333690243255,
        -0.7156796556062741,
        -0.7436933017092088,
        -0.7428572311833779,
        -0.7162249957768629,
        -0.7577593836372233,
        -0.7599565044305563,
        -0.7546952810780412
      ],
      [
        -0.6985373440567935,
        -0.7174569293872171,
        -0.33288057246672437,
        -0.7197253274498767,
        -0.6884612602652453,
        1.0,
        -0.5185818888396211,
        -0.33301156977971963,
        -0.7294873243684623,
        -0.6948549452952022,
        -0.7208023498732576,
        -0.7102766851927665,
        -0.7330279635753864,
        -0.7274284454820803,
        -0.7080348990884315,
        -0.6896444462464377,
        -0.4668152783637396,
        -0.7268545217753473,
        -0.7214793082347967,
        -0.7172473493771485,
        -0.7502733149426815,
        -0.7214388803520472,
        -0.7273531334480203,
        -0.7337151033163516,
        -0.7203116248025496,
        -0.7157454006345756,
        -0.7429331302366569,
        -0.7186481676447702,
        -0.7458790106891453,
        -0.44268440828406674,
        -0.7321717446345046,
        -0.7378629218718546,
        -0.5390473826431863,
        -0.7398262303615317,
        -0.73727891578419,
        -0.7274494810408648
      ],
      [
        -0.7400943238124074,
        -0.7538087801489817,
        -0.7300793900816039,
        -0.7337514841926548,
        -0.7347357040168174,
        -0.5185818888396211,
        1.0,
        -0.7160767169486504,
        -0.7573181447171988,
        -0.7488180546590463,
        -0.7414985507985965,
        -0.7196569617538178,
        -0.7531464724222183,
        -0.6968073894952759,
        -0.7282094988931207,
        -0.7456358111227244,
        -0.7158299767561229,
        -0.7377327126823299,
        -0.7479822000659473,
        -0.7535976226255838,
        -0.7567336849903483,
        -0.7539274348878733,
        -0.7592351563246404,
        -0.7574198016763631,
        -0.7372951883511089,
        -0.7428366082278988,
        -0.752642711838087,
        -0.7480229273513666,
        -0.7607125393635281,
        -0.4109686015355526,
        -0.7193585368159747,
        -0.7603739153093576,
        -0.7238812283705256,
        -0.7484187813989582,
        -0.7584270916923459,
        -0.7559035668509267
      ],
      [
        -0.7257892119339611,
        -0.738765360371106,
        -0.6970121228541101,
        -0.7281064191535951,
        -0.7246652901110917,
        -0.33301156977971963,
        -0.7160767169486504,
        1.0,
        -0.7403134621764509,
        -0.7200727409499039,
        -0.7289966707699718,
        -0.726794104694023,
        -0.7369986647535256,
        -0.7346330643398454,
        -0.7130969988537561,
        -0.724588438919934,
        -0.6949556484732013,
        -0.731618673955247,
        -0.7343378244281684,
        -0.7373254584839573,
        -0.7454977021193061,
        -0.742335777161166,
        -0.7035933549431095,
        -0.7465732128273559,
        -0.7318002574588611,
        -0.7089746109170789,
        -0.745178892162154,
        -0.7347601702521365,
        -0.7548871793299482,
        -0.7050527111601392,
        -0.736879763079265,
        -0.7283201324131581,
        -0.7133337875677549,
        -0.744412417801788,
        -0.7415324588913061,
        -0.7311670173435163
      ],
      [
        -0.7579312137611932,
        -0.7631631081538914,
        -0.7291894950499651,
        -0.7539476563840734,
        -0.7628807011491288,
        -0.7294873243684623,
        -0.7573181447171988,
        -0.7403134621764509,
        1.0,
        -0.752332520743884,
        -0.7609751517803728,
        -0.7542792929379303,
        -0.7486639533071745,
        -0.7597207313032235,
        -0.752320170171205,
        -0.7639793086604993,
        -0.7460657433708947,
        -0.7447072634695481,
        -0.7549208400407652,
        -0.7512504097356231,
        -0.7647362284838728,
        -0.6263511788772472,
        -0.7641840308014647,
        -0.754163032869439,
        -0.7603269608411196,
        -0.7514776719366505,
        -0.7626055753126932,
        -0.7546433999880927,
        -0.7626193731438617,
        -0.7475862100823374,
        -0.7571895361249986,
        -0.764615912622493,
        -0.7524398125378413,
        -0.7608984326427151,
        -0.7615767380505539,
        -0.7479579871164053
      ],
      [
        -0.7564745683797749,
        -0.7569599368535751,
        -0.5766409863577668,
        -0.752589822304605,
        -0.6124385764010614,
        -0.6948549452952022,
        -0.7488180546590463,
        -0.7200727409499039,
        -0.752332520743884,
        1.0,
        -0.7544710235084797,
        -0.752767196846636,
        -0.5801448316677851,
        -0.7557576960387988,
        -0.7397639872357626,
        -0.720690592028405,
        -0.7356471693881108,
        -0.7514486272851217,
        -0.7609229445382806,
        -0.6492749068926387,
        -0.7528606177797199,
        -0.6350170454884058,
        -0.7284177737349308,
        -0.7443654253837134,
        -0.7556134776057053,
        -0.7289328193735632,
        -0.7575884102102494,
        -0.7486801144037782,
        -0.618413608074555,
        -0.7272730804056134,
        -0.7570078530087446,
        -0.7389320159110464,
        -0.7480340763636011,
        -0.7588294011370109,
        -0.7619167858090787,
        -0.6155911139927291
      ],
      [
        -0.7433086952370729,
        -0.7578437884801694,
        -0.6791744939068287,
        -0.7293461330009068,
        -0.7557850222195857,
        -0.7208023498732576,
        -0.7414985507985965,
        -0.7289966707699718,
        -0.7609751517803728,
        -0.7544710235084797,
        1.0,
        -0.7494464886775813,
        -0.7631743483343676,
        -0.7517636412850346,
        -0.7316989111545265,
        -0.7507959037435175,
        -0.7443180271625822,
        -0.7027230887728989,
        -0.740618748826249,
        -0.7524222249871327,
        -0.7594037721493498,
        -0.753532306524562,
        -0.7599352053698217,
        -0.7642752072434987,
        -0.7465834177966081,
        -0.7581511048195904,
        -0.756381045355,
        -0.7467994309073185,
        -0.764490238953423,
        -0.7485812192626257,
        -0.7290275053557158,
        -0.7618875493675727,
        -0.7414660005501674,
        -0.746830270796353,
        -0.7514941282900467,
        -0.7530739527997733
      ],
      [
        -0.728214735717565,
        -0.7600083391702529,
        -0.7471606157294692,
        -0.6607959685436244,
        -0.7294724323201236,
        -0.7102766851927665,
        -0.7196569617538178,
        -0.726794104694023,
        -0.7542792929379303,
        -0.752767196846636,
        -0.7494464886775813,
        1.0,
        -0.7586463487603234,
        -0.6990313446453083,
        -0.7358631838101826,
        -0.7397895706689344,
        -0.6936162424244039,
        -0.7547172842646963,
        -0.7314110185761373,
        -0.7558698256366663,
        -0.7582891740771437,
        -0.7564117578120653,
        -0.7604698467763094,
        -0.763492569018958,
        -0.7225688085516626,
        -0.7439131733582937,
        -0.7581706124966298,
        -0.7351961462560073,
        -0.7536751798854234,
        -0.7433278547753932,
        -0.7427868725329256,
        -0.7640351487081851,
        -0.6994000511592449,
        -0.7550376582184328,
        -0.7597986218345184,
        -0.7600848134234852
      ],
      [
        -0.7618907786832061,
        -0.7495224695170224,
        -0.744450412745411,
        -0.7571378729002244,
        -0.6916035605523752,
        -0.7330279635753864,
        -0.7531464724222183,
        -0.7369986647535256,
        -0.7486639533071745,
        -0.5801448316677851,
        -0.7631743483343676,
        -0.7586463487603234,
        1.0,
        -0.7570835824853628,
        -0.7410545154592494,
        -0.752779120415566,
        -0.7396636370879577,
        -0.7517453750742868,
        -0.7601499785238526,
        -0.7572985768630414,
        -0.751062875872691,
        -0.7538723137341659,
        -0.7589267691642397,
        -0.7581948868624759,
        -0.7589644964455317,
        -0.7480470634557259,
        -0.756302536557514,
        -0.7576700605426432,
        -0.5981354562698213,
        -0.7389350362687299,
        -0.7559758813826094,
        -0.7604485974294634,
        -0.7565307628510635,
        -0.7570061943474884,
        -0.7605696071898302,
        -0.7555168143246562
      ],
      [
        -0.7468366144982364,
        -0.7627572799384228,
        -0.7554911323997735,
        -0.7319661276423791,
        -0.7490531417270634,
        -0.7274284454820803,
        -0.6968073894952759,
        -0.7346330643398454,
        -0.7597207313032235,
        -0.7557576960387988,
        -0.7517636412850346,
        -0.6990313446453083,
        -0.7570835824853628,
        1.0,
        -0.7502432246617954,
        -0.7431280517089545,
        -0.7395957515284616,
        -0.7519460345154501,
        -0.7429178113028871,
        -0.7581153572987384,
        -0.7630326852688425,
        -0.7630607265660225,
        -0.7606217291675066,
        -0.7635972822742401,
        -0.7418395991760012,
        -0.7542859649604089,
        -0.7588083308683015,
        -0.7454921265192953,
        -0.7629383150149467,
        -0.7407465930978067,
        -0.7411119969084996,
        -0.7636027061555961,
        -0.7126629301800269,
        -0.7584028399077526,
        -0.7615436255604922,
        -0.762509695730854
      ],
      [
        -0.7309193923414046,
        -0.7464660357069283,
        -0.717438446856175,
        -0.7418967122197377,
        -0.7327428946936119,
        -0.7080348990884315,
        -0.7282094988931207,
        -0.7130969988537561,
        -0.752320170171205,
        -0.7397639872357626,
        -0.7316989111545265,
        -0.7358631838101826,
        -0.7410545154592494,
        -0.7502432246617954,
        1.0,
        -0.7355193640550668,
        -0.7453949713768457,
        -0.7439788302252248,
        -0.7395102663664673,
        -0.7435884795194927,
        -0.7571572831846909,
        -0.7494739608006367,
        -0.7499513094703092,
        -0.7556371883495548,
        -0.7267125665064109,
        -0.7450838073310411,
        -0.7492584665077693,
        -0.7492441197424416,
        -0.7607996946020964,
        -0.7343523982606804,
        -0.7454139416618639,
        -0.7544657254650208,
        -0.7305429457455759,
        -0.7463834668369256,
        -0.7491830307554415,
        -0.7484960418100015
      ],
      [
        -0.7398492290188061,
        -0.6976962458140252,
        -0.7074963505879397,
        -0.7302715398929664,
        -0.5589882290099515,
        -0.6896444462464377,
        -0.7456358111227244,
        -0.724588438919934,
        -0.7639793086604993,
        -0.720690592028405,
        -0.7507959037435175,
        -0.7397895706689344,
        -0.752779120415566,
        -0.7431280517089545,
        -0.7355193640550668,
        1.0,
        -0.7238526044410404,
        -0.7533850713138597,
        -0.7551920544801731,
        -0.7423248047926276,
        -0.7274561932959,
        -0.7335836200274055,
        -0.7431326798763305,
        -0.7537068518257013,
        -0.7114607968398654,
        -0.7250280191916568,
        -0.762124035512286,
        -0.7378072416449971,
        -0.7597868841582989,
        -0.7299939623495202,
        -0.7439517535791902,
        -0.7444989856318768,
        -0.6937325060067709,
        -0.7611512366860123,
        -0.7623009897356473,
        -0.7463585261256913
      ],
      [
        -0.7110726416412534,
        -0.7487854727752241,
        -0.7132048589829139,
        -0.7364264694558385,
        -0.7194788681169408,
        -0.4668152783637396,
        -0.7158299767561229,
        -0.6949556484732013,
        -0.7460657433708947,
        -0.7356471693881108,
        -0.7443180271625822,
        -0.6936162424244039,
        -0.7396636370879577,
        -0.7395957515284616,
        -0.7453949713768457,
        -0.7238526044410404,
        1.0,
        -0.7438560211551094,
        -0.7362893058299946,
        -0.7517088468820041,
        -0.7557743869588981,
        -0.749578732988084,
        -0.7493482723447092,
        -0.7507922929697957,
        -0.7333464460134131,
        -0.7410909799116426,
        -0.7387490900890654,
        -0.7451507280725611,
        -0.7549716302024231,
        -0.7202315913365791,
        -0.7430997165293212,
        -0.7564598082420688,
        -0.5049038583016664,
        -0.753831657830038,
        -0.7312894818281347,
        -0.7472526438436915
      ],
      [
        -0.7568229542666368,
        -0.7457293128223664,
        -0.743251032635107,
        -0.7471352784581388,
        -0.7508599479189905,
        -0.7268545217753473,
        -0.7377327126823299,
        -0.731618673955247,
        -0.7447072634695481,
        -0.7514486272851217,
        -0.7027230887728989,
        -0.7547172842646963,
        -0.7517453750742868,
        -0.7519460345154501,
        -0.7439788302252248,
        -0.7533850713138597,
        -0.7438560211551094,
        1.0,
        -0.7530731934792965,
        -0.7513925212079292,
        -0.7186730622291497,
        -0.7466679668429516,
        -0.7474665141397274,
        -0.7591017363777246,
        -0.7564846619273952,
        -0.7528539672337375,
        -0.7512175130396067,
        -0.751569982031477,
        -0.7617329526065801,
        -0.7464049376724466,
        -0.7490743481259126,
        -0.7530642045713032,
        -0.75089069579243,
        -0.7519718049764756,
        -0.7479954229728414,
        -0.7499674362420063
      ],
      [
        -0.7273130239819028,
        -0.7609360852957627,
        -0.756484947371165,
        -0.7301182496600963,
        -0.7544853901255253,
        -0.7214793082347967,
        -0.7479822000659473,
        -0.7343378244281684,
        -0.7549208400407652,
        -0.7609229445382806,
        -0.740618748826249,
        -0.7314110185761373,
        -0.7601499785238526,
        -0.7429178113028871,
        -0.7395102663664673,
        -0.7551920544801731,
        -0.7362893058299946,
        -0.7530731934792965,
        1.0,
        -0.7614020447659506,
        -0.7604867977055537,
        -0.7586931515778402,
        -0.7633778969599725,
        -0.7633474894837176,
        -0.7492553495799796,
        -0.7558930624467102,
        -0.7588884236025923,
        -0.7430395939080072,
        -0.7634490739380178,
        -0.7518862612351517,
        -0.7495407479829217,
        -0.7630450145799186,
        -0.7058933200047179,
        -0.7530950133637702,
        -0.7512295280301915,
        -0.7593010813142991
      ],
      [
        -0.7552486513444223,
        -0.7578163552025456,
        -0.6081221557362902,
        -0.7479618203400885,
        -0.6954604995919065,
        -0.7172473493771485,
        -0.7535976226255838,
        -0.7373254584839573,
        -0.7512504097356231,
        -0.6492749068926387,
        -0.7524222249871327,
        -0.7558698256366663,
        -0.7572985768630414,
        -0.7581153572987384,
        -0.7435884795194927,
        -0.7423248047926276,
        -0.7517088468820041,
        -0.7513925212079292,
        -0.7614020447659506,
        1.0,
        -0.7576522048447283,
        -0.5895336592892092,
        -0.7600504649024704,
        -0.7558449072456916,
        -0.7564783751503688,
        -0.7164893343842569,
        -0.7608160414518743,
        -0.7441030808590635,
        -0.7438484194903903,
        -0.7433631437005387,
        -0.7503752427609653,
        -0.7543114550713872,
        -0.753959738908292,
        -0.7582619940017732,
        -0.7624040775629164,
        -0.6031141500096779
      ],
      [
        -0.7628957660820463,
        -0.7603337298080579,
        -0.7493076899532425,
        -0.7578884177323466,
        -0.7369634746955422,
        -0.7502733149426815,
        -0.7567336849903483,
        -0.7454977021193061,
        -0.7647362284838728,
        -0.7528606177797199,
        -0.7594037721493498,
        -0.7582891740771437,
        -0.751062875872691,
        -0.7630326852688425,
        -0.7571572831846909,
        -0.7274561932959,
        -0.7557743869588981,
        -0.7186730622291497,
        -0.7604867977055537,
        -0.7576522048447283,
        1.0,
        -0.7552592046949794,
        -0.7577303208592625,
        -0.7638326601001649,
        -0.7604558178787185,
        -0.7523416348904455,
        -0.7635932148747915,
        -0.742834717874893,
        -0.7609537903962323,
        -0.752784297291585,
        -0.7589581227656264,
        -0.7596579249279658,
        -0.7575780083528609,
        -0.7642818283789767,
        -0.7568420060190013,
        -0.7574382991750438
      ],
      [
        -0.759894025988646,
        -0.7586210211639124,
        -0.5938224103649787,
        -0.7563614378160889,
        -0.750902927420156,
        -0.7214388803520472,
        -0.7539274348878733,
        -0.742335777161166,
        -0.6263511788772472,
        -0.6350170454884058,
        -0.753532306524562,
        -0.7564117578120653,
        -0.7538723137341659,
        -0.7630607265660225,
        -0.7494739608006367,
        -0.7335836200274055,
        -0.749578732988084,
        -0.7466679668429516,
        -0.7586931515778402,
        -0.5895336592892092,
        -0.7552592046949794,
        1.0,
        -0.7596558513312337,
        -0.752669860365937,
        -0.759579600656286,
        -0.739272073400865,
        -0.7633473667247987,
        -0.7447015187037325,
        -0.7512881628415466,
        -0.747715116283433,
        -0.7514129214598837,
        -0.75540354674355,
        -0.7552142276069485,
        -0.7623171154909332,
        -0.7627909577329117,
        -0.5794206636115148
      ],
      [
        -0.7615156679554083,
        -0.7604373019386705,
        -0.7341594326850132,
        -0.7521297297220007,
        -0.7531928312177469,
        -0.7273531334480203,
        -0.7592351563246404,
        -0.7035933549431095,
        -0.7641840308014647,
        -0.7284177737349308,
        -0.7599352053698217,
        -0.7604698467763094,
        -0.7589267691642397,
        -0.7606217291675066,
        -0.7499513094703092,
        -0.7431326798763305,
        -0.7493482723447092,
        -0.7474665141397274,
        -0.7633778969599725,
        -0.7600504649024704,
        -0.7577303208592625,
        -0.7596558513312337,
        1.0,
        -0.7391414053167091,
        -0.7615048940773641,
        -0.7449832752786032,
        -0.7648697811340843,
        -0.728323404675814,
        -0.7643388046915023,
        -0.7482017665652008,
        -0.7590970558511271,
        -0.6041797949453818,
        -0.7573291529438679,
        -0.7599798761988104,
        -0.764342400718887,
        -0.67836498633515
      ],
      [
        -0.7633977292363744,
        -0.7532178739342903,
        -0.7251790253772359,
        -0.7630403808305213,
        -0.7604244536361606,
        -0.7337151033163516,
        -0.7574198016763631,
        -0.7465732128273559,
        -0.754163032869439,
        -0.7443654253837134,
        -0.7642752072434987,
        -0.763492569018958,
        -0.7581948868624759,
        -0.7635972822742401,
        -0.7556371883495548,
        -0.7537068518257013,
        -0.7507922929697957,
        -0.7591017363777246,
        -0.7633474894837176,
        -0.7558449072456916,
        -0.7638326601001649,
        -0.752669860365937,
        -0.7391414053167091,
        1.0,
        -0.7623708313017659,
        -0.7541545672133572,
        -0.7561040468799674,
        -0.7605575298799455,
        -0.7602553596280834,
        -0.7497656956908518,
        -0.7590596771790302,
        -0.7459650056787916,
        -0.758509094942198,
        -0.7612766053679463,
        -0.760586639734313,
        -0.7404015618709471
      ],
      [
        -0.7467500938452694,
        -0.7623057021991659,
        -0.7484826584499773,
        -0.7274626135950766,
        -0.7031512599178231,
        -0.7203116248025496,
        -0.7372951883511089,
        -0.7318002574588611,
        -0.7603269608411196,
        -0.7556134776057053,
        -0.7465834177966081,
        -0.7225688085516626,
        -0.7589644964455317,
        -0.7418395991760012,
        -0.7267125665064109,
        -0.7114607968398654,
        -0.7333464460134131,
        -0.7564846619273952,
        -0.7492553495799796,
        -0.7564783751503688,
        -0.7604558178787185,
        -0.759579600656286,
        -0.7615048940773641,
        -0.7623708313017659,
        1.0,
        -0.7523844893504126,
        -0.7584450534893681,
        -0.7436395229435481,
        -0.7614809340935461,
        -0.739129802503708,
        -0.7452049084152068,
        -0.7625663661473203,
        -0.6946649204340576,
        -0.7609562435872841,
        -0.7623623982307435,
        -0.7610487495323197
      ],
      [
        -0.7500898023411936,
        -0.7560087003863291,
        -0.7259554228324373,
        -0.7456622915988519,
        -0.7242670575763352,
        -0.7157454006345756,
        -0.7428366082278988,
        -0.7089746109170789,
        -0.7514776719366505,
        -0.7289328193735632,
        -0.7581511048195904,
        -0.7439131733582937,
        -0.7480470634557259,
        -0.7542859649604089,
        -0.7450838073310411,
        -0.7250280191916568,
        -0.7410909799116426,
        -0.7528539672337375,
        -0.7558930624467102,
        -0.7164893343842569,
        -0.7523416348904455,
        -0.739272073400865,
        -0.7449832752786032,
        -0.7541545672133572,
        -0.7523844893504126,
        1.0,
        -0.7596160610798643,
        -0.743310884963208,
        -0.7373268326577715,
        -0.7475707111014722,
        -0.7433835839451757,
        -0.7399971377362837,
        -0.7452058509377719,
        -0.7598014660270305,
        -0.7608587625927086,
        -0.7529795697414694
      ],
      [
        -0.7396280030500537,
        -0.7453800628730511,
        -0.7578979146162788,
        -0.7593074882281213,
        -0.7572461010192233,
        -0.7429331302366569,
        -0.752642711838087,
        -0.745178892162154,
        -0.7626055753126932,
        -0.7575884102102494,
        -0.756381045355,
        -0.7581706124966298,
        -0.756302536557514,
        -0.7588083308683015,
        -0.7492584665077693,
        -0.762124035512286,
        -0.7387490900890654,
        -0.7512175130396067,
        -0.7588884236025923,
        -0.7608160414518743,
        -0.7635932148747915,
        -0.7633473667247987,
        -0.7648697811340843,
        -0.7561040468799674,
        -0.7584450534893681,
        -0.7596160610798643,
        1.0,
        -0.760205183166516,
        -0.7603708444004675,
        -0.7508364528954772,
        -0.7574324454853801,
        -0.764687752472721,
        -0.7536683808573448,
        -0.6720854432661747,
        -0.6284921446728288,
        -0.7632359559098651
      ],
      [
        -0.7509123541317118,
        -0.7504746770966946,
        -0.7343442194295411,
        -0.43243451301893776,
        -0.7348256974584474,
        -0.7186481676447702,
        -0.7480229273513666,
        -0.7347601702521365,
        -0.7546433999880927,
        -0.7486801144037782,
        -0.7467994309073185,
        -0.7351961462560073,
        -0.7576700605426432,
        -0.7454921265192953,
        -0.7492441197424416,
        -0.7378072416449971,
        -0.7451507280725611,
        -0.751569982031477,
        -0.7430395939080072,
        -0.7441030808590635,
        -0.742834717874893,
        -0.7447015187037325,
        -0.728323404675814,
        -0.7605575298799455,
        -0.7436395229435481,
        -0.743310884963208,
        -0.760205183166516,
        1.0,
        -0.7347037461212972,
        -0.7435787456734738,
        -0.7437185004581566,
        -0.754537253425253,
        -0.7391648468909893,
        -0.7585269595775661,
        -0.7643206363632421,
        -0.7570929624989391
      ],
      [
        -0.7610201533568663,
        -0.7648380770541549,
        -0.7441531175738565,
        -0.7581491469354954,
        -0.7198333690243255,
        -0.7458790106891453,
        -0.7607125393635281,
        -0.7548871793299482,
        -0.7626193731438617,
        -0.618413608074555,
        -0.764490238953423,
        -0.7536751798854234,
        -0.5981354562698213,
        -0.7629383150149467,
        -0.7607996946020964,
        -0.7597868841582989,
        -0.7549716302024231,
        -0.7617329526065801,
        -0.7634490739380178,
        -0.7438484194903903,
        -0.7609537903962323,
        -0.7512881628415466,
        -0.7643388046915023,
        -0.7602553596280834,
        -0.7614809340935461,
        -0.7373268326577715,
        -0.7603708444004675,
        -0.7347037461212972,
        1.0,
        -0.7534800997292319,
        -0.7616998944592084,
        -0.7634031485485053,
        -0.7581250801899769,
        -0.7621297798786161,
        -0.7630172529402062,
        -0.7565706438412102
      ],
      [
        -0.7386078068241891,
        -0.7311266790554369,
        -0.7023091596853401,
        -0.7430942149076554,
        -0.7156796556062741,
        -0.44268440828406674,
        -0.4109686015355526,
        -0.7050527111601392,
        -0.7475862100823374,
        -0.7272730804056134,
        -0.7485812192626257,
        -0.7433278547753932,
        -0.7389350362687299,
        -0.7407465930978067,
        -0.7343523982606804,
        -0.7299939623495202,
        -0.7202315913365791,
        -0.7464049376724466,
        -0.7518862612351517,
        -0.7433631437005387,
        -0.752784297291585,
        -0.747715116283433,
        -0.7482017665652008,
        -0.7497656956908518,
        -0.739129802503708,
        -0.7475707111014722,
        -0.7508364528954772,
        -0.7435787456734738,
        -0.7534800997292319,
        1.0,
        -0.7421777551378953,
        -0.750953854238672,
        -0.7320381332667587,
        -0.7451470436965977,
        -0.7492315962871695,
        -0.7485868867222557
      ],
      [
        -0.6997451024816287,
        -0.757394611983619,
        -0.7446877919165509,
        -0.741316869626365,
        -0.7436933017092088,
        -0.7321717446345046,
        -0.7193585368159747,
        -0.736879763079265,
        -0.7571895361249986,
        -0.7570078530087446,
        -0.7290275053557158,
        -0.7427868725329256,
        -0.7559758813826094,
        -0.7411119969084996,
        -0.7454139416618639,
        -0.7439517535791902,
        -0.7430997165293212,
        -0.7490743481259126,
        -0.7495407479829217,
        -0.7503752427609653,
        -0.7589581227656264,
        -0.7514129214598837,
        -0.7590970558511271,
        -0.7590596771790302,
        -0.7452049084152068,
        -0.7433835839451757,
        -0.7574324454853801,
        -0.7437185004581566,
        -0.7616998944592084,
        -0.7421777551378953,
        1.0,
        -0.761499219145909,
        -0.7395713433391802,
        -0.7539430418573456,
        -0.7520052767783705,
        -0.7593503673508655
      ],
      [
        -0.7634388369218692,
        -0.7612143672590108,
        -0.7220343029253182,
        -0.7600875451279522,
        -0.7428572311833779,
        -0.7378629218718546,
        -0.7603739153093576,
        -0.7283201324131581,
        -0.764615912622493,
        -0.7389320159110464,
        -0.7618875493675727,
        -0.7640351487081851,
        -0.7604485974294634,
        -0.7636027061555961,
        -0.7544657254650208,
        -0.7444989856318768,
        -0.7564598082420688,
        -0.7530642045713032,
        -0.7630450145799186,
        -0.7543114550713872,
        -0.7596579249279658,
        -0.75540354674355,
        -0.6041797949453818,
        -0.7459650056787916,
        -0.7625663661473203,
        -0.7399971377362837,
        -0.764687752472721,
        -0.754537253425253,
        -0.7634031485485053,
        -0.750953854238672,
        -0.761499219145909,
        1.0,
        -0.7603565003913572,
        -0.7608078032123948,
        -0.7644094743664579,
        -0.6607497198823049
      ],
      [
        -0.7159338673005805,
        -0.7560932404191401,
        -0.7313470092460164,
        -0.7302001225957305,
        -0.7162249957768629,
        -0.5390473826431863,
        -0.7238812283705256,
        -0.7133337875677549,
        -0.7524398125378413,
        -0.7480340763636011,
        -0.7414660005501674,
        -0.6994000511592449,
        -0.7565307628510635,
        -0.7126629301800269,
        -0.7305429457455759,
        -0.6937325060067709,
        -0.5049038583016664,
        -0.75089069579243,
        -0.7058933200047179,
        -0.753959738908292,
        -0.7575780083528609,
        -0.7552142276069485,
        -0.7573291529438679,
        -0.758509094942198,
        -0.6946649204340576,
        -0.7452058509377719,
        -0.7536683808573448,
        -0.7391648468909893,
        -0.7581250801899769,
        -0.7320381332667587,
        -0.7395713433391802,
        -0.7603565003913572,
        1.0,
        -0.7541280733300963,
        -0.7459356213509699,
        -0.757524588856687
      ],
      [
        -0.6813835399755819,
        -0.7543709087849677,
        -0.7533340635595467,
        -0.7567339560743724,
        -0.7577593836372233,
        -0.7398262303615317,
        -0.7484187813989582,
        -0.744412417801788,
        -0.7608984326427151,
        -0.7588294011370109,
        -0.746830270796353,
        -0.7550376582184328,
        -0.7570061943474884,
        -0.7584028399077526,
        -0.7463834668369256,
        -0.7611512366860123,
        -0.753831657830038,
        -0.7519718049764756,
        -0.7530950133637702,
        -0.7582619940017732,
        -0.7642818283789767,
        -0.7623171154909332,
        -0.7599798761988104,
        -0.7612766053679463,
        -0.7609562435872841,
        -0.7598014660270305,
        -0.6720854432661747,
        -0.7585269595775661,
        -0.7621297798786161,
        -0.7451470436965977,
        -0.7539430418573456,
        -0.7608078032123948,
        -0.7541280733300963,
        1.0,
        -0.7556295847858252,
        -0.7613651674734203
      ],
      [
        -0.7454662580780127,
        -0.75985453325585,
        -0.7576024684733309,
        -0.7627807141363598,
        -0.7599565044305563,
        -0.73727891578419,
        -0.7584270916923459,
        -0.7415324588913061,
        -0.7615767380505539,
        -0.7619167858090787,
        -0.7514941282900467,
        -0.7597986218345184,
        -0.7605696071898302,
        -0.7615436255604922,
        -0.7491830307554415,
        -0.7623009897356473,
        -0.7312894818281347,
        -0.7479954229728414,
        -0.7512295280301915,
        -0.7624040775629164,
        -0.7568420060190013,
        -0.7627909577329117,
        -0.764342400718887,
        -0.760586639734313,
        -0.7623623982307435,
        -0.7608587625927086,
        -0.6284921446728288,
        -0.7643206363632421,
        -0.7630172529402062,
        -0.7492315962871695,
        -0.7520052767783705,
        -0.7644094743664579,
        -0.7459356213509699,
        -0.7556295847858252,
        1.0,
        -0.7635641135283626
      ],
      [
        -0.7623350168394754,
        -0.7544583321816066,
        -0.5873953217414847,
        -0.7600620978567236,
        -0.7546952810780412,
        -0.7274494810408648,
        -0.7559035668509267,
        -0.7311670173435163,
        -0.7479579871164053,
        -0.6155911139927291,
        -0.7530739527997733,
        -0.7600848134234852,
        -0.7555168143246562,
        -0.762509695730854,
        -0.7484960418100015,
        -0.7463585261256913,
        -0.7472526438436915,
        -0.7499674362420063,
        -0.7593010813142991,
        -0.6031141500096779,
        -0.7574382991750438,
        -0.5794206636115148,
        -0.67836498633515,
        -0.7404015618709471,
        -0.7610487495323197,
        -0.7529795697414694,
        -0.7632359559098651,
        -0.7570929624989391,
        -0.7565706438412102,
        -0.7485868867222557,
        -0.7593503673508655,
        -0.6607497198823049,
        -0.757524588856687,
        -0.7613651674734203,
        -0.7635641135283626,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        65,
        1,
        16,
        8,
        13,
        10,
        10,
        6,
        11,
        5,
        6,
        13,
        5,
        8,
        7,
        11,
        31,
        7,
        5,
        1,
        2,
        3,
        8,
        1,
        7,
        6,
        6,
        0,
        1,
        10,
        10,
        0,
        15,
        6,
        4,
        1
      ],
      "2020-02": [
        105,
        5,
        13,
        6,
        12,
        10,
        4,
        5,
        7,
        9,
        5,
        19,
        5,
        8,
        15,
        10,
        28,
        4,
        8,
        5,
        0,
        4,
        6,
        1,
        5,
        7,
        6,
        4,
        1,
        10,
        15,
        4,
        15,
        6,
        2,
        1
      ],
      "2020-03": [
        100,
        18,
        16,
        10,
        8,
        23,
        5,
        9,
        7,
        9,
        12,
        25,
        2,
        8,
        10,
        11,
        33,
        5,
        13,
        1,
        0,
        7,
        8,
        4,
        11,
        8,
        15,
        3,
        0,
        12,
        18,
        3,
        15,
        13,
        5,
        1
      ],
      "2020-04": [
        110,
        51,
        22,
        6,
        17,
        25,
        12,
        11,
        19,
        8,
        10,
        32,
        5,
        13,
        12,
        4,
        29,
        3,
        13,
        6,
        1,
        1,
        7,
        10,
        8,
        13,
        10,
        1,
        2,
        15,
        19,
        3,
        14,
        13,
        3,
        2
      ],
      "2020-05": [
        91,
        27,
        19,
        9,
        9,
        12,
        6,
        8,
        10,
        11,
        10,
        20,
        3,
        14,
        14,
        16,
        26,
        10,
        12,
        2,
        4,
        4,
        9,
        6,
        11,
        12,
        5,
        3,
        0,
        13,
        28,
        4,
        12,
        8,
        1,
        0
      ],
      "2020-06": [
        95,
        33,
        25,
        6,
        12,
        8,
        8,
        6,
        11,
        10,
        11,
        13,
        4,
        12,
        7,
        13,
        34,
        1,
        10,
        6,
        1,
        3,
        7,
        0,
        10,
        9,
        10,
        4,
        1,
        10,
        19,
        6,
        18,
        9,
        3,
        2
      ],
      "2020-07": [
        81,
        30,
        28,
        10,
        8,
        15,
        11,
        10,
        16,
        11,
        6,
        23,
        7,
        22,
        9,
        11,
        32,
        5,
        12,
        3,
        3,
        7,
        7,
        7,
        16,
        6,
        3,
        2,
        3,
        14,
        18,
        4,
        9,
        13,
        2,
        2
      ],
      "2020-08": [
        85,
        27,
        12,
        6,
        20,
        13,
        3,
        9,
        6,
        10,
        7,
        24,
        6,
        8,
        6,
        14,
        27,
        7,
        12,
        11,
        2,
        2,
        10,
        4,
        9,
        10,
        9,
        3,
        3,
        13,
        16,
        2,
        20,
        10,
        2,
        1
      ],
      "2020-09": [
        86,
        32,
        19,
        6,
        5,
        12,
        10,
        5,
        10,
        8,
        3,
        27,
        8,
        8,
        4,
        14,
        10,
        3,
        3,
        4,
        2,
        3,
        16,
        5,
        3,
        11,
        8,
        4,
        2,
        13,
        13,
        1,
        11,
        8,
        3,
        1
      ],
      "2020-10": [
        74,
        28,
        26,
        7,
        16,
        20,
        2,
        5,
        12,
        17,
        5,
        21,
        6,
        14,
        3,
        12,
        13,
        1,
        7,
        7,
        1,
        3,
        8,
        5,
        9,
        8,
        10,
        4,
        2,
        6,
        10,
        1,
        10,
        5,
        2,
        0
      ],
      "2020-11": [
        63,
        28,
        17,
        8,
        8,
        11,
        8,
        5,
        5,
        13,
        4,
        19,
        7,
        9,
        7,
        9,
        14,
        2,
        8,
        6,
        0,
        0,
        7,
        6,
        11,
        12,
        6,
        5,
        2,
        8,
        10,
        5,
        15,
        3,
        2,
        0
      ],
      "2020-12": [
        56,
        30,
        18,
        6,
        9,
        15,
        13,
        5,
        9,
        14,
        3,
        20,
        5,
        13,
        4,
        10,
        20,
        3,
        9,
        7,
        0,
        1,
        9,
        2,
        9,
        11,
        4,
        0,
        0,
        4,
        10,
        2,
        5,
        3,
        1,
        5
      ],
      "2021-01": [
        53,
        18,
        21,
        5,
        6,
        13,
        4,
        7,
        4,
        12,
        3,
        18,
        8,
        10,
        5,
        9,
        12,
        11,
        14,
        3,
        0,
        4,
        9,
        8,
        5,
        8,
        6,
        2,
        0,
        5,
        12,
        3,
        15,
        2,
        1,
        5
      ],
      "2021-02": [
        47,
        19,
        16,
        7,
        8,
        8,
        7,
        7,
        2,
        6,
        6,
        20,
        1,
        7,
        6,
        13,
        18,
        1,
        6,
        3,
        3,
        1,
        6,
        4,
        6,
        5,
        11,
        2,
        0,
        2,
        9,
        1,
        9,
        3,
        2,
        1
      ],
      "2021-03": [
        72,
        24,
        32,
        5,
        14,
        12,
        8,
        3,
        8,
        11,
        5,
        23,
        8,
        19,
        6,
        14,
        17,
        4,
        10,
        3,
        0,
        2,
        2,
        3,
        13,
        10,
        6,
        3,
        3,
        11,
        11,
        3,
        12,
        4,
        1,
        2
      ],
      "2021-04": [
        61,
        28,
        13,
        6,
        9,
        14,
        6,
        2,
        5,
        9,
        2,
        18,
        6,
        13,
        6,
        13,
        18,
        3,
        4,
        2,
        1,
        0,
        3,
        6,
        4,
        6,
        10,
        1,
        0,
        9,
        6,
        0,
        11,
        0,
        3,
        1
      ],
      "2021-05": [
        52,
        27,
        18,
        9,
        20,
        9,
        8,
        2,
        4,
        8,
        2,
        22,
        4,
        12,
        5,
        7,
        15,
        5,
        4,
        1,
        2,
        4,
        5,
        5,
        10,
        8,
        4,
        2,
        1,
        8,
        10,
        7,
        7,
        2,
        3,
        4
      ],
      "2021-06": [
        60,
        16,
        15,
        9,
        10,
        14,
        4,
        5,
        9,
        4,
        1,
        26,
        4,
        12,
        7,
        12,
        25,
        2,
        10,
        2,
        2,
        2,
        5,
        4,
        8,
        6,
        10,
        2,
        3,
        7,
        15,
        7,
        4,
        5,
        0,
        2
      ],
      "2021-07": [
        60,
        21,
        20,
        7,
        8,
        8,
        5,
        5,
        13,
        6,
        1,
        16,
        6,
        8,
        3,
        13,
        22,
        0,
        9,
        8,
        2,
        5,
        6,
        2,
        4,
        12,
        8,
        2,
        5,
        11,
        10,
        2,
        18,
        2,
        1,
        2
      ],
      "2021-08": [
        49,
        13,
        13,
        4,
        10,
        20,
        10,
        5,
        13,
        12,
        4,
        17,
        4,
        7,
        3,
        8,
        15,
        4,
        9,
        2,
        0,
        1,
        8,
        2,
        6,
        6,
        3,
        2,
        2,
        6,
        10,
        1,
        11,
        2,
        0,
        1
      ],
      "2021-09": [
        55,
        24,
        24,
        4,
        14,
        21,
        4,
        5,
        13,
        13,
        2,
        14,
        3,
        9,
        5,
        10,
        15,
        0,
        7,
        8,
        1,
        5,
        9,
        7,
        2,
        8,
        5,
        5,
        2,
        11,
        8,
        1,
        8,
        3,
        1,
        0
      ],
      "2021-10": [
        60,
        19,
        17,
        2,
        5,
        21,
        7,
        6,
        12,
        10,
        3,
        14,
        3,
        6,
        5,
        15,
        12,
        5,
        15,
        7,
        2,
        3,
        7,
        6,
        10,
        10,
        8,
        1,
        2,
        7,
        6,
        4,
        12,
        4,
        0,
        3
      ],
      "2021-11": [
        58,
        14,
        19,
        4,
        13,
        11,
        7,
        9,
        5,
        11,
        2,
        24,
        7,
        11,
        6,
        7,
        15,
        1,
        9,
        2,
        0,
        5,
        10,
        4,
        3,
        7,
        6,
        5,
        3,
        7,
        16,
        2,
        10,
        4,
        1,
        4
      ],
      "2021-12": [
        69,
        11,
        17,
        5,
        8,
        10,
        6,
        7,
        7,
        12,
        0,
        20,
        2,
        12,
        2,
        7,
        8,
        3,
        12,
        1,
        0,
        3,
        10,
        5,
        7,
        13,
        3,
        1,
        0,
        12,
        10,
        4,
        10,
        1,
        1,
        0
      ],
      "2022-01": [
        65,
        15,
        21,
        6,
        16,
        5,
        9,
        6,
        6,
        14,
        0,
        23,
        4,
        10,
        6,
        12,
        14,
        2,
        6,
        6,
        0,
        1,
        10,
        2,
        10,
        6,
        3,
        7,
        2,
        7,
        5,
        1,
        12,
        1,
        1,
        3
      ],
      "2022-02": [
        67,
        11,
        24,
        7,
        11,
        10,
        1,
        8,
        13,
        9,
        10,
        22,
        2,
        9,
        7,
        12,
        11,
        4,
        10,
        1,
        2,
        3,
        13,
        3,
        10,
        16,
        8,
        2,
        1,
        10,
        11,
        3,
        6,
        2,
        0,
        3
      ],
      "2022-03": [
        98,
        11,
        51,
        5,
        22,
        12,
        6,
        5,
        14,
        15,
        7,
        22,
        8,
        15,
        12,
        19,
        14,
        3,
        10,
        5,
        3,
        0,
        13,
        7,
        10,
        13,
        9,
        0,
        4,
        19,
        12,
        3,
        20,
        6,
        1,
        7
      ],
      "2022-04": [
        64,
        4,
        16,
        4,
        9,
        15,
        8,
        8,
        4,
        8,
        5,
        28,
        5,
        11,
        9,
        9,
        12,
        3,
        9,
        2,
        2,
        5,
        7,
        5,
        5,
        11,
        5,
        3,
        2,
        13,
        13,
        0,
        15,
        2,
        0,
        4
      ],
      "2022-05": [
        61,
        14,
        12,
        6,
        10,
        14,
        6,
        6,
        5,
        4,
        0,
        29,
        6,
        18,
        3,
        13,
        18,
        2,
        6,
        7,
        1,
        3,
        6,
        3,
        4,
        9,
        3,
        1,
        5,
        11,
        16,
        1,
        12,
        4,
        2,
        3
      ],
      "2022-06": [
        83,
        23,
        20,
        4,
        14,
        10,
        12,
        3,
        11,
        16,
        3,
        24,
        10,
        3,
        9,
        5,
        18,
        6,
        9,
        7,
        5,
        1,
        8,
        2,
        4,
        12,
        6,
        3,
        4,
        15,
        15,
        4,
        12,
        2,
        0,
        4
      ],
      "2022-07": [
        96,
        25,
        25,
        5,
        10,
        10,
        6,
        4,
        9,
        9,
        3,
        28,
        6,
        8,
        4,
        10,
        19,
        4,
        13,
        4,
        4,
        4,
        7,
        5,
        10,
        6,
        7,
        4,
        1,
        13,
        11,
        5,
        13,
        2,
        1,
        1
      ],
      "2022-08": [
        58,
        15,
        25,
        3,
        10,
        7,
        3,
        4,
        7,
        10,
        4,
        14,
        8,
        4,
        4,
        12,
        22,
        2,
        10,
        5,
        0,
        3,
        4,
        3,
        7,
        13,
        6,
        2,
        4,
        10,
        15,
        6,
        7,
        2,
        1,
        3
      ],
      "2022-09": [
        65,
        13,
        19,
        7,
        15,
        7,
        2,
        4,
        11,
        9,
        5,
        23,
        10,
        10,
        7,
        9,
        11,
        0,
        12,
        13,
        2,
        4,
        7,
        3,
        6,
        5,
        6,
        1,
        6,
        6,
        14,
        2,
        17,
        8,
        1,
        1
      ],
      "2022-10": [
        76,
        17,
        21,
        7,
        16,
        11,
        6,
        7,
        8,
        14,
        7,
        23,
        11,
        6,
        9,
        10,
        10,
        4,
        10,
        5,
        2,
        3,
        5,
        5,
        4,
        6,
        2,
        1,
        0,
        8,
        9,
        2,
        7,
        2,
        0,
        1
      ],
      "2022-11": [
        82,
        17,
        21,
        2,
        18,
        19,
        9,
        7,
        3,
        14,
        3,
        32,
        12,
        7,
        2,
        13,
        15,
        3,
        11,
        5,
        1,
        4,
        11,
        7,
        8,
        9,
        7,
        4,
        4,
        11,
        16,
        2,
        11,
        1,
        1,
        3
      ],
      "2022-12": [
        44,
        11,
        15,
        4,
        8,
        13,
        5,
        3,
        8,
        9,
        7,
        15,
        4,
        1,
        6,
        11,
        18,
        1,
        10,
        2,
        1,
        2,
        1,
        1,
        5,
        5,
        2,
        1,
        0,
        5,
        10,
        3,
        6,
        2,
        1,
        1
      ],
      "2023-01": [
        46,
        12,
        16,
        2,
        10,
        15,
        3,
        4,
        2,
        11,
        2,
        8,
        3,
        12,
        5,
        5,
        11,
        3,
        4,
        3,
        1,
        3,
        5,
        3,
        3,
        6,
        3,
        1,
        0,
        8,
        7,
        4,
        8,
        2,
        0,
        5
      ],
      "2023-02": [
        56,
        6,
        21,
        1,
        7,
        10,
        2,
        3,
        13,
        6,
        2,
        18,
        4,
        7,
        2,
        8,
        13,
        1,
        5,
        3,
        1,
        2,
        3,
        1,
        3,
        3,
        7,
        4,
        0,
        10,
        11,
        3,
        10,
        1,
        0,
        3
      ],
      "2023-03": [
        90,
        17,
        29,
        3,
        11,
        37,
        5,
        15,
        10,
        8,
        3,
        31,
        7,
        20,
        10,
        19,
        25,
        4,
        14,
        0,
        0,
        2,
        5,
        5,
        8,
        15,
        10,
        3,
        2,
        15,
        18,
        8,
        12,
        3,
        3,
        2
      ],
      "2023-04": [
        66,
        8,
        30,
        5,
        13,
        17,
        9,
        13,
        10,
        19,
        3,
        16,
        2,
        10,
        2,
        12,
        12,
        3,
        10,
        2,
        0,
        4,
        15,
        4,
        7,
        5,
        6,
        2,
        3,
        12,
        12,
        4,
        14,
        4,
        1,
        3
      ],
      "2023-05": [
        75,
        6,
        24,
        5,
        11,
        14,
        10,
        0,
        9,
        22,
        3,
        23,
        7,
        11,
        2,
        11,
        12,
        6,
        12,
        4,
        1,
        4,
        11,
        9,
        13,
        14,
        5,
        3,
        2,
        9,
        15,
        7,
        12,
        1,
        2,
        2
      ],
      "2023-06": [
        75,
        8,
        33,
        1,
        19,
        24,
        12,
        5,
        7,
        9,
        4,
        21,
        8,
        13,
        5,
        15,
        10,
        6,
        11,
        4,
        1,
        2,
        6,
        6,
        5,
        11,
        4,
        4,
        1,
        12,
        14,
        2,
        9,
        1,
        0,
        4
      ],
      "2023-07": [
        78,
        6,
        38,
        1,
        12,
        18,
        8,
        10,
        6,
        10,
        4,
        26,
        6,
        17,
        9,
        18,
        13,
        4,
        15,
        4,
        2,
        8,
        7,
        7,
        6,
        19,
        3,
        1,
        1,
        8,
        18,
        4,
        17,
        4,
        0,
        4
      ],
      "2023-08": [
        76,
        9,
        32,
        5,
        16,
        14,
        18,
        4,
        9,
        11,
        2,
        16,
        7,
        12,
        3,
        11,
        14,
        2,
        10,
        3,
        1,
        7,
        14,
        4,
        6,
        16,
        5,
        8,
        3,
        13,
        20,
        4,
        12,
        3,
        0,
        4
      ],
      "2023-09": [
        76,
        9,
        25,
        3,
        19,
        14,
        8,
        7,
        8,
        10,
        4,
        14,
        11,
        11,
        5,
        17,
        14,
        6,
        11,
        5,
        1,
        1,
        4,
        5,
        10,
        12,
        5,
        4,
        1,
        5,
        14,
        5,
        12,
        5,
        4,
        1
      ],
      "2023-10": [
        62,
        9,
        29,
        4,
        14,
        10,
        7,
        7,
        5,
        11,
        2,
        19,
        13,
        11,
        2,
        15,
        13,
        2,
        10,
        7,
        0,
        5,
        13,
        10,
        7,
        12,
        2,
        2,
        5,
        14,
        22,
        3,
        11,
        1,
        0,
        3
      ],
      "2023-11": [
        65,
        13,
        30,
        4,
        8,
        10,
        12,
        7,
        14,
        8,
        4,
        22,
        6,
        21,
        6,
        18,
        19,
        6,
        4,
        8,
        1,
        5,
        20,
        4,
        8,
        10,
        3,
        2,
        3,
        13,
        13,
        2,
        6,
        1,
        0,
        0
      ],
      "2023-12": [
        75,
        9,
        18,
        0,
        16,
        12,
        6,
        6,
        8,
        8,
        2,
        20,
        7,
        13,
        2,
        15,
        12,
        5,
        16,
        4,
        2,
        2,
        14,
        6,
        4,
        3,
        5,
        4,
        3,
        9,
        9,
        2,
        15,
        6,
        1,
        3
      ],
      "2024-01": [
        69,
        11,
        26,
        1,
        10,
        10,
        9,
        6,
        7,
        12,
        3,
        19,
        8,
        12,
        3,
        10,
        5,
        4,
        3,
        2,
        0,
        2,
        11,
        4,
        2,
        7,
        3,
        1,
        6,
        12,
        10,
        1,
        8,
        2,
        1,
        2
      ],
      "2024-02": [
        84,
        1,
        20,
        4,
        13,
        9,
        7,
        5,
        6,
        12,
        6,
        21,
        3,
        13,
        3,
        21,
        8,
        5,
        9,
        4,
        0,
        5,
        5,
        2,
        10,
        11,
        5,
        3,
        2,
        11,
        12,
        2,
        14,
        0,
        0,
        0
      ],
      "2024-03": [
        91,
        19,
        39,
        8,
        18,
        19,
        5,
        5,
        6,
        16,
        3,
        26,
        16,
        10,
        10,
        13,
        18,
        5,
        16,
        8,
        1,
        5,
        11,
        4,
        10,
        14,
        5,
        2,
        4,
        17,
        17,
        7,
        20,
        3,
        2,
        4
      ],
      "2024-04": [
        75,
        6,
        21,
        5,
        8,
        15,
        7,
        9,
        6,
        8,
        5,
        24,
        2,
        13,
        3,
        10,
        23,
        5,
        13,
        5,
        0,
        4,
        13,
        1,
        5,
        9,
        6,
        4,
        2,
        7,
        9,
        5,
        8,
        2,
        2,
        0
      ],
      "2024-05": [
        86,
        8,
        24,
        1,
        15,
        10,
        7,
        6,
        7,
        9,
        4,
        23,
        6,
        11,
        4,
        16,
        14,
        4,
        7,
        4,
        0,
        1,
        10,
        4,
        11,
        8,
        4,
        1,
        2,
        14,
        9,
        1,
        7,
        2,
        0,
        4
      ],
      "2024-06": [
        74,
        2,
        37,
        6,
        11,
        14,
        7,
        7,
        8,
        11,
        2,
        22,
        6,
        16,
        9,
        25,
        14,
        4,
        14,
        5,
        2,
        7,
        13,
        6,
        6,
        15,
        1,
        0,
        1,
        13,
        11,
        9,
        5,
        2,
        1,
        5
      ],
      "2024-07": [
        93,
        9,
        34,
        4,
        13,
        24,
        8,
        10,
        9,
        9,
        5,
        18,
        11,
        13,
        3,
        14,
        12,
        3,
        5,
        7,
        0,
        6,
        10,
        5,
        8,
        14,
        6,
        4,
        6,
        16,
        11,
        5,
        22,
        2,
        1,
        7
      ],
      "2024-08": [
        76,
        7,
        25,
        2,
        8,
        12,
        5,
        12,
        13,
        7,
        3,
        21,
        7,
        14,
        4,
        19,
        14,
        2,
        5,
        5,
        1,
        3,
        11,
        5,
        0,
        15,
        4,
        5,
        3,
        9,
        10,
        1,
        12,
        4,
        1,
        2
      ],
      "2024-09": [
        102,
        4,
        32,
        4,
        15,
        15,
        6,
        6,
        13,
        18,
        5,
        28,
        7,
        13,
        4,
        16,
        18,
        5,
        14,
        4,
        1,
        7,
        16,
        6,
        4,
        12,
        8,
        3,
        3,
        12,
        16,
        6,
        10,
        3,
        0,
        1
      ],
      "2024-10": [
        92,
        9,
        28,
        8,
        17,
        17,
        6,
        8,
        12,
        17,
        1,
        32,
        14,
        10,
        2,
        14,
        14,
        6,
        6,
        11,
        3,
        2,
        10,
        6,
        6,
        13,
        8,
        2,
        3,
        16,
        14,
        6,
        9,
        3,
        0,
        1
      ],
      "2024-11": [
        88,
        5,
        20,
        0,
        18,
        10,
        7,
        14,
        11,
        16,
        3,
        21,
        9,
        16,
        2,
        12,
        19,
        3,
        9,
        6,
        1,
        6,
        12,
        7,
        11,
        14,
        7,
        2,
        6,
        15,
        13,
        6,
        17,
        0,
        1,
        1
      ],
      "2024-12": [
        96,
        4,
        27,
        2,
        11,
        9,
        11,
        11,
        6,
        13,
        8,
        20,
        7,
        14,
        3,
        11,
        11,
        4,
        14,
        7,
        1,
        2,
        4,
        3,
        8,
        11,
        4,
        0,
        2,
        8,
        13,
        3,
        15,
        3,
        0,
        3
      ],
      "2025-01": [
        78,
        9,
        30,
        2,
        17,
        12,
        12,
        8,
        14,
        11,
        3,
        15,
        11,
        14,
        3,
        16,
        11,
        4,
        5,
        2,
        2,
        5,
        13,
        7,
        8,
        9,
        3,
        2,
        3,
        8,
        12,
        10,
        10,
        0,
        0,
        1
      ],
      "2025-02": [
        70,
        3,
        27,
        2,
        10,
        6,
        11,
        5,
        8,
        11,
        7,
        13,
        6,
        11,
        1,
        13,
        13,
        7,
        4,
        4,
        2,
        2,
        5,
        1,
        16,
        11,
        6,
        2,
        2,
        11,
        8,
        7,
        9,
        2,
        1,
        4
      ],
      "2025-03": [
        138,
        8,
        31,
        2,
        11,
        10,
        14,
        9,
        15,
        16,
        7,
        22,
        7,
        14,
        2,
        10,
        17,
        3,
        15,
        4,
        1,
        6,
        14,
        7,
        13,
        16,
        3,
        1,
        5,
        20,
        18,
        2,
        14,
        4,
        0,
        3
      ],
      "2025-04": [
        64,
        4,
        24,
        2,
        9,
        11,
        12,
        6,
        9,
        9,
        6,
        14,
        3,
        9,
        4,
        11,
        7,
        2,
        7,
        5,
        0,
        4,
        5,
        4,
        6,
        6,
        2,
        0,
        2,
        6,
        13,
        2,
        13,
        2,
        2,
        2
      ],
      "2025-05": [
        117,
        7,
        23,
        4,
        16,
        7,
        9,
        8,
        13,
        12,
        4,
        29,
        8,
        18,
        6,
        12,
        17,
        1,
        15,
        7,
        0,
        3,
        12,
        3,
        14,
        17,
        6,
        3,
        4,
        17,
        20,
        5,
        13,
        1,
        1,
        3
      ],
      "2025-06": [
        84,
        4,
        27,
        5,
        18,
        18,
        8,
        3,
        5,
        10,
        1,
        26,
        7,
        15,
        3,
        17,
        6,
        5,
        14,
        6,
        0,
        5,
        12,
        7,
        8,
        8,
        4,
        2,
        6,
        9,
        21,
        7,
        16,
        2,
        0,
        0
      ],
      "2025-07": [
        95,
        7,
        31,
        10,
        19,
        10,
        8,
        5,
        6,
        15,
        5,
        15,
        6,
        19,
        5,
        16,
        11,
        5,
        12,
        11,
        4,
        6,
        15,
        6,
        5,
        13,
        4,
        2,
        2,
        7,
        5,
        5,
        8,
        2,
        0,
        3
      ],
      "2025-08": [
        114,
        7,
        16,
        1,
        16,
        9,
        5,
        11,
        3,
        7,
        5,
        20,
        9,
        8,
        4,
        16,
        11,
        4,
        8,
        7,
        1,
        4,
        19,
        6,
        6,
        14,
        5,
        1,
        1,
        11,
        12,
        4,
        7,
        1,
        2,
        2
      ],
      "2025-09": [
        34,
        2,
        7,
        2,
        3,
        2,
        2,
        5,
        5,
        3,
        0,
        4,
        4,
        5,
        1,
        2,
        3,
        1,
        2,
        2,
        0,
        1,
        1,
        1,
        3,
        7,
        4,
        1,
        3,
        3,
        5,
        0,
        4,
        3,
        0,
        0
      ]
    },
    "papers": {
      "0": [
        {
          "title": "Selective compression learning of latent representations for variable-rate image compression",
          "year": "2022-11",
          "abstract": "Recently, many neural network-based image compression methods have shown\npromising results superior to the existing tool-based conventional codecs.\nHowever, most of them are often trained as separate models for different target\nbit rates, thus increasing the model complexity. Therefore, several studies\nhave been conducted for learned compression that supports variable rates with\nsingle models, but they require additional network modules, layers, or inputs\nthat often lead to complexity overhead, or do not provide sufficient coding\nefficiency. In this paper, we firstly propose a selective compression method\nthat partially encodes the latent representations in a fully generalized manner\nfor deep learning-based variable-rate image compression. The proposed method\nadaptively determines essential representation elements for compression of\ndifferent target quality levels. For this, we first generate a 3D importance\nmap as the nature of input content to represent the underlying importance of\nthe representation elements. The 3D importance map is then adjusted for\ndifferent target quality levels using importance adjustment curves. The\nadjusted 3D importance map is finally converted into a 3D binary mask to\ndetermine the essential representation elements for compression. The proposed\nmethod can be easily integrated with the existing compression models with a\nnegligible amount of overhead increase. Our method can also enable continuously\nvariable-rate compression via simple interpolation of the importance adjustment\ncurves among different quality levels. The extensive experimental results show\nthat the proposed method can achieve comparable compression efficiency as those\nof the separately trained reference compression models and can reduce decoding\ntime owing to the selective compression. The sample codes are publicly\navailable at https://github.com/JooyoungLeeETRI/SCR.",
          "arxiv_id": "2211.04104v1"
        },
        {
          "title": "JND-Based Perceptual Optimization For Learned Image Compression",
          "year": "2023-02",
          "abstract": "Recently, learned image compression schemes have achieved remarkable\nimprovements in image fidelity (e.g., PSNR and MS-SSIM) compared to\nconventional hybrid image coding ones due to their high-efficiency non-linear\ntransform, end-to-end optimization frameworks, etc. However, few of them take\nthe Just Noticeable Difference (JND) characteristic of the Human Visual System\n(HVS) into account and optimize learned image compression towards perceptual\nquality. To address this issue, a JND-based perceptual quality loss is\nproposed. Considering that the amounts of distortion in the compressed image at\ndifferent training epochs under different Quantization Parameters (QPs) are\ndifferent, we develop a distortion-aware adjustor. After combining them\ntogether, we can better assign the distortion in the compressed image with the\nguidance of JND to preserve the high perceptual quality. All these designs\nenable the proposed method to be flexibly applied to various learned image\ncompression schemes with high scalability and plug-and-play advantages.\nExperimental results on the Kodak dataset demonstrate that the proposed method\nhas led to better perceptual quality than the baseline model under the same bit\nrate.",
          "arxiv_id": "2302.13092v2"
        },
        {
          "title": "Adversarial Distortion for Learned Video Compression",
          "year": "2020-04",
          "abstract": "In this paper, we present a novel adversarial lossy video compression model.\nAt extremely low bit-rates, standard video coding schemes suffer from\nunpleasant reconstruction artifacts such as blocking, ringing etc. Existing\nlearned neural approaches to video compression have achieved reasonable success\non reducing the bit-rate for efficient transmission and reduce the impact of\nartifacts to an extent. However, they still tend to produce blurred results\nunder extreme compression. In this paper, we present a deep adversarial learned\nvideo compression model that minimizes an auxiliary adversarial distortion\nobjective. We find this adversarial objective to correlate better with human\nperceptual quality judgement relative to traditional quality metrics such as\nMS-SSIM and PSNR. Our experiments using a state-of-the-art learned video\ncompression system demonstrate a reduction of perceptual artifacts and\nreconstruction of detail lost especially under extremely high compression.",
          "arxiv_id": "2004.09508v3"
        }
      ],
      "1": [
        {
          "title": "MobileCaps: A Lightweight Model for Screening and Severity Analysis of COVID-19 Chest X-Ray Images",
          "year": "2021-08",
          "abstract": "The world is going through a challenging phase due to the disastrous effect\ncaused by the COVID-19 pandemic on the healthcare system and the economy. The\nrate of spreading, post-COVID-19 symptoms, and the occurrence of new strands of\nCOVID-19 have put the healthcare systems in disruption across the globe. Due to\nthis, the task of accurately screening COVID-19 cases has become of utmost\npriority. Since the virus infects the respiratory system, Chest X-Ray is an\nimaging modality that is adopted extensively for the initial screening. We have\nperformed a comprehensive study that uses CXR images to identify COVID-19 cases\nand realized the necessity of having a more generalizable model. We utilize\nMobileNetV2 architecture as the feature extractor and integrate it into Capsule\nNetworks to construct a fully automated and lightweight model termed as\nMobileCaps. MobileCaps is trained and evaluated on the publicly available\ndataset with the model ensembling and Bayesian optimization strategies to\nefficiently classify CXR images of patients with COVID-19 from non-COVID-19\npneumonia and healthy cases. The proposed model is further evaluated on two\nadditional RT-PCR confirmed datasets to demonstrate the generalizability. We\nalso introduce MobileCaps-S and leverage it for performing severity assessment\nof CXR images of COVID-19 based on the Radiographic Assessment of Lung Edema\n(RALE) scoring technique. Our classification model achieved an overall recall\nof 91.60, 94.60, 92.20, and a precision of 98.50, 88.21, 92.62 for COVID-19,\nnon-COVID-19 pneumonia, and healthy cases, respectively. Further, the severity\nassessment model attained an R$^2$ coefficient of 70.51. Owing to the fact that\nthe proposed models have fewer trainable parameters than the state-of-the-art\nmodels reported in the literature, we believe our models will go a long way in\naiding healthcare systems in the battle against the pandemic.",
          "arxiv_id": "2108.08775v1"
        },
        {
          "title": "COVID-19 Infection Localization and Severity Grading from Chest X-ray Images",
          "year": "2021-03",
          "abstract": "Coronavirus disease 2019 (COVID-19) has been the main agenda of the whole\nworld, since it came into sight in December 2019 as it has significantly\naffected the world economy and healthcare system. Given the effects of COVID-19\non pulmonary tissues, chest radiographic imaging has become a necessity for\nscreening and monitoring the disease. Numerous studies have proposed Deep\nLearning approaches for the automatic diagnosis of COVID-19. Although these\nmethods achieved astonishing performance in detection, they have used limited\nchest X-ray (CXR) repositories for evaluation, usually with a few hundred\nCOVID-19 CXR images only. Thus, such data scarcity prevents reliable evaluation\nwith the potential of overfitting. In addition, most studies showed no or\nlimited capability in infection localization and severity grading of COVID-19\npneumonia. In this study, we address this urgent need by proposing a systematic\nand unified approach for lung segmentation and COVID-19 localization with\ninfection quantification from CXR images. To accomplish this, we have\nconstructed the largest benchmark dataset with 33,920 CXR images, including\n11,956 COVID-19 samples, where the annotation of ground-truth lung segmentation\nmasks is performed on CXRs by a novel human-machine collaborative approach. An\nextensive set of experiments was performed using the state-of-the-art\nsegmentation networks, U-Net, U-Net++, and Feature Pyramid Networks (FPN). The\ndeveloped network, after an extensive iterative process, reached a superior\nperformance for lung region segmentation with Intersection over Union (IoU) of\n96.11% and Dice Similarity Coefficient (DSC) of 97.99%. Furthermore, COVID-19\ninfections of various shapes and types were reliably localized with 83.05% IoU\nand 88.21% DSC. Finally, the proposed approach has achieved an outstanding\nCOVID-19 detection performance with both sensitivity and specificity values\nabove 99%.",
          "arxiv_id": "2103.07985v1"
        },
        {
          "title": "Auto-Lesion Segmentation with a Novel Intensity Dark Channel Prior for COVID-19 Detection",
          "year": "2023-09",
          "abstract": "During the COVID-19 pandemic, medical imaging techniques like computed\ntomography (CT) scans have demonstrated effectiveness in combating the rapid\nspread of the virus. Therefore, it is crucial to conduct research on\ncomputerized models for the detection of COVID-19 using CT imaging. A novel\nprocessing method has been developed, utilizing radiomic features, to assist in\nthe CT-based diagnosis of COVID-19. Given the lower specificity of traditional\nfeatures in distinguishing between different causes of pulmonary diseases, the\nobjective of this study is to develop a CT-based radiomics framework for the\ndifferentiation of COVID-19 from other lung diseases. The model is designed to\nfocus on outlining COVID-19 lesions, as traditional features often lack\nspecificity in this aspect. The model categorizes images into three classes:\nCOVID-19, non-COVID-19, or normal. It employs enhancement auto-segmentation\nprinciples using intensity dark channel prior (IDCP) and deep neural networks\n(ALS-IDCP-DNN) within a defined range of analysis thresholds. A publicly\navailable dataset comprising COVID-19, normal, and non-COVID-19 classes was\nutilized to validate the proposed model's effectiveness. The best performing\nclassification model, Residual Neural Network with 50 layers (Resnet-50),\nattained an average accuracy, precision, recall, and F1-score of 98.8%, 99%,\n98%, and 98% respectively. These results demonstrate the capability of our\nmodel to accurately classify COVID-19 images, which could aid radiologists in\ndiagnosing suspected COVID-19 patients. Furthermore, our model's performance\nsurpasses that of more than 10 current state-of-the-art studies conducted on\nthe same dataset.",
          "arxiv_id": "2309.12638v2"
        }
      ],
      "2": [
        {
          "title": "AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation",
          "year": "2024-03",
          "abstract": "In the past decades, deep neural networks, particularly convolutional neural\nnetworks, have achieved state-of-the-art performance in a variety of medical\nimage segmentation tasks. Recently, the introduction of the vision transformer\n(ViT) has significantly altered the landscape of deep segmentation models.\nThere has been a growing focus on ViTs, driven by their excellent performance\nand scalability. However, we argue that the current design of the vision\ntransformer-based UNet (ViT-UNet) segmentation models may not effectively\nhandle the heterogeneous appearance (e.g., varying shapes and sizes) of objects\nof interest in medical image segmentation tasks. To tackle this challenge, we\npresent a structured approach to introduce spatially dynamic components to the\nViT-UNet. This adaptation enables the model to effectively capture features of\ntarget objects with diverse appearances. This is achieved by three main\ncomponents: \\textbf{(i)} deformable patch embedding; \\textbf{(ii)} spatially\ndynamic multi-head attention; \\textbf{(iii)} deformable positional encoding.\nThese components were integrated into a novel architecture, termed AgileFormer.\nAgileFormer is a spatially agile ViT-UNet designed for medical image\nsegmentation. Experiments in three segmentation tasks using publicly available\ndatasets demonstrated the effectiveness of the proposed method. The code is\navailable at\n\\href{https://github.com/sotiraslab/AgileFormer}{https://github.com/sotiraslab/AgileFormer}.",
          "arxiv_id": "2404.00122v2"
        },
        {
          "title": "Towards Segment Anything Model (SAM) for Medical Image Segmentation: A Survey",
          "year": "2023-05",
          "abstract": "Due to the flexibility of prompting, foundation models have become the\ndominant force in the domains of natural language processing and image\ngeneration. With the recent introduction of the Segment Anything Model (SAM),\nthe prompt-driven paradigm has entered the realm of image segmentation,\nbringing with a range of previously unexplored capabilities. However, it\nremains unclear whether it can be applicable to medical image segmentation due\nto the significant differences between natural images and medical images.In\nthis work, we summarize recent efforts to extend the success of SAM to medical\nimage segmentation tasks, including both empirical benchmarking and\nmethodological adaptations, and discuss potential future directions for SAM in\nmedical image segmentation. Although directly applying SAM to medical image\nsegmentation cannot obtain satisfying performance on multi-modal and\nmulti-target medical datasets, many insights are drawn to guide future research\nto develop foundation models for medical image analysis. To facilitate future\nresearch, we maintain an active repository that contains up-to-date paper list\nand open-source project summary at https://github.com/YichiZhang98/SAM4MIS.",
          "arxiv_id": "2305.03678v3"
        },
        {
          "title": "Computer-Vision Benchmark Segment-Anything Model (SAM) in Medical Images: Accuracy in 12 Datasets",
          "year": "2023-04",
          "abstract": "Background: The segment-anything model (SAM), introduced in April 2023, shows\npromise as a benchmark model and a universal solution to segment various\nnatural images. It comes without previously-required re-training or fine-tuning\nspecific to each new dataset.\n  Purpose: To test SAM's accuracy in various medical image segmentation tasks\nand investigate potential factors that may affect its accuracy in medical\nimages.\n  Methods: SAM was tested on 12 public medical image segmentation datasets\ninvolving 7,451 subjects. The accuracy was measured by the Dice overlap between\nthe algorithm-segmented and ground-truth masks. SAM was compared with five\nstate-of-the-art algorithms specifically designed for medical image\nsegmentation tasks. Associations of SAM's accuracy with six factors were\ncomputed, independently and jointly, including segmentation difficulties as\nmeasured by segmentation ability score and by Dice overlap in U-Net, image\ndimension, size of the target region, image modality, and contrast.\n  Results: The Dice overlaps from SAM were significantly lower than the five\nmedical-image-based algorithms in all 12 medical image segmentation datasets,\nby a margin of 0.1-0.5 and even 0.6-0.7 Dice. SAM-Semantic was significantly\nassociated with medical image segmentation difficulty and the image modality,\nand SAM-Point and SAM-Box were significantly associated with image segmentation\ndifficulty, image dimension, target region size, and target-vs-background\ncontrast. All these 3 variations of SAM were more accurate in 2D medical\nimages, larger target region sizes, easier cases with a higher Segmentation\nAbility score and higher U-Net Dice, and higher foreground-background contrast.",
          "arxiv_id": "2304.09324v3"
        }
      ],
      "3": [
        {
          "title": "Self-supervised physics-informed generative networks for phase retrieval from a single X-ray hologram",
          "year": "2025-08",
          "abstract": "X-ray phase contrast imaging significantly improves the visualization of\nstructures with weak or uniform absorption, broadening its applications across\na wide range of scientific disciplines. Propagation-based phase contrast is\nparticularly suitable for time- or dose-critical in vivo/in situ/operando\n(tomography) experiments because it requires only a single intensity\nmeasurement. However, the phase information of the wave field is lost during\nthe measurement and must be recovered. Conventional algebraic and iterative\nmethods often rely on specific approximations or boundary conditions that may\nnot be met by many samples or experimental setups. In addition, they require\nmanual tuning of reconstruction parameters by experts, making them less\nadaptable for complex or variable conditions. Here we present a self-learning\napproach for solving the inverse problem of phase retrieval in the near-field\nregime of Fresnel theory using a single intensity measurement (hologram). A\nphysics-informed generative adversarial network is employed to reconstruct both\nthe phase and absorbance of the unpropagated wave field in the sample plane\nfrom a single hologram. Unlike most deep learning approaches for phase\nretrieval, our approach does not require paired, unpaired, or simulated\ntraining data. This significantly broadens the applicability of our approach,\nas acquiring or generating suitable training data remains a major challenge due\nto the wide variability in sample types and experimental configurations. The\nalgorithm demonstrates robust and consistent performance across diverse imaging\nconditions and sample types, delivering quantitative, high-quality\nreconstructions for both simulated data and experimental datasets acquired at\nbeamline P05 at PETRA III (DESY, Hamburg), operated by Helmholtz-Zentrum\nHereon. Furthermore, it enables the simultaneous retrieval of both phase and\nabsorption information.",
          "arxiv_id": "2508.15530v1"
        },
        {
          "title": "Holographic single particle imaging for weakly scattering, heterogeneous nanoscale objects",
          "year": "2022-10",
          "abstract": "Single particle imaging (SPI) at X-ray free electron lasers (XFELs) is a\ntechnique to determine the 3D structure of nanoscale objects like biomolecules\nfrom a large number of diffraction patterns of copies of these objects in\nrandom orientations. Millions of low signal-to-noise diffraction patterns with\nunknown orientation are collected during an X-ray SPI experiment. The patterns\nare then analyzed and merged using a reconstruction algorithm to retrieve the\nfull 3D-structure of particle. The resolution of reconstruction is limited by\nbackground noise, signal-to-noise ratio in diffraction patterns and total\namount of data collected. We recently introduced a reference-enhanced\nholographic single particle imaging methodology [Optica 7,593-601(2020)] to\ncollect high enough signal-to-noise and background tolerant patterns and a\nreconstruction algorithm to recover missing parameters beyond orientation and\nthen directly retrieve the full Fourier model of the sample of interest. Here\nwe describe a phase retrieval algorithm based on maximum likelihood estimation\nusing pattern search dubbed as MaxLP, with better scalability for fine sampling\nof latent parameters and much better performance in the low signal limit.\nFurthermore, we show that structural variations within the target particle are\naveraged in real space, significantly improving robustness to conformational\nheterogeneity in comparison to conventional SPI. With these computational\nimprovements, we believe reference-enhanced SPI is capable of reaching sub-nm\nresolution biomolecule imaging.",
          "arxiv_id": "2210.10611v1"
        },
        {
          "title": "SSR-PR: Single-shot Super-Resolution Phase Retrieval based two prior calibration tests",
          "year": "2021-08",
          "abstract": "We propose a novel approach and algorithm based on two preliminary tests of\nthe optical system elements to enhance the super-resolved complex-valued\nimaging. The approach is developed for inverse phase imaging in a single-shot\nlensless optical setup. Imaging is based on wavefront modulation by a single\nbinary phase mask. The preliminary tests compensate errors in the optical\nsystem and correct a carrying wavefront, reducing the gap between real-life\nexperiments and computational modeling, which improve imaging significantly\nboth qualitatively and quantitatively. These two tests are performed for\nobservation of the laser beam and phase mask along, and might be considered as\na preliminary system calibration. The corrected carrying wavefront is embedded\ninto the proposed iterative Single-shot Super-Resolution Phase Retrieval\n(SSR-PR) algorithm. Improved initial diffraction pattern upsampling, and a\ncombination of sparse and deep learning based filters achieves the\nsuper-resolved reconstructions. Simulations and physical experiments\ndemonstrate the high-quality super-resolution phase imaging. In the\nsimulations, we showed that the SSR-PR algorithm corrects the errors of the\nproposed optical system and reconstructs phase details 4x smaller than the\nsensor pixel size. In physical experiment 2um thick lines of USAF phase-target\nwere resolved, which is almost 2x smaller than the sensor pixel size and\ncorresponds to the smallest resolvable group of used test target. For phase\nbio-imaging, we provide Buccal Epithelial Cells reconstructed in computational\nsuper-resolution and the quality was of the same level as a digital holographic\nsystem with 40x magnification objective. Furthermore, the single-shot advantage\nprovides the possibility to record dynamic scenes, where the framerate is\nlimited only by the used camera. We provide amplitude-phase video clip of a\nmoving alive single-celled eukaryote.",
          "arxiv_id": "2108.05616v1"
        }
      ],
      "4": [
        {
          "title": "MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing pulmonary MRI based on 3D Gaussian representation",
          "year": "2025-05",
          "abstract": "This study presents an unsupervised, motion-resolved reconstruction framework\nfor high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI),\nutilizing a three-dimensional Gaussian representation (3DGS). The proposed\nmethod leverages 3DGS to address the challenges of motion-resolved 3D isotropic\npulmonary MRI reconstruction by enabling data smoothing between voxels for\ncontinuous spatial representation. Pulmonary MRI data acquisition is performed\nusing a golden-angle radial sampling trajectory, with respiratory motion\nsignals extracted from the center of k-space in each radial spoke. Based on the\nestimated motion signal, the k-space data is sorted into multiple respiratory\nphases. A 3DGS framework is then applied to reconstruct a reference image\nvolume from the first motion state. Subsequently, a patient-specific\nconvolutional neural network is trained to estimate the deformation vector\nfields (DVFs), which are used to generate the remaining motion states through\nspatial transformation of the reference volume. The proposed reconstruction\npipeline is evaluated on six datasets from six subjects and bench-marked\nagainst three state-of-the-art reconstruction methods. The experimental\nfindings demonstrate that the proposed reconstruction framework effectively\nreconstructs high-resolution, motion-resolved pulmonary MR images. Compared\nwith existing approaches, it achieves superior image quality, reflected by\nhigher signal-to-noise ratio and contrast-to-noise ratio. The proposed\nunsupervised 3DGS-based reconstruction method enables accurate motion-resolved\npulmonary MRI with isotropic spatial resolution. Its superior performance in\nimage quality metrics over state-of-the-art methods highlights its potential as\na robust solution for clinical pulmonary MR imaging.",
          "arxiv_id": "2505.04959v1"
        },
        {
          "title": "Joint Supervised and Self-supervised Learning for MRI Reconstruction",
          "year": "2023-11",
          "abstract": "Magnetic Resonance Imaging (MRI) represents an important diagnostic modality;\nhowever, its inherently slow acquisition process poses challenges in obtaining\nfully-sampled $k$-space data under motion. In the absence of fully-sampled\nacquisitions, serving as ground truths, training deep learning algorithms in a\nsupervised manner to predict the underlying ground truth image becomes\nchallenging. To address this limitation, self-supervised methods have emerged\nas a viable alternative, leveraging available subsampled $k$-space data to\ntrain deep neural networks for MRI reconstruction. Nevertheless, these\napproaches often fall short when compared to supervised methods. We propose\nJoint Supervised and Self-supervised Learning (JSSL), a novel training approach\nfor deep learning-based MRI reconstruction algorithms aimed at enhancing\nreconstruction quality in cases where target datasets containing fully-sampled\n$k$-space measurements are unavailable. JSSL operates by simultaneously\ntraining a model in a self-supervised learning setting, using subsampled data\nfrom the target dataset(s), and in a supervised learning manner, utilizing\ndatasets with fully-sampled $k$-space data, referred to as proxy datasets. We\ndemonstrate JSSL's efficacy using subsampled prostate or cardiac MRI data as\nthe target datasets, with fully-sampled brain and knee, or brain, knee and\nprostate $k$-space acquisitions, respectively, as proxy datasets. Our results\nshowcase substantial improvements over conventional self-supervised methods,\nvalidated using common image quality metrics. Furthermore, we provide\ntheoretical motivations for JSSL and establish \"rule-of-thumb\" guidelines for\ntraining MRI reconstruction models. JSSL effectively enhances MRI\nreconstruction quality in scenarios where fully-sampled $k$-space data is not\navailable, leveraging the strengths of supervised learning by incorporating\nproxy datasets.",
          "arxiv_id": "2311.15856v3"
        },
        {
          "title": "CL-MRI: Self-Supervised Contrastive Learning to Improve the Accuracy of Undersampled MRI Reconstruction",
          "year": "2023-06",
          "abstract": "In Magnetic Resonance Imaging (MRI), image acquisitions are often\nundersampled in the measurement domain to accelerate the scanning process, at\nthe expense of image quality. However, image quality is a crucial factor that\ninfluences the accuracy of clinical diagnosis; hence, high-quality image\nreconstruction from undersampled measurements has been a key area of research.\nRecently, deep learning (DL) methods have emerged as the state-of-the-art for\nMRI reconstruction, typically involving deep neural networks to transform\nundersampled MRI images into high-quality MRI images through data-driven\nprocesses. Nevertheless, there is clear and significant room for improvement in\nundersampled DL MRI reconstruction to meet the high standards required for\nclinical diagnosis, in terms of eliminating aliasing artifacts and reducing\nimage noise. In this paper, we introduce a self-supervised pretraining\nprocedure using contrastive learning to improve the accuracy of undersampled DL\nMRI reconstruction. We use contrastive learning to transform the MRI image\nrepresentations into a latent space that maximizes mutual information among\ndifferent undersampled representations and optimizes the information content at\nthe input of the downstream DL reconstruction models. Our experiments\ndemonstrate improved reconstruction accuracy across a range of acceleration\nfactors and datasets, both quantitatively and qualitatively. Furthermore, our\nextended experiments validate the proposed framework's robustness under\nadversarial conditions, such as measurement noise, different k-space sampling\npatterns, and pathological abnormalities, and also prove the transfer learning\ncapabilities on MRI datasets with completely different anatomy. Additionally,\nwe conducted experiments to visualize and analyze the properties of the\nproposed MRI contrastive learning latent space.",
          "arxiv_id": "2306.00530v3"
        }
      ],
      "5": [
        {
          "title": "Cross Modality Medical Image Synthesis for Improving Liver Segmentation",
          "year": "2025-03",
          "abstract": "Deep learning-based computer-aided diagnosis (CAD) of medical images requires\nlarge datasets. However, the lack of large publicly available labeled datasets\nlimits the development of deep learning-based CAD systems. Generative\nAdversarial Networks (GANs), in particular, CycleGAN, can be used to generate\nnew cross-domain images without paired training data. However, most\nCycleGAN-based synthesis methods lack the potential to overcome alignment and\nasymmetry between the input and generated data. We propose a two-stage\ntechnique for the synthesis of abdominal MRI using cross-modality translation\nof abdominal CT. We show that the synthetic data can help improve the\nperformance of the liver segmentation network. We increase the number of\nabdominal MRI images through cross-modality image transformation of unpaired CT\nimages using a CycleGAN inspired deformation invariant network called EssNet.\nSubsequently, we combine the synthetic MRI images with the original MRI images\nand use them to improve the accuracy of the U-Net on a liver segmentation task.\nWe train the U-Net on real MRI images and then on real and synthetic MRI\nimages. Consequently, by comparing both scenarios, we achieve an improvement in\nthe performance of U-Net. In summary, the improvement achieved in the\nIntersection over Union (IoU) is 1.17%. The results show potential to address\nthe data scarcity challenge in medical imaging.",
          "arxiv_id": "2503.00945v1"
        },
        {
          "title": "Medical Image Generation using Generative Adversarial Networks",
          "year": "2020-05",
          "abstract": "Generative adversarial networks (GANs) are unsupervised Deep Learning\napproach in the computer vision community which has gained significant\nattention from the last few years in identifying the internal structure of\nmultimodal medical imaging data. The adversarial network simultaneously\ngenerates realistic medical images and corresponding annotations, which proven\nto be useful in many cases such as image augmentation, image registration,\nmedical image generation, image reconstruction, and image-to-image translation.\nThese properties bring the attention of the researcher in the field of medical\nimage analysis and we are witness of rapid adaption in many novel and\ntraditional applications. This chapter provides state-of-the-art progress in\nGANs-based clinical application in medical image generation, and cross-modality\nsynthesis. The various framework of GANs which gained popularity in the\ninterpretation of medical images, such as Deep Convolutional GAN (DCGAN),\nLaplacian GAN (LAPGAN), pix2pix, CycleGAN, and unsupervised image-to-image\ntranslation model (UNIT), continue to improve their performance by\nincorporating additional hybrid architecture, has been discussed. Further, some\nof the recent applications of these frameworks for image reconstruction, and\nsynthesis, and future research directions in the area have been covered.",
          "arxiv_id": "2005.10687v1"
        },
        {
          "title": "Self-Attentive Spatial Adaptive Normalization for Cross-Modality Domain Adaptation",
          "year": "2021-03",
          "abstract": "Despite the successes of deep neural networks on many challenging vision\ntasks, they often fail to generalize to new test domains that are not\ndistributed identically to the training data. The domain adaptation becomes\nmore challenging for cross-modality medical data with a notable domain shift.\nGiven that specific annotated imaging modalities may not be accessible nor\ncomplete. Our proposed solution is based on the cross-modality synthesis of\nmedical images to reduce the costly annotation burden by radiologists and\nbridge the domain gap in radiological images. We present a novel approach for\nimage-to-image translation in medical images, capable of supervised or\nunsupervised (unpaired image data) setups. Built upon adversarial training, we\npropose a learnable self-attentive spatial normalization of the deep\nconvolutional generator network's intermediate activations. Unlike previous\nattention-based image-to-image translation approaches, which are either\ndomain-specific or require distortion of the source domain's structures, we\nunearth the importance of the auxiliary semantic information to handle the\ngeometric changes and preserve anatomical structures during image translation.\nWe achieve superior results for cross-modality segmentation between unpaired\nMRI and CT data for multi-modality whole heart and multi-modal brain tumor MRI\n(T1/T2) datasets compared to the state-of-the-art methods. We also observe\nencouraging results in cross-modality conversion for paired MRI and CT images\non a brain dataset. Furthermore, a detailed analysis of the cross-modality\nimage translation, thorough ablation studies confirm our proposed method's\nefficacy.",
          "arxiv_id": "2103.03781v1"
        }
      ],
      "6": [
        {
          "title": "Deep Learning for Rapid Landslide Detection using Synthetic Aperture Radar (SAR) Datacubes",
          "year": "2022-11",
          "abstract": "With climate change predicted to increase the likelihood of landslide events,\nthere is a growing need for rapid landslide detection technologies that help\ninform emergency responses. Synthetic Aperture Radar (SAR) is a remote sensing\ntechnique that can provide measurements of affected areas independent of\nweather or lighting conditions. Usage of SAR, however, is hindered by domain\nknowledge that is necessary for the pre-processing steps and its interpretation\nrequires expert knowledge. We provide simplified, pre-processed,\nmachine-learning ready SAR datacubes for four globally located landslide events\nobtained from several Sentinel-1 satellite passes before and after a landslide\ntriggering event together with segmentation maps of the landslides. From this\ndataset, using the Hokkaido, Japan datacube, we study the feasibility of\nSAR-based landslide detection with supervised deep learning (DL). Our results\ndemonstrate that DL models can be used to detect landslides from SAR data,\nachieving an Area under the Precision-Recall curve exceeding 0.7. We find that\nadditional satellite visits enhance detection performance, but that early\ndetection is possible when SAR data is combined with terrain information from a\ndigital elevation model. This can be especially useful for time-critical\nemergency interventions. Code is made publicly available at\nhttps://github.com/iprapas/landslide-sar-unet.",
          "arxiv_id": "2211.02869v1"
        },
        {
          "title": "Predicting Gradient is Better: Exploring Self-Supervised Learning for SAR ATR with a Joint-Embedding Predictive Architecture",
          "year": "2023-11",
          "abstract": "The growing Synthetic Aperture Radar (SAR) data has the potential to build a\nfoundation model through Self-Supervised Learning (SSL) methods, which can\nachieve various SAR Automatic Target Recognition (ATR) tasks with pre-training\nin large-scale unlabeled data and fine-tuning in small labeled samples. SSL\naims to construct supervision signals directly from the data, which minimizes\nthe need for expensive expert annotation and maximizes the use of the expanding\ndata pool for a foundational model. This study investigates an effective SSL\nmethod for SAR ATR, which can pave the way for a foundation model in SAR ATR.\nThe primary obstacles faced in SSL for SAR ATR are the small targets in remote\nsensing and speckle noise in SAR images, corresponding to the SSL approach and\nsignals. To overcome these challenges, we present a novel Joint-Embedding\nPredictive Architecture for SAR ATR (SAR-JEPA), which leverages local masked\npatches to predict the multi-scale SAR gradient representations of unseen\ncontext. The key aspect of SAR-JEPA is integrating SAR domain features to\nensure high-quality self-supervised signals as target features. Besides, we\nemploy local masks and multi-scale features to accommodate the various small\ntargets in remote sensing. By fine-tuning and evaluating our framework on three\ntarget recognition datasets (vehicle, ship, and aircraft) with four other\ndatasets as pre-training, we demonstrate its outperformance over other SSL\nmethods and its effectiveness with increasing SAR data. This study showcases\nthe potential of SSL for SAR target recognition across diverse targets, scenes,\nand sensors.Our codes and weights are available in\n\\url{https://github.com/waterdisappear/SAR-JEPA.",
          "arxiv_id": "2311.15153v6"
        },
        {
          "title": "OpenEarthMap-SAR: A Benchmark Synthetic Aperture Radar Dataset for Global High-Resolution Land Cover Mapping",
          "year": "2025-01",
          "abstract": "High-resolution land cover mapping plays a crucial role in addressing a wide\nrange of global challenges, including urban planning, environmental monitoring,\ndisaster response, and sustainable development. However, creating accurate,\nlarge-scale land cover datasets remains a significant challenge due to the\ninherent complexities of geospatial data, such as diverse terrain, varying\nsensor modalities, and atmospheric conditions. Synthetic Aperture Radar (SAR)\nimagery, with its ability to penetrate clouds and capture data in all-weather,\nday-and-night conditions, offers unique advantages for land cover mapping.\nDespite these strengths, the lack of benchmark datasets tailored for SAR\nimagery has limited the development of robust models specifically designed for\nthis data modality. To bridge this gap and facilitate advancements in SAR-based\ngeospatial analysis, we introduce OpenEarthMap-SAR, a benchmark SAR dataset,\nfor global high-resolution land cover mapping. OpenEarthMap-SAR consists of 1.5\nmillion segments of 5033 aerial and satellite images with the size of\n1024$\\times$1024 pixels, covering 35 regions from Japan, France, and the USA,\nwith partially manually annotated and fully pseudo 8-class land cover labels at\na ground sampling distance of 0.15--0.5 m. We evaluated the performance of\nstate-of-the-art methods for semantic segmentation and present challenging\nproblem settings suitable for further technical development. The dataset also\nserves the official dataset for IEEE GRSS Data Fusion Contest Track I. The\ndataset has been made publicly available at\nhttps://zenodo.org/records/14622048.",
          "arxiv_id": "2501.10891v2"
        }
      ],
      "7": [
        {
          "title": "Unsupervised Latent Stain Adaptation for Computational Pathology",
          "year": "2024-06",
          "abstract": "In computational pathology, deep learning (DL) models for tasks such as\nsegmentation or tissue classification are known to suffer from domain shifts\ndue to different staining techniques. Stain adaptation aims to reduce the\ngeneralization error between different stains by training a model on source\nstains that generalizes to target stains. Despite the abundance of target stain\ndata, a key challenge is the lack of annotations. To address this, we propose a\njoint training between artificially labeled and unlabeled data including all\navailable stained images called Unsupervised Latent Stain Adaptation (ULSA).\nOur method uses stain translation to enrich labeled source images with\nsynthetic target images in order to increase the supervised signals. Moreover,\nwe leverage unlabeled target stain images using stain-invariant feature\nconsistency learning. With ULSA we present a semi-supervised strategy for\nefficient stain adaptation without access to annotated target stain data.\nRemarkably, ULSA is task agnostic in patch-level analysis for whole slide\nimages (WSIs). Through extensive evaluation on external datasets, we\ndemonstrate that ULSA achieves state-of-the-art (SOTA) performance in kidney\ntissue segmentation and breast cancer classification across a spectrum of\nstaining variations. Our findings suggest that ULSA is an important framework\nfor stain adaptation in computational pathology.",
          "arxiv_id": "2406.19081v2"
        },
        {
          "title": "LESS: Label-efficient Multi-scale Learning for Cytological Whole Slide Image Screening",
          "year": "2023-06",
          "abstract": "In computational pathology, multiple instance learning (MIL) is widely used\nto circumvent the computational impasse in giga-pixel whole slide image (WSI)\nanalysis. It usually consists of two stages: patch-level feature extraction and\nslide-level aggregation. Recently, pretrained models or self-supervised\nlearning have been used to extract patch features, but they suffer from low\neffectiveness or inefficiency due to overlooking the task-specific supervision\nprovided by slide labels. Here we propose a weakly-supervised Label-Efficient\nWSI Screening method, dubbed LESS, for cytological WSI analysis with only\nslide-level labels, which can be effectively applied to small datasets. First,\nwe suggest using variational positive-unlabeled (VPU) learning to uncover\nhidden labels of both benign and malignant patches. We provide appropriate\nsupervision by using slide-level labels to improve the learning of patch-level\nfeatures. Next, we take into account the sparse and random arrangement of cells\nin cytological WSIs. To address this, we propose a strategy to crop patches at\nmultiple scales and utilize a cross-attention vision transformer (CrossViT) to\ncombine information from different scales for WSI classification. The\ncombination of our two steps achieves task-alignment, improving effectiveness\nand efficiency. We validate the proposed label-efficient method on a urine\ncytology WSI dataset encompassing 130 samples (13,000 patches) and FNAC 2019\ndataset with 212 samples (21,200 patches). The experiment shows that the\nproposed LESS reaches 84.79%, 85.43%, 91.79% and 78.30% on a urine cytology WSI\ndataset, and 96.88%, 96.86%, 98.95%, 97.06% on FNAC 2019 dataset in terms of\naccuracy, AUC, sensitivity and specificity. It outperforms state-of-the-art MIL\nmethods on pathology WSIs and realizes automatic cytological WSI cancer\nscreening.",
          "arxiv_id": "2306.03407v2"
        },
        {
          "title": "Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images",
          "year": "2020-04",
          "abstract": "The rapidly emerging field of computational pathology has the potential to\nenable objective diagnosis, therapeutic response prediction and identification\nof new morphological features of clinical relevance. However, deep\nlearning-based computational pathology approaches either require manual\nannotation of gigapixel whole slide images (WSIs) in fully-supervised settings\nor thousands of WSIs with slide-level labels in a weakly-supervised setting.\nMoreover, whole slide level computational pathology methods also suffer from\ndomain adaptation and interpretability issues. These challenges have prevented\nthe broad adaptation of computational pathology for clinical and research\npurposes. Here we present CLAM - Clustering-constrained attention multiple\ninstance learning, an easy-to-use, high-throughput, and interpretable WSI-level\nprocessing and learning method that only requires slide-level labels while\nbeing data efficient, adaptable and capable of handling multi-class subtyping\nproblems. CLAM is a deep-learning-based weakly-supervised method that uses\nattention-based learning to automatically identify sub-regions of high\ndiagnostic value in order to accurately classify the whole slide, while also\nutilizing instance-level clustering over the representative regions identified\nto constrain and refine the feature space. In three separate analyses, we\ndemonstrate the data efficiency and adaptability of CLAM and its superior\nperformance over standard weakly-supervised classification. We demonstrate that\nCLAM models are interpretable and can be used to identify well-known and new\nmorphological features. We further show that models trained using CLAM are\nadaptable to independent test cohorts, cell phone microscopy images, and\nbiopsies. CLAM is a general-purpose and adaptable method that can be used for a\nvariety of different computational pathology tasks in both clinical and\nresearch settings.",
          "arxiv_id": "2004.09666v2"
        }
      ],
      "8": [
        {
          "title": "OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods",
          "year": "2023-12",
          "abstract": "Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.",
          "arxiv_id": "2312.08255v4"
        },
        {
          "title": "Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images",
          "year": "2023-08",
          "abstract": "Optical Coherence Tomography (OCT) is a novel and effective screening tool\nfor ophthalmic examination. Since collecting OCT images is relatively more\nexpensive than fundus photographs, existing methods use multi-modal learning to\ncomplement limited OCT data with additional context from fundus images.\nHowever, the multi-modal framework requires eye-paired datasets of both\nmodalities, which is impractical for clinical use. To address this problem, we\npropose a novel fundus-enhanced disease-aware distillation model (FDDM), for\nretinal disease classification from OCT images. Our framework enhances the OCT\nmodel during training by utilizing unpaired fundus images and does not require\nthe use of fundus images during testing, which greatly improves the\npracticality and efficiency of our method for clinical use. Specifically, we\npropose a novel class prototype matching to distill disease-related information\nfrom the fundus model to the OCT model and a novel class similarity alignment\nto enforce consistency between disease distribution of both modalities.\nExperimental results show that our proposed approach outperforms single-modal,\nmulti-modal, and state-of-the-art distillation methods for retinal disease\nclassification. Code is available at https://github.com/xmed-lab/FDDM.",
          "arxiv_id": "2308.00291v1"
        },
        {
          "title": "ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model",
          "year": "2020-07",
          "abstract": "Optical Coherence Tomography Angiography (OCT-A) is a non-invasive imaging\ntechnique, and has been increasingly used to image the retinal vasculature at\ncapillary level resolution. However, automated segmentation of retinal vessels\nin OCT-A has been under-studied due to various challenges such as low capillary\nvisibility and high vessel complexity, despite its significance in\nunderstanding many eye-related diseases. In addition, there is no publicly\navailable OCT-A dataset with manually graded vessels for training and\nvalidation. To address these issues, for the first time in the field of retinal\nimage analysis we construct a dedicated Retinal OCT-A SEgmentation dataset\n(ROSE), which consists of 229 OCT-A images with vessel annotations at either\ncenterline-level or pixel level. This dataset has been released for public\naccess to assist researchers in the community in undertaking research in\nrelated topics. Secondly, we propose a novel Split-based Coarse-to-Fine vessel\nsegmentation network (SCF-Net), with the ability to detect thick and thin\nvessels separately. In the SCF-Net, a split-based coarse segmentation (SCS)\nmodule is first introduced to produce a preliminary confidence map of vessels,\nand a split-based refinement (SRN) module is then used to optimize the\nshape/contour of the retinal microvasculature. Thirdly, we perform a thorough\nevaluation of the state-of-the-art vessel segmentation models and our SCF-Net\non the proposed ROSE dataset. The experimental results demonstrate that our\nSCF-Net yields better vessel segmentation performance in OCT-A than both\ntraditional methods and other deep learning methods.",
          "arxiv_id": "2007.05201v2"
        }
      ],
      "9": [
        {
          "title": "ME-Net: Multi-Encoder Net Framework for Brain Tumor Segmentation",
          "year": "2022-03",
          "abstract": "Glioma is the most common and aggressive brain tumor. Magnetic resonance\nimaging (MRI) plays a vital role to evaluate tumors for the arrangement of\ntumor surgery and the treatment of subsequent procedures. However, the manual\nsegmentation of the MRI image is strenuous, which limits its clinical\napplication. With the development of deep learning, a large number of automatic\nsegmentation methods have been developed, but most of them stay in 2D images,\nwhich leads to subpar performance. Moreover, the serious voxel imbalance\nbetween the brain tumor and the background as well as the different sizes and\nlocations of the brain tumor makes the segmentation of 3D images a challenging\nproblem. Aiming at segmenting 3D MRI, we propose a model for brain tumor\nsegmentation with multiple encoders. The structure contains four encoders and\none decoder. The four encoders correspond to the four modalities of the MRI\nimage, perform one-to-one feature extraction, and then merge the feature maps\nof the four modalities into the decoder. This method reduces the difficulty of\nfeature extraction and greatly improves model performance. We also introduced a\nnew loss function named \"Categorical Dice\", and set different weights for\ndifferent segmented regions at the same time, which solved the problem of voxel\nimbalance. We evaluated our approach using the online BraTS 2020 Challenge\nverification. Our proposed method can achieve promising results in the\nvalidation set compared to the state-of-the-art approaches with Dice scores of\n0.70249, 0.88267, and 0.73864 for the intact tumor, tumor core, and enhanced\ntumor, respectively.",
          "arxiv_id": "2203.11213v1"
        },
        {
          "title": "HI-Net: Hyperdense Inception 3D UNet for Brain Tumor Segmentation",
          "year": "2020-12",
          "abstract": "The brain tumor segmentation task aims to classify tissue into the whole\ntumor (WT), tumor core (TC), and enhancing tumor (ET) classes using multimodel\nMRI images. Quantitative analysis of brain tumors is critical for clinical\ndecision making. While manual segmentation is tedious, time-consuming, and\nsubjective, this task is at the same time very challenging to automatic\nsegmentation methods. Thanks to the powerful learning ability, convolutional\nneural networks (CNNs), mainly fully convolutional networks, have shown\npromising brain tumor segmentation. This paper further boosts the performance\nof brain tumor segmentation by proposing hyperdense inception 3D UNet (HI-Net),\nwhich captures multi-scale information by stacking factorization of 3D weighted\nconvolutional layers in the residual inception block. We use hyper dense\nconnections among factorized convolutional layers to extract more contexual\ninformation, with the help of features reusability. We use a dice loss function\nto cope with class imbalances. We validate the proposed architecture on the\nmulti-modal brain tumor segmentation challenges (BRATS) 2020 testing dataset.\nPreliminary results on the BRATS 2020 testing set show that achieved by our\nproposed approach, the dice (DSC) scores of ET, WT, and TC are 0.79457,\n0.87494, and 0.83712, respectively.",
          "arxiv_id": "2012.06760v1"
        },
        {
          "title": "QuickTumorNet: Fast Automatic Multi-Class Segmentation of Brain Tumors",
          "year": "2020-12",
          "abstract": "Non-invasive techniques such as magnetic resonance imaging (MRI) are widely\nemployed in brain tumor diagnostics. However, manual segmentation of brain\ntumors from 3D MRI volumes is a time-consuming task that requires trained\nexpert radiologists. Due to the subjectivity of manual segmentation, there is\nlow inter-rater reliability which can result in diagnostic discrepancies. As\nthe success of many brain tumor treatments depends on early intervention, early\ndetection is paramount. In this context, a fully automated segmentation method\nfor brain tumor segmentation is necessary as an efficient and reliable method\nfor brain tumor detection and quantification. In this study, we propose an\nend-to-end approach for brain tumor segmentation, capitalizing on a modified\nversion of QuickNAT, a brain tissue type segmentation deep convolutional neural\nnetwork (CNN). Our method was evaluated on a data set of 233 patient's T1\nweighted images containing three tumor type classes annotated (meningioma,\nglioma, and pituitary). Our model, QuickTumorNet, demonstrated fast, reliable,\nand accurate brain tumor segmentation that can be utilized to assist clinicians\nin diagnosis and treatment.",
          "arxiv_id": "2012.12410v1"
        }
      ],
      "10": [
        {
          "title": "Real-time Fusion Network for RGB-D Semantic Segmentation Incorporating Unexpected Obstacle Detection for Road-driving Images",
          "year": "2020-02",
          "abstract": "Semantic segmentation has made striking progress due to the success of deep\nconvolutional neural networks. Considering the demands of autonomous driving,\nreal-time semantic segmentation has become a research hotspot these years.\nHowever, few real-time RGB-D fusion semantic segmentation studies are carried\nout despite readily accessible depth information nowadays. In this paper, we\npropose a real-time fusion semantic segmentation network termed RFNet that\neffectively exploits complementary cross-modal information. Building on an\nefficient network architecture, RFNet is capable of running swiftly, which\nsatisfies autonomous vehicles applications. Multi-dataset training is leveraged\nto incorporate unexpected small obstacle detection, enriching the recognizable\nclasses required to face unforeseen hazards in the real world. A comprehensive\nset of experiments demonstrates the effectiveness of our framework. On\nCityscapes, Our method outperforms previous state-of-the-art semantic\nsegmenters, with excellent accuracy and 22Hz inference speed at the full\n2048x1024 resolution, outperforming most existing RGB-D networks.",
          "arxiv_id": "2002.10570v2"
        },
        {
          "title": "SemanticVoxels: Sequential Fusion for 3D Pedestrian Detection using LiDAR Point Cloud and Semantic Segmentation",
          "year": "2020-09",
          "abstract": "3D pedestrian detection is a challenging task in automated driving because\npedestrians are relatively small, frequently occluded and easily confused with\nnarrow vertical objects. LiDAR and camera are two commonly used sensor\nmodalities for this task, which should provide complementary information.\nUnexpectedly, LiDAR-only detection methods tend to outperform multisensor\nfusion methods in public benchmarks. Recently, PointPainting has been presented\nto eliminate this performance drop by effectively fusing the output of a\nsemantic segmentation network instead of the raw image information. In this\npaper, we propose a generalization of PointPainting to be able to apply fusion\nat different levels. After the semantic augmentation of the point cloud, we\nencode raw point data in pillars to get geometric features and semantic point\ndata in voxels to get semantic features and fuse them in an effective way.\nExperimental results on the KITTI test set show that SemanticVoxels achieves\nstate-of-the-art performance in both 3D and bird's eye view pedestrian\ndetection benchmarks. In particular, our approach demonstrates its strength in\ndetecting challenging pedestrian cases and outperforms current state-of-the-art\napproaches.",
          "arxiv_id": "2009.12276v1"
        },
        {
          "title": "3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View Spatial Feature Fusion for 3D Object Detection",
          "year": "2020-04",
          "abstract": "In this paper, we propose a new deep architecture for fusing camera and LiDAR\nsensors for 3D object detection. Because the camera and LiDAR sensor signals\nhave different characteristics and distributions, fusing these two modalities\nis expected to improve both the accuracy and robustness of 3D object detection.\nOne of the challenges presented by the fusion of cameras and LiDAR is that the\nspatial feature maps obtained from each modality are represented by\nsignificantly different views in the camera and world coordinates; hence, it is\nnot an easy task to combine two heterogeneous feature maps without loss of\ninformation. To address this problem, we propose a method called 3D-CVF that\ncombines the camera and LiDAR features using the cross-view spatial feature\nfusion strategy. First, the method employs auto-calibrated projection, to\ntransform the 2D camera features to a smooth spatial feature map with the\nhighest correspondence to the LiDAR features in the bird's eye view (BEV)\ndomain. Then, a gated feature fusion network is applied to use the spatial\nattention maps to mix the camera and LiDAR features appropriately according to\nthe region. Next, camera-LiDAR feature fusion is also achieved in the\nsubsequent proposal refinement stage. The camera feature is used from the 2D\ncamera-view domain via 3D RoI grid pooling and fused with the BEV feature for\nproposal refinement. Our evaluations, conducted on the KITTI and nuScenes 3D\nobject detection datasets demonstrate that the camera-LiDAR fusion offers\nsignificant performance gain over single modality and that the proposed 3D-CVF\nachieves state-of-the-art performance in the KITTI benchmark.",
          "arxiv_id": "2004.12636v2"
        }
      ],
      "11": [
        {
          "title": "A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution",
          "year": "2021-07",
          "abstract": "Recently, most of state-of-the-art single image super-resolution (SISR)\nmethods have attained impressive performance by using deep convolutional neural\nnetworks (DCNNs). The existing SR methods have limited performance due to a\nfixed degradation settings, i.e. usually a bicubic downscaling of\nlow-resolution (LR) image. However, in real-world settings, the LR degradation\nprocess is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR,\nor real LR. Therefore, most SR methods are ineffective and inefficient in\nhandling more than one degradation settings within a single network. To handle\nthe multiple degradation, i.e. refers to multi-domain image super-resolution,\nwe propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and\nscalable approach that super-resolves the LR images for the multiple LR domains\nusing only a single model. The proposed scheme is trained in a StarGAN like\nnetwork topology with a single generator and discriminator networks. We\ndemonstrate the effectiveness of our proposed approach in quantitative and\nqualitative experiments compared to other state-of-the-art methods.",
          "arxiv_id": "2107.03145v1"
        },
        {
          "title": "Real-World Single Image Super-Resolution Under Rainy Condition",
          "year": "2022-06",
          "abstract": "Image super-resolution is an important research area in computer vision that\nhas a wide variety of applications including surveillance, medical imaging etc.\nReal-world signal image super-resolution has become very popular now-a-days due\nto its real-time application. There are still a lot of scopes to improve\nreal-world single image super-resolution specially during challenging weather\nscenarios. In this paper, we have proposed a new algorithm to perform\nreal-world single image super-resolution during rainy condition. Our proposed\nmethod can mitigate the influence of rainy conditions during image\nsuper-resolution. Our experiment results show that our proposed algorithm can\nperform image super-resolution decreasing the negative effects of the rain.",
          "arxiv_id": "2206.08345v1"
        },
        {
          "title": "Deep Cyclic Generative Adversarial Residual Convolutional Networks for Real Image Super-Resolution",
          "year": "2020-09",
          "abstract": "Recent deep learning based single image super-resolution (SISR) methods\nmostly train their models in a clean data domain where the low-resolution (LR)\nand the high-resolution (HR) images come from noise-free settings (same domain)\ndue to the bicubic down-sampling assumption. However, such degradation process\nis not available in real-world settings. We consider a deep cyclic network\nstructure to maintain the domain consistency between the LR and HR data\ndistributions, which is inspired by the recent success of CycleGAN in the\nimage-to-image translation applications. We propose the Super-Resolution\nResidual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a\ngenerative adversarial network (GAN) framework for the LR to HR domain\ntranslation in an end-to-end manner. We demonstrate our proposed approach in\nthe quantitative and qualitative experiments that generalize well to the real\nimage super-resolution and it is easy to deploy for the mobile/embedded\ndevices. In addition, our SR results on the AIM 2020 Real Image SR Challenge\ndatasets demonstrate that the proposed SR approach achieves comparable results\nas the other state-of-art methods.",
          "arxiv_id": "2009.03693v1"
        }
      ],
      "12": [
        {
          "title": "An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization",
          "year": "2021-07",
          "abstract": "Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal\nperiod mild cognitive impairment (MCI) is essential for the delayed disease\nprogression and the improved quality of patients'life. The emerging\ncomputer-aided diagnostic methods that combine deep learning with structural\nmagnetic resonance imaging (sMRI) have achieved encouraging results, but some\nof them are limit of issues such as data leakage and unexplainable diagnosis.\nIn this research, we propose a novel end-to-end deep learning approach for\nautomated diagnosis of AD and localization of important brain regions related\nto the disease from sMRI data. This approach is based on a 2D single model\nstrategy and has the following differences from the current approaches: 1)\nConvolutional Neural Network (CNN) models of different structures and\ncapacities are evaluated systemically and the most suitable model is adopted\nfor AD diagnosis; 2) a data augmentation strategy named Two-stage Random\nRandAugment (TRRA) is proposed to alleviate the overfitting issue caused by\nlimited training data and to improve the classification performance in AD\ndiagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the\nvisually explainable heatmaps that localize and highlight the brain regions\nthat our model focuses on and to make our model more transparent. Our approach\nhas been evaluated on two publicly accessible datasets for two classification\ntasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable\nMCI (sMCI). The experimental results indicate that our approach outperforms the\nstate-of-the-art approaches, including those using multi-model and 3D CNN\nmethods. The resultant localization heatmaps from our approach also highlight\nthe lateral ventricle and some disease-relevant regions of cortex, coincident\nwith the commonly affected regions during the development of AD.",
          "arxiv_id": "2107.13200v1"
        },
        {
          "title": "Brain-Aware Readout Layers in GNNs: Advancing Alzheimer's early Detection and Neuroimaging",
          "year": "2024-10",
          "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder characterized by\nprogressive memory and cognitive decline, affecting millions worldwide.\nDiagnosing AD is challenging due to its heterogeneous nature and variable\nprogression. This study introduces a novel brain-aware readout layer (BA\nreadout layer) for Graph Neural Networks (GNNs), designed to improve\ninterpretability and predictive accuracy in neuroimaging for early AD\ndiagnosis. By clustering brain regions based on functional connectivity and\nnode embedding, this layer improves the GNN's capability to capture complex\nbrain network characteristics. We analyzed neuroimaging data from 383\nparticipants, including both cognitively normal and preclinical AD individuals,\nusing T1-weighted MRI, resting-state fMRI, and FBB-PET to construct brain\ngraphs. Our results show that GNNs with the BA readout layer significantly\noutperform traditional models in predicting the Preclinical Alzheimer's\nCognitive Composite (PACC) score, demonstrating higher robustness and\nstability. The adaptive BA readout layer also offers enhanced interpretability\nby highlighting task-specific brain regions critical to cognitive functions\nimpacted by AD. These findings suggest that our approach provides a valuable\ntool for the early diagnosis and analysis of Alzheimer's disease.",
          "arxiv_id": "2410.14683v1"
        },
        {
          "title": "Predicting conversion of mild cognitive impairment to Alzheimer's disease",
          "year": "2022-03",
          "abstract": "Alzheimer's disease (AD) is the most common age-related dementia. Mild\ncognitive impairment (MCI) is the early stage of cognitive decline before AD.\nIt is crucial to predict the MCI-to-AD conversion for precise management, which\nremains challenging due to the diversity of patients. Previous evidence shows\nthat the brain network generated from diffusion MRI promises to classify\ndementia using deep learning. However, the limited availability of diffusion\nMRI challenges the model training. In this study, we develop a self-supervised\ncontrastive learning approach to generate structural brain networks from\nroutine anatomical MRI under the guidance of diffusion MRI. The generated brain\nnetworks are applied to train a learning framework for predicting the MCI-to-AD\nconversion. Instead of directly modelling the AD brain networks, we train a\ngraph encoder and a variational autoencoder to model the healthy ageing\ntrajectories from brain networks of healthy controls. To predict the MCI-to-AD\nconversion, we further design a recurrent neural networks based approach to\nmodel the longitudinal deviation of patients' brain networks from the healthy\nageing trajectory. Numerical results show that the proposed methods outperform\nthe benchmarks in the prediction task. We also visualize the model\ninterpretation to explain the prediction and identify abnormal changes of white\nmatter tracts.",
          "arxiv_id": "2203.04725v1"
        }
      ],
      "13": [
        {
          "title": "Learning Spatial-Spectral Prior for Super-Resolution of Hyperspectral Imagery",
          "year": "2020-05",
          "abstract": "Recently, single gray/RGB image super-resolution reconstruction task has been\nextensively studied and made significant progress by leveraging the advanced\nmachine learning techniques based on deep convolutional neural networks\n(DCNNs). However, there has been limited technical development focusing on\nsingle hyperspectral image super-resolution due to the high-dimensional and\ncomplex spectral patterns in hyperspectral image. In this paper, we make a step\nforward by investigating how to adapt state-of-the-art residual learning based\nsingle gray/RGB image super-resolution approaches for computationally efficient\nsingle hyperspectral image super-resolution, referred as SSPSR. Specifically,\nwe introduce a spatial-spectral prior network (SSPN) to fully exploit the\nspatial information and the correlation between the spectra of the\nhyperspectral data. Considering that the hyperspectral training samples are\nscarce and the spectral dimension of hyperspectral image data is very high, it\nis nontrivial to train a stable and effective deep network. Therefore, a group\nconvolution (with shared network parameters) and progressive upsampling\nframework is proposed. This will not only alleviate the difficulty in feature\nextraction due to high-dimension of the hyperspectral data, but also make the\ntraining process more stable. To exploit the spatial and spectral prior, we\ndesign a spatial-spectral block (SSB), which consists of a spatial residual\nmodule and a spectral attention residual module. Experimental results on some\nhyperspectral images demonstrate that the proposed SSPSR method enhances the\ndetails of the recovered high-resolution hyperspectral images, and outperforms\nstate-of-the-arts. The source code is available at\n\\url{https://github.com/junjun-jiang/SSPSR",
          "arxiv_id": "2005.08752v2"
        },
        {
          "title": "Unsupervised Hyperspectral and Multispectral Image Blind Fusion Based on Deep Tucker Decomposition Network with Spatial-Spectral Manifold Learning",
          "year": "2024-09",
          "abstract": "Hyperspectral and multispectral image fusion aims to generate high spectral\nand spatial resolution hyperspectral images (HR-HSI) by fusing high-resolution\nmultispectral images (HR-MSI) and low-resolution hyperspectral images (LR-HSI).\nHowever, existing fusion methods encounter challenges such as unknown\ndegradation parameters, incomplete exploitation of the correlation between\nhigh-dimensional structures and deep image features. To overcome these issues,\nin this article, an unsupervised blind fusion method for hyperspectral and\nmultispectral images based on Tucker decomposition and spatial spectral\nmanifold learning (DTDNML) is proposed. We design a novel deep Tucker\ndecomposition network that maps LR-HSI and HR-MSI into a consistent feature\nspace, achieving reconstruction through decoders with shared parameter. To\nbetter exploit and fuse spatial-spectral features in the data, we design a core\ntensor fusion network that incorporates a spatial spectral attention mechanism\nfor aligning and fusing features at different scales. Furthermore, to enhance\nthe capacity in capturing global information, a Laplacian-based\nspatial-spectral manifold constraints is introduced in shared-decoders.\nSufficient experiments have validated that this method enhances the accuracy\nand efficiency of hyperspectral and multispectral fusion on different remote\nsensing datasets. The source code is available at\nhttps://github.com/Shawn-H-Wang/DTDNML.",
          "arxiv_id": "2409.09670v2"
        },
        {
          "title": "Cross-Scope Spatial-Spectral Information Aggregation for Hyperspectral Image Super-Resolution",
          "year": "2023-11",
          "abstract": "Hyperspectral image super-resolution has attained widespread prominence to\nenhance the spatial resolution of hyperspectral images. However,\nconvolution-based methods have encountered challenges in harnessing the global\nspatial-spectral information. The prevailing transformer-based methods have not\nadequately captured the long-range dependencies in both spectral and spatial\ndimensions. To alleviate this issue, we propose a novel cross-scope\nspatial-spectral Transformer (CST) to efficiently investigate long-range\nspatial and spectral similarities for single hyperspectral image\nsuper-resolution. Specifically, we devise cross-attention mechanisms in spatial\nand spectral dimensions to comprehensively model the long-range\nspatial-spectral characteristics. By integrating global information into the\nrectangle-window self-attention, we first design a cross-scope spatial\nself-attention to facilitate long-range spatial interactions. Then, by\nleveraging appropriately characteristic spatial-spectral features, we construct\na cross-scope spectral self-attention to effectively capture the intrinsic\ncorrelations among global spectral bands. Finally, we elaborate a concise\nfeed-forward neural network to enhance the feature representation capacity in\nthe Transformer structure. Extensive experiments over three hyperspectral\ndatasets demonstrate that the proposed CST is superior to other\nstate-of-the-art methods both quantitatively and visually. The code is\navailable at \\url{https://github.com/Tomchenshi/CST.git}.",
          "arxiv_id": "2311.17340v1"
        }
      ],
      "14": [
        {
          "title": "Neuromorphic-P2M: Processing-in-Pixel-in-Memory Paradigm for Neuromorphic Image Sensors",
          "year": "2023-01",
          "abstract": "Edge devices equipped with computer vision must deal with vast amounts of\nsensory data with limited computing resources. Hence, researchers have been\nexploring different energy-efficient solutions such as near-sensor processing,\nin-sensor processing, and in-pixel processing, bringing the computation closer\nto the sensor. In particular, in-pixel processing embeds the computation\ncapabilities inside the pixel array and achieves high energy efficiency by\ngenerating low-level features instead of the raw data stream from CMOS image\nsensors. Many different in-pixel processing techniques and approaches have been\ndemonstrated on conventional frame-based CMOS imagers, however, the\nprocessing-in-pixel approach for neuromorphic vision sensors has not been\nexplored so far. In this work, we for the first time, propose an asynchronous\nnon-von-Neumann analog processing-in-pixel paradigm to perform convolution\noperations by integrating in-situ multi-bit multi-channel convolution inside\nthe pixel array performing analog multiply and accumulate (MAC) operations that\nconsume significantly less energy than their digital MAC alternative. To make\nthis approach viable, we incorporate the circuit's non-ideality, leakage, and\nprocess variations into a novel hardware-algorithm co-design framework that\nleverages extensive HSpice simulations of our proposed circuit using the GF22nm\nFD-SOI technology node. We verified our framework on state-of-the-art\nneuromorphic vision sensor datasets and show that our solution consumes ~2x\nlower backend-processor energy while maintaining almost similar front-end\n(sensor) energy on the IBM DVS128-Gesture dataset than the state-of-the-art\nwhile maintaining a high test accuracy of 88.36%.",
          "arxiv_id": "2301.09111v1"
        },
        {
          "title": "Hardware-Algorithm Co-design Enabling Processing-in-Pixel-in-Memory (P2M) for Neuromorphic Vision Sensors",
          "year": "2023-10",
          "abstract": "The high volume of data transmission between the edge sensor and the cloud\nprocessor leads to energy and throughput bottlenecks for resource-constrained\nedge devices focused on computer vision. Hence, researchers are investigating\ndifferent approaches (e.g., near-sensor processing, in-sensor processing,\nin-pixel processing) by executing computations closer to the sensor to reduce\nthe transmission bandwidth. Specifically, in-pixel processing for neuromorphic\nvision sensors (e.g., dynamic vision sensors (DVS)) involves incorporating\nasynchronous multiply-accumulate (MAC) operations within the pixel array,\nresulting in improved energy efficiency. In a CMOS implementation, low overhead\nenergy-efficient analog MAC accumulates charges on a passive capacitor;\nhowever, the capacitor's limited charge retention time affects the algorithmic\nintegration time choices, impacting the algorithmic accuracy, bandwidth,\nenergy, and training efficiency. Consequently, this results in a design\ntrade-off on the hardware aspect-creating a need for a low-leakage compute unit\nwhile maintaining the area and energy benefits. In this work, we present a\nholistic analysis of the hardware-algorithm co-design trade-off based on the\nlimited integration time posed by the hardware and techniques to improve the\nleakage performance of the in-pixel analog MAC operations.",
          "arxiv_id": "2310.16844v1"
        },
        {
          "title": "Embedded event based object detection with spiking neural network",
          "year": "2024-06",
          "abstract": "The complexity of event-based object detection (OD) poses considerable\nchallenges. Spiking Neural Networks (SNNs) show promising results and pave the\nway for efficient event-based OD. Despite this success, the path to efficient\nSNNs on embedded devices remains a challenge. This is due to the size of the\nnetworks required to accomplish the task and the ability of devices to take\nadvantage of SNNs benefits. Even when \"edge\" devices are considered, they\ntypically use embedded GPUs that consume tens of watts. In response to these\nchallenges, our research introduces an embedded neuromorphic testbench that\nutilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator.\nUsing an extended version of the Qualia framework, we can train, evaluate,\nquantize, and deploy spiking neural networks on an FPGA implementation of\nSPLEAT. We used this testbench to load a state-of-the-art SNN solution,\nestimate the performance loss associated with deploying the network on\ndedicated hardware, and run real-world event-based OD on neuromorphic hardware\nspecifically designed for low-power spiking neural networks. Remarkably, our\nembedded spiking solution, which includes a model with 1.08 million parameters,\noperates efficiently with 490 mJ per prediction.",
          "arxiv_id": "2406.17617v1"
        }
      ],
      "15": [
        {
          "title": "Self-Supervised Training For Low Dose CT Reconstruction",
          "year": "2020-10",
          "abstract": "Ionizing radiation has been the biggest concern in CT imaging. To reduce the\ndose level without compromising the image quality, low-dose CT reconstruction\nhas been offered with the availability of compressed sensing based\nreconstruction methods. Recently, data-driven methods got attention with the\nrise of deep learning, the availability of high computational power, and big\ndatasets. Deep learning based methods have also been used in low-dose CT\nreconstruction problem in different manners. Usually, the success of these\nmethods depends on labeled data. However, recent studies showed that training\ncan be achieved successfully with noisy datasets. In this study, we defined a\ntraining scheme to use low-dose sinograms as their own training targets. We\napplied the self-supervision principle in the projection domain where the noise\nis element-wise independent which is a requirement for self-supervised training\nmethods. Using the self-supervised training, the filtering part of the FBP\nmethod and the parameters of a denoiser neural network are optimized. We\ndemonstrate that our method outperforms both conventional and compressed\nsensing based iterative reconstruction methods qualitatively and quantitatively\nin the reconstruction of analytic CT phantoms and real-world CT images in\nlow-dose CT reconstruction task.",
          "arxiv_id": "2010.13232v2"
        },
        {
          "title": "Diffusion Probabilistic Priors for Zero-Shot Low-Dose CT Image Denoising",
          "year": "2023-05",
          "abstract": "Denoising low-dose computed tomography (CT) images is a critical task in\nmedical image computing. Supervised deep learning-based approaches have made\nsignificant advancements in this area in recent years. However, these methods\ntypically require pairs of low-dose and normal-dose CT images for training,\nwhich are challenging to obtain in clinical settings. Existing unsupervised\ndeep learning-based methods often require training with a large number of\nlow-dose CT images or rely on specially designed data acquisition processes to\nobtain training data. To address these limitations, we propose a novel\nunsupervised method that only utilizes normal-dose CT images during training,\nenabling zero-shot denoising of low-dose CT images. Our method leverages the\ndiffusion model, a powerful generative model. We begin by training a cascaded\nunconditional diffusion model capable of generating high-quality normal-dose CT\nimages from low-resolution to high-resolution. The cascaded architecture makes\nthe training of high-resolution diffusion models more feasible. Subsequently,\nwe introduce low-dose CT images into the reverse process of the diffusion model\nas likelihood, combined with the priors provided by the diffusion model and\niteratively solve multiple maximum a posteriori (MAP) problems to achieve\ndenoising. Additionally, we propose methods to adaptively adjust the\ncoefficients that balance the likelihood and prior in MAP estimations, allowing\nfor adaptation to different noise levels in low-dose CT images. We test our\nmethod on low-dose CT datasets of different regions with varying dose levels.\nThe results demonstrate that our method outperforms the state-of-the-art\nunsupervised method and surpasses several supervised deep learning-based\nmethods. Codes are available in https://github.com/DeepXuan/Dn-Dp.",
          "arxiv_id": "2305.15887v2"
        },
        {
          "title": "End-to-End Deep Learning for Interior Tomography with Low-Dose X-ray CT",
          "year": "2025-01",
          "abstract": "Objective: There exist several X-ray computed tomography (CT) scanning\nstrategies to reduce a radiation dose, such as (1) sparse-view CT, (2) low-dose\nCT, and (3) region-of-interest (ROI) CT (called interior tomography). To\nfurther reduce the dose, the sparse-view and/or low-dose CT settings can be\napplied together with interior tomography. Interior tomography has various\nadvantages in terms of reducing the number of detectors and decreasing the\nX-ray radiation dose. However, a large patient or small field-of-view (FOV)\ndetector can cause truncated projections, and then the reconstructed images\nsuffer from severe cupping artifacts. In addition, although the low-dose CT can\nreduce the radiation exposure dose, analytic reconstruction algorithms produce\nimage noise. Recently, many researchers have utilized image-domain deep\nlearning (DL) approaches to remove each artifact and demonstrated impressive\nperformances, and the theory of deep convolutional framelets supports the\nreason for the performance improvement. Approach: In this paper, we found that\nthe image-domain convolutional neural network (CNN) is difficult to solve\ncoupled artifacts, based on deep convolutional framelets. Significance: To\naddress the coupled problem, we decouple it into two sub-problems: (i) image\ndomain noise reduction inside truncated projection to solve low-dose CT problem\nand (ii) extrapolation of projection outside truncated projection to solve the\nROI CT problem. The decoupled sub-problems are solved directly with a novel\nproposed end-to-end learning using dual-domain CNNs. Main results: We\ndemonstrate that the proposed method outperforms the conventional image-domain\ndeep learning methods, and a projection-domain CNN shows better performance\nthan the image-domain CNNs which are commonly used by many researchers.",
          "arxiv_id": "2501.05085v1"
        }
      ],
      "16": [
        {
          "title": "Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey",
          "year": "2021-11",
          "abstract": "This is a tutorial and survey paper on Generative Adversarial Network (GAN),\nadversarial autoencoders, and their variants. We start with explaining\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\nand DCGAN. The mode collapse problem is introduced and various methods,\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\nintroduce some applications of GAN such as image-to-image translation\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\nGAN), text-to-image translation (including StackGAN), and mixing image\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\nautoencoders based on adversarial learning including adversarial autoencoder,\nPixelGAN, and implicit autoencoder.",
          "arxiv_id": "2111.13282v1"
        },
        {
          "title": "Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey",
          "year": "2023-02",
          "abstract": "Generative Adversarial Networks (GANs) have been very successful for\nsynthesizing the images in a given dataset. The artificially generated images\nby GANs are very realistic. The GANs have shown potential usability in several\ncomputer vision applications, including image generation, image-to-image\ntranslation, video synthesis, and others. Conventionally, the generator network\nis the backbone of GANs, which generates the samples and the discriminator\nnetwork is used to facilitate the training of the generator network. The\ndiscriminator network is usually a Convolutional Neural Network (CNN). Whereas,\nthe generator network is usually either an Up-CNN for image generation or an\nEncoder-Decoder network for image-to-image translation. The convolution-based\nnetworks exploit the local relationship in a layer, which requires the deep\nnetworks to extract the abstract features. Hence, CNNs suffer to exploit the\nglobal relationship in the feature space. However, recently developed\nTransformer networks are able to exploit the global relationship at every\nlayer. The Transformer networks have shown tremendous performance improvement\nfor several problems in computer vision. Motivated from the success of\nTransformer networks and GANs, recent works have tried to exploit the\nTransformers in GAN framework for the image/video synthesis. This paper\npresents a comprehensive survey on the developments and advancements in GANs\nutilizing the Transformer networks for computer vision applications. The\nperformance comparison for several applications on benchmark datasets is also\nperformed and analyzed. The conducted survey will be very useful to deep\nlearning and computer vision community to understand the research trends \\&\ngaps related with Transformer-based GANs and to develop the advanced GAN\narchitectures by exploiting the global and local relationships for different\napplications.",
          "arxiv_id": "2302.08641v1"
        },
        {
          "title": "Image Synthesis with Adversarial Networks: a Comprehensive Survey and Case Studies",
          "year": "2020-12",
          "abstract": "Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains such as computer vision, medicine, and natural\nlanguage processing. Moreover, transforming an object or person to a desired\nshape become a well-studied research in the GANs. GANs are powerful models for\nlearning complex distributions to synthesize semantically meaningful samples.\nHowever, there is a lack of comprehensive review in this field, especially lack\nof a collection of GANs loss-variant, evaluation metrics, remedies for diverse\nimage generation, and stable training. Given the current fast GANs development,\nin this survey, we provide a comprehensive review of adversarial models for\nimage synthesis. We summarize the synthetic image generation methods, and\ndiscuss the categories including image-to-image translation, fusion image\ngeneration, label-to-image mapping, and text-to-image translation. We organize\nthe literature based on their base models, developed ideas related to\narchitectures, constraints, loss functions, evaluation metrics, and training\ndatasets. We present milestones of adversarial models, review an extensive\nselection of previous works in various categories, and present insights on the\ndevelopment route from the model-based to data-driven methods. Further, we\nhighlight a range of potential future research directions. One of the unique\nfeatures of this review is that all software implementations of these GAN\nmethods and datasets have been collected and made available in one place at\nhttps://github.com/pshams55/GAN-Case-Study.",
          "arxiv_id": "2012.13736v1"
        }
      ],
      "17": [
        {
          "title": "Segment Anything in Defect Detection",
          "year": "2023-11",
          "abstract": "Defect detection plays a crucial role in infrared non-destructive testing\nsystems, offering non-contact, safe, and efficient inspection capabilities.\nHowever, challenges such as low resolution, high noise, and uneven heating in\ninfrared thermal images hinder comprehensive and accurate defect detection. In\nthis study, we propose DefectSAM, a novel approach for segmenting defects on\nhighly noisy thermal images based on the widely adopted model, Segment Anything\n(SAM)\\cite{kirillov2023segany}. Harnessing the power of a meticulously curated\ndataset generated through labor-intensive lab experiments and valuable prompts\nfrom experienced experts, DefectSAM surpasses existing state-of-the-art\nsegmentation algorithms and achieves significant improvements in defect\ndetection rates. Notably, DefectSAM excels in detecting weaker and smaller\ndefects on complex and irregular surfaces, reducing the occurrence of missed\ndetections and providing more accurate defect size estimations. Experimental\nstudies conducted on various materials have validated the effectiveness of our\nsolutions in defect detection, which hold significant potential to expedite the\nevolution of defect detection tools, enabling enhanced inspection capabilities\nand accuracy in defect identification.",
          "arxiv_id": "2311.10245v1"
        },
        {
          "title": "A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization",
          "year": "2025-07",
          "abstract": "Surface defect detection of steel, especially the recognition of multi-scale\ndefects, has always been a major challenge in industrial manufacturing. Steel\nsurfaces not only have defects of various sizes and shapes, which limit the\naccuracy of traditional image processing and detection methods in complex\nenvironments. However, traditional defect detection methods face issues of\ninsufficient accuracy and high miss-detection rates when dealing with small\ntarget defects. To address this issue, this study proposes a detection\nframework based on deep learning, specifically YOLOv9s, combined with the\nC3Ghost module, SCConv module, and CARAFE upsampling operator, to improve\ndetection accuracy and model performance. First, the SCConv module is used to\nreduce feature redundancy and optimize feature representation by reconstructing\nthe spatial and channel dimensions. Second, the C3Ghost module is introduced to\nenhance the model's feature extraction ability by reducing redundant\ncomputations and parameter volume, thereby improving model efficiency. Finally,\nthe CARAFE upsampling operator, which can more finely reorganize feature maps\nin a content-aware manner, optimizes the upsampling process and ensures\ndetailed restoration of high-resolution defect regions. Experimental results\ndemonstrate that the proposed model achieves higher accuracy and robustness in\nsteel surface defect detection tasks compared to other methods, effectively\naddressing defect detection problems.",
          "arxiv_id": "2507.15476v2"
        },
        {
          "title": "A Novel Approach for Defect Detection of Wind Turbine Blade Using Virtual Reality and Deep Learning",
          "year": "2023-12",
          "abstract": "Wind turbines are subjected to continuous rotational stresses and unusual\nexternal forces such as storms, lightning, strikes by flying objects, etc.,\nwhich may cause defects in turbine blades. Hence, it requires a periodical\ninspection to ensure proper functionality and avoid catastrophic failure. The\ntask of inspection is challenging due to the remote location and inconvenient\nreachability by human inspection. Researchers used images with cropped defects\nfrom the wind turbine in the literature. They neglected possible background\nbiases, which may hinder real-time and autonomous defect detection using aerial\nvehicles such as drones or others. To overcome such challenges, in this paper,\nwe experiment with defect detection accuracy by having the defects with the\nbackground using a two-step deep-learning methodology. In the first step, we\ndevelop virtual models of wind turbines to synthesize the near-reality images\nfor four types of common defects - cracks, leading edge erosion, bending, and\nlight striking damage. The Unity perception package is used to generate wind\nturbine blade defects images with variations in background, randomness, camera\nangle, and light effects. In the second step, a customized U-Net architecture\nis trained to classify and segment the defect in turbine blades. The outcomes\nof U-Net architecture have been thoroughly tested and compared with 5-fold\nvalidation datasets. The proposed methodology provides reasonable defect\ndetection accuracy, making it suitable for autonomous and remote inspection\nthrough aerial vehicles.",
          "arxiv_id": "2401.00237v1"
        }
      ],
      "18": [
        {
          "title": "Unsupervised Low-light Image Enhancement with Decoupled Networks",
          "year": "2020-05",
          "abstract": "In this paper, we tackle the problem of enhancing real-world low-light images\nwith significant noise in an unsupervised fashion. Conventional unsupervised\nlearning-based approaches usually tackle the low-light image enhancement\nproblem using an image-to-image translation model. They focus primarily on\nillumination or contrast enhancement but fail to suppress the noise that\nubiquitously exists in images taken under real-world low-light conditions. To\naddress this issue, we explicitly decouple this task into two sub-tasks:\nillumination enhancement and noise suppression. We propose to learn a two-stage\nGAN-based framework to enhance the real-world low-light images in a fully\nunsupervised fashion. To facilitate the unsupervised training of our model, we\nconstruct samples with pseudo labels. Furthermore, we propose an adaptive\ncontent loss to suppress real image noise in different regions based on\nillumination intensity. In addition to conventional benchmark datasets, a new\nunpaired low-light image enhancement dataset is built and used to thoroughly\nevaluate the performance of our model. Extensive experiments show that our\nproposed method outperforms the state-of-the-art unsupervised image enhancement\nmethods in terms of both illumination enhancement and noise reduction.",
          "arxiv_id": "2005.02818v2"
        },
        {
          "title": "Deep Bilateral Retinex for Low-Light Image Enhancement",
          "year": "2020-07",
          "abstract": "Low-light images, i.e. the images captured in low-light conditions, suffer\nfrom very poor visibility caused by low contrast, color distortion and\nsignificant measurement noise. Low-light image enhancement is about improving\nthe visibility of low-light images. As the measurement noise in low-light\nimages is usually significant yet complex with spatially-varying\ncharacteristic, how to handle the noise effectively is an important yet\nchallenging problem in low-light image enhancement. Based on the Retinex\ndecomposition of natural images, this paper proposes a deep learning method for\nlow-light image enhancement with a particular focus on handling the measurement\nnoise. The basic idea is to train a neural network to generate a set of\npixel-wise operators for simultaneously predicting the noise and the\nillumination layer, where the operators are defined in the bilateral space.\nSuch an integrated approach allows us to have an accurate prediction of the\nreflectance layer in the presence of significant spatially-varying measurement\nnoise. Extensive experiments on several benchmark datasets have shown that the\nproposed method is very competitive to the state-of-the-art methods, and has\nsignificant advantage over others when processing images captured in extremely\nlow lighting conditions.",
          "arxiv_id": "2007.02018v1"
        },
        {
          "title": "Simplifying Low-Light Image Enhancement Networks with Relative Loss Functions",
          "year": "2023-04",
          "abstract": "Image enhancement is a common technique used to mitigate issues such as\nsevere noise, low brightness, low contrast, and color deviation in low-light\nimages. However, providing an optimal high-light image as a reference for\nlow-light image enhancement tasks is impossible, which makes the learning\nprocess more difficult than other image processing tasks. As a result, although\nseveral low-light image enhancement methods have been proposed, most of them\nare either too complex or insufficient in addressing all the issues in\nlow-light images. In this paper, to make the learning easier in low-light image\nenhancement, we introduce FLW-Net (Fast and LightWeight Network) and two\nrelative loss functions. Specifically, we first recognize the challenges of the\nneed for a large receptive field to obtain global contrast and the lack of an\nabsolute reference, which limits the simplification of network structures in\nthis task. Then, we propose an efficient global feature information extraction\ncomponent and two loss functions based on relative information to overcome\nthese challenges. Finally, we conducted comparative experiments to demonstrate\nthe effectiveness of the proposed method, and the results confirm that the\nproposed method can significantly reduce the complexity of supervised low-light\nimage enhancement networks while improving processing effect. The code is\navailable at \\url{https://github.com/hitzhangyu/FLW-Net}.",
          "arxiv_id": "2304.02978v2"
        }
      ],
      "19": [
        {
          "title": "Mesh-based 3D Motion Tracking in Cardiac MRI using Deep Learning",
          "year": "2022-09",
          "abstract": "3D motion estimation from cine cardiac magnetic resonance (CMR) images is\nimportant for the assessment of cardiac function and diagnosis of\ncardiovascular diseases. Most of the previous methods focus on estimating\npixel-/voxel-wise motion fields in the full image space, which ignore the fact\nthat motion estimation is mainly relevant and useful within the object of\ninterest, e.g., the heart. In this work, we model the heart as a 3D geometric\nmesh and propose a novel deep learning-based method that can estimate 3D motion\nof the heart mesh from 2D short- and long-axis CMR images. By developing a\ndifferentiable mesh-to-image rasterizer, the method is able to leverage the\nanatomical shape information from 2D multi-view CMR images for 3D motion\nestimation. The differentiability of the rasterizer enables us to train the\nmethod end-to-end. One advantage of the proposed method is that by tracking the\nmotion of each vertex, it is able to keep the vertex correspondence of 3D\nmeshes between time frames, which is important for quantitative assessment of\nthe cardiac function on the mesh. We evaluate the proposed method on CMR images\nacquired from the UK Biobank study. Experimental results show that the proposed\nmethod quantitatively and qualitatively outperforms both conventional and\nlearning-based cardiac motion tracking methods.",
          "arxiv_id": "2209.02004v1"
        },
        {
          "title": "CNN-based Cardiac Motion Extraction to Generate Deformable Geometric Left Ventricle Myocardial Models from Cine MRI",
          "year": "2021-03",
          "abstract": "Patient-specific left ventricle (LV) myocardial models have the potential to\nbe used in a variety of clinical scenarios for improved diagnosis and treatment\nplans. Cine cardiac magnetic resonance (MR) imaging provides high resolution\nimages to reconstruct patient-specific geometric models of the LV myocardium.\nWith the advent of deep learning, accurate segmentation of cardiac chambers\nfrom cine cardiac MR images and unsupervised learning for image registration\nfor cardiac motion estimation on a large number of image datasets is\nattainable. Here, we propose a deep leaning-based framework for the development\nof patient-specific geometric models of LV myocardium from cine cardiac MR\nimages, using the Automated Cardiac Diagnosis Challenge (ACDC) dataset. We use\nthe deformation field estimated from the VoxelMorph-based convolutional neural\nnetwork (CNN) to propagate the isosurface mesh and volume mesh of the\nend-diastole (ED) frame to the subsequent frames of the cardiac cycle. We\nassess the CNN-based propagated models against segmented models at each cardiac\nphase, as well as models propagated using another traditional nonrigid image\nregistration technique.",
          "arxiv_id": "2103.16695v1"
        },
        {
          "title": "Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark Study from Multi-Sequence Cardiac MR Segmentation Challenge",
          "year": "2020-06",
          "abstract": "Accurate computing, analysis and modeling of the ventricles and myocardium\nfrom medical images are important, especially in the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). Late\ngadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an\nimportant protocol to visualize MI. However, automated segmentation of LGE CMR\nis still challenging, due to the indistinguishable boundaries, heterogeneous\nintensity distribution and complex enhancement patterns of pathological\nmyocardium from LGE CMR. Furthermore, compared with the other sequences LGE CMR\nimages with gold standard labels are particularly limited, which represents\nanother obstacle for developing novel algorithms for automatic segmentation of\nLGE CMR. This paper presents the selective results from the Multi-Sequence\nCardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019.\nThe challenge offered a data set of paired MS-CMR images, including auxiliary\nCMR sequences as well as LGE CMR, from 45 patients who underwent\ncardiomyopathy. It was aimed to develop new algorithms, as well as benchmark\nexisting ones for LGE CMR segmentation and compare them objectively. In\naddition, the paired MS-CMR images could enable algorithms to combine the\ncomplementary information from the other sequences for the segmentation of LGE\nCMR. Nine representative works were selected for evaluation and comparisons,\namong which three methods are unsupervised methods and the other six are\nsupervised. The results showed that the average performance of the nine methods\nwas comparable to the inter-observer variations. The success of these methods\nwas mainly attributed to the inclusion of the auxiliary sequences from the\nMS-CMR images, which provide important label information for the training of\ndeep neural networks.",
          "arxiv_id": "2006.12434v2"
        }
      ],
      "20": [
        {
          "title": "Deep learning-based algorithm for assessment of knee osteoarthritis severity in radiographs matches performance of radiologists",
          "year": "2022-07",
          "abstract": "A fully-automated deep learning algorithm matched performance of radiologists\nin assessment of knee osteoarthritis severity in radiographs using the\nKellgren-Lawrence grading system.\n  To develop an automated deep learning-based algorithm that jointly uses\nPosterior-Anterior (PA) and Lateral (LAT) views of knee radiographs to assess\nknee osteoarthritis severity according to the Kellgren-Lawrence grading system.\n  We used a dataset of 9739 exams from 2802 patients from Multicenter\nOsteoarthritis Study (MOST). The dataset was divided into a training set of\n2040 patients, a validation set of 259 patients and a test set of 503 patients.\nA novel deep learning-based method was utilized for assessment of knee OA in\ntwo steps: (1) localization of knee joints in the images, (2) classification\naccording to the KL grading system. Our method used both PA and LAT views as\nthe input to the model. The scores generated by the algorithm were compared to\nthe grades provided in the MOST dataset for the entire test set as well as\ngrades provided by 5 radiologists at our institution for a subset of the test\nset.\n  The model obtained a multi-class accuracy of 71.90% on the entire test set\nwhen compared to the ratings provided in the MOST dataset. The quadratic\nweighted Kappa coefficient for this set was 0.9066. The average quadratic\nweighted Kappa between all pairs of radiologists from our institution who took\na part of study was 0.748. The average quadratic-weighted Kappa between the\nalgorithm and the radiologists at our institution was 0.769.\n  The proposed model performed demonstrated equivalency of KL classification to\nMSK radiologists, but clearly superior reproducibility. Our model also agreed\nwith radiologists at our institution to the same extent as the radiologists\nwith each other. The algorithm could be used to provide reproducible assessment\nof knee osteoarthritis severity.",
          "arxiv_id": "2207.12521v1"
        },
        {
          "title": "CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning",
          "year": "2023-07",
          "abstract": "Background: MR-based subchondral bone effectively predicts knee\nosteoarthritis. However, its clinical application is limited by the cost and\ntime of MR. Purpose: We aim to develop a novel distillation-learning-based\nmethod named SRRD for subchondral bone microstructural analysis using\neasily-acquired CT images, which leverages paired MR images to enhance the\nCT-based analysis model during training. Materials and Methods: Knee joint\nimages of both CT and MR modalities were collected from October 2020 to May\n2021. Firstly, we developed a GAN-based generative model to transform MR images\ninto CT images, which was used to establish the anatomical correspondence\nbetween the two modalities. Next, we obtained numerous patches of subchondral\nbone regions of MR images, together with their trabecular parameters (BV / TV,\nTb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression.\nThe distillation-learning technique was used to train the regression model and\ntransfer MR structural information to the CT-based model. The regressed\ntrabecular parameters were further used for knee osteoarthritis classification.\nResults: A total of 80 participants were evaluated. CT-based regression results\nof trabecular parameters achieved intra-class correlation coefficients (ICCs)\nof 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N,\nrespectively. The use of distillation learning significantly improved the\nperformance of the CT-based knee osteoarthritis classification method using the\nCNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of\n0.658 (95% CI, 0.574-0.742) (p<.001). Conclusions: The proposed SRRD method\nshowed high reliability and validity in MR-CT registration, regression, and\nknee osteoarthritis classification, indicating the feasibility of subchondral\nbone microstructural analysis based on CT images.",
          "arxiv_id": "2307.04390v2"
        },
        {
          "title": "Automated anomaly-aware 3D segmentation of bones and cartilages in knee MR images from the Osteoarthritis Initiative",
          "year": "2022-11",
          "abstract": "In medical image analysis, automated segmentation of multi-component\nanatomical structures, which often have a spectrum of potential anomalies and\npathologies, is a challenging task. In this work, we develop a multi-step\napproach using U-Net-based neural networks to initially detect anomalies (bone\nmarrow lesions, bone cysts) in the distal femur, proximal tibia and patella\nfrom 3D magnetic resonance (MR) images of the knee in individuals with varying\ngrades of osteoarthritis. Subsequently, the extracted data are used for\ndownstream tasks involving semantic segmentation of individual bone and\ncartilage volumes as well as bone anomalies. For anomaly detection, the\nU-Net-based models were developed to reconstruct the bone profiles of the femur\nand tibia in images via inpainting so anomalous bone regions could be replaced\nwith close to normal appearances. The reconstruction error was used to detect\nbone anomalies. A second anomaly-aware network, which was compared to\nanomaly-na\\\"ive segmentation networks, was used to provide a final automated\nsegmentation of the femoral, tibial and patellar bones and cartilages from the\nknee MR images containing a spectrum of bone anomalies. The anomaly-aware\nsegmentation approach provided up to 58% reduction in Hausdorff distances for\nbone segmentations compared to the results from the anomaly-na\\\"ive\nsegmentation networks. In addition, the anomaly-aware networks were able to\ndetect bone lesions in the MR images with greater sensitivity and specificity\n(area under the receiver operating characteristic curve [AUC] up to 0.896)\ncompared to the anomaly-na\\\"ive segmentation networks (AUC up to 0.874).",
          "arxiv_id": "2211.16696v2"
        }
      ],
      "21": [
        {
          "title": "NeCA: 3D Coronary Artery Tree Reconstruction from Two 2D Projections via Neural Implicit Representation",
          "year": "2024-09",
          "abstract": "Cardiovascular diseases (CVDs) are the most common health threats worldwide.\n2D X-ray invasive coronary angiography (ICA) remains the most widely adopted\nimaging modality for CVD assessment during real-time cardiac interventions.\nHowever, it is often difficult for cardiologists to interpret the 3D geometry\nof coronary vessels based on 2D planes. Moreover, due to the radiation limit,\noften only two angiographic projections are acquired, providing limited\ninformation of the vessel geometry and necessitating 3D coronary tree\nreconstruction based only on two ICA projections. In this paper, we propose a\nself-supervised deep learning method called NeCA, which is based on neural\nimplicit representation using the multiresolution hash encoder and\ndifferentiable cone-beam forward projector layer, in order to achieve 3D\ncoronary artery tree reconstruction from two 2D projections. We validate our\nmethod using six different metrics on a dataset generated from coronary\ncomputed tomography angiography of right coronary artery and left anterior\ndescending artery. The evaluation results demonstrate that our NeCA method,\nwithout requiring 3D ground truth for supervision or large datasets for\ntraining, achieves promising performance in both vessel topology and\nbranch-connectivity preservation compared to the supervised deep learning\nmodel.",
          "arxiv_id": "2409.04596v2"
        },
        {
          "title": "Segmentation and Vascular Vectorization for Coronary Artery by Geometry-based Cascaded Neural Network",
          "year": "2023-05",
          "abstract": "Segmentation of the coronary artery is an important task for the quantitative\nanalysis of coronary computed tomography angiography (CCTA) images and is being\nstimulated by the field of deep learning. However, the complex structures with\ntiny and narrow branches of the coronary artery bring it a great challenge.\nCoupled with the medical image limitations of low resolution and poor contrast,\nfragmentations of segmented vessels frequently occur in the prediction.\nTherefore, a geometry-based cascaded segmentation method is proposed for the\ncoronary artery, which has the following innovations: 1) Integrating geometric\ndeformation networks, we design a cascaded network for segmenting the coronary\nartery and vectorizing results. The generated meshes of the coronary artery are\ncontinuous and accurate for twisted and sophisticated coronary artery\nstructures, without fragmentations. 2) Different from mesh annotations\ngenerated by the traditional marching cube method from voxel-based labels, a\nfiner vectorized mesh of the coronary artery is reconstructed with the\nregularized morphology. The novel mesh annotation benefits the geometry-based\nsegmentation network, avoiding bifurcation adhesion and point cloud dispersion\nin intricate branches. 3) A dataset named CCA-200 is collected, consisting of\n200 CCTA images with coronary artery disease. The ground truths of 200 cases\nare coronary internal diameter annotations by professional radiologists.\nExtensive experiments verify our method on our collected dataset CCA-200 and\npublic ASOCA dataset, with a Dice of 0.778 on CCA-200 and 0.895 on ASOCA,\nshowing superior results. Especially, our geometry-based model generates an\naccurate, intact and smooth coronary artery, devoid of any fragmentations of\nsegmented vessels.",
          "arxiv_id": "2305.04208v1"
        },
        {
          "title": "A Deep Learning Model for Coronary Artery Segmentation and Quantitative Stenosis Detection in Angiographic Images",
          "year": "2024-06",
          "abstract": "Coronary artery disease (CAD) is a leading cause of cardiovascular-related\nmortality, and accurate stenosis detection is crucial for effective clinical\ndecision-making. Coronary angiography remains the gold standard for diagnosing\nCAD, but manual analysis of angiograms is prone to errors and subjectivity.\nThis study aims to develop a deep learning-based approach for the automatic\nsegmentation of coronary arteries from angiographic images and the quantitative\ndetection of stenosis, thereby improving the accuracy and efficiency of CAD\ndiagnosis. We propose a novel deep learning-based method for the automatic\nsegmentation of coronary arteries in angiographic images, coupled with a\ndynamic cohort method for stenosis detection. The segmentation model combines\nthe MedSAM and VM-UNet architectures to achieve high-performance results. After\nsegmentation, the vascular centerline is extracted, vessel diameter is\ncomputed, and the degree of stenosis is measured with high precision, enabling\naccurate identification of arterial stenosis. On the mixed dataset (including\nthe ARCADE, DCA1, and GH datasets), the model achieved an average IoU of\n0.6308, with sensitivity and specificity of 0.9772 and 0.9903, respectively. On\nthe ARCADE dataset, the average IoU was 0.6303, with sensitivity of 0.9832 and\nspecificity of 0.9933. Additionally, the stenosis detection algorithm achieved\na true positive rate (TPR) of 0.5867 and a positive predictive value (PPV) of\n0.5911, demonstrating the effectiveness of our model in analyzing coronary\nangiography images. SAM-VMNet offers a promising tool for the automated\nsegmentation and detection of coronary artery stenosis. The model's high\naccuracy and robustness provide significant clinical value for the early\ndiagnosis and treatment planning of CAD. The code and examples are available at\nhttps://github.com/qimingfan10/SAM-VMNet.",
          "arxiv_id": "2406.00492v2"
        }
      ],
      "22": [
        {
          "title": "Joint 2D-3D Breast Cancer Classification",
          "year": "2020-02",
          "abstract": "Breast cancer is the malignant tumor that causes the highest number of cancer\ndeaths in females. Digital mammograms (DM or 2D mammogram) and digital breast\ntomosynthesis (DBT or 3D mammogram) are the two types of mammography imagery\nthat are used in clinical practice for breast cancer detection and diagnosis.\nRadiologists usually read both imaging modalities in combination; however,\nexisting computer-aided diagnosis tools are designed using only one imaging\nmodality. Inspired by clinical practice, we propose an innovative convolutional\nneural network (CNN) architecture for breast cancer classification, which uses\nboth 2D and 3D mammograms, simultaneously. Our experiment shows that the\nproposed method significantly improves the performance of breast cancer\nclassification. By assembling three CNN classifiers, the proposed model\nachieves 0.97 AUC, which is 34.72% higher than the methods using only one\nimaging modality.",
          "arxiv_id": "2002.12392v1"
        },
        {
          "title": "Deep-LIBRA: Artificial intelligence method for robust quantification of breast density with independent validation in breast cancer risk assessment",
          "year": "2020-11",
          "abstract": "Breast density is an important risk factor for breast cancer that also\naffects the specificity and sensitivity of screening mammography. Current\nfederal legislation mandates reporting of breast density for all women\nundergoing breast screening. Clinically, breast density is assessed visually\nusing the American College of Radiology Breast Imaging Reporting And Data\nSystem (BI-RADS) scale. Here, we introduce an artificial intelligence (AI)\nmethod to estimate breast percentage density (PD) from digital mammograms. Our\nmethod leverages deep learning (DL) using two convolutional neural network\narchitectures to accurately segment the breast area. A machine-learning\nalgorithm combining superpixel generation, texture feature analysis, and\nsupport vector machine is then applied to differentiate dense from non-dense\ntissue regions, from which PD is estimated. Our method has been trained and\nvalidated on a multi-ethnic, multi-institutional dataset of 15,661 images\n(4,437 women), and then tested on an independent dataset of 6,368 digital\nmammograms (1,702 women; cases=414) for both PD estimation and discrimination\nof breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and\nan expert reader were strongly correlated (Spearman correlation coefficient =\n0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination\nperformance (area under the ROC curve, AUC = 0.611 [95% confidence interval\n(CI): 0.583, 0.639]) compared to four other widely-used research and commercial\nPD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong\nagreement of PD estimates between Deep-LIBRA and gold-standard assessment by an\nexpert reader, as well as improved performance in breast cancer risk assessment\nover state-of-the-art open-source and commercial methods.",
          "arxiv_id": "2011.08001v3"
        },
        {
          "title": "Deep Learning Predicts Mammographic Breast Density in Clinical Breast Ultrasound Images",
          "year": "2024-10",
          "abstract": "Background: Breast density, as derived from mammographic images and defined\nby the American College of Radiology's Breast Imaging Reporting and Data System\n(BI-RADS), is one of the strongest risk factors for breast cancer. Breast\nultrasound (BUS) is an alternative breast cancer screening modality,\nparticularly useful for early detection in low-resource, rural contexts. The\npurpose of this study was to explore an artificial intelligence (AI) model to\npredict BI-RADS mammographic breast density category from clinical, handheld\nBUS imaging. Methods: All data are sourced from the Hawaii and Pacific Islands\nMammography Registry. We compared deep learning methods from BUS imaging, as\nwell as machine learning models from image statistics alone. The use of\nAI-derived BUS density as a risk factor for breast cancer was then compared to\nclinical BI-RADS breast density while adjusting for age. The BUS data were\nsplit by individual into 70/20/10% groups for training, validation, and\ntesting. Results: 405,120 clinical BUS images from 14.066 women were selected\nfor inclusion in this study, resulting in 9.846 women for training (302,574\nimages), 2,813 for validation (11,223 images), and 1,406 for testing (4,042\nimages). On the held-out testing set, the strongest AI model achieves AUROC\n0.854 predicting BI-RADS mammographic breast density from BUS imaging and\noutperforms all shallow machine learning methods based on image statistics. In\ncancer risk prediction, age-adjusted AI BUS breast density predicted 5-year\nbreast cancer risk with 0.633 AUROC, as compared to 0.637 AUROC from\nage-adjusted clinical breast density. Conclusions: BI-RADS mammographic breast\ndensity can be estimated from BUS imaging with high accuracy using a deep\nlearning model. Furthermore, we demonstrate that AI-derived BUS breast density\nis predictive of 5-year breast cancer risk in our population.",
          "arxiv_id": "2411.00891v2"
        }
      ],
      "23": [
        {
          "title": "Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification",
          "year": "2022-02",
          "abstract": "Convolutional Neural Networks have demonstrated human-level performance in\nthe classification of melanoma and other skin lesions, but evident performance\ndisparities between differing skin tones should be addressed before widespread\ndeployment. In this work, we propose an efficient yet effective algorithm for\nautomatically labelling the skin tone of lesion images, and use this to\nannotate the benchmark ISIC dataset. We subsequently use these automated labels\nas the target for two leading bias unlearning techniques towards mitigating\nskin tone bias. Our experimental results provide evidence that our skin tone\ndetection algorithm outperforms existing solutions and that unlearning skin\ntone may improve generalisation and can reduce the performance disparity\nbetween melanoma detection in lighter and darker skin tones.",
          "arxiv_id": "2202.02832v4"
        },
        {
          "title": "Deep Learning based Novel Cascaded Approach for Skin Lesion Analysis",
          "year": "2023-01",
          "abstract": "Automatic lesion analysis is critical in skin cancer diagnosis and ensures\neffective treatment. The computer aided diagnosis of such skin cancer in\ndermoscopic images can significantly reduce the clinicians workload and help\nimprove diagnostic accuracy. Although researchers are working extensively to\naddress this problem, early detection and accurate identification of skin\nlesions remain challenging. This research focuses on a two step framework for\nskin lesion segmentation followed by classification for lesion analysis. We\nexplored the effectiveness of deep convolutional neural network based\narchitectures by designing an encoder-decoder architecture for skin lesion\nsegmentation and CNN based classification network. The proposed approaches are\nevaluated quantitatively in terms of the Accuracy, mean Intersection over Union\nand Dice Similarity Coefficient. Our cascaded end to end deep learning based\napproach is the first of its kind, where the classification accuracy of the\nlesion is significantly improved because of prior segmentation.",
          "arxiv_id": "2301.06226v1"
        },
        {
          "title": "Knowledge-aware Deep Framework for Collaborative Skin Lesion Segmentation and Melanoma Recognition",
          "year": "2021-06",
          "abstract": "Deep learning techniques have shown their superior performance in\ndermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a\nchallenging task due to the difficulty of incorporating the useful\ndermatologist clinical knowledge into the learning process. In this paper, we\npropose a novel knowledge-aware deep framework that incorporates some clinical\nknowledge into collaborative learning of two important melanoma diagnosis\ntasks, i.e., skin lesion segmentation and melanoma recognition. Specifically,\nto exploit the knowledge of morphological expressions of the lesion region and\nalso the periphery region for melanoma identification, a lesion-based pooling\nand shape extraction (LPSE) scheme is designed, which transfers the structure\ninformation obtained from skin lesion segmentation into melanoma recognition.\nMeanwhile, to pass the skin lesion diagnosis knowledge from melanoma\nrecognition to skin lesion segmentation, an effective diagnosis guided feature\nfusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual\nlearning mechanism that further promotes the inter-task cooperation, and thus\niteratively improves the joint learning capability of the model for both skin\nlesion segmentation and melanoma recognition. Experimental results on two\npublicly available skin lesion datasets show the effectiveness of the proposed\nmethod for melanoma analysis.",
          "arxiv_id": "2106.03455v2"
        }
      ],
      "24": [
        {
          "title": "FiRe: Fixed-points of Restoration Priors for Solving Inverse Problems",
          "year": "2024-11",
          "abstract": "Selecting an appropriate prior to compensate for information loss due to the\nmeasurement operator is a fundamental challenge in imaging inverse problems.\nImplicit priors based on denoising neural networks have become central to\nwidely-used frameworks such as Plug-and-Play (PnP) algorithms. In this work, we\nintroduce Fixed-points of Restoration (FiRe) priors as a new framework for\nexpanding the notion of priors in PnP to general restoration models beyond\ntraditional denoising models. The key insight behind FiRe is that smooth images\nemerge as fixed points of the composition of a degradation operator with the\ncorresponding restoration model. This enables us to derive an explicit formula\nfor our implicit prior by quantifying invariance of images under this composite\noperation. Adopting this fixed-point perspective, we show how various\nrestoration networks can effectively serve as priors for solving inverse\nproblems. The FiRe framework further enables ensemble-like combinations of\nmultiple restoration models as well as acquisition-informed restoration\nnetworks, all within a unified optimization approach. Experimental results\nvalidate the effectiveness of FiRe across various inverse problems,\nestablishing a new paradigm for incorporating pretrained restoration models\ninto PnP-like algorithms. Code available at\nhttps://github.com/matthieutrs/fire.",
          "arxiv_id": "2411.18970v2"
        },
        {
          "title": "Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse Problems",
          "year": "2023-06",
          "abstract": "Plug-and-Play (PnP) methods are efficient iterative algorithms for solving\nill-posed image inverse problems. PnP methods are obtained by using deep\nGaussian denoisers instead of the proximal operator or the gradient-descent\nstep within proximal algorithms. Current PnP schemes rely on data-fidelity\nterms that have either Lipschitz gradients or closed-form proximal operators,\nwhich is not applicable to Poisson inverse problems. Based on the observation\nthat the Gaussian noise is not the adequate noise model in this setting, we\npropose to generalize PnP using theBregman Proximal Gradient (BPG) method. BPG\nreplaces the Euclidean distance with a Bregman divergence that can better\ncapture the smoothness properties of the problem. We introduce the Bregman\nScore Denoiser specifically parametrized and trained for the new Bregman\ngeometry and prove that it corresponds to the proximal operator of a nonconvex\npotential. We propose two PnP algorithms based on the Bregman Score Denoiser\nfor solving Poisson inverse problems. Extending the convergence results of BPG\nin the nonconvex settings, we show that the proposed methods converge,\ntargeting stationary points of an explicit global functional. Experimental\nevaluations conducted on various Poisson inverse problems validate the\nconvergence results and showcase effective restoration performance.",
          "arxiv_id": "2306.03466v1"
        },
        {
          "title": "Block Coordinate Plug-and-Play Methods for Blind Inverse Problems",
          "year": "2023-05",
          "abstract": "Plug-and-play (PnP) prior is a well-known class of methods for solving\nimaging inverse problems by computing fixed-points of operators combining\nphysical measurement models and learned image denoisers. While PnP methods have\nbeen extensively used for image recovery with known measurement operators,\nthere is little work on PnP for solving blind inverse problems. We address this\ngap by presenting a new block-coordinate PnP (BC-PnP) method that efficiently\nsolves this joint estimation problem by introducing learned denoisers as priors\non both the unknown image and the unknown measurement operator. We present a\nnew convergence theory for BC-PnP compatible with blind inverse problems by\nconsidering nonconvex data-fidelity terms and expansive denoisers. Our theory\nanalyzes the convergence of BC-PnP to a stationary point of an implicit\nfunction associated with an approximate minimum mean-squared error (MMSE)\ndenoiser. We numerically validate our method on two blind inverse problems:\nautomatic coil sensitivity estimation in magnetic resonance imaging (MRI) and\nblind image deblurring. Our results show that BC-PnP provides an efficient and\nprincipled framework for using denoisers as PnP priors for jointly estimating\nmeasurement operators and images.",
          "arxiv_id": "2305.12672v2"
        }
      ],
      "25": [
        {
          "title": "Deep Learning for Medical Image Registration: A Comprehensive Review",
          "year": "2022-04",
          "abstract": "Image registration is a critical component in the applications of various\nmedical image analyses. In recent years, there has been a tremendous surge in\nthe development of deep learning (DL)-based medical image registration models.\nThis paper provides a comprehensive review of medical image registration.\nFirstly, a discussion is provided for supervised registration categories, for\nexample, fully supervised, dual supervised, and weakly supervised registration.\nNext, similarity-based as well as generative adversarial network (GAN)-based\nregistration are presented as part of unsupervised registration. Deep iterative\nregistration is then described with emphasis on deep similarity-based and\nreinforcement learning-based registration. Moreover, the application areas of\nmedical image registration are reviewed. This review focuses on monomodal and\nmultimodal registration and associated imaging, for instance, X-ray, CT scan,\nultrasound, and MRI. The existing challenges are highlighted in this review,\nwhere it is shown that a major challenge is the absence of a training dataset\nwith known transformations. Finally, a discussion is provided on the promising\nfuture research areas in the field of DL-based medical image registration.",
          "arxiv_id": "2204.11341v1"
        },
        {
          "title": "A survey on deep learning in medical image registration: new technologies, uncertainty, evaluation metrics, and beyond",
          "year": "2023-07",
          "abstract": "Deep learning technologies have dramatically reshaped the field of medical\nimage registration over the past decade. The initial developments, such as\nregression-based and U-Net-based networks, established the foundation for deep\nlearning in image registration. Subsequent progress has been made in various\naspects of deep learning-based registration, including similarity measures,\ndeformation regularizations, network architectures, and uncertainty estimation.\nThese advancements have not only enriched the field of image registration but\nhave also facilitated its application in a wide range of tasks, including atlas\nconstruction, multi-atlas segmentation, motion estimation, and 2D-3D\nregistration. In this paper, we present a comprehensive overview of the most\nrecent advancements in deep learning-based image registration. We begin with a\nconcise introduction to the core concepts of deep learning-based image\nregistration. Then, we delve into innovative network architectures, loss\nfunctions specific to registration, and methods for estimating registration\nuncertainty. Additionally, this paper explores appropriate evaluation metrics\nfor assessing the performance of deep learning models in registration tasks.\nFinally, we highlight the practical applications of these novel techniques in\nmedical imaging and discuss the future prospects of deep learning-based image\nregistration.",
          "arxiv_id": "2307.15615v4"
        },
        {
          "title": "Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration",
          "year": "2023-07",
          "abstract": "Image registration is a fundamental requirement for medical image analysis.\nDeep registration methods based on deep learning have been widely recognized\nfor their capabilities to perform fast end-to-end registration. Many deep\nregistration methods achieved state-of-the-art performance by performing\ncoarse-to-fine registration, where multiple registration steps were iterated\nwith cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE)\nregistration methods have been proposed to perform coarse-to-fine registration\nin a single network and showed advantages in both registration accuracy and\nruntime. However, existing NICE registration methods mainly focus on deformable\nregistration, while affine registration, a common prerequisite, is still\nreliant on time-consuming traditional optimization-based methods or extra\naffine registration networks. In addition, existing NICE registration methods\nare limited by the intrinsic locality of convolution operations. Transformers\nmay address this limitation for their capabilities to capture long-range\ndependency, but the benefits of using transformers for NICE registration have\nnot been explored. In this study, we propose a Non-Iterative Coarse-to-finE\nTransformer network (NICE-Trans) for image registration. Our NICE-Trans is the\nfirst deep registration method that (i) performs joint affine and deformable\ncoarse-to-fine registration within a single network, and (ii) embeds\ntransformers into a NICE registration framework to model long-range relevance\nbetween images. Extensive experiments with seven public datasets show that our\nNICE-Trans outperforms state-of-the-art registration methods on both\nregistration accuracy and runtime.",
          "arxiv_id": "2307.03421v1"
        }
      ],
      "26": [
        {
          "title": "Learning Contextually Fused Audio-visual Representations for Audio-visual Speech Recognition",
          "year": "2022-02",
          "abstract": "With the advance in self-supervised learning for audio and visual modalities,\nit has become possible to learn a robust audio-visual speech representation.\nThis would be beneficial for improving the audio-visual speech recognition\n(AVSR) performance, as the multi-modal inputs contain more fruitful information\nin principle. In this paper, based on existing self-supervised representation\nlearning methods for audio modality, we therefore propose an audio-visual\nrepresentation learning approach. The proposed approach explores both the\ncomplementarity of audio-visual modalities and long-term context dependency\nusing a transformer-based fusion module and a flexible masking strategy. After\npre-training, the model is able to extract fused representations required by\nAVSR. Without loss of generality, it can be applied to single-modal tasks, e.g.\naudio/visual speech recognition by simply masking out one modality in the\nfusion module. The proposed pre-trained model is evaluated on speech\nrecognition and lipreading tasks using one or two modalities, where the\nsuperiority is revealed.",
          "arxiv_id": "2202.07428v2"
        },
        {
          "title": "Unsupervised active speaker detection in media content using cross-modal information",
          "year": "2022-09",
          "abstract": "We present a cross-modal unsupervised framework for active speaker detection\nin media content such as TV shows and movies. Machine learning advances have\nenabled impressive performance in identifying individuals from speech and\nfacial images. We leverage speaker identity information from speech and faces,\nand formulate active speaker detection as a speech-face assignment task such\nthat the active speaker's face and the underlying speech identify the same\nperson (character). We express the speech segments in terms of their associated\nspeaker identity distances, from all other speech segments, to capture a\nrelative identity structure for the video. Then we assign an active speaker's\nface to each speech segment from the concurrently appearing faces such that the\nobtained set of active speaker faces displays a similar relative identity\nstructure. Furthermore, we propose a simple and effective approach to address\nspeech segments where speakers are present off-screen. We evaluate the proposed\nsystem on three benchmark datasets -- Visual Person Clustering dataset,\nAVA-active speaker dataset, and Columbia dataset -- consisting of videos from\nentertainment and broadcast media, and show competitive performance to\nstate-of-the-art fully supervised methods.",
          "arxiv_id": "2209.11896v1"
        },
        {
          "title": "An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation",
          "year": "2020-08",
          "abstract": "Speech enhancement and speech separation are two related tasks, whose purpose\nis to extract either one or more target speech signals, respectively, from a\nmixture of sounds generated by several sources. Traditionally, these tasks have\nbeen tackled using signal processing and machine learning techniques applied to\nthe available acoustic signals. Since the visual aspect of speech is\nessentially unaffected by the acoustic environment, visual information from the\ntarget speakers, such as lip movements and facial expressions, has also been\nused for speech enhancement and speech separation systems. In order to\nefficiently fuse acoustic and visual information, researchers have exploited\nthe flexibility of data-driven approaches, specifically deep learning,\nachieving strong performance. The ceaseless proposal of a large number of\ntechniques to extract features and fuse multimodal information has highlighted\nthe need for an overview that comprehensively describes and discusses\naudio-visual speech enhancement and separation based on deep learning. In this\npaper, we provide a systematic survey of this research topic, focusing on the\nmain elements that characterise the systems in the literature: acoustic\nfeatures; visual features; deep learning methods; fusion techniques; training\ntargets and objective functions. In addition, we review deep-learning-based\nmethods for speech reconstruction from silent videos and audio-visual sound\nsource separation for non-speech signals, since these methods can be more or\nless directly applied to audio-visual speech enhancement and separation.\nFinally, we survey commonly employed audio-visual speech datasets, given their\ncentral role in the development of data-driven approaches, and evaluation\nmethods, because they are generally used to compare different systems and\ndetermine their performance.",
          "arxiv_id": "2008.09586v2"
        }
      ],
      "27": [
        {
          "title": "Deep Learning-based Synthetic High-Resolution In-Depth Imaging Using an Attachable Dual-element Endoscopic Ultrasound Probe",
          "year": "2023-09",
          "abstract": "Endoscopic ultrasound (EUS) imaging has a trade-off between resolution and\npenetration depth. By considering the in-vivo characteristics of human organs,\nit is necessary to provide clinicians with appropriate hardware specifications\nfor precise diagnosis. Recently, super-resolution (SR) ultrasound imaging\nstudies, including the SR task in deep learning fields, have been reported for\nenhancing ultrasound images. However, most of those studies did not consider\nultrasound imaging natures, but rather they were conventional SR techniques\nbased on downsampling of ultrasound images. In this study, we propose a novel\ndeep learning-based high-resolution in-depth imaging probe capable of offering\nlow- and high-frequency ultrasound image pairs. We developed an attachable\ndual-element EUS probe with customized low- and high-frequency ultrasound\ntransducers under small hardware constraints. We also designed a special geared\nstructure to enable the same image plane. The proposed system was evaluated\nwith a wire phantom and a tissue-mimicking phantom. After the evaluation, 442\nultrasound image pairs from the tissue-mimicking phantom were acquired. We then\napplied several deep learning models to obtain synthetic high-resolution\nin-depth images, thus demonstrating the feasibility of our approach for\nclinical unmet needs. Furthermore, we quantitatively and qualitatively analyzed\nthe results to find a suitable deep-learning model for our task. The obtained\nresults demonstrate that our proposed dual-element EUS probe with an\nimage-to-image translation network has the potential to provide synthetic\nhigh-frequency ultrasound images deep inside tissues.",
          "arxiv_id": "2309.06770v1"
        },
        {
          "title": "Super-Resolution Ultrasound Localization Microscopy Based on a High Frame-rate Clinical Ultrasound Scanner: An In-human Feasibility Study",
          "year": "2020-09",
          "abstract": "Non-invasive detection of microvascular alterations in deep tissues in vivo\nprovides critical information for clinical diagnosis and evaluation of a\nbroad-spectrum of pathologies. Recently, the emergence of super-resolution\nultrasound localization microscopy (ULM) offers new possibilities for clinical\nimaging of microvasculature at capillary level. Currently, the clinical utility\nof ULM on clinical ultrasound scanners is hindered by the technical\nlimitations, such as long data acquisition time, and compromised tracking\nperformance associated with low imaging frame-rate. Here we present an in-human\nULM on a high frame-rate (HFR) clinical ultrasound scanner to achieve\nsuper-resolution microvessel imaging using a short acquisition time (<10s).\nUltrasound MB data were acquired from different human tissues, (liver, kidney,\npancreatic, and breast tumor) using an HFR clinical scanner. By leveraging the\nHFR and advanced processing techniques including sub-pixel motion registration,\nMB signal separation, and Kalman filter-based tracking, MBs can be robustly\nlocalized and tracked for successful ULM under the circumstances of relatively\nhigh MB concentration and limited data acquisition time in humans. Subtle\nmorphological and hemodynamic information were demonstrated on data acquired\nwith single breath-hold and free-hand scanning. Compared with contrast-enhanced\npower Doppler generated based on the same MB dataset, ULM showed a 5.7-fold\nresolution improvement in a vessel, and provided a wide-range flow speed\nmeasurement that is Doppler angle-independent. This study demonstrated the\nfeasibility of ultrafast in-human ULM in various human tissues based on a\nclinical scanner that supports HFR imaging, and showed a great potential for\nthe implementation of super-resolution ultrasound microvessel imaging in a\nmyriad of clinical applications involving microvascular abnormalities and\npathologies.",
          "arxiv_id": "2009.13477v1"
        },
        {
          "title": "Learning-based sound speed estimation and aberration correction in linear-array photoacoustic imaging",
          "year": "2023-06",
          "abstract": "Photoacoustic (PA) image reconstruction involves acoustic inversion that\nnecessitates the specification of the speed of sound (SoS) within the medium of\npropagation. Due to the lack of information on the spatial distribution of the\nSoS within heterogeneous soft tissue, a homogeneous SoS distribution (such as\n1540 m/s) is typically assumed in PA image reconstruction, similar to that of\nultrasound (US) imaging. Failure to compensate the SoS variations leads to\naberration artefacts, deteriorating the image quality. Various methods have\nbeen proposed to address this issue, but they usually involve complex hardware\nand/or time-consuming algorithms, hindering clinical translation. In this work,\nwe introduce a deep learning framework for SoS estimation and subsequent\naberration correction in a dual-modal PA/US imaging system exploiting a\nclinical US probe. As the acquired PA and US images were inherently\nco-registered, the estimated SoS distribution from US channel data using a deep\nneural network was incorporated for accurate PA image reconstruction. The\nframework comprised an initial pre-training stage based on digital phantoms,\nwhich was further enhanced through transfer learning using physical phantom\ndata and associated SoS maps obtained from measurements. This framework\nachieved a root mean square error of 10.2 m/s and 15.2 m/s for SoS estimation\non digital and physical phantoms, respectively and structural similarity index\nmeasures of up to 0.86 for PA reconstructions as compared to the conventional\napproach of 0.69. A maximum of 1.2 times improvement in signal-to-noise ratio\nof PA images was further demonstrated with a human volunteer study. Our results\nshow that the proposed framework could be valuable in various clinical and\npreclinical applications to enhance PA image reconstruction.",
          "arxiv_id": "2306.11034v2"
        }
      ],
      "28": [
        {
          "title": "AFFIRM: Affinity Fusion-based Framework for Iteratively Random Motion correction of multi-slice fetal brain MRI",
          "year": "2022-05",
          "abstract": "Multi-slice magnetic resonance images of the fetal brain are usually\ncontaminated by severe and arbitrary fetal and maternal motion. Hence, stable\nand robust motion correction is necessary to reconstruct high-resolution 3D\nfetal brain volume for clinical diagnosis and quantitative analysis. However,\nthe conventional registration-based correction has a limited capture range and\nis insufficient for detecting relatively large motions. Here, we present a\nnovel Affinity Fusion-based Framework for Iteratively Random Motion (AFFIRM)\ncorrection of the multi-slice fetal brain MRI. It learns the sequential motion\nfrom multiple stacks of slices and integrates the features between 2D slices\nand reconstructed 3D volume using affinity fusion, which resembles the\niterations between slice-to-volume registration and volumetric reconstruction\nin the regular pipeline. The method accurately estimates the motion regardless\nof brain orientations and outperforms other state-of-the-art learning-based\nmethods on the simulated motion-corrupted data, with a 48.4% reduction of mean\nabsolute error for rotation and 61.3% for displacement. We then incorporated\nAFFIRM into the multi-resolution slice-to-volume registration and tested it on\nthe real-world fetal MRI scans at different gestation stages. The results\nindicated that adding AFFIRM to the conventional pipeline improved the success\nrate of fetal brain super-resolution reconstruction from 77.2% to 91.9%.",
          "arxiv_id": "2205.05851v1"
        },
        {
          "title": "FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with Conditional Diffusion Model",
          "year": "2024-03",
          "abstract": "The quality of fetal MRI is significantly affected by unpredictable and\nsubstantial fetal motion, leading to the introduction of artifacts even when\nfast acquisition sequences are employed. The development of 3D real-time fetal\npose estimation approaches on volumetric EPI fetal MRI opens up a promising\navenue for fetal motion monitoring and prediction. Challenges arise in fetal\npose estimation due to limited number of real scanned fetal MR training images,\nhindering model generalization when the acquired fetal MRI lacks adequate pose.\n  In this study, we introduce FetalDiffusion, a novel approach utilizing a\nconditional diffusion model to generate 3D synthetic fetal MRI with\ncontrollable pose. Additionally, an auxiliary pose-level loss is adopted to\nenhance model performance. Our work demonstrates the success of this proposed\nmodel by producing high-quality synthetic fetal MRI images with accurate and\nrecognizable fetal poses, comparing favorably with in-vivo real fetal MRI.\nFurthermore, we show that the integration of synthetic fetal MR images enhances\nthe fetal pose estimation model's performance, particularly when the number of\navailable real scanned data is limited resulting in 15.4% increase in PCK and\n50.2% reduced in mean error. All experiments are done on a single 32GB V100\nGPU. Our method holds promise for improving real-time tracking models, thereby\naddressing fetal motion issues more effectively.",
          "arxiv_id": "2404.00132v1"
        },
        {
          "title": "Fetal-BET: Brain Extraction Tool for Fetal MRI",
          "year": "2023-10",
          "abstract": "Fetal brain extraction is a necessary first step in most computational fetal\nbrain MRI pipelines. However, it has been a very challenging task due to\nnon-standard fetal head pose, fetal movements during examination, and vastly\nheterogeneous appearance of the developing fetal brain and the neighboring\nfetal and maternal anatomy across various sequences and scanning conditions.\nDevelopment of a machine learning method to effectively address this task\nrequires a large and rich labeled dataset that has not been previously\navailable. As a result, there is currently no method for accurate fetal brain\nextraction on various fetal MRI sequences. In this work, we first built a large\nannotated dataset of approximately 72,000 2D fetal brain MRI images. Our\ndataset covers the three common MRI sequences including T2-weighted,\ndiffusion-weighted, and functional MRI acquired with different scanners.\nMoreover, it includes normal and pathological brains. Using this dataset, we\ndeveloped and validated deep learning methods, by exploiting the power of the\nU-Net style architectures, the attention mechanism, multi-contrast feature\nlearning, and data augmentation for fast, accurate, and generalizable automatic\nfetal brain extraction. Our approach leverages the rich information from\nmulti-contrast (multi-sequence) fetal MRI data, enabling precise delineation of\nthe fetal brain structures. Evaluations on independent test data show that our\nmethod achieves accurate brain extraction on heterogeneous test data acquired\nwith different scanners, on pathological brains, and at various gestational\nstages. This robustness underscores the potential utility of our deep learning\nmodel for fetal brain imaging and image analysis.",
          "arxiv_id": "2310.01523v2"
        }
      ],
      "29": [
        {
          "title": "Future-Proofing Medical Imaging with Privacy-Preserving Federated Learning and Uncertainty Quantification: A Review",
          "year": "2024-09",
          "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in\nautomating various medical imaging tasks, which could soon become routine in\nclinical practice for disease diagnosis, prognosis, treatment planning, and\npost-treatment surveillance. However, the privacy concerns surrounding patient\ndata present a major barrier to the widespread adoption of AI in medical\nimaging, as large, diverse training datasets are essential for developing\naccurate, generalizable, and robust Artificial intelligence models. Federated\nLearning (FL) offers a solution that enables organizations to train AI models\ncollaboratively without sharing sensitive data. federated learning exchanges\nmodel training information, such as gradients, between the participating sites.\nDespite its promise, federated learning is still in its developmental stages\nand faces several challenges. Notably, sensitive information can still be\ninferred from the gradients shared during model training. Quantifying AI\nmodels' uncertainty is vital due to potential data distribution shifts\npost-deployment, which can affect model performance. Uncertainty quantification\n(UQ) in FL is particularly challenging due to data heterogeneity across\nparticipating sites. This review provides a comprehensive examination of FL,\nprivacy-preserving FL (PPFL), and UQ in FL. We identify key gaps in current FL\nmethodologies and propose future research directions to enhance data privacy\nand trustworthiness in medical imaging applications.",
          "arxiv_id": "2409.16340v1"
        },
        {
          "title": "ST-FL: Style Transfer Preprocessing in Federated Learning for COVID-19 Segmentation",
          "year": "2022-03",
          "abstract": "Chest Computational Tomography (CT) scans present low cost, speed and\nobjectivity for COVID-19 diagnosis and deep learning methods have shown great\npromise in assisting the analysis and interpretation of these images. Most\nhospitals or countries can train their own models using in-house data, however\nempirical evidence shows that those models perform poorly when tested on new\nunseen cases, surfacing the need for coordinated global collaboration. Due to\nprivacy regulations, medical data sharing between hospitals and nations is\nextremely difficult. We propose a GAN-augmented federated learning model,\ndubbed ST-FL (Style Transfer Federated Learning), for COVID-19 image\nsegmentation. Federated learning (FL) permits a centralised model to be learned\nin a secure manner from heterogeneous datasets located in disparate private\ndata silos. We demonstrate that the widely varying data quality on FL client\nnodes leads to a sub-optimal centralised FL model for COVID-19 chest CT image\nsegmentation. ST-FL is a novel FL framework that is robust in the face of\nhighly variable data quality at client nodes. The robustness is achieved by a\ndenoising CycleGAN model at each client of the federation that maps arbitrary\nquality images into the same target quality, counteracting the severe data\nvariability evident in real-world FL use-cases. Each client is provided with\nthe target style, which is the same for all clients, and trains their own\ndenoiser. Our qualitative and quantitative results suggest that this FL model\nperforms comparably to, and in some cases better than, a model that has\ncentralised access to all the training data.",
          "arxiv_id": "2203.13680v1"
        },
        {
          "title": "Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation",
          "year": "2023-08",
          "abstract": "Federated Learning (FL) is a distributed machine learning approach that\nsafeguards privacy by creating an impartial global model while respecting the\nprivacy of individual client data. However, the conventional FL method can\nintroduce security risks when dealing with diverse client data, potentially\ncompromising privacy and data integrity. To address these challenges, we\npresent a differential privacy (DP) federated deep learning framework in\nmedical image segmentation. In this paper, we extend our similarity weight\naggregation (SimAgg) method to DP-SimAgg algorithm, a differentially private\nsimilarity-weighted aggregation algorithm for brain tumor segmentation in\nmulti-modal magnetic resonance imaging (MRI). Our DP-SimAgg method not only\nenhances model segmentation capabilities but also provides an additional layer\nof privacy preservation. Extensive benchmarking and evaluation of our\nframework, with computational performance as a key consideration, demonstrate\nthat DP-SimAgg enables accurate and robust brain tumor segmentation while\nminimizing communication costs during model training. This advancement is\ncrucial for preserving the privacy of medical image data and safeguarding\nsensitive information. In conclusion, adding a differential privacy layer in\nthe global weight aggregation phase of the federated brain tumor segmentation\nprovides a promising solution to privacy concerns without compromising\nsegmentation model efficacy. By leveraging DP, we ensure the protection of\nclient data against adversarial attacks and malicious participants.",
          "arxiv_id": "2308.00856v1"
        }
      ],
      "30": [
        {
          "title": "Real-Time Spatio-Temporal LiDAR Point Cloud Compression",
          "year": "2020-08",
          "abstract": "Compressing massive LiDAR point clouds in real-time is critical to autonomous\nmachines such as drones and self-driving cars. While most of the recent prior\nwork has focused on compressing individual point cloud frames, this paper\nproposes a novel system that effectively compresses a sequence of point clouds.\nThe idea to exploit both the spatial and temporal redundancies in a sequence of\npoint cloud frames. We first identify a key frame in a point cloud sequence and\nspatially encode the key frame by iterative plane fitting. We then exploit the\nfact that consecutive point clouds have large overlaps in the physical space,\nand thus spatially encoded data can be (re-)used to encode the temporal stream.\nTemporal encoding by reusing spatial encoding data not only improves the\ncompression rate, but also avoids redundant computations, which significantly\nimproves the compression speed. Experiments show that our compression system\nachieves 40x to 90x compression rate, significantly higher than the MPEG's\nLiDAR point cloud compression standard, while retaining high end-to-end\napplication accuracies. Meanwhile, our compression system has a compression\nspeed that matches the point cloud generation rate by today LiDARs and\nout-performs existing compression systems, enabling real-time point cloud\ntransmission.",
          "arxiv_id": "2008.06972v1"
        },
        {
          "title": "GRASP-Net: Geometric Residual Analysis and Synthesis for Point Cloud Compression",
          "year": "2022-09",
          "abstract": "Point cloud compression (PCC) is a key enabler for various 3-D applications,\nowing to the universality of the point cloud format. Ideally, 3D point clouds\nendeavor to depict object/scene surfaces that are continuous. Practically, as a\nset of discrete samples, point clouds are locally disconnected and sparsely\ndistributed. This sparse nature is hindering the discovery of local correlation\namong points for compression. Motivated by an analysis with fractal dimension,\nwe propose a heterogeneous approach with deep learning for lossy point cloud\ngeometry compression. On top of a base layer compressing a coarse\nrepresentation of the input, an enhancement layer is designed to cope with the\nchallenging geometric residual/details. Specifically, a point-based network is\napplied to convert the erratic local details to latent features residing on the\ncoarse point cloud. Then a sparse convolutional neural network operating on the\ncoarse point cloud is launched. It utilizes the continuity/smoothness of the\ncoarse geometry to compress the latent features as an enhancement bit-stream\nthat greatly benefits the reconstruction quality. When this bit-stream is\nunavailable, e.g., due to packet loss, we support a skip mode with the same\narchitecture which generates geometric details from the coarse point cloud\ndirectly. Experimentation on both dense and sparse point clouds demonstrate the\nstate-of-the-art compression performance achieved by our proposal. Our code is\navailable at https://github.com/InterDigitalInc/GRASP-Net.",
          "arxiv_id": "2209.04401v1"
        },
        {
          "title": "Performance analysis of Deep Learning-based Lossy Point Cloud Geometry Compression Coding Solutions",
          "year": "2024-02",
          "abstract": "The quality evaluation of three deep learning-based coding solutions for\npoint cloud geometry, notably ADLPCC, PCC GEO CNNv2, and PCGCv2, is presented.\nThe MPEG G-PCC was used as an anchor. Furthermore, LUT SR, which uses\nmulti-resolution Look-Up tables, was also considered. A set of six point clouds\nrepresenting landscapes and objects were used. As point cloud texture has a\ngreat influence on the perceived quality, two different subjective studies that\ndiffer in the texture addition model are reported and statistically compared.\nIn the first experiment, the dataset was first encoded with the identified\ncodecs. Then, the texture of the original point cloud was mapped to the decoded\npoint cloud using the Meshlab software, resulting in a point cloud with both\ngeometry and texture information. Finally, the resulting point cloud was\nencoded with G-PCC using the lossless-geometry-lossy-atts mode, while in the\nsecond experiment the texture was mapped directly onto the distorted geometry.\nMoreover, both subjective evaluations were used to benchmark a set of objective\npoint cloud quality metrics. The two experiments were shown to be statistically\ndifferent, and the tested metrics revealed quite different behaviors for the\ntwo sets of data. The results reveal that the preferred method of evaluation is\nthe encoding of texture information with G-PCC after mapping the texture of the\noriginal point cloud to the distorted point cloud. The results suggest that\ncurrent objective metrics are not suitable to evaluate distortions created by\nmachine learning-based codecs.",
          "arxiv_id": "2402.05192v1"
        }
      ],
      "31": [
        {
          "title": "Comprehensive study of good model training for prostate segmentation in volumetric MRI",
          "year": "2022-08",
          "abstract": "Prostate cancer was the third most common cancer in 2020 internationally,\ncoming after breast cancer and lung cancer. Furthermore, in recent years\nprostate cancer has shown an increasing trend. According to clinical\nexperience, if this problem is detected and treated early, there can be a high\nchance of survival for the patient. One task that helps diagnose prostate\ncancer is prostate segmentation from magnetic resonance imaging. Manual\nsegmentation performed by clinical experts has its drawbacks such as: the high\ntime and concentration required from observers; and inter- and intra-observer\nvariability. This is why in recent years automatic approaches to segment a\nprostate based on convolutional neural networks have emerged. Many of them have\nnovel proposed architectures. In this paper I make an exhaustive study of\nseveral deep learning models by adjusting them to the task of prostate\nprediction. I do not use novel architectures, but focus my work more on how to\ntrain the networks. My approach is based on a ResNext101 3D encoder and a\nUnet3D decoder. I provide a study of the importance of resolutions in\nresampling data, something that no one else has done before.",
          "arxiv_id": "2208.13671v1"
        },
        {
          "title": "Mask Enhanced Deeply Supervised Prostate Cancer Detection on B-mode Micro-Ultrasound",
          "year": "2024-12",
          "abstract": "Prostate cancer is a leading cause of cancer-related deaths among men. The\nrecent development of high frequency, micro-ultrasound imaging offers improved\nresolution compared to conventional ultrasound and potentially a better ability\nto differentiate clinically significant cancer from normal tissue. However, the\nfeatures of prostate cancer remain subtle, with ambiguous borders with normal\ntissue and large variations in appearance, making it challenging for both\nmachine learning and humans to localize it on micro-ultrasound images.\n  We propose a novel Mask Enhanced Deeply-supervised Micro-US network, termed\nMedMusNet, to automatically and more accurately segment prostate cancer to be\nused as potential targets for biopsy procedures. MedMusNet leverages predicted\nmasks of prostate cancer to enforce the learned features layer-wisely within\nthe network, reducing the influence of noise and improving overall consistency\nacross frames.\n  MedMusNet successfully detected 76% of clinically significant cancer with a\nDice Similarity Coefficient of 0.365, significantly outperforming the baseline\nSwin-M2F in specificity and accuracy (Wilcoxon test, Bonferroni correction,\np-value<0.05). While the lesion-level and patient-level analyses showed\nimproved performance compared to human experts and different baseline, the\nimprovements did not reach statistical significance, likely on account of the\nsmall cohort.\n  We have presented a novel approach to automatically detect and segment\nclinically significant prostate cancer on B-mode micro-ultrasound images. Our\nMedMusNet model outperformed other models, surpassing even human experts. These\npreliminary results suggest the potential for aiding urologists in prostate\ncancer diagnosis via biopsy and treatment decision-making.",
          "arxiv_id": "2412.10997v1"
        },
        {
          "title": "Using deep learning to detect patients at risk for prostate cancer despite benign biopsies",
          "year": "2021-06",
          "abstract": "Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.",
          "arxiv_id": "2106.14256v3"
        }
      ],
      "32": [
        {
          "title": "Learning to Translate Noise for Robust Image Denoising",
          "year": "2024-12",
          "abstract": "Deep learning-based image denoising techniques often struggle with poor\ngeneralization performance to out-of-distribution real-world noise. To tackle\nthis challenge, we propose a novel noise translation framework that performs\ndenoising on an image with translated noise rather than directly denoising an\noriginal noisy image. Specifically, our approach translates complex, unknown\nreal-world noise into Gaussian noise, which is spatially uncorrelated and\nindependent of image content, through a noise translation network. The\ntranslated noisy images are then processed by an image denoising network\npretrained to effectively remove Gaussian noise, enabling robust and consistent\ndenoising performance. We also design well-motivated loss functions and\narchitectures for the noise translation network by leveraging the mathematical\nproperties of Gaussian noise. Experimental results demonstrate that the\nproposed method substantially improves robustness and generalizability,\noutperforming state-of-the-art methods across diverse benchmarks. Visualized\ndenoising results and the source code are available on our project page.",
          "arxiv_id": "2412.04727v1"
        },
        {
          "title": "Unpaired Learning of Deep Image Denoising",
          "year": "2020-08",
          "abstract": "We investigate the task of learning blind image denoising networks from an\nunpaired set of clean and noisy images. Such problem setting generally is\npractical and valuable considering that it is feasible to collect unpaired\nnoisy and clean images in most real-world applications. And we further assume\nthat the noise can be signal dependent but is spatially uncorrelated. In order\nto facilitate unpaired learning of denoising network, this paper presents a\ntwo-stage scheme by incorporating self-supervised learning and knowledge\ndistillation. For self-supervised learning, we suggest a dilated blind-spot\nnetwork (D-BSN) to learn denoising solely from real noisy images. Due to the\nspatial independence of noise, we adopt a network by stacking 1x1 convolution\nlayers to estimate the noise level map for each image. Both the D-BSN and\nimage-specific noise model (CNN\\_est) can be jointly trained via maximizing the\nconstrained log-likelihood. Given the output of D-BSN and estimated noise level\nmap, improved denoising performance can be further obtained based on the Bayes'\nrule. As for knowledge distillation, we first apply the learned noise models to\nclean images to synthesize a paired set of training images, and use the real\nnoisy images and the corresponding denoising results in the first stage to form\nanother paired set. Then, the ultimate denoising model can be distilled by\ntraining an existing denoising network using these two paired sets. Experiments\nshow that our unpaired learning method performs favorably on both synthetic\nnoisy images and real-world noisy photographs in terms of quantitative and\nqualitative evaluation.",
          "arxiv_id": "2008.13711v1"
        },
        {
          "title": "Image Blind Denoising Using Dual Convolutional Neural Network with Skip Connection",
          "year": "2023-04",
          "abstract": "In recent years, deep convolutional neural networks have shown fascinating\nperformance in the field of image denoising. However, deeper network\narchitectures are often accompanied with large numbers of model parameters,\nleading to high training cost and long inference time, which limits their\napplication in practical denoising tasks. In this paper, we propose a novel\ndual convolutional blind denoising network with skip connection (DCBDNet),\nwhich is able to achieve a desirable balance between the denoising effect and\nnetwork complexity. The proposed DCBDNet consists of a noise estimation network\nand a dual convolutional neural network (CNN). The noise estimation network is\nused to estimate the noise level map, which improves the flexibility of the\nproposed model. The dual CNN contains two branches: a u-shaped sub-network is\ndesigned for the upper branch, and the lower branch is composed of the dilated\nconvolution layers. Skip connections between layers are utilized in both the\nupper and lower branches. The proposed DCBDNet was evaluated on several\nsynthetic and real-world image denoising benchmark datasets. Experimental\nresults have demonstrated that the proposed DCBDNet can effectively remove\ngaussian noise in a wide range of levels, spatially variant noise and real\nnoise. With a simple model structure, our proposed DCBDNet still can obtain\ncompetitive denoising performance compared to the state-of-the-art image\ndenoising models containing complex architectures. Namely, a favorable\ntrade-off between denoising performance and model complexity is achieved. Codes\nare available at https://github.com/WenCongWu/DCBDNet.",
          "arxiv_id": "2304.01620v1"
        }
      ],
      "33": [
        {
          "title": "Unifying Graph Embedding Features with Graph Convolutional Networks for Skeleton-based Action Recognition",
          "year": "2020-03",
          "abstract": "Combining skeleton structure with graph convolutional networks has achieved\nremarkable performance in human action recognition. Since current research\nfocuses on designing basic graph for representing skeleton data, these\nembedding features contain basic topological information, which cannot learn\nmore systematic perspectives from skeleton data. In this paper, we overcome\nthis limitation by proposing a novel framework, which unifies 15 graph\nembedding features into the graph convolutional network for human action\nrecognition, aiming to best take advantage of graph information to distinguish\nkey joints, bones, and body parts in human action, instead of being exclusive\nto a single feature or domain. Additionally, we fully investigate how to find\nthe best graph features of skeleton structure for improving human action\nrecognition. Besides, the topological information of the skeleton sequence is\nexplored to further enhance the performance in a multi-stream framework.\nMoreover, the unified graph features are extracted by the adaptive methods on\nthe training process, which further yields improvements. Our model is validated\nby three large-scale datasets, namely NTU-RGB+D, Kinetics and SYSU-3D, and\noutperforms the state-of-the-art methods. Overall, our work unified graph\nembedding features to promotes systematic research on human action recognition.",
          "arxiv_id": "2003.03007v2"
        },
        {
          "title": "Complex Human Action Recognition in Live Videos Using Hybrid FR-DL Method",
          "year": "2020-07",
          "abstract": "Automated human action recognition is one of the most attractive and\npractical research fields in computer vision, in spite of its high\ncomputational costs. In such systems, the human action labelling is based on\nthe appearance and patterns of the motions in the video sequences; however, the\nconventional methodologies and classic neural networks cannot use temporal\ninformation for action recognition prediction in the upcoming frames in a video\nsequence. On the other hand, the computational cost of the preprocessing stage\nis high. In this paper, we address challenges of the preprocessing phase, by an\nautomated selection of representative frames among the input sequences.\nFurthermore, we extract the key features of the representative frame rather\nthan the entire features. We propose a hybrid technique using background\nsubtraction and HOG, followed by application of a deep neural network and\nskeletal modelling method. The combination of a CNN and the LSTM recursive\nnetwork is considered for feature selection and maintaining the previous\ninformation, and finally, a Softmax-KNN classifier is used for labelling human\nactivities. We name our model as Feature Reduction & Deep Learning based action\nrecognition method, or FR-DL in short. To evaluate the proposed method, we use\nthe UCF dataset for the benchmarking which is widely-used among researchers in\naction recognition research. The dataset includes 101 complicated activities in\nthe wild. Experimental results show a significant improvement in terms of\naccuracy and speed in comparison with six state-of-the-art articles.",
          "arxiv_id": "2007.02811v1"
        },
        {
          "title": "TinyVIRAT: Low-resolution Video Action Recognition",
          "year": "2020-07",
          "abstract": "The existing research in action recognition is mostly focused on high-quality\nvideos where the action is distinctly visible. In real-world surveillance\nenvironments, the actions in videos are captured at a wide range of\nresolutions. Most activities occur at a distance with a small resolution and\nrecognizing such activities is a challenging problem. In this work, we focus on\nrecognizing tiny actions in videos. We introduce a benchmark dataset,\nTinyVIRAT, which contains natural low-resolution activities. The actions in\nTinyVIRAT videos have multiple labels and they are extracted from surveillance\nvideos which makes them realistic and more challenging. We propose a novel\nmethod for recognizing tiny actions in videos which utilizes a progressive\ngenerative approach to improve the quality of low-resolution actions. The\nproposed method also consists of a weakly trained attention mechanism which\nhelps in focusing on the activity regions in the video. We perform extensive\nexperiments to benchmark the proposed TinyVIRAT dataset and observe that the\nproposed method significantly improves the action recognition performance over\nbaselines. We also evaluate the proposed approach on synthetically resized\naction recognition datasets and achieve state-of-the-art results when compared\nwith existing methods. The dataset and code is publicly available at\nhttps://github.com/UgurDemir/Tiny-VIRAT.",
          "arxiv_id": "2007.07355v1"
        }
      ],
      "34": [
        {
          "title": "Beyond PRNU: Learning Robust Device-Specific Fingerprint for Source Camera Identification",
          "year": "2021-11",
          "abstract": "Source camera identification tools assist image forensic investigators to\nassociate an image in question with a suspect camera. Various techniques have\nbeen developed based on the analysis of the subtle traces left in the images\nduring the acquisition. The Photo Response Non Uniformity (PRNU) noise pattern\ncaused by sensor imperfections has been proven to be an effective way to\nidentify the source camera. The existing literature suggests that the PRNU is\nthe only fingerprint that is device-specific and capable of identifying the\nexact source device. However, the PRNU is susceptible to camera settings, image\ncontent, image processing operations, and counter-forensic attacks. A forensic\ninvestigator unaware of counter-forensic attacks or incidental image\nmanipulations is at the risk of getting misled. The spatial synchronization\nrequirement during the matching of two PRNUs also represents a major limitation\nof the PRNU. In recent years, deep learning based approaches have been\nsuccessful in identifying source camera models. However, the identification of\nindividual cameras of the same model through these data-driven approaches\nremains unsatisfactory. In this paper, we bring to light the existence of a new\nrobust data-driven device-specific fingerprint in digital images which is\ncapable of identifying the individual cameras of the same model. It is\ndiscovered that the new device fingerprint is location-independent, stochastic,\nand globally available, which resolve the spatial synchronization issue. Unlike\nthe PRNU, which resides in the high-frequency band, the new device fingerprint\nis extracted from the low and mid-frequency bands, which resolves the fragility\nissue that the PRNU is unable to contend with. Our experiments on various\ndatasets demonstrate that the new fingerprint is highly resilient to image\nmanipulations such as rotation, gamma correction, and aggressive JPEG\ncompression.",
          "arxiv_id": "2111.02144v1"
        },
        {
          "title": "A Transferable Anti-Forensic Attack on Forensic CNNs Using A Generative Adversarial Network",
          "year": "2021-01",
          "abstract": "With the development of deep learning, convolutional neural networks (CNNs)\nhave become widely used in multimedia forensics for tasks such as detecting and\nidentifying image forgeries. Meanwhile, anti-forensic attacks have been\ndeveloped to fool these CNN-based forensic algorithms. Previous anti-forensic\nattacks often were designed to remove forgery traces left by a single\nmanipulation operation as opposed to a set of manipulations. Additionally,\nrecent research has shown that existing anti-forensic attacks against forensic\nCNNs have poor transferability, i.e. they are unable to fool other forensic\nCNNs that were not explicitly used during training. In this paper, we propose a\nnew anti-forensic attack framework designed to remove forensic traces left by a\nvariety of manipulation operations. This attack is transferable, i.e. it can be\nused to attack forensic CNNs are unknown to the attacker, and it introduces\nonly minimal distortions that are imperceptible to human eyes. Our proposed\nattack utilizes a generative adversarial network (GAN) to build a generator\nthat can attack color images of any size. We achieve attack transferability\nthrough the use of a new training strategy and loss function. We conduct\nextensive experiment to demonstrate that our attack can fool many state-of-art\nforensic CNNs with varying levels of knowledge available to the attacker.",
          "arxiv_id": "2101.09568v1"
        },
        {
          "title": "Perception Matters: Exploring Imperceptible and Transferable Anti-forensics for GAN-generated Fake Face Imagery Detection",
          "year": "2020-10",
          "abstract": "Recently, generative adversarial networks (GANs) can generate photo-realistic\nfake facial images which are perceptually indistinguishable from real face\nphotos, promoting research on fake face detection. Though fake face forensics\ncan achieve high detection accuracy, their anti-forensic counterparts are less\ninvestigated. Here we explore more \\textit{imperceptible} and\n\\textit{transferable} anti-forensics for fake face imagery detection based on\nadversarial attacks. Since facial and background regions are often smooth, even\nsmall perturbation could cause noticeable perceptual impairment in fake face\nimages. Therefore it makes existing adversarial attacks ineffective as an\nanti-forensic method. Our perturbation analysis reveals the intuitive reason of\nthe perceptual degradation issue when directly applying existing attacks. We\nthen propose a novel adversarial attack method, better suitable for image\nanti-forensics, in the transformed color domain by considering visual\nperception. Simple yet effective, the proposed method can fool both deep\nlearning and non-deep learning based forensic detectors, achieving higher\nattack success rate and significantly improved visual quality. Specially, when\nadversaries consider imperceptibility as a constraint, the proposed\nanti-forensic method can improve the average attack success rate by around 30\\%\non fake face images over two baseline attacks. \\textit{More imperceptible} and\n\\textit{more transferable}, the proposed method raises new security concerns to\nfake face imagery detection. We have released our code for public use, and\nhopefully the proposed method can be further explored in related forensic\napplications as an anti-forensic benchmark.",
          "arxiv_id": "2010.15886v1"
        }
      ],
      "35": [
        {
          "title": "Automatic Polyp Segmentation using Fully Convolutional Neural Network",
          "year": "2021-01",
          "abstract": "Colorectal cancer is one of fatal cancer worldwide. Colonoscopy is the\nstandard treatment for examination, localization, and removal of colorectal\npolyps. However, it has been shown that the miss-rate of colorectal polyps\nduring colonoscopy is between 6 to 27%. The use of an automated, accurate, and\nreal-time polyp segmentation during colonoscopy examinations can help the\nclinicians to eliminate missing lesions and prevent further progression of\ncolorectal cancer. The ``Medico automatic polyp segmentation challenge''\nprovides an opportunity to study polyp segmentation and build a fast\nsegmentation model. The challenge organizers provide a Kvasir-SEG dataset to\ntrain the model. Then it is tested on a separate unseen dataset to validate the\nefficiency and speed of the segmentation model. The experiments demonstrate\nthat the model trained on the Kvasir-SEG dataset and tested on an unseen\ndataset achieves a dice coefficient of 0.7801, mIoU of 0.6847, recall of\n0.8077, and precision of 0.8126, demonstrating the generalization ability of\nour model. The model has achieved 80.60 FPS on the unseen dataset with an image\nresolution of $512 \\times 512$.",
          "arxiv_id": "2101.04001v1"
        },
        {
          "title": "Automatic Polyp Segmentation using U-Net-ResNet50",
          "year": "2020-12",
          "abstract": "Polyps are the predecessors to colorectal cancer which is considered as one\nof the leading causes of cancer-related deaths worldwide. Colonoscopy is the\nstandard procedure for the identification, localization, and removal of\ncolorectal polyps. Due to variability in shape, size, and surrounding tissue\nsimilarity, colorectal polyps are often missed by the clinicians during\ncolonoscopy. With the use of an automatic, accurate, and fast polyp\nsegmentation method during the colonoscopy, many colorectal polyps can be\neasily detected and removed. The ``Medico automatic polyp segmentation\nchallenge'' provides an opportunity to study polyp segmentation and build an\nefficient and accurate segmentation algorithm. We use the U-Net with\npre-trained ResNet50 as the encoder for the polyp segmentation. The model is\ntrained on Kvasir-SEG dataset provided for the challenge and tested on the\norganizer's dataset and achieves a dice coefficient of 0.8154, Jaccard of\n0.7396, recall of 0.8533, precision of 0.8532, accuracy of 0.9506, and F2 score\nof 0.8272, demonstrating the generalization ability of our model.",
          "arxiv_id": "2012.15247v1"
        },
        {
          "title": "TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation",
          "year": "2022-06",
          "abstract": "Colorectal cancer (CRC) is one of the most common causes of cancer and\ncancer-related mortality worldwide. Performing colon cancer screening in a\ntimely fashion is the key to early detection. Colonoscopy is the primary\nmodality used to diagnose colon cancer. However, the miss rate of polyps,\nadenomas and advanced adenomas remains significantly high. Early detection of\npolyps at the precancerous stage can help reduce the mortality rate and the\neconomic burden associated with colorectal cancer. Deep learning-based\ncomputer-aided diagnosis (CADx) system may help gastroenterologists to identify\npolyps that may otherwise be missed, thereby improving the polyp detection\nrate. Additionally, CADx system could prove to be a cost-effective system that\nimproves long-term colorectal cancer prevention. In this study, we proposed a\ndeep learning-based architecture for automatic polyp segmentation, called\nTransformer ResU-Net (TransResU-Net). Our proposed architecture is built upon\nresidual blocks with ResNet-50 as the backbone and takes the advantage of\ntransformer self-attention mechanism as well as dilated convolution(s). Our\nexperimental results on two publicly available polyp segmentation benchmark\ndatasets showed that TransResU-Net obtained a highly promising dice score and a\nreal-time speed. With high efficacy in our performance metrics, we concluded\nthat TransResU-Net could be a strong benchmark for building a real-time polyp\ndetection system for the early diagnosis, treatment, and prevention of\ncolorectal cancer. The source code of the proposed TransResU-Net is publicly\navailable at https://github.com/nikhilroxtomar/TransResUNet.",
          "arxiv_id": "2206.08985v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:24:44Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}