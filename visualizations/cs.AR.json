{
  "topics": {
    "data": {
      "0": {
        "name": "0_energy_hardware_memory_performance",
        "keywords": [
          [
            "energy",
            0.016978632956455002
          ],
          [
            "hardware",
            0.016748211666966606
          ],
          [
            "memory",
            0.014237428437171707
          ],
          [
            "performance",
            0.012648228501251831
          ],
          [
            "efficiency",
            0.012117592881808577
          ],
          [
            "design",
            0.01173228223990224
          ],
          [
            "accuracy",
            0.011624076492499675
          ],
          [
            "In",
            0.011537221240758564
          ],
          [
            "power",
            0.010986375709631967
          ],
          [
            "neural",
            0.010813933616941676
          ]
        ],
        "count": 2472
      },
      "1": {
        "name": "1_performance_FPGA_hardware_data",
        "keywords": [
          [
            "performance",
            0.01912271579535267
          ],
          [
            "FPGA",
            0.015480384798502573
          ],
          [
            "hardware",
            0.015104344227212844
          ],
          [
            "data",
            0.013512982714934988
          ],
          [
            "memory",
            0.01339279947005847
          ],
          [
            "high",
            0.012658917117494868
          ],
          [
            "RISC",
            0.012622663772714529
          ],
          [
            "applications",
            0.012534835070724502
          ],
          [
            "design",
            0.011831638041251862
          ],
          [
            "level",
            0.011099069185617741
          ]
        ],
        "count": 505
      },
      "2": {
        "name": "2_DRAM_memory_data_performance",
        "keywords": [
          [
            "DRAM",
            0.04230296412704067
          ],
          [
            "memory",
            0.04145048395306144
          ],
          [
            "data",
            0.025478172955677433
          ],
          [
            "performance",
            0.021889358514298293
          ],
          [
            "cache",
            0.01842846151322563
          ],
          [
            "storage",
            0.012887843030646197
          ],
          [
            "systems",
            0.012748594180055807
          ],
          [
            "latency",
            0.012738431909511596
          ],
          [
            "access",
            0.011878008826148753
          ],
          [
            "Memory",
            0.011817317980912683
          ]
        ],
        "count": 440
      },
      "3": {
        "name": "3_security_attacks_attack_hardware",
        "keywords": [
          [
            "security",
            0.03971665082799818
          ],
          [
            "attacks",
            0.0339188413560754
          ],
          [
            "attack",
            0.019549817409995247
          ],
          [
            "hardware",
            0.018956222528511514
          ],
          [
            "cache",
            0.017548148931749045
          ],
          [
            "secure",
            0.015615110414435392
          ],
          [
            "channel",
            0.014647887706637884
          ],
          [
            "In",
            0.014290902301416766
          ],
          [
            "execution",
            0.01328455623224081
          ],
          [
            "performance",
            0.013260811482016202
          ]
        ],
        "count": 335
      },
      "4": {
        "name": "4_design_circuit_circuits_placement",
        "keywords": [
          [
            "design",
            0.03341747804260833
          ],
          [
            "circuit",
            0.031921268403481876
          ],
          [
            "circuits",
            0.0197188616576832
          ],
          [
            "placement",
            0.017816345683523027
          ],
          [
            "learning",
            0.017584723366076725
          ],
          [
            "optimization",
            0.015829826430833183
          ],
          [
            "synthesis",
            0.013537338299398774
          ],
          [
            "framework",
            0.012582991706215593
          ],
          [
            "graph",
            0.012225002266961844
          ],
          [
            "3D",
            0.012154797375142593
          ]
        ],
        "count": 197
      },
      "5": {
        "name": "5_LLMs_code_design_Verilog",
        "keywords": [
          [
            "LLMs",
            0.04648525736349214
          ],
          [
            "code",
            0.044153395878373525
          ],
          [
            "design",
            0.03427341026778699
          ],
          [
            "Verilog",
            0.03387525939170383
          ],
          [
            "generation",
            0.0336491993630162
          ],
          [
            "RTL",
            0.032668713415604546
          ],
          [
            "LLM",
            0.0303105848526588
          ],
          [
            "language",
            0.02426878624729128
          ],
          [
            "hardware",
            0.020227377843156962
          ],
          [
            "code generation",
            0.01932375031137425
          ]
        ],
        "count": 186
      },
      "6": {
        "name": "6_quantum_Quantum_qubits_circuits",
        "keywords": [
          [
            "quantum",
            0.11458692236872475
          ],
          [
            "Quantum",
            0.051114412344557034
          ],
          [
            "qubits",
            0.033963654957794555
          ],
          [
            "circuits",
            0.025597610941695634
          ],
          [
            "circuit",
            0.025093313700692566
          ],
          [
            "quantum computing",
            0.022078475661993523
          ],
          [
            "gates",
            0.02059302773922827
          ],
          [
            "computing",
            0.019335495889056933
          ],
          [
            "error",
            0.018763898076042475
          ],
          [
            "computers",
            0.018519986986540695
          ]
        ],
        "count": 183
      },
      "7": {
        "name": "7_graph_GNN_Graph_GNNs",
        "keywords": [
          [
            "graph",
            0.051920434050120735
          ],
          [
            "GNN",
            0.03833502298525061
          ],
          [
            "Graph",
            0.031077002516619785
          ],
          [
            "GNNs",
            0.021365837647437127
          ],
          [
            "memory",
            0.020293075233145837
          ],
          [
            "graphs",
            0.020250634060362264
          ],
          [
            "data",
            0.019621366813645146
          ],
          [
            "GCN",
            0.01889670897315708
          ],
          [
            "irregular",
            0.014823954739224886
          ],
          [
            "performance",
            0.014657946583924995
          ]
        ],
        "count": 135
      },
      "8": {
        "name": "8_approximate_arithmetic_multiplier_bit",
        "keywords": [
          [
            "approximate",
            0.02247495355141278
          ],
          [
            "arithmetic",
            0.01993546104627111
          ],
          [
            "multiplier",
            0.01990627233126034
          ],
          [
            "bit",
            0.019420968665038618
          ],
          [
            "posit",
            0.017814981943715863
          ],
          [
            "proposed",
            0.017655466431317688
          ],
          [
            "multipliers",
            0.01705306279834997
          ],
          [
            "adder",
            0.01703837336585658
          ],
          [
            "design",
            0.016992669222051816
          ],
          [
            "power",
            0.016680069284602986
          ]
        ],
        "count": 130
      },
      "9": {
        "name": "9_FHE_HE_data_Encryption",
        "keywords": [
          [
            "FHE",
            0.03864451701666903
          ],
          [
            "HE",
            0.027117477350764134
          ],
          [
            "data",
            0.020100511966683998
          ],
          [
            "Encryption",
            0.019794787330772088
          ],
          [
            "hardware",
            0.018778618830507837
          ],
          [
            "encryption",
            0.016197805327369367
          ],
          [
            "computation",
            0.015538214749100967
          ],
          [
            "operations",
            0.015214456496925124
          ],
          [
            "multiplication",
            0.014944168914448722
          ],
          [
            "privacy",
            0.0147539388380621
          ]
        ],
        "count": 128
      }
    },
    "correlations": [
      [
        1.0,
        -0.6363190159786176,
        -0.6520522908876618,
        -0.7150995598616745,
        -0.6981697162297694,
        -0.6890624891792145,
        -0.7555964981346344,
        -0.7403936949000632,
        -0.6677844197558962,
        -0.7477771301906435
      ],
      [
        -0.6363190159786176,
        1.0,
        -0.40602479584341655,
        -0.7031137349159859,
        -0.6910221125000022,
        -0.6893104517265372,
        -0.75031083245371,
        -0.7349240047172192,
        -0.7048391099272165,
        -0.743250711386535
      ],
      [
        -0.6520522908876618,
        -0.40602479584341655,
        1.0,
        -0.6738345854612771,
        -0.69759888177639,
        -0.673454320567806,
        -0.7567178140574299,
        -0.7204984593100656,
        -0.7190005538638206,
        -0.7415299618057202
      ],
      [
        -0.7150995598616745,
        -0.7031137349159859,
        -0.6738345854612771,
        1.0,
        -0.7147502072746339,
        -0.7080330082480434,
        -0.7412559566512967,
        -0.7439809585185637,
        -0.7365556180392947,
        -0.7264167755275324
      ],
      [
        -0.6981697162297694,
        -0.6910221125000022,
        -0.69759888177639,
        -0.7147502072746339,
        1.0,
        -0.44721223845931957,
        -0.7187729648746729,
        -0.6885273050168508,
        -0.6947078247121792,
        -0.7562184189794521
      ],
      [
        -0.6890624891792145,
        -0.6893104517265372,
        -0.673454320567806,
        -0.7080330082480434,
        -0.44721223845931957,
        1.0,
        -0.7490727380810955,
        -0.7218271280618744,
        -0.7187346709834,
        -0.7579526303460438
      ],
      [
        -0.7555964981346344,
        -0.75031083245371,
        -0.7567178140574299,
        -0.7412559566512967,
        -0.7187729648746729,
        -0.7490727380810955,
        1.0,
        -0.7568158227117265,
        -0.753222883210965,
        -0.7390174509945477
      ],
      [
        -0.7403936949000632,
        -0.7349240047172192,
        -0.7204984593100656,
        -0.7439809585185637,
        -0.6885273050168508,
        -0.7218271280618744,
        -0.7568158227117265,
        1.0,
        -0.7530614676416294,
        -0.761183553704814
      ],
      [
        -0.6677844197558962,
        -0.7048391099272165,
        -0.7190005538638206,
        -0.7365556180392947,
        -0.6947078247121792,
        -0.7187346709834,
        -0.753222883210965,
        -0.7530614676416294,
        1.0,
        -0.757837825295639
      ],
      [
        -0.7477771301906435,
        -0.743250711386535,
        -0.7415299618057202,
        -0.7264167755275324,
        -0.7562184189794521,
        -0.7579526303460438,
        -0.7390174509945477,
        -0.761183553704814,
        -0.757837825295639,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        8,
        0,
        3,
        1,
        3,
        0,
        2,
        0,
        0,
        0
      ],
      "2020-02": [
        9,
        1,
        5,
        1,
        3,
        1,
        1,
        0,
        0,
        0
      ],
      "2020-03": [
        5,
        4,
        4,
        2,
        2,
        0,
        1,
        0,
        3,
        0
      ],
      "2020-04": [
        21,
        2,
        7,
        4,
        2,
        1,
        0,
        1,
        3,
        1
      ],
      "2020-05": [
        13,
        0,
        8,
        4,
        2,
        2,
        0,
        0,
        5,
        1
      ],
      "2020-06": [
        21,
        0,
        7,
        2,
        2,
        0,
        1,
        0,
        1,
        0
      ],
      "2020-07": [
        21,
        8,
        12,
        9,
        6,
        4,
        2,
        4,
        0,
        0
      ],
      "2020-08": [
        17,
        3,
        11,
        5,
        7,
        0,
        2,
        0,
        4,
        0
      ],
      "2020-09": [
        27,
        4,
        14,
        7,
        8,
        7,
        2,
        2,
        2,
        2
      ],
      "2020-10": [
        15,
        4,
        8,
        4,
        6,
        5,
        2,
        5,
        7,
        0
      ],
      "2020-11": [
        29,
        4,
        9,
        4,
        6,
        2,
        0,
        3,
        4,
        0
      ],
      "2020-12": [
        18,
        2,
        13,
        7,
        8,
        2,
        1,
        1,
        1,
        0
      ],
      "2021-01": [
        22,
        3,
        6,
        5,
        1,
        4,
        1,
        1,
        4,
        0
      ],
      "2021-02": [
        32,
        2,
        4,
        8,
        2,
        5,
        2,
        2,
        3,
        0
      ],
      "2021-03": [
        34,
        2,
        5,
        8,
        3,
        4,
        0,
        9,
        2,
        1
      ],
      "2021-04": [
        24,
        5,
        7,
        10,
        7,
        5,
        1,
        8,
        2,
        0
      ],
      "2021-05": [
        28,
        2,
        12,
        8,
        8,
        2,
        0,
        5,
        2,
        0
      ],
      "2021-06": [
        21,
        7,
        8,
        11,
        5,
        4,
        3,
        1,
        4,
        0
      ],
      "2021-07": [
        22,
        3,
        11,
        13,
        4,
        1,
        2,
        1,
        2,
        0
      ],
      "2021-08": [
        19,
        3,
        10,
        6,
        6,
        2,
        3,
        4,
        3,
        0
      ],
      "2021-09": [
        20,
        4,
        12,
        6,
        2,
        1,
        2,
        3,
        5,
        2
      ],
      "2021-10": [
        22,
        3,
        7,
        5,
        3,
        3,
        1,
        2,
        6,
        1
      ],
      "2021-11": [
        21,
        4,
        4,
        3,
        3,
        7,
        2,
        9,
        3,
        0
      ],
      "2021-12": [
        25,
        6,
        9,
        7,
        3,
        2,
        2,
        6,
        3,
        3
      ],
      "2022-01": [
        24,
        2,
        7,
        5,
        5,
        5,
        3,
        6,
        5,
        0
      ],
      "2022-02": [
        20,
        2,
        6,
        6,
        7,
        5,
        3,
        3,
        6,
        1
      ],
      "2022-03": [
        23,
        1,
        11,
        7,
        7,
        3,
        1,
        5,
        11,
        0
      ],
      "2022-04": [
        24,
        3,
        9,
        6,
        6,
        4,
        1,
        2,
        4,
        3
      ],
      "2022-05": [
        33,
        5,
        18,
        8,
        11,
        6,
        3,
        5,
        2,
        2
      ],
      "2022-06": [
        28,
        1,
        11,
        7,
        8,
        1,
        1,
        5,
        4,
        0
      ],
      "2022-07": [
        28,
        6,
        7,
        6,
        1,
        1,
        3,
        6,
        3,
        1
      ],
      "2022-08": [
        29,
        1,
        8,
        4,
        3,
        1,
        12,
        2,
        4,
        0
      ],
      "2022-09": [
        28,
        3,
        9,
        7,
        4,
        3,
        4,
        5,
        3,
        1
      ],
      "2022-10": [
        26,
        2,
        6,
        3,
        5,
        3,
        7,
        3,
        2,
        1
      ],
      "2022-11": [
        23,
        2,
        13,
        8,
        11,
        5,
        4,
        4,
        3,
        2
      ],
      "2022-12": [
        22,
        2,
        6,
        4,
        3,
        4,
        4,
        2,
        2,
        2
      ],
      "2023-01": [
        24,
        4,
        7,
        4,
        2,
        2,
        3,
        4,
        4,
        0
      ],
      "2023-02": [
        31,
        1,
        9,
        11,
        3,
        4,
        2,
        0,
        11,
        1
      ],
      "2023-03": [
        24,
        4,
        13,
        3,
        8,
        5,
        6,
        10,
        8,
        1
      ],
      "2023-04": [
        28,
        5,
        9,
        9,
        8,
        9,
        3,
        7,
        7,
        1
      ],
      "2023-05": [
        34,
        5,
        12,
        7,
        3,
        12,
        3,
        6,
        5,
        1
      ],
      "2023-06": [
        31,
        3,
        12,
        5,
        3,
        3,
        6,
        6,
        6,
        0
      ],
      "2023-07": [
        31,
        3,
        4,
        7,
        5,
        7,
        4,
        2,
        2,
        0
      ],
      "2023-08": [
        33,
        3,
        7,
        9,
        8,
        7,
        6,
        12,
        4,
        2
      ],
      "2023-09": [
        36,
        6,
        10,
        7,
        5,
        5,
        4,
        3,
        5,
        2
      ],
      "2023-10": [
        29,
        2,
        8,
        8,
        7,
        13,
        2,
        1,
        1,
        2
      ],
      "2023-11": [
        38,
        2,
        10,
        4,
        6,
        11,
        9,
        4,
        6,
        2
      ],
      "2023-12": [
        28,
        5,
        8,
        3,
        6,
        11,
        3,
        7,
        3,
        0
      ],
      "2024-01": [
        37,
        1,
        10,
        9,
        7,
        20,
        3,
        5,
        5,
        1
      ],
      "2024-02": [
        27,
        4,
        12,
        5,
        12,
        11,
        2,
        5,
        7,
        0
      ],
      "2024-03": [
        37,
        4,
        12,
        5,
        10,
        12,
        4,
        4,
        4,
        1
      ],
      "2024-04": [
        42,
        5,
        11,
        6,
        14,
        11,
        3,
        5,
        3,
        1
      ],
      "2024-05": [
        43,
        3,
        11,
        7,
        2,
        14,
        4,
        2,
        7,
        1
      ],
      "2024-06": [
        35,
        5,
        13,
        10,
        4,
        12,
        4,
        6,
        6,
        0
      ],
      "2024-07": [
        51,
        2,
        17,
        9,
        12,
        23,
        3,
        7,
        3,
        0
      ],
      "2024-08": [
        28,
        2,
        21,
        7,
        8,
        21,
        6,
        4,
        3,
        0
      ],
      "2024-09": [
        34,
        1,
        9,
        10,
        11,
        24,
        8,
        4,
        3,
        1
      ],
      "2024-10": [
        44,
        5,
        10,
        8,
        8,
        19,
        4,
        3,
        3,
        3
      ],
      "2024-11": [
        42,
        4,
        11,
        7,
        17,
        16,
        7,
        9,
        9,
        1
      ],
      "2024-12": [
        41,
        4,
        7,
        5,
        6,
        29,
        5,
        5,
        4,
        2
      ],
      "2025-01": [
        47,
        5,
        21,
        7,
        9,
        16,
        3,
        4,
        5,
        2
      ],
      "2025-02": [
        47,
        3,
        20,
        7,
        10,
        31,
        5,
        9,
        5,
        1
      ],
      "2025-03": [
        51,
        4,
        19,
        3,
        17,
        31,
        4,
        3,
        4,
        3
      ],
      "2025-04": [
        52,
        2,
        18,
        10,
        11,
        43,
        4,
        6,
        7,
        1
      ],
      "2025-05": [
        64,
        5,
        16,
        9,
        11,
        31,
        4,
        6,
        6,
        0
      ],
      "2025-06": [
        55,
        6,
        16,
        8,
        4,
        27,
        6,
        5,
        6,
        2
      ],
      "2025-07": [
        50,
        5,
        10,
        5,
        8,
        36,
        1,
        8,
        6,
        2
      ],
      "2025-08": [
        65,
        5,
        14,
        5,
        7,
        32,
        5,
        8,
        4,
        1
      ],
      "2025-09": [
        19,
        5,
        4,
        4,
        3,
        14,
        1,
        3,
        2,
        1
      ]
    },
    "papers": {
      "0": [
        {
          "title": "OPIMA: Optical Processing-In-Memory for Convolutional Neural Network Acceleration",
          "year": "2024-07",
          "abstract": "Recent advances in machine learning (ML) have spotlighted the pressing need\nfor computing architectures that bridge the gap between memory bandwidth and\nprocessing power. The advent of deep neural networks has pushed traditional Von\nNeumann architectures to their limits due to the high latency and energy\nconsumption costs associated with data movement between the processor and\nmemory for these workloads. One of the solutions to overcome this bottleneck is\nto perform computation within the main memory through processing-in-memory\n(PIM), thereby limiting data movement and the costs associated with it.\nHowever, DRAM-based PIM struggles to achieve high throughput and energy\nefficiency due to internal data movement bottlenecks and the need for frequent\nrefresh operations. In this work, we introduce OPIMA, a PIM-based ML\naccelerator, architected within an optical main memory. OPIMA has been designed\nto leverage the inherent massive parallelism within main memory while\nperforming high-speed, low-energy optical computation to accelerate ML models\nbased on convolutional neural networks. We present a comprehensive analysis of\nOPIMA to guide design choices and operational mechanisms. Additionally, we\nevaluate the performance and energy consumption of OPIMA, comparing it with\nconventional electronic computing systems and emerging photonic PIM\narchitectures. The experimental results show that OPIMA can achieve 2.98x\nhigher throughput and 137x better energy efficiency than the best-known prior\nwork.",
          "arxiv_id": "2407.08205v1"
        },
        {
          "title": "NeuroNAS: Enhancing Efficiency of Neuromorphic In-Memory Computing for Intelligent Mobile Agents through Hardware-Aware Spiking Neural Architecture Search",
          "year": "2024-06",
          "abstract": "Intelligent mobile agents (e.g., UGVs and UAVs) typically demand low\npower/energy consumption when solving their machine learning (ML)-based tasks,\nsince they are usually powered by portable batteries with limited capacity. A\npotential solution is employing neuromorphic computing with Spiking Neural\nNetworks (SNNs), which leverages event-based computation to enable ultra-low\npower/energy ML algorithms. To maximize the performance efficiency of SNN\ninference, the In-Memory Computing (IMC)-based hardware accelerators with\nemerging device technologies (e.g., RRAM) can be employed. However, SNN models\nare typically developed without considering constraints from the application\nand the underlying IMC hardware, thereby hindering SNNs from reaching their\nfull potential in performance and efficiency. To address this, we propose\nNeuroNAS, a novel framework for developing energyefficient neuromorphic IMC for\nintelligent mobile agents using hardware-aware spiking neural architecture\nsearch (NAS), i.e., by quickly finding an SNN architecture that offers high\naccuracy under the given constraints (e.g., memory, area, latency, and energy\nconsumption). Its key steps include: optimizing SNN operations to enable\nefficient NAS, employing quantization to minimize the memory footprint,\ndeveloping an SNN architecture that facilitates an effective learning, and\ndevising a systematic hardware-aware search algorithm to meet the constraints.\nCompared to the state-of-the-art techniques, NeuroNAS quickly finds SNN\narchitectures (with 8bit weight precision) that maintain high accuracy by up to\n6.6x search time speed-ups, while achieving up to 92% area savings, 1.2x\nlatency improvements, 84% energy savings across different datasets (i.e.,\nCIFAR-10, CIFAR-100, and TinyImageNet-200); while the state-of-the-art fail to\nmeet all constraints at once.",
          "arxiv_id": "2407.00641v3"
        },
        {
          "title": "BPLight-CNN: A Photonics-based Backpropagation Accelerator for Deep Learning",
          "year": "2021-02",
          "abstract": "Training deep learning networks involves continuous weight updates across the\nvarious layers of the deep network while using a backpropagation algorithm\n(BP). This results in expensive computation overheads during training.\nConsequently, most deep learning accelerators today employ pre-trained weights\nand focus only on improving the design of the inference phase. The recent trend\nis to build a complete deep learning accelerator by incorporating the training\nmodule. Such efforts require an ultra-fast chip architecture for executing the\nBP algorithm. In this article, we propose a novel photonics-based\nbackpropagation accelerator for high performance deep learning training. We\npresent the design for a convolutional neural network, BPLight-CNN, which\nincorporates the silicon photonics-based backpropagation accelerator.\nBPLight-CNN is a first-of-its-kind photonic and memristor-based CNN\narchitecture for end-to-end training and prediction. We evaluate BPLight-CNN\nusing a photonic CAD framework (IPKISS) on deep learning benchmark models\nincluding LeNet and VGG-Net. The proposed design achieves (i) at least 34x\nspeedup, 34x improvement in computational efficiency, and 38.5x energy savings,\nduring training; and (ii) 29x speedup, 31x improvement in computational\nefficiency, and 38.7x improvement in energy savings, during inference compared\nto the state-of-the-art designs. All these comparisons are done at a 16-bit\nresolution; and BPLight-CNN achieves these improvements at a cost of\napproximately 6% lower accuracy compared to the state-of-the-art.",
          "arxiv_id": "2102.10140v1"
        }
      ],
      "1": [
        {
          "title": "High Throughput Multidimensional Tridiagonal Systems Solvers on FPGAs",
          "year": "2022-01",
          "abstract": "We present a design space exploration for synthesizing optimized,\nhigh-throughput implementations of multiple multi-dimensional tridiagonal\nsystem solvers on FPGAs. Re-evaluating the characteristics of algorithms for\nthe direct solution of tridiagonal systems, we develop a new tridiagonal solver\nlibrary aimed at implementing high-performance computing applications on Xilinx\nFPGA hardware. Key new features of the library are (1) the unification of\nstandard state-of-the-art techniques for implementing implicit numerical\nsolvers with a number of novel high-gain optimizations such as vectorization\nand batching, motivated by multi-dimensional systems in real-world\napplications, (2) data-flow techniques that provide application specific\noptimizations for both 2D and 3D problems, including integration of explicit\nloops commonplace in real workloads, and (3) the development of an analytic\nmodel to explore the design space, and obtain rapid performance estimates. The\nnew library provide an order of magnitude better performance for solving large\nbatches of systems compared to Xilinx's current tridiagonal solver library. Two\nrepresentative applications are implemented using the new solver on a Xilinx\nAlveo U280 FPGA, demonstrating over 85% predictive model accuracy. These are\ncompared with a current state-of-the-art GPU library for solving\nmulti-dimensional tridiagonal systems on an Nvidia V100 GPU, analyzing time to\nsolution, bandwidth, and energy consumption. Results show the FPGAs achieving\ncompetitive or better runtime performance for a range of multi-dimensional\nproblems compared to the V100 GPU. Additionally, the significant energy savings\noffered by FPGA implementations, over 30% for the most complex application, are\nquantified. We discuss the algorithmic trade-offs required to obtain good\nperformance on FPGAs, giving insights into the feasibility and profitability of\nFPGA implementations.",
          "arxiv_id": "2201.03950v1"
        },
        {
          "title": "High-Level FPGA Accelerator Design for Structured-Mesh-Based Explicit Numerical Solvers",
          "year": "2021-01",
          "abstract": "This paper presents a workflow for synthesizing near-optimal FPGA\nimplementations for structured-mesh based stencil applications for explicit\nsolvers. It leverages key characteristics of the application class, its\ncomputation-communication pattern, and the architectural capabilities of the\nFPGA to accelerate solvers from the high-performance computing domain. Key new\nfeatures of the workflow are (1) the unification of standard state-of-the-art\ntechniques with a number of high-gain optimizations such as batching and\nspatial blocking/tiling, motivated by increasing throughput for real-world work\nloads and (2) the development and use of a predictive analytic model for\nexploring the design space, resource estimates and performance. Three\nrepresentative applications are implemented using the design workflow on a\nXilinx Alveo U280 FPGA, demonstrating near-optimal performance and over 85%\npredictive model accuracy. These are compared with equivalent highly-optimized\nimplementations of the same applications on modern HPC-grade GPUs (Nvidia V100)\nanalyzing time to solution, bandwidth and energy consumption. Performance\nresults indicate equivalent runtime performance of the FPGA implementations to\nthe V100 GPU, with over 2x energy savings, for the largest non-trivial\napplication synthesized on the FPGA compared to the best performing GPU-based\nsolution. Our investigation shows the considerable challenges in gaining high\nperformance on current generation FPGAs compared to traditional architectures.\nWe discuss determinants for a given stencil code to be amenable to FPGA\nimplementation, providing insights into the feasibility and profitability of a\ndesign and its resulting performance.",
          "arxiv_id": "2101.01177v2"
        },
        {
          "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded Heterogeneous SoCs",
          "year": "2025-02",
          "abstract": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
          "arxiv_id": "2502.17398v1"
        }
      ],
      "2": [
        {
          "title": "CLR-DRAM: A Low-Cost DRAM Architecture Enabling Dynamic Capacity-Latency Trade-Off",
          "year": "2020-05",
          "abstract": "DRAM is the prevalent main memory technology, but its long access latency can\nlimit the performance of many workloads. Although prior works provide DRAM\ndesigns that reduce DRAM access latency, their reduced storage capacities\nhinder the performance of workloads that need large memory capacity. Because\nthe capacity-latency trade-off is fixed at design time, previous works cannot\nachieve maximum performance under very different and dynamic workload demands.\n  This paper proposes Capacity-Latency-Reconfigurable DRAM (CLR-DRAM), a new\nDRAM architecture that enables dynamic capacity-latency trade-off at low cost.\nCLR-DRAM allows dynamic reconfiguration of any DRAM row to switch between two\noperating modes: 1) max-capacity mode, where every DRAM cell operates\nindividually to achieve approximately the same storage density as a\ndensity-optimized commodity DRAM chip and 2) high-performance mode, where two\nadjacent DRAM cells in a DRAM row and their sense amplifiers are coupled to\noperate as a single low-latency logical cell driven by a single logical sense\namplifier.\n  We implement CLR-DRAM by adding isolation transistors in each DRAM subarray.\nOur evaluations show that CLR-DRAM can improve system performance and DRAM\nenergy consumption by 18.6% and 29.7% on average with four-core multiprogrammed\nworkloads. We believe that CLR-DRAM opens new research directions for a system\nto adapt to the diverse and dynamically changing memory capacity and access\nlatency demands of workloads.",
          "arxiv_id": "2005.12775v1"
        },
        {
          "title": "Improving DRAM Performance, Reliability, and Security by Rigorously Understanding Intrinsic DRAM Operation",
          "year": "2023-03",
          "abstract": "DRAM is the primary technology used for main memory in modern systems.\nUnfortunately, as DRAM scales down to smaller technology nodes, it faces key\nchallenges in both data integrity and latency, which strongly affect overall\nsystem reliability, security, and performance. To develop reliable, secure, and\nhigh-performance DRAM-based main memory for future systems, it is critical to\nrigorously characterize, analyze, and understand various aspects (e.g.,\nreliability, retention, latency, RowHammer vulnerability) of existing DRAM\nchips and their architecture. The goal of this dissertation is to 1) develop\ntechniques and infrastructures to enable such rigorous characterization,\nanalysis, and understanding, and 2) enable new mechanisms to improve DRAM\nperformance, reliability, and security based on the developed understanding.\n  To this end, in this dissertation, we 1) design, implement, and prototype a\nnew practical-to-use and flexible FPGA-based DRAM characterization\ninfrastructure (called SoftMC), 2) use the DRAM characterization infrastructure\nto develop a new experimental methodology (called U-TRR) to uncover the\noperation of existing proprietary in-DRAM RowHammer protection mechanisms and\ncraft new RowHammer access patterns to efficiently circumvent these RowHammer\nprotection mechanisms, 3) propose a new DRAM architecture, called SelfManaging\nDRAM, for enabling autonomous and efficient in-DRAM maintenance operations that\nenable not only better performance, efficiency, and reliability but also faster\nand easier adoption of changes to DRAM chips, and 4) propose a versatile DRAM\nsubstrate, called the Copy-Row (CROW) substrate, that enables new mechanisms\nfor improving DRAM performance, energy consumption, and reliability.",
          "arxiv_id": "2303.07445v1"
        },
        {
          "title": "Sectored DRAM: A Practical Energy-Efficient and High-Performance Fine-Grained DRAM Architecture",
          "year": "2022-07",
          "abstract": "We propose Sectored DRAM, a new, low-overhead DRAM substrate that reduces\nwasted energy by enabling fine-grained DRAM data transfers and DRAM row\nactivation. Sectored DRAM leverages two key ideas to enable fine-grained data\ntransfers and row activation at low chip area cost. First, a cache block\ntransfer between main memory and the memory controller happens in a fixed\nnumber of clock cycles where only a small portion of the cache block (a word)\nis transferred in each cycle. Sectored DRAM augments the memory controller and\nthe DRAM chip to execute cache block transfers in a variable number of clock\ncycles based on the workload access pattern with minor modifications to the\nmemory controller's and the DRAM chip's circuitry. Second, a large DRAM row, by\ndesign, is already partitioned into smaller independent physically isolated\nregions. Sectored DRAM provides the memory controller with the ability to\nactivate each such region based on the workload access pattern via small\nmodifications to the DRAM chip's array access circuitry. Activating smaller\nregions of a large row relaxes DRAM power delivery constraints and allows the\nmemory controller to schedule DRAM accesses faster.\n  Compared to a system with coarse-grained DRAM, Sectored DRAM reduces the DRAM\nenergy consumption of highly-memory-intensive workloads by up to (on average)\n33% (20%) while improving their performance by up to (on average) 36% (17%).\nSectored DRAM's DRAM energy savings, combined with its system performance\nimprovement, allows system-wide energy savings of up to 23%. Sectored DRAM's\nDRAM chip area overhead is 1.7% the area of a modern DDR4 chip. We hope and\nbelieve that Sectored DRAM's ideas and results will help to enable more\nefficient and high-performance memory systems. To this end, we open source\nSectored DRAM at https://github.com/CMU-SAFARI/Sectored-DRAM.",
          "arxiv_id": "2207.13795v4"
        }
      ],
      "3": [
        {
          "title": "Random and Safe Cache Architecture to Defeat Cache Timing Attacks",
          "year": "2023-09",
          "abstract": "Caches have been exploited to leak secret information due to the different\ntimes they take to handle memory accesses. Cache timing attacks include\nnon-speculative cache side and covert channel attacks and cache-based\nspeculative execution attacks. We first present a systematic view of the attack\nand defense space and show that no existing defense has addressed all cache\ntiming attacks, which we do in this paper. We propose Random and Safe (RaS)\ncache architectures to decorrelate cache state changes from memory requests.\nRaS fills the cache with ``safe'' cache lines that are likely to be used in the\nfuture, rather than with demand-fetched, security-sensitive lines. RaS lifts\nthe restriction on cache fills for accesses that become safe when speculative\nexecution is resolved and authorized. Our RaS-Spec design against cache-based\nspeculative execution attacks has a low 3.8% average performance overhead. RaS+\nvariants against both speculative and non-speculative attacks have\nsecurity-performance trade-offs ranging from 7.9% to 45.2% average overhead.",
          "arxiv_id": "2309.16172v2"
        },
        {
          "title": "New Models for Understanding and Reasoning about Speculative Execution Attacks",
          "year": "2020-09",
          "abstract": "Spectre and Meltdown attacks and their variants exploit hardware performance\noptimization features to cause security breaches. Secret information is\naccessed and leaked through covert or side channels. New attack variants keep\nappearing and we do not have a systematic way to capture the critical\ncharacteristics of these attacks and evaluate why they succeed or fail.\n  In this paper, we provide a new attack-graph model for reasoning about\nspeculative execution attacks. We model attacks as ordered dependency graphs,\nand prove that a race condition between two nodes can occur if there is a\nmissing dependency edge between them. We define a new concept, \"security\ndependency\", between a resource access and its prior authorization operation.\nWe show that a missing security dependency is equivalent to a race condition\nbetween authorization and access, which is a root cause of speculative\nexecution attacks. We show detailed examples of how our attack graph models the\nSpectre and Meltdown attacks, and is generalizable to all the attack variants\npublished so far. This attack model is also very useful for identifying new\nattacks and for generalizing defense strategies. We identify several defense\nstrategies with different performance-security tradeoffs. We show that the\ndefenses proposed so far all fit under one of our defense strategies. We also\nexplain how attack graphs can be constructed and point to this as promising\nfuture work for tool designers.",
          "arxiv_id": "2009.07998v2"
        },
        {
          "title": "Protecting Cache States Against Both Speculative Execution Attacks and Side-channel Attacks",
          "year": "2023-02",
          "abstract": "Hardware caches are essential performance optimization features in modern\nprocessors to reduce the effective memory access time. Unfortunately, they are\nalso the prime targets for attacks on computer processors because they are\nhigh-bandwidth and reliable side or covert channels for leaking secrets.\nConventional cache timing attacks typically leak secret encryption keys, while\nrecent speculative execution attacks typically leak arbitrary\nillegally-obtained secrets through cache timing channels. While many hardware\ndefenses have been proposed for each class of attacks, we show that those for\nconventional (non-speculative) cache timing channels do not work for all\nspeculative execution attacks, and vice versa. We maintain that a cache is not\nsecure unless it can defend against both of these major attack classes.\n  We propose a new methodology and framework for covering such relatively large\nattack surfaces to produce a Speculative and Timing Attack Resilient (STAR)\ncache subsystem. We use this to design two comprehensive secure cache\narchitectures, STAR-FARR and STAR-NEWS, that have very low performance\noverheads of 5.6% and 6.8%, respectively. To the best of our knowledge, these\nare the first secure cache designs that cover both non-speculative cache side\nchannels and cache-based speculative execution attacks.\n  Our methodology can be used to compose and check other secure cache designs.\nIt can also be extended to other attack classes and hardware systems.\nAdditionally, we also highlight the intrinsic security and performance benefits\nof a randomized cache like a real Fully Associative cache with Random\nReplacement (FARR) and a lower-latency, speculation-aware version (NEWS).",
          "arxiv_id": "2302.00732v2"
        }
      ],
      "4": [
        {
          "title": "CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design",
          "year": "2025-05",
          "abstract": "The rapid advancements of AI rely on the support of ICs. However, the growing\ncomplexity of digital ICs makes the traditional IC design process costly and\ntime-consuming. In recent years, AI-assisted IC design methods have\ndemonstrated great potential, but most methods are task-specific or focus\nsolely on the circuit structure in graph format, overlooking other circuit\nmodalities with rich functional information. In this paper, we introduce\nCircuitFusion, the first multimodal and implementation-aware circuit encoder.\nIt encodes circuits into general representations that support different\ndownstream circuit design tasks. To learn from circuits, we propose to fuse\nthree circuit modalities: hardware code, structural graph, and functionality\nsummary. More importantly, we identify four unique properties of circuits:\nparallel execution, functional equivalent transformation, multiple design\nstages, and circuit reusability. Based on these properties, we propose new\nstrategies for both the development and application of CircuitFusion: 1) During\ncircuit preprocessing, utilizing the parallel nature of circuits, we split each\ncircuit into multiple sub-circuits based on sequential-element boundaries, each\nsub-circuit in three modalities. 2) During CircuitFusion pre-training, we\nintroduce three self-supervised tasks that utilize equivalent transformations\nboth within and across modalities. 3) When applying CircuitFusion to downstream\ntasks, we propose a new retrieval-augmented inference method, which retrieves\nsimilar known circuits as a reference for predictions. It improves fine-tuning\nperformance and even enables zero-shot inference. Evaluated on five different\ncircuit design tasks, CircuitFusion consistently outperforms the SOTA\nsupervised method specifically developed for every single task, demonstrating\nits generalizability and ability to learn circuits' inherent properties.",
          "arxiv_id": "2505.02168v1"
        },
        {
          "title": "Domain Knowledge-Based Automated Analog Circuit Design with Deep Reinforcement Learning",
          "year": "2022-02",
          "abstract": "The design automation of analog circuits is a longstanding challenge in the\nintegrated circuit field. This paper presents a deep reinforcement learning\nmethod to expedite the design of analog circuits at the pre-layout stage, where\nthe goal is to find device parameters to fulfill desired circuit\nspecifications. Our approach is inspired by experienced human designers who\nrely on domain knowledge of analog circuit design (e.g., circuit topology and\ncouplings between circuit specifications) to tackle the problem. Unlike all\nprior methods, our method originally incorporates such key domain knowledge\ninto policy learning with a graph-based policy network, thereby best modeling\nthe relations between circuit parameters and design targets. Experimental\nresults on exemplary circuits show it achieves human-level design accuracy\n(~99%) with 1.5x efficiency of existing best-performing methods. Our method\nalso shows better generalization ability to unseen specifications and\noptimality in circuit performance optimization. Moreover, it applies to\ndesigning diverse analog circuits across different semiconductor technologies,\nbreaking the limitations of prior ad-hoc methods in designing one particular\ntype of analog circuits with conventional semiconductor technology.",
          "arxiv_id": "2202.13185v1"
        },
        {
          "title": "Domain Knowledge-Infused Deep Learning for Automated Analog/Radio-Frequency Circuit Parameter Optimization",
          "year": "2022-04",
          "abstract": "The design automation of analog circuits is a longstanding challenge. This\npaper presents a reinforcement learning method enhanced by graph learning to\nautomate the analog circuit parameter optimization at the pre-layout stage,\ni.e., finding device parameters to fulfill desired circuit specifications.\nUnlike all prior methods, our approach is inspired by human experts who rely on\ndomain knowledge of analog circuit design (e.g., circuit topology and couplings\nbetween circuit specifications) to tackle the problem. By originally\nincorporating such key domain knowledge into policy training with a multimodal\nnetwork, the method best learns the complex relations between circuit\nparameters and design targets, enabling optimal decisions in the optimization\nprocess. Experimental results on exemplary circuits show it achieves\nhuman-level design accuracy (99%) 1.5X efficiency of existing best-performing\nmethods. Our method also shows better generalization ability to unseen\nspecifications and optimality in circuit performance optimization. Moreover, it\napplies to design radio-frequency circuits on emerging semiconductor\ntechnologies, breaking the limitations of prior learning methods in designing\nconventional analog circuits.",
          "arxiv_id": "2204.12948v2"
        }
      ],
      "5": [
        {
          "title": "RTL++: Graph-enhanced LLM for RTL Code Generation",
          "year": "2025-05",
          "abstract": "As hardware design complexity escalates, there is an urgent need for advanced\nautomation in electronic design automation (EDA). Traditional register transfer\nlevel (RTL) design methods are manual, time-consuming, and prone to errors.\nWhile commercial (instruction-tuned) large language models (LLMs) shows\npromising performance for automation, they pose security and privacy concerns.\nOpen-source models offer alternatives; however, they frequently fall short in\nquality/correctness, largely due to limited, high-quality RTL code data\nessential for effective training and generalization. This paper proposes RTL++,\na first-of-its-kind LLM-assisted method for RTL code generation that utilizes\ngraph representations of code structures to enhance the quality of generated\ncode. By encoding RTL code into a textualized control flowgraphs (CFG) and data\nflow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and\nrelationships within the code. This structured graph-based approach enhances\nthe context available to LLMs, enabling them to better understand and generate\ninstructions. By focusing on data generation through graph representations,\nRTL++ addresses the limitations of previous approaches that rely solely on code\nand suffer from lack of diversity. Experimental results demonstrate that RTL++\noutperforms state-of-the-art models fine-tuned for RTL generation, as evaluated\nusing the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1\nmodel, which highlight the effectiveness of graph-enhanced context in advancing\nthe capabilities of LLM-assisted RTL code generation.",
          "arxiv_id": "2505.13479v1"
        },
        {
          "title": "AutoVCoder: A Systematic Framework for Automated Verilog Code Generation using LLMs",
          "year": "2024-07",
          "abstract": "Recently, the use of large language models (LLMs) for software code\ngeneration, e.g., C/C++ and Python, has proven a great success. However, LLMs\nstill suffer from low syntactic and functional correctness when it comes to the\ngeneration of register-transfer level (RTL) code, such as Verilog. To address\nthis issue, in this paper, we develop AutoVCoder, a systematic open-source\nframework that significantly improves the LLMs' correctness of generating\nVerilog code and enhances the quality of its output at the same time. Our\nframework integrates three novel techniques, including a high-quality hardware\ndataset generation approach, a two-round LLM fine-tuning method and a\ndomain-specific retrieval-augmented generation (RAG) mechanism. Experimental\nresults demonstrate that AutoVCoder outperforms both industrial and academic\nLLMs in Verilog code generation. Specifically, AutoVCoder shows a 0.5% and 2.2%\nimprovement in functional correctness on the EvalMachine and EvalHuman\nbenchmarks compared with BetterV, and also achieves a 3.4% increase in syntax\ncorrectness and a 3.4% increase in functional correctness on the RTLLM\nbenchmark compared with RTLCoder.",
          "arxiv_id": "2407.18333v1"
        },
        {
          "title": "MAGE: A Multi-Agent Engine for Automated RTL Code Generation",
          "year": "2024-12",
          "abstract": "The automatic generation of RTL code (e.g., Verilog) through natural language\ninstructions has emerged as a promising direction with the advancement of large\nlanguage models (LLMs). However, producing RTL code that is both syntactically\nand functionally correct remains a significant challenge. Existing\nsingle-LLM-agent approaches face substantial limitations because they must\nnavigate between various programming languages and handle intricate generation,\nverification, and modification tasks. To address these challenges, this paper\nintroduces MAGE, the first open-source multi-agent AI system designed for\nrobust and accurate Verilog RTL code generation. We propose a novel\nhigh-temperature RTL candidate sampling and debugging system that effectively\nexplores the space of code candidates and significantly improves the quality of\nthe candidates. Furthermore, we design a novel Verilog-state checkpoint\nchecking mechanism that enables early detection of functional errors and\ndelivers precise feedback for targeted fixes, significantly enhancing the\nfunctional correctness of the generated RTL code. MAGE achieves a 95.7% rate of\nsyntactic and functional correctness code generation on VerilogEval-Human 2\nbenchmark, surpassing the state-of-the-art Claude-3.5-sonnet by 23.3 %,\ndemonstrating a robust and reliable approach for AI-driven RTL design\nworkflows.",
          "arxiv_id": "2412.07822v1"
        }
      ],
      "6": [
        {
          "title": "QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum Circuits",
          "year": "2024-01",
          "abstract": "Parameterized Quantum Circuits (PQC) have obtained increasing popularity\nthanks to their great potential for near-term Noisy Intermediate-Scale Quantum\n(NISQ) computers. Achieving quantum advantages usually requires a large number\nof qubits and quantum circuits with enough capacity. However, limited coherence\ntime and massive quantum noises severely constrain the size of quantum circuits\nthat can be executed reliably on real machines. To address these two pain\npoints, we propose QuantumSEA, an in-time sparse exploration for noise-adaptive\nquantum circuits, aiming to achieve two key objectives: (1) implicit circuits\ncapacity during training - by dynamically exploring the circuit's sparse\nconnectivity and sticking a fixed small number of quantum gates throughout the\ntraining which satisfies the coherence time and enjoy light noises, enabling\nfeasible executions on real quantum devices; (2) noise robustness - by jointly\noptimizing the topology and parameters of quantum circuits under real device\nnoise models. In each update step of sparsity, we leverage the moving average\nof historical gradients to grow necessary gates and utilize salience-based\npruning to eliminate insignificant gates. Extensive experiments are conducted\nwith 7 Quantum Machine Learning (QML) and Variational Quantum Eigensolver (VQE)\nbenchmarks on 6 simulated or real quantum computers, where QuantumSEA\nconsistently surpasses noise-aware search, human-designed, and randomly\ngenerated quantum circuit baselines by a clear performance margin. For example,\neven in the most challenging on-chip training regime, our method establishes\nstate-of-the-art results with only half the number of quantum gates and ~2x\ntime saving of circuit executions. Codes are available at\nhttps://github.com/VITA-Group/QuantumSEA.",
          "arxiv_id": "2401.05571v1"
        },
        {
          "title": "A Uniform Quantum Computing Model Based on Virtual Quantum Processors",
          "year": "2023-02",
          "abstract": "Quantum Computers, one fully realized, can represent an exponential boost in\ncomputing power. However, the computational power of the current quantum\ncomputers, referred to as Noisy Internediate Scale Quantum, or NISQ, is\nseverely limited because of environmental and intrinsic noise, as well as the\nvery low connectivity between qubits compared to their total amount. We propose\na virtual quantum processor that emulates a generic hybrid quantum machine\nwhich can serve as a logical version of quantum computing hardware. This hybrid\nclassical quantum machine powers quantum-logical computations which are\nsubstitutable by future native quantum processors.",
          "arxiv_id": "2302.12750v1"
        },
        {
          "title": "Optimising Iteration Scheduling for Full-State Vector Simulation of Quantum Circuits on FPGAs",
          "year": "2024-11",
          "abstract": "As the field of quantum computing grows, novel algorithms which take\nadvantage of quantum phenomena need to be developed. As we are currently in the\nNISQ (noisy intermediate scale quantum) era, quantum algorithm researchers\ncannot reliably test their algorithms on real quantum hardware, which is still\ntoo limited. Instead, quantum computing simulators on classical computing\nsystems are used. In the quantum circuit model, quantum bits (qubits) are\noperated on by quantum gates. A quantum circuit is a sequence of such quantum\ngates operating on some number of qubits. A quantum gate applied to a qubit can\nbe controlled by other qubits in the circuit. This applies the gate only to the\nstates which satisfy the required control qubit state. We particularly target\nFPGAs as our main simulation platform, as these offer potential energy savings\nwhen compared to running simulations on CPUs/GPUs.\n  In this work, we present a memory access pattern to optimise the number of\niterations that need to be scheduled to execute a quantum gate such that only\nthe iterations which access the required pairs (determined according to the\ncontrol qubits imposed on the gate) are scheduled. We show that this approach\nresults in a significant reduction in the time required to simulate a gate for\neach added control qubit. We also show that this approach benefits the\nsimulation time on FPGAs more than CPUs and GPUs and allows to outperform both\nCPU and GPU platforms in terms of energy efficiency, which is the main factor\nfor scalability of the simulations.",
          "arxiv_id": "2411.18354v1"
        }
      ],
      "7": [
        {
          "title": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis",
          "year": "2022-05",
          "abstract": "Graph neural networks (GNNs) are among the most powerful tools in deep\nlearning. They routinely solve complex problems on unstructured networks, such\nas node classification, graph classification, or link prediction, with high\naccuracy. However, both inference and training of GNNs are complex, and they\nuniquely combine the features of irregular graph processing with dense and\nregular computations. This complexity makes it very challenging to execute GNNs\nefficiently on modern massively parallel architectures. To alleviate this, we\nfirst design a taxonomy of parallelism in GNNs, considering data and model\nparallelism, and different forms of pipelining. Then, we use this taxonomy to\ninvestigate the amount of parallelism in numerous GNN models, GNN-driven\nmachine learning tasks, software frameworks, or hardware accelerators. We use\nthe work-depth model, and we also assess communication volume and\nsynchronization. We specifically focus on the sparsity/density of the\nassociated tensors, in order to understand how to effectively apply techniques\nsuch as vectorization. We also formally analyze GNN pipelining, and we\ngeneralize the established Message-Passing class of GNN models to cover\narbitrary pipeline depths, facilitating future optimizations. Finally, we\ninvestigate different forms of asynchronicity, navigating the path for future\nasynchronous parallel GNN pipelines. The outcomes of our analysis are\nsynthesized in a set of insights that help to maximize GNN performance, and a\ncomprehensive list of challenges and opportunities for further research into\nefficient GNN computations. Our work will help to advance the design of future\nGNNs.",
          "arxiv_id": "2205.09702v7"
        },
        {
          "title": "Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses",
          "year": "2023-06",
          "abstract": "Graph Neural Networks (GNNs) are emerging as a powerful tool for learning\nfrom graph-structured data and performing sophisticated inference tasks in\nvarious application domains. Although GNNs have been shown to be effective on\nmodest-sized graphs, training them on large-scale graphs remains a significant\nchallenge due to lack of efficient data access and data movement methods.\nExisting frameworks for training GNNs use CPUs for graph sampling and feature\naggregation, while the training and updating of model weights are executed on\nGPUs. However, our in-depth profiling shows the CPUs cannot achieve the\nthroughput required to saturate GNN model training throughput, causing gross\nunder-utilization of expensive GPU resources. Furthermore, when the graph and\nits embeddings do not fit in the CPU memory, the overhead introduced by the\noperating system, say for handling page-faults, comes in the critical path of\nexecution.\n  To address these issues, we propose the GPU Initiated Direct Storage Access\n(GIDS) dataloader, to enable GPU-oriented GNN training for large-scale graphs\nwhile efficiently utilizing all hardware resources, such as CPU memory,\nstorage, and GPU memory with a hybrid data placement strategy. By enabling GPU\nthreads to fetch feature vectors directly from storage, GIDS dataloader solves\nthe memory capacity problem for GPU-oriented GNN training. Moreover, GIDS\ndataloader leverages GPU parallelism to tolerate storage latency and eliminates\nexpensive page-fault overhead. Doing so enables us to design novel\noptimizations for exploiting locality and increasing effective bandwidth for\nGNN training. Our evaluation using a single GPU on terabyte-scale GNN datasets\nshows that GIDS dataloader accelerates the overall DGL GNN training pipeline by\nup to 392X when compared to the current, state-of-the-art DGL dataloader.",
          "arxiv_id": "2306.16384v2"
        },
        {
          "title": "GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching",
          "year": "2021-05",
          "abstract": "Graph neural networks (GNN) analysis engines are vital for real-world\nproblems that use large graph models. Challenges for a GNN hardware platform\ninclude the ability to (a) host a variety of GNNs, (b) handle high sparsity in\ninput vertex feature vectors and the graph adjacency matrix and the\naccompanying random memory access patterns, and (c) maintain load-balanced\ncomputation in the face of uneven workloads, induced by high sparsity and\npower-law vertex degree distributions. This paper proposes GNNIE, an\naccelerator designed to run a broad range of GNNs. It tackles workload\nimbalance by (i)~splitting vertex feature operands into blocks, (ii)~reordering\nand redistributing computations, (iii)~using a novel flexible MAC architecture.\nIt adopts a graph-specific, degree-aware caching policy that is well suited to\nreal-world graph characteristics. The policy enhances on-chip data reuse and\navoids random memory access to DRAM.\n  GNNIE achieves average speedups of 21233x over a CPU and 699x over a GPU over\nmultiple datasets on graph attention networks (GATs), graph convolutional\nnetworks (GCNs), GraphSAGE, GINConv, and DiffPool. Compared to prior\napproaches, GNNIE achieves an average speedup of 35x over HyGCN (which cannot\nimplement GATs) for GCN, GraphSAGE, and GINConv, and, using 3.4x fewer\nprocessing units, an average speedup of 2.1x over AWB-GCN (which runs only\nGCNs).",
          "arxiv_id": "2105.10554v2"
        }
      ],
      "8": [
        {
          "title": "Evaluation of Posits for Spectral Analysis Using a Software-Defined Dataflow Architecture",
          "year": "2024-06",
          "abstract": "Spectral analysis plays an important role in detection of damage in\nstructures and deep learning. The choice of a floating-point format plays a\ncrucial role in determining the accuracy and performance of spectral analysis.\nThe IEEE Std 754\\textsuperscript{TM} floating-point format (IEEE~754 for short)\nis supported by most major hardware vendors for ``normal'' floats. However, it\nhas several limitations. Previous work has attempted to evaluate posit format\nwith respect to accuracy and performance. The accuracy of the posit has been\nestablished over IEEE~754 for a variety of applications. For example, our\nanalysis of the Fast Fourier Transform shows 2x better accuracy when using a\n32-bit posit vs. a 32-bit IEEE754 format. For spectral analysis, 32-bit posits\nare substantially more accurate than 32-bit IEEE~754 floats. Although posit has\nshown better accuracy than IEEE~754, a fair evaluation of posit with IEEE~754\nformat using a real hardware implementation has been lacking so far. A software\nsimulation of posit format on an x86 CPU is about $\\mathbf{69.3\\times}$ slower\nthan native IEEE~754 hardware for normal floats for a Fast Fourier Transform\n(FFT) of $\\mathbf{2^{28}}$ points. We propose the use of a software-defined\ndataflow architecture to evaluate performance and accuracy of posits in\nspectral analysis. Our dataflow architecture uses reconfigurable logical\nelements that express algorithms using only integer operations. Our\narchitecture does not have an FPU, and we express both IEEE~754 and posit\narithmetic using the same integer operations within the hardware. On our\ndataflow architecture, the posit format is only $\\mathbf{1.8\\times}$ slower\nthan IEEE~754 for a Fast Fourier Transform (FFT) of $\\mathbf{2^{28}\\approx\n268}$ million points. With this implementation, we empirically propose a new\nlower bound for the performance of posit compared to IEEE~754 format.",
          "arxiv_id": "2406.05398v1"
        },
        {
          "title": "CLARINET: A RISC-V Based Framework for Posit Arithmetic Empiricism",
          "year": "2020-05",
          "abstract": "Many engineering and scientific applications require high precision\narithmetic. IEEE~754-2008 compliant (floating-point) arithmetic is the de facto\nstandard for performing these computations. Recently, posit arithmetic has been\nproposed as a drop-in replacement for floating-point arithmetic. The\nposit\\texttrademark data representation and arithmetic claim several absolute\nadvantages over the floating-point format and arithmetic, including higher\ndynamic range, better accuracy, and superior performance-area trade-offs.\nHowever, there does not exist any accessible, holistic framework that\nfacilitates the validation of these claims of posit arithmetic, especially when\nthe claims involve long accumulations (quire).\n  In this paper, we present a consolidated general-purpose processor-based\nframework to support posit arithmetic empiricism. The end-users of the\nframework have the liberty to seamlessly experiment with their applications\nusing posit and floating-point arithmetic since the framework is designed for\nthe two number systems to coexist. Melodica is a posit arithmetic core that\nimplements parametric fused operations that uniquely involve the quire data\ntype. Clarinet is a Melodica-enabled processor based on the RISC-V ISA. To the\nbest of our knowledge, this is the first-ever integration of quire with a\nRISC-V core. To show the effectiveness of the Clarinet platform, we perform an\nextensive application study and benchmark some of the common linear algebra and\ncomputer vision kernels. We emulate Clarinet on a Xilinx FPGA and present\nutilization and timing data. Clarinet and Melodica remain actively under\ndevelopment and is available in open-source for posit arithmetic empiricism.",
          "arxiv_id": "2006.00364v4"
        },
        {
          "title": "Fixed-Posit: A Floating-Point Representation for Error-Resilient Applications",
          "year": "2021-04",
          "abstract": "Today, almost all computer systems use IEEE-754 floating point to represent\nreal numbers. Recently, posit was proposed as an alternative to IEEE-754\nfloating point as it has better accuracy and a larger dynamic range. The\nconfigurable nature of posit, with varying number of regime and exponent bits,\nhas acted as a deterrent to its adoption. To overcome this shortcoming, we\npropose fixed-posit representation where the number of regime and exponent bits\nare fixed, and present the design of a fixed-posit multiplier. We evaluate the\nfixed-posit multiplier on error-resilient applications of AxBench and OpenBLAS\nbenchmarks as well as neural networks. The proposed fixed-posit multiplier has\n47%, 38.5%, 22% savings for power, area and delay respectively when compared to\nposit multipliers and up to 70%, 66%, 26% savings in power, area and delay\nrespectively when compared to 32-bit IEEE-754 multiplier. These savings are\naccompanied with minimal output quality loss (1.2% average relative error)\nacross OpenBLAS and AxBench workloads. Further, for neural networks like\nResNet-18 on ImageNet we observe a negligible accuracy loss (0.12%) on using\nthe fixed-posit multiplier.",
          "arxiv_id": "2104.04763v1"
        }
      ],
      "9": [
        {
          "title": "ARK: Fully Homomorphic Encryption Accelerator with Runtime Data Generation and Inter-Operation Key Reuse",
          "year": "2022-05",
          "abstract": "Homomorphic Encryption (HE) is one of the most promising post-quantum\ncryptographic schemes that enable privacy-preserving computation on servers.\nHowever, noise accumulates as we perform operations on HE-encrypted data,\nrestricting the number of possible operations. Fully HE (FHE) removes this\nrestriction by introducing the bootstrapping operation, which refreshes the\ndata; however, FHE schemes are highly memory-bound. Bootstrapping, in\nparticular, requires loading GBs of evaluation keys and plaintexts from\noff-chip memory, which makes FHE acceleration fundamentally bottlenecked by the\noff-chip memory bandwidth.\n  In this paper, we propose ARK, an Accelerator for FHE with Runtime data\ngeneration and inter-operation Key reuse. ARK enables practical FHE workloads\nwith a novel algorithm-architecture co-design to accelerate bootstrapping. We\nfirst eliminate the off-chip memory bandwidth bottleneck through runtime data\ngeneration and inter-operation key reuse. This approach enables ARK to fully\nexploit on-chip memory by substantially reducing the size of the working set.\nOn top of such algorithmic enhancements, we build ARK microarchitecture that\nminimizes on-chip data movement through an efficient, alternating data\ndistribution policy based on the data access patterns and a streamlined\ndataflow organization of the tailored functional units -- including base\nconversion, number-theoretic transform, and automorphism units. Overall, our\nco-design effectively handles the heavy computation and data movement overheads\nof FHE, drastically reducing the cost of HE operations, including\nbootstrapping.",
          "arxiv_id": "2205.00922v3"
        },
        {
          "title": "BTS: An Accelerator for Bootstrappable Fully Homomorphic Encryption",
          "year": "2021-12",
          "abstract": "Homomorphic encryption (HE) enables the secure offloading of computations to\nthe cloud by providing computation on encrypted data (ciphertexts). HE is based\non noisy encryption schemes in which noise accumulates as more computations are\napplied to the data. The limited number of operations applicable to the data\nprevents practical applications from exploiting HE. Bootstrapping enables an\nunlimited number of operations or fully HE (FHE) by refreshing the ciphertext.\nUnfortunately, bootstrapping requires a significant amount of additional\ncomputation and memory bandwidth as well. Prior works have proposed hardware\naccelerators for computation primitives of FHE. However, to the best of our\nknowledge, this is the first to propose a hardware FHE accelerator that\nsupports bootstrapping as a first-class citizen.\n  In particular, we propose BTS - Bootstrappable, Technologydriven, Secure\naccelerator architecture for FHE. We identify the challenges of supporting\nbootstrapping in the accelerator and analyze the off-chip memory bandwidth and\ncomputation required. In particular, given the limitations of modern memory\ntechnology, we identify the HE parameter sets that are efficient for FHE\nacceleration. Based on the insights gained from our analysis, we propose BTS,\nwhich effectively exploits the parallelism innate in HE operations by arranging\na massive number of processing elements in a grid. We present the design and\nmicroarchitecture of BTS, including a network-on-chip design that exploits a\ndeterministic communication pattern. BTS shows 5,556x and 1,306x improved\nexecution time on ResNet-20 and logistic regression over a CPU, with a chip\narea of 373.6mm^2 and up to 163.2W of power.",
          "arxiv_id": "2112.15479v2"
        },
        {
          "title": "MemFHE: End-to-End Computing with Fully Homomorphic Encryption in Memory",
          "year": "2022-04",
          "abstract": "The increasing amount of data and the growing complexity of problems has\nresulted in an ever-growing reliance on cloud computing. However, many\napplications, most notably in healthcare, finance or defense, demand security\nand privacy which today's solutions cannot fully address. Fully homomorphic\nencryption (FHE) elevates the bar of today's solutions by adding\nconfidentiality of data during processing. It allows computation on fully\nencrypted data without the need for decryption, thus fully preserving privacy.\nTo enable processing encrypted data at usable levels of classic security, e.g.,\n128-bit, the encryption procedure introduces noticeable data size expansion -\nthe ciphertext is much bigger than the native aggregate of native data types.\nIn this paper, we present MemFHE which is the first accelerator of both client\nand server for the latest Ring-GSW (Gentry, Sahai, and Waters) based\nhomomorphic encryption schemes using Processing In Memory (PIM). PIM alleviates\nthe data movement issues with large FHE encrypted data, while providing in-situ\nexecution and extensive parallelism needed for FHE's polynomial operations.\nWhile the client-PIM can homomorphically encrypt and decrypt data, the\nserver-PIM can process homomorphically encrypted data without decryption.\nMemFHE's server-PIM is pipelined and is designed to provide flexible\nbootstrapping, allowing two encryption techniques and various FHE\nsecurity-levels based on the application requirements. We evaluate MemFHE for\nvarious security-levels and compare it with state-of-the-art CPU\nimplementations for Ring-GSW based FHE. MemFHE is up to 20kx (265x) faster than\nCPU (GPU) for FHE arithmetic operations and provides on average 2007x higher\nthroughput than the state-of-the-art while implementing neural networks with\nFHE.",
          "arxiv_id": "2204.12557v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:20:52Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}