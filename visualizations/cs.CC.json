{
  "topics": {
    "data": {
      "0": {
        "name": "0_graphs_graph_problem_time",
        "keywords": [
          [
            "graphs",
            0.03295463363482961
          ],
          [
            "graph",
            0.031208950196233175
          ],
          [
            "problem",
            0.030676218726093444
          ],
          [
            "time",
            0.02098342566483001
          ],
          [
            "NP",
            0.017705813042440545
          ],
          [
            "problems",
            0.016573413694941645
          ],
          [
            "vertex",
            0.01602535948141955
          ],
          [
            "algorithm",
            0.015421532864670092
          ],
          [
            "number",
            0.015106361251494787
          ],
          [
            "set",
            0.014919826048149676
          ]
        ],
        "count": 1147
      },
      "1": {
        "name": "1_quantum_classical_Quantum_state",
        "keywords": [
          [
            "quantum",
            0.07448983297612358
          ],
          [
            "classical",
            0.02679755151556073
          ],
          [
            "Quantum",
            0.019340241048998313
          ],
          [
            "state",
            0.01853045097577866
          ],
          [
            "states",
            0.018409331184744665
          ],
          [
            "complexity",
            0.01775301850946724
          ],
          [
            "algorithm",
            0.014848312933970353
          ],
          [
            "circuits",
            0.01482206869539325
          ],
          [
            "problem",
            0.013109494759571095
          ],
          [
            "circuit",
            0.012248962437652704
          ]
        ],
        "count": 694
      },
      "2": {
        "name": "2_complexity_NP_proof_automata",
        "keywords": [
          [
            "complexity",
            0.02000398956550705
          ],
          [
            "NP",
            0.018434431135193496
          ],
          [
            "proof",
            0.017847331815340935
          ],
          [
            "automata",
            0.01567015222569048
          ],
          [
            "problem",
            0.015074335848128133
          ],
          [
            "problems",
            0.014834265087352614
          ],
          [
            "time",
            0.014682198440969369
          ],
          [
            "complete",
            0.013544682713599009
          ],
          [
            "finite",
            0.013235976908151693
          ],
          [
            "paper",
            0.012564714048765328
          ]
        ],
        "count": 602
      },
      "3": {
        "name": "3_polynomial_rank_polynomials_tensor",
        "keywords": [
          [
            "polynomial",
            0.029324859015652648
          ],
          [
            "rank",
            0.027573082271833034
          ],
          [
            "polynomials",
            0.026274331465282404
          ],
          [
            "tensor",
            0.021606600375172556
          ],
          [
            "problem",
            0.01977312318671941
          ],
          [
            "tensors",
            0.01919451090994263
          ],
          [
            "complexity",
            0.017740914937509965
          ],
          [
            "algorithm",
            0.01748748160670302
          ],
          [
            "algebraic",
            0.017142557919867046
          ],
          [
            "groups",
            0.016860479632577065
          ]
        ],
        "count": 322
      },
      "4": {
        "name": "4_complexity_function_learning_communication",
        "keywords": [
          [
            "complexity",
            0.023778411109940417
          ],
          [
            "function",
            0.022480246147055925
          ],
          [
            "learning",
            0.02220997627693942
          ],
          [
            "communication",
            0.02176769326637622
          ],
          [
            "functions",
            0.01980709044139977
          ],
          [
            "log",
            0.019507787772176766
          ],
          [
            "distribution",
            0.01816850723055044
          ],
          [
            "lower",
            0.01689576775932158
          ],
          [
            "bound",
            0.01663098771307179
          ],
          [
            "communication complexity",
            0.012116978560806329
          ]
        ],
        "count": 196
      },
      "5": {
        "name": "5_neural_networks_neural networks_models",
        "keywords": [
          [
            "neural",
            0.037592244233989054
          ],
          [
            "networks",
            0.03218412182427276
          ],
          [
            "neural networks",
            0.025064242822737438
          ],
          [
            "models",
            0.02216171105225606
          ],
          [
            "attention",
            0.021275623971806792
          ],
          [
            "complexity",
            0.020789773437121795
          ],
          [
            "network",
            0.019918670137383146
          ],
          [
            "Neural",
            0.017581065629473903
          ],
          [
            "training",
            0.016873558292241735
          ],
          [
            "model",
            0.01499603372961587
          ]
        ],
        "count": 172
      },
      "6": {
        "name": "6_games_equilibrium_agents_problem",
        "keywords": [
          [
            "games",
            0.03164086396834411
          ],
          [
            "equilibrium",
            0.02510617580718805
          ],
          [
            "agents",
            0.024639350814246265
          ],
          [
            "problem",
            0.024555467783666206
          ],
          [
            "Nash",
            0.02452922823469469
          ],
          [
            "complexity",
            0.01976378935314312
          ],
          [
            "equilibria",
            0.018327060343158584
          ],
          [
            "PPAD",
            0.017735071438423188
          ],
          [
            "agent",
            0.017481372753194384
          ],
          [
            "game",
            0.017143798391780818
          ]
        ],
        "count": 165
      },
      "7": {
        "name": "7_complexity_causal_information_models",
        "keywords": [
          [
            "complexity",
            0.02781723917129465
          ],
          [
            "causal",
            0.022530651498507602
          ],
          [
            "information",
            0.02190232076859416
          ],
          [
            "models",
            0.02083459473427992
          ],
          [
            "explanations",
            0.01652493829852814
          ],
          [
            "computational",
            0.015426161306799534
          ],
          [
            "algorithmic",
            0.014538584023739766
          ],
          [
            "AI",
            0.014354739250442922
          ],
          [
            "theory",
            0.01335197243315712
          ],
          [
            "reasoning",
            0.013326833691098322
          ]
        ],
        "count": 102
      },
      "8": {
        "name": "8_optimization_convex_methods_order",
        "keywords": [
          [
            "optimization",
            0.044727221406215344
          ],
          [
            "convex",
            0.030111197451960624
          ],
          [
            "methods",
            0.026535488731364576
          ],
          [
            "order",
            0.024078378645266917
          ],
          [
            "method",
            0.022514760186515572
          ],
          [
            "complexity",
            0.018736238568980598
          ],
          [
            "problems",
            0.01807877980162742
          ],
          [
            "stochastic",
            0.017912135667743738
          ],
          [
            "algorithm",
            0.01745175039056554
          ],
          [
            "function",
            0.016855036983256945
          ]
        ],
        "count": 84
      },
      "9": {
        "name": "9_random_low_degree_low degree",
        "keywords": [
          [
            "random",
            0.026488719749617938
          ],
          [
            "low",
            0.02366058833689067
          ],
          [
            "degree",
            0.02364497753813503
          ],
          [
            "low degree",
            0.01948893945587125
          ],
          [
            "matrix",
            0.016738096832231555
          ],
          [
            "problem",
            0.016554114856711877
          ],
          [
            "bounds",
            0.016348998402441164
          ],
          [
            "algorithms",
            0.015409108242867243
          ],
          [
            "sparse",
            0.015397359400443743
          ],
          [
            "hardness",
            0.014381395079512143
          ]
        ],
        "count": 76
      },
      "10": {
        "name": "10_codes_code_list_decoding",
        "keywords": [
          [
            "codes",
            0.0861920270976002
          ],
          [
            "code",
            0.046433153653387564
          ],
          [
            "list",
            0.041871771909017544
          ],
          [
            "decoding",
            0.03322249776790525
          ],
          [
            "rate",
            0.031687358983826436
          ],
          [
            "distance",
            0.02483199750617156
          ],
          [
            "Codes",
            0.02082219689590427
          ],
          [
            "Reed",
            0.020437401440074596
          ],
          [
            "linear",
            0.01810810644112792
          ],
          [
            "LDCs",
            0.016703298631570235
          ]
        ],
        "count": 72
      }
    },
    "correlations": [
      [
        1.0,
        -0.6565208378083023,
        -0.18267012215254488,
        -0.4507723372587552,
        -0.390313971829114,
        -0.7048687121165842,
        0.0836664964879435,
        -0.370061469451482,
        -0.6406646009888953,
        -0.6307618740432334,
        -0.717942067486081
      ],
      [
        -0.6565208378083023,
        1.0,
        -0.6320246380972507,
        -0.6748417284761088,
        -0.6023793553501473,
        -0.7481335231652764,
        -0.6366549600736713,
        -0.633845536419117,
        -0.7146726129184868,
        -0.7031669126430877,
        -0.7279646183272565
      ],
      [
        -0.18267012215254488,
        -0.6320246380972507,
        1.0,
        -0.469078820245146,
        -0.22104307866649348,
        -0.6789801779873572,
        -0.2344938850372628,
        -0.23095331857817472,
        -0.6583239959682373,
        -0.6326610240230023,
        -0.7154599232923029
      ],
      [
        -0.4507723372587552,
        -0.6748417284761088,
        -0.469078820245146,
        1.0,
        -0.5664078189035042,
        -0.7333682482984704,
        -0.5481001076721476,
        -0.6061508352126209,
        -0.7061848120065678,
        -0.6322939003147496,
        -0.7292576024540064
      ],
      [
        -0.390313971829114,
        -0.6023793553501473,
        -0.22104307866649348,
        -0.5664078189035042,
        1.0,
        -0.6936482467207781,
        -0.3004277104480848,
        -0.2539163373130917,
        -0.6627586044579705,
        -0.46340895477313004,
        -0.7063777795951605
      ],
      [
        -0.7048687121165842,
        -0.7481335231652764,
        -0.6789801779873572,
        -0.7333682482984704,
        -0.6936482467207781,
        1.0,
        -0.703511220818412,
        -0.6682766487307585,
        -0.7056281162860554,
        -0.7279511774109917,
        -0.7580637244320021
      ],
      [
        0.0836664964879435,
        -0.6366549600736713,
        -0.2344938850372628,
        -0.5481001076721476,
        -0.3004277104480848,
        -0.703511220818412,
        1.0,
        -0.3043426610134472,
        -0.6450248221014655,
        -0.6347778445966425,
        -0.7232632694842921
      ],
      [
        -0.370061469451482,
        -0.633845536419117,
        -0.23095331857817472,
        -0.6061508352126209,
        -0.2539163373130917,
        -0.6682766487307585,
        -0.3043426610134472,
        1.0,
        -0.6782724811372443,
        -0.6601376448369196,
        -0.725924629847386
      ],
      [
        -0.6406646009888953,
        -0.7146726129184868,
        -0.6583239959682373,
        -0.7061848120065678,
        -0.6627586044579705,
        -0.7056281162860554,
        -0.6450248221014655,
        -0.6782724811372443,
        1.0,
        -0.6998649269117365,
        -0.7407516940625501
      ],
      [
        -0.6307618740432334,
        -0.7031669126430877,
        -0.6326610240230023,
        -0.6322939003147496,
        -0.46340895477313004,
        -0.7279511774109917,
        -0.6347778445966425,
        -0.6601376448369196,
        -0.6998649269117365,
        1.0,
        -0.711943708680497
      ],
      [
        -0.717942067486081,
        -0.7279646183272565,
        -0.7154599232923029,
        -0.7292576024540064,
        -0.7063777795951605,
        -0.7580637244320021,
        -0.7232632694842921,
        -0.725924629847386,
        -0.7407516940625501,
        -0.711943708680497,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        18,
        8,
        7,
        3,
        5,
        2,
        2,
        2,
        1,
        0,
        0
      ],
      "2020-02": [
        39,
        8,
        9,
        7,
        11,
        1,
        5,
        4,
        3,
        1,
        1
      ],
      "2020-03": [
        15,
        4,
        14,
        3,
        3,
        1,
        3,
        3,
        0,
        1,
        0
      ],
      "2020-04": [
        29,
        6,
        8,
        3,
        8,
        2,
        0,
        0,
        3,
        4,
        1
      ],
      "2020-05": [
        27,
        7,
        8,
        5,
        7,
        4,
        6,
        5,
        2,
        5,
        2
      ],
      "2020-06": [
        21,
        11,
        8,
        5,
        3,
        7,
        3,
        2,
        2,
        1,
        0
      ],
      "2020-07": [
        29,
        10,
        6,
        3,
        13,
        0,
        4,
        4,
        4,
        1,
        2
      ],
      "2020-08": [
        26,
        12,
        5,
        3,
        7,
        1,
        3,
        3,
        1,
        2,
        0
      ],
      "2020-09": [
        24,
        5,
        8,
        6,
        6,
        1,
        2,
        3,
        3,
        3,
        4
      ],
      "2020-10": [
        28,
        6,
        11,
        1,
        7,
        2,
        3,
        6,
        1,
        2,
        1
      ],
      "2020-11": [
        33,
        8,
        8,
        5,
        6,
        3,
        6,
        2,
        2,
        1,
        3
      ],
      "2020-12": [
        15,
        8,
        10,
        4,
        14,
        1,
        3,
        3,
        2,
        4,
        3
      ],
      "2021-01": [
        20,
        5,
        11,
        3,
        4,
        5,
        1,
        4,
        2,
        0,
        0
      ],
      "2021-02": [
        25,
        7,
        6,
        4,
        6,
        4,
        1,
        3,
        5,
        0,
        1
      ],
      "2021-03": [
        22,
        9,
        7,
        6,
        7,
        1,
        5,
        1,
        1,
        1,
        0
      ],
      "2021-04": [
        24,
        5,
        8,
        5,
        4,
        1,
        2,
        4,
        3,
        2,
        0
      ],
      "2021-05": [
        29,
        9,
        9,
        1,
        5,
        4,
        2,
        2,
        1,
        2,
        3
      ],
      "2021-06": [
        24,
        6,
        10,
        7,
        8,
        2,
        4,
        2,
        1,
        1,
        0
      ],
      "2021-07": [
        33,
        5,
        6,
        3,
        18,
        5,
        2,
        2,
        1,
        2,
        3
      ],
      "2021-08": [
        24,
        6,
        7,
        2,
        6,
        3,
        1,
        3,
        1,
        0,
        4
      ],
      "2021-09": [
        19,
        7,
        16,
        3,
        6,
        0,
        3,
        0,
        3,
        1,
        1
      ],
      "2021-10": [
        24,
        14,
        9,
        2,
        5,
        1,
        3,
        2,
        4,
        0,
        2
      ],
      "2021-11": [
        27,
        14,
        7,
        1,
        16,
        2,
        5,
        5,
        5,
        1,
        6
      ],
      "2021-12": [
        23,
        9,
        9,
        2,
        4,
        1,
        5,
        1,
        1,
        1,
        2
      ],
      "2022-01": [
        16,
        8,
        17,
        4,
        6,
        2,
        1,
        4,
        2,
        0,
        1
      ],
      "2022-02": [
        35,
        6,
        10,
        4,
        8,
        3,
        7,
        1,
        1,
        1,
        4
      ],
      "2022-03": [
        19,
        8,
        9,
        7,
        10,
        2,
        5,
        3,
        2,
        2,
        0
      ],
      "2022-04": [
        19,
        11,
        6,
        2,
        2,
        3,
        2,
        1,
        0,
        1,
        2
      ],
      "2022-05": [
        32,
        4,
        13,
        4,
        5,
        2,
        1,
        6,
        1,
        4,
        2
      ],
      "2022-06": [
        25,
        12,
        7,
        1,
        4,
        3,
        2,
        3,
        4,
        0,
        3
      ],
      "2022-07": [
        17,
        10,
        7,
        2,
        5,
        4,
        7,
        0,
        1,
        4,
        1
      ],
      "2022-08": [
        17,
        2,
        7,
        1,
        10,
        3,
        2,
        3,
        1,
        1,
        2
      ],
      "2022-09": [
        19,
        13,
        10,
        4,
        8,
        2,
        3,
        2,
        6,
        1,
        2
      ],
      "2022-10": [
        21,
        9,
        6,
        2,
        4,
        2,
        1,
        2,
        3,
        0,
        3
      ],
      "2022-11": [
        36,
        12,
        8,
        9,
        14,
        1,
        7,
        3,
        2,
        6,
        2
      ],
      "2022-12": [
        16,
        9,
        4,
        7,
        6,
        1,
        1,
        4,
        2,
        1,
        0
      ],
      "2023-01": [
        17,
        13,
        10,
        2,
        8,
        1,
        7,
        0,
        0,
        1,
        0
      ],
      "2023-02": [
        26,
        12,
        11,
        2,
        5,
        8,
        4,
        4,
        2,
        2,
        3
      ],
      "2023-03": [
        23,
        11,
        4,
        3,
        6,
        4,
        1,
        0,
        3,
        1,
        1
      ],
      "2023-04": [
        31,
        7,
        9,
        0,
        12,
        0,
        0,
        4,
        2,
        1,
        2
      ],
      "2023-05": [
        27,
        10,
        11,
        5,
        12,
        4,
        6,
        5,
        3,
        2,
        1
      ],
      "2023-06": [
        30,
        17,
        8,
        4,
        10,
        2,
        5,
        3,
        5,
        0,
        2
      ],
      "2023-07": [
        22,
        6,
        6,
        4,
        9,
        1,
        2,
        7,
        3,
        2,
        0
      ],
      "2023-08": [
        12,
        10,
        6,
        1,
        6,
        4,
        2,
        0,
        0,
        2,
        3
      ],
      "2023-09": [
        30,
        13,
        13,
        1,
        11,
        4,
        4,
        2,
        0,
        2,
        3
      ],
      "2023-10": [
        16,
        9,
        9,
        4,
        8,
        0,
        3,
        2,
        1,
        3,
        5
      ],
      "2023-11": [
        23,
        12,
        3,
        5,
        11,
        0,
        4,
        3,
        3,
        4,
        2
      ],
      "2023-12": [
        30,
        11,
        12,
        4,
        8,
        0,
        3,
        4,
        5,
        1,
        1
      ],
      "2024-01": [
        25,
        11,
        4,
        5,
        5,
        1,
        1,
        3,
        4,
        2,
        1
      ],
      "2024-02": [
        33,
        14,
        12,
        4,
        12,
        5,
        5,
        2,
        0,
        6,
        4
      ],
      "2024-03": [
        23,
        15,
        7,
        1,
        7,
        2,
        5,
        2,
        3,
        1,
        6
      ],
      "2024-04": [
        37,
        8,
        8,
        6,
        8,
        2,
        6,
        2,
        1,
        4,
        6
      ],
      "2024-05": [
        34,
        8,
        8,
        3,
        10,
        3,
        4,
        8,
        2,
        5,
        3
      ],
      "2024-06": [
        29,
        4,
        2,
        0,
        5,
        3,
        7,
        3,
        0,
        1,
        1
      ],
      "2024-07": [
        31,
        14,
        6,
        2,
        4,
        2,
        1,
        4,
        1,
        1,
        4
      ],
      "2024-08": [
        16,
        14,
        12,
        4,
        2,
        2,
        2,
        4,
        3,
        0,
        0
      ],
      "2024-09": [
        28,
        10,
        9,
        3,
        11,
        2,
        2,
        3,
        3,
        0,
        4
      ],
      "2024-10": [
        30,
        33,
        2,
        5,
        9,
        7,
        4,
        5,
        2,
        2,
        5
      ],
      "2024-11": [
        31,
        29,
        11,
        9,
        21,
        3,
        3,
        5,
        6,
        4,
        5
      ],
      "2024-12": [
        24,
        8,
        10,
        3,
        7,
        3,
        5,
        3,
        5,
        1,
        1
      ],
      "2025-01": [
        26,
        8,
        4,
        1,
        3,
        3,
        4,
        1,
        5,
        2,
        3
      ],
      "2025-02": [
        33,
        5,
        12,
        5,
        12,
        1,
        6,
        8,
        3,
        3,
        3
      ],
      "2025-03": [
        25,
        8,
        5,
        4,
        6,
        4,
        2,
        3,
        3,
        0,
        5
      ],
      "2025-04": [
        32,
        11,
        14,
        7,
        14,
        4,
        3,
        8,
        1,
        4,
        7
      ],
      "2025-05": [
        25,
        8,
        11,
        5,
        6,
        1,
        2,
        1,
        2,
        4,
        1
      ],
      "2025-06": [
        33,
        14,
        8,
        6,
        11,
        0,
        2,
        6,
        2,
        3,
        5
      ],
      "2025-07": [
        39,
        16,
        10,
        4,
        14,
        1,
        3,
        3,
        4,
        2,
        1
      ],
      "2025-08": [
        20,
        13,
        8,
        1,
        3,
        2,
        0,
        3,
        1,
        3,
        0
      ],
      "2025-09": [
        12,
        9,
        2,
        7,
        3,
        0,
        2,
        1,
        0,
        1,
        1
      ]
    },
    "papers": {
      "0": [
        {
          "title": "On the (In)Approximability of the Monitoring Edge Geodetic Set Problem",
          "year": "2025-07",
          "abstract": "We study the minimum \\emph{Monitoring Edge Geodetic Set} (\\megset) problem\nintroduced in [Foucaud et al., CALDAM'23]: given a graph $G$, we say that an\nedge is monitored by a pair $u,v$ of vertices if \\emph{all} shortest paths\nbetween $u$ and $v$ traverse $e$; the goal of the problem consists in finding a\nsubset $M$ of vertices of $G$ such that each edge of $G$ is monitored by at\nleast one pair of vertices in $M$, and $|M|$ is minimized.\n  In this paper, we prove that all polynomial-time approximation algorithms for\nthe minimum \\megset problem must have an approximation ratio of $\\Omega(\\log\nn)$, unless \\p = \\np. To the best of our knowledge, this is the first\nnon-constant inapproximability result known for this problem. We also\nstrengthen the known \\np-hardness of the problem on $2$-apex graphs by showing\nthat the same result holds for $1$-apex graphs. This leaves open the problem of\ndetermining whether the problem remains \\np-hard on planar (i.e., $0$-apex)\ngraphs.\n  On the positive side, we design an algorithm that computes good approximate\nsolutions for hereditary graph classes that admit efficiently computable\nbalanced separators of truly sublinear size. This immediately results in\npolynomial-time approximation algorithms achieving an approximation ratio of\n$O(n^{\\frac{1}{4}} \\sqrt{\\log n})$ on planar graphs, graphs with bounded genus,\nand $k$-apex graphs with $k=O(n^{\\frac{1}{4}})$. On graphs with bounded\ntreewidth, we obtain an approximation ratio of $O(\\log^{3/2} n)$ for any\nconstant $\\varepsilon > 0$. This compares favorably with the best-known\napproximation algorithm for general graphs, which achieves an approximation\nratio of $O(\\sqrt{n \\log n})$ via a simple reduction to the \\textsc{Set Cover}\nproblem.",
          "arxiv_id": "2507.00708v2"
        },
        {
          "title": "On the Parameterized Complexity of Odd Coloring",
          "year": "2025-03",
          "abstract": "A proper vertex coloring of a connected graph $G$ is called an odd coloring\nif, for every vertex $v$ in $G$, there exists a color that appears odd number\nof times in the open neighborhood of $v$. The minimum number of colors required\nto obtain an odd coloring of $G$ is called the \\emph{odd chromatic number} of\n$G$, denoted by $\\chi_{o}(G)$. Determining $\\chi_o(G)$ known to be ${\\sf\nNP}$-hard. Given a graph $G$ and an integer $k$, the \\odc{} problem is to\ndecide whether $\\chi_o(G)$ is at most $k$. In this paper, we study the\nparameterized complexity of the problem, particularly with respect to\nstructural graph parameters. We obtain the following results: \\begin{itemize}\n  \\item We prove that the problem admits a polynomial kernel when parameterized\nby the distance to clique.\n  \\item We show that the problem cannot have a polynomial kernel when\nparameterized by the vertex cover number unless ${\\sf NP} \\subseteq {\\sf Co\n{\\text -} NP/poly}$.\n  \\item We show that the problem is fixed-parameter tractable when\nparameterized by distance to cluster, distance to co-cluster, or neighborhood\ndiversity.\n  \\item We show that the problem is ${\\sf W[1]}$-hard parameterized by\nclique-width. \\end{itemize}\n  Finally, we study the complexity of the problem on restricted graph classes.\nWe show that it can be solved in polynomial time on cographs and split graphs\nbut remains NP-complete on certain subclasses of bipartite graphs.",
          "arxiv_id": "2503.05312v1"
        },
        {
          "title": "The Harmless Set Problem",
          "year": "2021-11",
          "abstract": "Given a graph $G = (V,E)$, a threshold function $t~ :~ V \\rightarrow\n\\mathbb{N}$ and an integer $k$, we study the Harmless Set problem, where the\ngoal is to find a subset of vertices $S \\subseteq V$ of size at least $k$ such\nthat every vertex $v\\in V$ has less than $t(v)$ neighbors in $S$. We enhance\nour understanding of the problem from the viewpoint of parameterized\ncomplexity. Our focus lies on parameters that measure the structural properties\nof the input instance. We show that the problem is W[1]-hard parameterized by a\nwide range of fairly restrictive structural parameters such as the feedback\nvertex set number, pathwidth, treedepth, and even the size of a minimum vertex\ndeletion set into graphs of pathwidth and treedepth at most three. On dense\ngraphs, we show that the problem is W[1]-hard parameterized by cluster vertex\ndeletion number. We also show that the Harmless Set problem with majority\nthresholds is W[1]-hard when parameterized by the treewidth of the input graph.\nWe prove that the Harmless Set problem can be solved in polynomial time on\ngraph with bounded cliquewidth. On the positive side, we obtain fixed-parameter\nalgorithms for the problem with respect to neighbourhood diversity, twin cover\nand vertex integrity of the input graph. We show that the problem parameterized\nby the solution size is fixed parameter tractable on planar graphs. We thereby\nresolve two open questions stated in C. Bazgan and M. Chopin (2014) concerning\nthe complexity of {\\sc Harmless Set} parameterized by the treewidth of the\ninput graph and on planar graphs with respect to the solution size.",
          "arxiv_id": "2111.06267v2"
        }
      ],
      "1": [
        {
          "title": "The power of shallow-depth Toffoli and qudit quantum circuits",
          "year": "2024-04",
          "abstract": "The relevance of shallow-depth quantum circuits has recently increased,\nmainly due to their applicability to near-term devices. In this context, one of\nthe main goals of quantum circuit complexity is to find problems that can be\nsolved by quantum shallow circuits but require more computational resources\nclassically.\n  Our first contribution in this work is to prove new separations between\nclassical and quantum constant-depth circuits. Firstly, we show a separation\nbetween constant-depth quantum circuits with quantum advice\n$\\mathsf{QNC}^0/\\mathsf{qpoly}$, and $\\mathsf{AC}^0[p]$, which is the class of\nclassical constant-depth circuits with unbounded-fan in and $\\pmod{p}$ gates.\nIn addition, we show a separation between $\\mathsf{QAC}^0$, which additionally\nhas Toffoli gates with unbounded control, and $\\mathsf{AC}^0[p]$. This\nestablishes the first such separation for a shallow-depth quantum class that\ndoes not involve quantum fan-out gates.\n  Secondly, we consider $\\mathsf{QNC}^0$ circuits with infinite-size gate sets.\nWe show that these circuits, along with (classical or quantum) prime modular\ngates, can implement threshold gates, showing that\n$\\mathsf{QNC}^0[p]=\\mathsf{QTC}^0$. Finally, we also show that in the\ninfinite-size gateset case, these quantum circuit classes for\nhigher-dimensional Hilbert spaces do not offer any advantage to standard qubit\nimplementations.",
          "arxiv_id": "2404.18104v1"
        },
        {
          "title": "Wasserstein Complexity of Quantum Circuits",
          "year": "2022-08",
          "abstract": "Given a unitary transformation, what is the size of the smallest quantum\ncircuit that implements it? This quantity, known as the quantum circuit\ncomplexity, is a fundamental property of quantum evolutions that has widespread\napplications in many fields, including quantum computation, quantum field\ntheory, and black hole physics. In this letter, we obtain a new lower bound for\nthe quantum circuit complexity in terms of a novel complexity measure that we\npropose for quantum circuits, which we call the quantum Wasserstein complexity.\nOur proposed measure is based on the quantum Wasserstein distance of order one\n(also called the quantum earth mover's distance), a metric on the space of\nquantum states. We also prove several fundamental and important properties of\nour new complexity measure, which stand to be of independent interest. Finally,\nwe show that our new measure also provides a lower bound for the experimental\ncost of implementing quantum circuits, which implies a quantum limit on\nconverting quantum resources to computational resources. Our results provide\nnovel applications of the quantum Wasserstein distance and pave the way for a\ndeeper understanding of the resources needed to implement a quantum\ncomputation.",
          "arxiv_id": "2208.06306v1"
        },
        {
          "title": "Quantum commitments and signatures without one-way functions",
          "year": "2021-12",
          "abstract": "In the classical world, the existence of commitments is equivalent to the\nexistence of one-way functions. In the quantum setting, on the other hand,\ncommitments are not known to imply one-way functions, but all known\nconstructions of quantum commitments use at least one-way functions. Are\none-way functions really necessary for commitments in the quantum world? In\nthis work, we show that non-interactive quantum commitments (for classical\nmessages) with computational hiding and statistical binding exist if\npseudorandom quantum states exist. Pseudorandom quantum states are sets of\nquantum states that are efficiently generated but their polynomially many\ncopies are computationally indistinguishable from the same number of copies of\nHaar random states [Ji, Liu, and Song, CRYPTO 2018]. It is known that\npseudorandom quantum states exist even if $\\BQP=\\QMA$ (relative to a quantum\noracle) [Kretschmer, TQC 2021], which means that pseudorandom quantum states\ncan exist even if no quantum-secure classical cryptographic primitive exists.\nOur result therefore shows that quantum commitments can exist even if no\nquantum-secure classical cryptographic primitive exists. In particular, quantum\ncommitments can exist even if no quantum-secure one-way function exists. In\nthis work, we also consider digital signatures, which are other fundamental\nprimitives in cryptography. We show that one-time secure digital signatures\nwith quantum public keys exist if pseudorandom quantum states exist. In the\nclassical setting, the existence of digital signatures is equivalent to the\nexistence of one-way functions. Our result, on the other hand, shows that\nquantum signatures can exist even if no quantum-secure classical cryptographic\nprimitive (including quantum-secure one-way functions) exists.",
          "arxiv_id": "2112.06369v3"
        }
      ],
      "2": [
        {
          "title": "Rice-like complexity lower bounds for Boolean and uniform automata networks",
          "year": "2024-09",
          "abstract": "Automata networks are a versatile model of finite discrete dynamical systems\ncomposed of interacting entities (the automata), able to embed any directed\ngraph as a dynamics on its space of configurations (the set of vertices,\nrepresenting all the assignments of a state to each entity). In this world,\nvirtually any question is decidable by a simple exhaustive search. We lever the\nRice-like complexity lower bound, stating that any non-trivial monadic second\norder logic question on the graph of its dynamics is NP-hard or coNP-hard\n(given the automata network description), to bounded alphabets (including the\nBoolean case). This restriction is particularly meaningful for applications to\n\"complex systems\", where each entity has a restricted set of possible states\n(its alphabet). For the non-deterministic case, trivial questions are solvable\nin constant time, hence there is a sharp gap in complexity for the algorithmic\nsolving of concrete problems on them. For the non-deterministic case,\nnon-triviality is defined at bounded treewidth, which offers a structure to\nestablish metatheorems of complexity lower bounds.",
          "arxiv_id": "2409.08762v1"
        },
        {
          "title": "On a Class of Constrained Synchronization Problems in NP",
          "year": "2020-06",
          "abstract": "The class of known constraint automata for which the constrained\nsynchronization problem is in NP all admit a special form. In this work, we\ntake a closer look at them. We characterize a wider class of constraint\nautomata that give constrained synchronization problems in NP, which\nencompasses all known problems in NP. We call these automata polycyclic\nautomata. The corresponding language class of polycyclic languages is\nintroduced. We show various characterizations and closure properties for this\nnew language class. We then give a criterion for NP-completeness and a\ncriterion for polynomial time solvability for polycyclic constraint languages.",
          "arxiv_id": "2006.01903v2"
        },
        {
          "title": "Metamathematics of Resolution Lower Bounds: A TFNP Perspective",
          "year": "2024-11",
          "abstract": "This paper studies the *refuter* problems, a family of decision-tree\n$\\mathsf{TFNP}$ problems capturing the metamathematical difficulty of proving\nproof complexity lower bounds. Suppose $\\varphi$ is a hard tautology that does\nnot admit any length-$s$ proof in some proof system $P$. In the corresponding\nrefuter problem, we are given (query access to) a purported length-$s$ proof\n$\\pi$ in $P$ that claims to have proved $\\varphi$, and our goal is to find an\ninvalid derivation inside $\\pi$. As suggested by witnessing theorems in bounded\narithmetic, the *computational complexity* of these refuter problems is closely\ntied to the *metamathematics* of the underlying proof complexity lower bounds.\n  We focus on refuter problems corresponding to lower bounds for *resolution*,\nwhich is arguably the single most studied system in proof complexity. We\nintroduce a new class $\\mathrm{rwPHP}(\\mathsf{PLS})$ in decision-tree\n$\\mathsf{TFNP}$, which can be seen as a randomized version of $\\mathsf{PLS}$,\nand argue that this class effectively captures the metamathematics of proving\nresolution lower bounds.\n  We view these results as a contribution to the *bounded reverse mathematics*\nof complexity lower bounds: when interpreted in relativized bounded arithmetic,\nour results show that the theory $\\mathsf{T}^1_2(\\alpha) +\n\\mathrm{dwPHP}(\\mathsf{PV}(\\alpha))$ characterizes the \"reasoning power\"\nrequired to prove (the \"easiest\") resolution lower bounds. An intriguing\ncorollary of our results is that the combinatorial principle, \"the pigeonhole\nprinciple requires exponential-size resolution proofs\", captures the class of\n$\\mathsf{TFNP}$ problems whose totality is provable in $\\mathsf{T}^1_2 +\n\\mathrm{dwPHP}(\\mathsf{PV})$.",
          "arxiv_id": "2411.15515v1"
        }
      ],
      "3": [
        {
          "title": "On Identity Testing and Noncommutative Rank Computation over the Free Skew Field",
          "year": "2022-09",
          "abstract": "The identity testing of rational formulas (RIT) in the free skew field\nefficiently reduces to computing the rank of a matrix whose entries are linear\npolynomials in noncommuting variables\\cite{HW15}. This rank computation problem\nhas deterministic polynomial-time white-box algorithms \\cite{GGOW16, IQS18} and\na randomized polynomial-time algorithm in the black-box setting \\cite{DM17}. In\nthis paper, we propose a new approach for efficient derandomization of\n\\emph{black-box} RIT. Additionally, we obtain results for matrix rank\ncomputation over the free skew field, and construct efficient linear pencil\nrepresentations for a new class of rational expressions. More precisely, we\nshow the following results:\n  1. Under the hardness assumption that the ABP (algebraic branching program)\ncomplexity of every polynomial identity for the $k\\times k$ matrix algebra is\n$2^{\\Omega(k)}$ \\cite{BW05}, we obtain a subexponential-time black-box\nalgorithm for RIT in almost general setting. This can be seen as the first\n\"hardness implies derandomization\" type theorem for rational formulas.\n  2. We show that the noncommutative rank of any matrix over the free skew\nfield whose entries have small linear pencil representations can be computed in\ndeterministic polynomial time. Prior to this, an efficient rank computation was\nonly known for matrices with noncommutative formulas as entries\\cite{GGOW20}.\nAs special cases of our algorithm, we obtain the first deterministic\npolynomial-time algorithms for rank computation of matrices whose entries are\nnoncommutative ABPs or rational formulas.\n  3. Motivated by the definition given by Bergman\\cite{Ber76}, we define a new\nclass that contains noncommutative ABPs and rational formulas. We obtain a\npolynomial-size linear pencil representation for this class. As a by-product,\nwe obtain a white-box deterministic polynomial-time identity testing algorithm\nfor the class.",
          "arxiv_id": "2209.04797v1"
        },
        {
          "title": "Asymptotic tensor rank is characterized by polynomials",
          "year": "2024-11",
          "abstract": "Asymptotic tensor rank is notoriously difficult to determine. Indeed,\ndetermining its value for the $2\\times 2$ matrix multiplication tensor would\ndetermine the matrix multiplication exponent, a long-standing open problem. On\nthe other hand, Strassen's asymptotic rank conjecture makes the bold claim that\nasymptotic tensor rank equals the largest dimension of the tensor and is thus\nas easy to compute as matrix rank. Despite tremendous interest, much is still\nunknown about the structural and computational properties of asymptotic rank;\nfor instance whether it is computable.\n  We prove that asymptotic tensor rank is \"computable from above\", that is, for\nany real number $r$ there is an (efficient) algorithm that determines, given a\ntensor $T$, if the asymptotic tensor rank of $T$ is at most $r$. The algorithm\nhas a simple structure; it consists of evaluating a finite list of polynomials\non the tensor. Indeed, we prove that the sublevel sets of asymptotic rank are\nZariski-closed (just like matrix rank). While we do not exhibit these\npolynomials explicitly, their mere existence has strong implications on the\nstructure of asymptotic rank.\n  As one such implication, we find that the values that asymptotic tensor rank\ntakes, on all tensors, is a well-ordered set. In other words, any\nnon-increasing sequence of asymptotic ranks stabilizes (\"discreteness from\nabove\"). In particular, for the matrix multiplication exponent (which is an\nasymptotic rank) there is no sequence of exponents of bilinear maps that\napproximates it arbitrarily closely from above without being eventually\nconstant. In other words, any upper bound on the matrix multiplication exponent\nthat is close enough, will \"snap\" to it. Previously such discreteness results\nwere only known for finite fields or for other tensor parameters (e.g.,\nasymptotic slice rank). We obtain them for infinite fields like the complex\nnumbers.",
          "arxiv_id": "2411.15789v1"
        },
        {
          "title": "Reconstruction Algorithms for Low-Rank Tensors and Depth-3 Multilinear Circuits",
          "year": "2021-05",
          "abstract": "We give new and efficient black-box reconstruction algorithms for some\nclasses of depth-$3$ arithmetic circuits. As a consequence, we obtain the first\nefficient algorithm for computing the tensor rank and for finding the optimal\ntensor decomposition as a sum of rank-one tensors when then input is a\nconstant-rank tensor. More specifically, we provide efficient learning\nalgorithms that run in randomized polynomial time over general fields and in\ndeterministic polynomial time over the reals and the complex numbers for the\nfollowing classes:\n  (1) Set-multilinear depth-$3$ circuits of constant top fan-in\n$\\Sigma\\Pi\\Sigma\\{\\sqcup_j X_j\\}(k)$ circuits). As a consequence of our\nalgorithm, we obtain the first polynomial time algorithm for tensor rank\ncomputation and optimal tensor decomposition of constant-rank tensors. This\nresult holds for $d$ dimensional tensors for any $d$, but is interesting even\nfor $d=3$.\n  (2) Sums of powers of constantly many linear forms ($\\Sigma\\wedge\\Sigma$\ncircuits). As a consequence we obtain the first polynomial-time algorithm for\ntensor rank computation and optimal tensor decomposition of constant-rank\nsymmetric tensors.\n  (3) Multilinear depth-3 circuits of constant top fan-in (multilinear\n$\\Sigma\\Pi\\Sigma(k)$ circuits). Our algorithm works over all fields of\ncharacteristic 0 or large enough characteristic. Prior to our work the only\nefficient algorithms known were over polynomially-sized finite fields (see.\nKarnin-Shpilka 09').\n  Prior to our work, the only polynomial-time or even subexponential-time\nalgorithms known (deterministic or randomized) for subclasses of\n$\\Sigma\\Pi\\Sigma(k)$ circuits that also work over large/infinite fields were\nfor the setting when the top fan-in $k$ is at most $2$ (see Sinha 16' and Sinha\n20').",
          "arxiv_id": "2105.01751v1"
        }
      ],
      "4": [
        {
          "title": "Fine-Grained Distribution-Dependent Learning Curves",
          "year": "2022-08",
          "abstract": "Learning curves plot the expected error of a learning algorithm as a function\nof the number of labeled samples it receives from a target distribution. They\nare widely used as a measure of an algorithm's performance, but classic PAC\nlearning theory cannot explain their behavior.\n  As observed by Antos and Lugosi (1996 , 1998), the classic `No Free Lunch'\nlower bounds only trace the upper envelope above all learning curves of\nspecific target distributions. For a concept class with VC dimension $d$ the\nclassic bound decays like $d/n$, yet it is possible that the learning curve for\n\\emph{every} specific distribution decays exponentially. In this case, for each\n$n$ there exists a different `hard' distribution requiring $d/n$ samples. Antos\nand Lugosi asked which concept classes admit a `strong minimax lower bound' --\na lower bound of $d'/n$ that holds for a fixed distribution for infinitely many\n$n$.\n  We solve this problem in a principled manner, by introducing a combinatorial\ndimension called VCL that characterizes the best $d'$ for which $d'/n$ is a\nstrong minimax lower bound. Our characterization strengthens the lower bounds\nof Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021), and it refines\ntheir theory of learning curves, by showing that for classes with finite VCL\nthe learning rate can be decomposed into a linear component that depends only\non the hypothesis class and an exponential component that depends also on the\ntarget distribution. As a corollary, we recover the lower bound of Antos and\nLugosi (1996 , 1998) for half-spaces in $\\mathbb{R}^d$.\n  Finally, to provide another viewpoint on our work and how it compares to\ntraditional PAC learning bounds, we also present an alternative formulation of\nour results in a language that is closer to the PAC setting.",
          "arxiv_id": "2208.14615v2"
        },
        {
          "title": "On (Simple) Decision Tree Rank",
          "year": "2022-09",
          "abstract": "In the decision tree computation model for Boolean functions, the depth\ncorresponds to query complexity, and size corresponds to storage space. The\ndepth measure is the most well-studied one, and is known to be polynomially\nrelated to several non-computational complexity measures of functions such as\ncertificate complexity. The size measure is also studied, but to a lesser\nextent. Another decision tree measure that has received very little attention\nis the minimal rank of the decision tree, first introduced by Ehrenfeucht and\nHaussler in 1989. This measure is closely related to the logarithm of the size,\nbut is not polynomially related to depth, and hence it can reveal additional\ninformation about the complexity of a function. It is characterised by the\nvalue of a Prover-Delayer game first proposed by Pudl\\'ak and Impagliazzo in\nthe context of tree-like resolution proofs. In this paper we study this measure\nfurther. We obtain an upper bound on depth in terms of rank and Fourier\nsparsity. We obtain upper and lower bounds on rank in terms of (variants of)\ncertificate complexity. We also obtain upper and lower bounds on the rank for\ncomposed functions in terms of the depth of the outer function and the rank of\nthe inner function. This allow us to easily recover known asympotical lower\nbounds on logarithm of the size for Iterated AND-OR and Iterated 3-bit\nMajority. We compute the rank exactly for several natural functions and use\nthem to show that all the bounds we have obtained are tight. We also show that\nrank in the simple decision tree model can be used to bound query complexity,\nor depth, in the more general conjunctive decision tree model. Finally, we\nimprove upon the known size lower bound for the Tribes function and conclude\nthat in the size-rank relationship for decision trees, obtained by Ehrenfeucht\nand Haussler, the upper bound for Tribes is asymptotically tight.",
          "arxiv_id": "2209.12877v1"
        },
        {
          "title": "Decision Tree Complexity versus Block Sensitivity and Degree",
          "year": "2022-09",
          "abstract": "Relations between the decision tree complexity and various other complexity\nmeasures of Boolean functions is a thriving topic of research in computational\ncomplexity. It is known that decision tree complexity is bounded above by the\ncube of block sensitivity, and the cube of polynomial degree. However, the\nwidest separation between decision tree complexity and each of block\nsensitivity and degree that is witnessed by known Boolean functions is\nquadratic. In this work, we investigate the tightness of the existing cubic\nupper bounds.\n  We improve the cubic upper bounds for many interesting classes of Boolean\nfunctions. We show that for graph properties and for functions with a constant\nnumber of alternations, both of the cubic upper bounds can be improved to\nquadratic. We define a class of Boolean functions, which we call the zebra\nfunctions, that comprises Boolean functions where each monotone path from 0^n\nto 1^n has an equal number of alternations. This class contains the symmetric\nand monotone functions as its subclasses. We show that for any zebra function,\ndecision tree complexity is at most the square of block sensitivity, and\ncertificate complexity is at most the square of degree.\n  Finally, we show using a lifting theorem of communication complexity by\nG{\\\"{o}}{\\\"{o}}s, Pitassi and Watson that the task of proving an improved upper\nbound on the decision tree complexity for all functions is in a sense\nequivalent to the potentially easier task of proving a similar upper bound on\ncommunication complexity for each bi-partition of the input variables, for all\nfunctions. In particular, this implies that to bound the decision tree\ncomplexity it suffices to bound smaller measures like parity decision tree\ncomplexity, subcube decision tree complexity and decision tree rank, that are\ndefined in terms of models that can be efficiently simulated by communication\nprotocols.",
          "arxiv_id": "2209.08042v1"
        }
      ],
      "5": [
        {
          "title": "Neural Spectrahedra and Semidefinite Lifts: Global Convex Optimization of Polynomial Activation Neural Networks in Fully Polynomial-Time",
          "year": "2021-01",
          "abstract": "The training of two-layer neural networks with nonlinear activation functions\nis an important non-convex optimization problem with numerous applications and\npromising performance in layerwise deep learning. In this paper, we develop\nexact convex optimization formulations for two-layer neural networks with\nsecond degree polynomial activations based on semidefinite programming.\nRemarkably, we show that semidefinite lifting is always exact and therefore\ncomputational complexity for global optimization is polynomial in the input\ndimension and sample size for all input data. The developed convex formulations\nare proven to achieve the same global optimal solution set as their non-convex\ncounterparts. More specifically, the globally optimal two-layer neural network\nwith polynomial activations can be found by solving a semidefinite program\n(SDP) and decomposing the solution using a procedure we call Neural\nDecomposition. Moreover, the choice of regularizers plays a crucial role in the\ncomputational tractability of neural network training. We show that the\nstandard weight decay regularization formulation is NP-hard, whereas other\nsimple convex penalties render the problem tractable in polynomial time via\nconvex programming. We extend the results beyond the fully connected\narchitecture to different neural network architectures including networks with\nvector outputs and convolutional architectures with pooling. We provide\nextensive numerical simulations showing that the standard backpropagation\napproach often fails to achieve the global optimum of the training loss. The\nproposed approach is significantly faster to obtain better test accuracy\ncompared to the standard backpropagation procedure.",
          "arxiv_id": "2101.02429v1"
        },
        {
          "title": "Information contraction in noisy binary neural networks and its implications",
          "year": "2021-01",
          "abstract": "Neural networks have gained importance as the machine learning models that\nachieve state-of-the-art performance on large-scale image classification,\nobject detection and natural language processing tasks. In this paper, we\nconsider noisy binary neural networks, where each neuron has a non-zero\nprobability of producing an incorrect output. These noisy models may arise from\nbiological, physical and electronic contexts and constitute an important class\nof models that are relevant to the physical world. Intuitively, the number of\nneurons in such systems has to grow to compensate for the noise while\nmaintaining the same level of expressive power and computation reliability. Our\nkey finding is a lower bound for the required number of neurons in noisy neural\nnetworks, which is first of its kind. To prove this lower bound, we take an\ninformation theoretic approach and obtain a novel strong data processing\ninequality (SDPI), which not only generalizes the Evans-Schulman results for\nbinary symmetric channels to general channels, but also improves the tightness\ndrastically when applied to estimate end-to-end information contraction in\nnetworks. Our SDPI can be applied to various information processing systems,\nincluding neural networks and cellular automata. Applying the SDPI in noisy\nbinary neural networks, we obtain our key lower bound and investigate its\nimplications on network depth-width trade-offs, our results suggest a\ndepth-width trade-off for noisy neural networks that is very different from the\nestablished understanding regarding noiseless neural networks. Furthermore, we\napply the SDPI to study fault-tolerant cellular automata and obtain bounds on\nthe error correction overheads and the relaxation time. This paper offers new\nunderstanding of noisy information processing systems through the lens of\ninformation theory.",
          "arxiv_id": "2101.11750v2"
        },
        {
          "title": "Descriptive complexity for neural networks via Boolean networks",
          "year": "2023-08",
          "abstract": "We investigate the expressive power of neural networks from the point of view\nof descriptive complexity. We study neural networks that use floating-point\nnumbers and piecewise polynomial activation functions from two perspectives: 1)\nthe general scenario where neural networks run for an unlimited number of\nrounds and have unrestricted topologies, and 2) classical feedforward neural\nnetworks that have the topology of layered acyclic graphs and run for only a\nconstant number of rounds. We characterize these neural networks via Boolean\nnetworks formalized via a recursive rule-based logic. In particular, we show\nthat the sizes of the neural networks and the corresponding Boolean rule\nformulae are polynomially related. In fact, in the direction from Boolean rules\nto neural networks, the blow-up is only linear. Our translations result in a\ntime delay, which is the number of rounds that it takes to simulate a single\ncomputation step. In the translation from neural networks to Boolean rules, the\ntime delay of the resulting formula is polylogarithmic in the size of the\nneural network. In the converse translation, the time delay of the neural\nnetwork is linear in the formula size. Ultimately, we obtain translations\nbetween neural networks, Boolean networks, the diamond-free fragment of modal\nsubstitution calculus, and a class of recursive Boolean circuits. Our\ntranslations offer a method, for almost any activation function F, of\ntranslating any neural network in our setting into an equivalent neural network\nthat uses F at each node. This even includes linear activation functions, which\nis possible due to using floats rather than actual reals!",
          "arxiv_id": "2308.06277v4"
        }
      ],
      "6": [
        {
          "title": "Reducing the complexity of computing the values of a Nash equilibrium",
          "year": "2025-07",
          "abstract": "The Colonel Blotto game, formulated by Emile Borel, involves players\nallocating limited resources to multiple battlefields simultaneously, with the\nwinner being the one who allocates more resources to each battlefield.\nComputation of the Nash equilibrium, including of two person, zero sum, mixed\nstrategy Colonel Blotto games have encountered issues of scalability and\ncomplexity owing to their PPAD completeness. This paper proposes an algorithm\nthat computes the same value as the Nash equilibrium but cannot be\ncharacterized by the Fixed point Theorems of Tarski, Kakutani and Brouwer. The\nreduced complexity of the proposed algorithm is based on dispensing with the\nneed for computing both players Nash strategies in Colonel Blotto games. The\nsame algorithm can, therefore, be extended to all two person, zero sum games to\ncompute the value of the Nash equilibrium. The theoretical superiority of the\nproposed algorithm over both LP solvers and another method that computes the\nsame value of the game as its Nash equilibrium by a random assignment of\nprobabilities to the active strategy set of the defending player, is also\nproposed.",
          "arxiv_id": "2507.22819v1"
        },
        {
          "title": "On the Computational Complexity of Decision Problems about Multi-Player Nash Equilibria",
          "year": "2020-01",
          "abstract": "We study the computational complexity of decision problems about Nash\nequilibria in $m$-player games. Several such problems have recently been shown\nto be computationally equivalent to the decision problem for the existential\ntheory of the reals, or stated in terms of complexity classes,\n$\\exists\\mathbb{R}$-complete, when $m\\geq 3$. We show that, unless they turn\ninto trivial problems, they are $\\exists\\mathbb{R}$-hard even for 3-player\nzero-sum games.\n  We also obtain new results about several other decision problems. We show\nthat when $m\\geq 3$ the problems of deciding if a game has a Pareto optimal\nNash equilibrium or deciding if a game has a strong Nash equilibrium are\n$\\exists\\mathbb{R}$-complete. The latter result rectifies a previous claim of\nNP-completeness in the literature. We show that deciding if a game has an\nirrational valued Nash equilibrium is $\\exists\\mathbb{R}$-hard, answering a\nquestion of Bil\\`o and Mavronicolas, and address also the computational\ncomplexity of deciding if a game has a rational valued Nash equilibrium. These\nresults also hold for 3-player zero-sum games.\n  Our proof methodology applies to corresponding decision problems about\nsymmetric Nash equilibria in symmetric games as well, and in particular our new\nresults carry over to the symmetric setting. Finally we show that deciding\nwhether a symmetric $m$-player games has a non-symmetric Nash equilibrium is\n$\\exists\\mathbb{R}$-complete when $m\\geq 3$, answering a question of Garg,\nMehta, Vazirani, and Yazdanbod.",
          "arxiv_id": "2001.05196v1"
        },
        {
          "title": "Simultaneous Contests with Equal Sharing Allocation of Prizes: Computational Complexity and Price of Anarchy",
          "year": "2022-07",
          "abstract": "We study a general scenario of simultaneous contests that allocate prizes\nbased on equal sharing: each contest awards its prize to all players who\nsatisfy some contest-specific criterion, and the value of this prize to a\nwinner decreases as the number of winners increases. The players produce\noutputs for a set of activities, and the winning criteria of the contests are\nbased on these outputs. We consider two variations of the model: (i) players\nhave costs for producing outputs; (ii) players do not have costs but have\ngeneralized budget constraints. We observe that these games are exact potential\ngames and hence always have a pure-strategy Nash equilibrium. The price of\nanarchy is $2$ for the budget model, but can be unbounded for the cost model.\nOur main results are for the computational complexity of these games. We prove\nthat for general versions of the model exactly or approximately computing a\nbest response is NP-hard. For natural restricted versions where best response\nis easy to compute, we show that finding a pure-strategy Nash equilibrium is\nPLS-complete, and finding a mixed-strategy Nash equilibrium is\n(PPAD$\\cap$PLS)-complete. On the other hand, an approximate pure-strategy Nash\nequilibrium can be found in pseudo-polynomial time. These games are a strict\nbut natural subclass of explicit congestion games, but they still have the same\nequilibrium hardness results.",
          "arxiv_id": "2207.08151v1"
        }
      ],
      "7": [
        {
          "title": "On the Computational Complexity of Ethics: Moral Tractability for Minds and Machines",
          "year": "2023-02",
          "abstract": "Why should moral philosophers, moral psychologists, and machine ethicists\ncare about computational complexity? Debates on whether artificial intelligence\n(AI) can or should be used to solve problems in ethical domains have mainly\nbeen driven by what AI can or cannot do in terms of human capacities. In this\npaper, we tackle the problem from the other end by exploring what kind of moral\nmachines are possible based on what computational systems can or cannot do. To\ndo so, we analyze normative ethics through the lens of computational\ncomplexity. First, we introduce computational complexity for the uninitiated\nreader and discuss how the complexity of ethical problems can be framed within\nMarr's three levels of analysis. We then study a range of ethical problems\nbased on consequentialism, deontology, and virtue ethics, with the aim of\nelucidating the complexity associated with the problems themselves (e.g., due\nto combinatorics, uncertainty, strategic dynamics), the computational methods\nemployed (e.g., probability, logic, learning), and the available resources\n(e.g., time, knowledge, learning). The results indicate that most problems the\nnormative frameworks pose lead to tractability issues in every category\nanalyzed. Our investigation also provides several insights about the\ncomputational nature of normative ethics, including the differences between\nrule- and outcome-based moral strategies, and the implementation-variance with\nregard to moral resources. We then discuss the consequences complexity results\nhave for the prospect of moral machines in virtue of the trade-off between\noptimality and efficiency. Finally, we elucidate how computational complexity\ncan be used to inform both philosophical and cognitive-psychological research\non human morality by advancing the Moral Tractability Thesis (MTT).",
          "arxiv_id": "2302.04218v1"
        },
        {
          "title": "Hard to Explain: On the Computational Hardness of In-Distribution Model Interpretation",
          "year": "2024-08",
          "abstract": "The ability to interpret Machine Learning (ML) models is becoming\nincreasingly essential. However, despite significant progress in the field,\nthere remains a lack of rigorous characterization regarding the innate\ninterpretability of different models. In an attempt to bridge this gap, recent\nwork has demonstrated that it is possible to formally assess interpretability\nby studying the computational complexity of explaining the decisions of various\nmodels. In this setting, if explanations for a particular model can be obtained\nefficiently, the model is considered interpretable (since it can be explained\n``easily''). However, if generating explanations over an ML model is\ncomputationally intractable, it is considered uninterpretable. Prior research\nidentified two key factors that influence the complexity of interpreting an ML\nmodel: (i) the type of the model (e.g., neural networks, decision trees, etc.);\nand (ii) the form of explanation (e.g., contrastive explanations, Shapley\nvalues, etc.). In this work, we claim that a third, important factor must also\nbe considered for this analysis -- the underlying distribution over which the\nexplanation is obtained. Considering the underlying distribution is key in\navoiding explanations that are socially misaligned, i.e., convey information\nthat is biased and unhelpful to users. We demonstrate the significant influence\nof the underlying distribution on the resulting overall interpretation\ncomplexity, in two settings: (i) prediction models paired with an external\nout-of-distribution (OOD) detector; and (ii) prediction models designed to\ninherently generate socially aligned explanations. Our findings prove that the\nexpressiveness of the distribution can significantly influence the overall\ncomplexity of interpretation, and identify essential prerequisites that a model\nmust possess to generate socially aligned explanations.",
          "arxiv_id": "2408.03915v1"
        },
        {
          "title": "Local vs. Global Interpretability: A Computational Complexity Perspective",
          "year": "2024-06",
          "abstract": "The local and global interpretability of various ML models has been studied\nextensively in recent years. However, despite significant progress in the\nfield, many known results remain informal or lack sufficient mathematical\nrigor. We propose a framework for bridging this gap, by using computational\ncomplexity theory to assess local and global perspectives of interpreting ML\nmodels. We begin by proposing proofs for two novel insights that are essential\nfor our analysis: (1) a duality between local and global forms of explanations;\nand (2) the inherent uniqueness of certain global explanation forms. We then\nuse these insights to evaluate the complexity of computing explanations, across\nthree model types representing the extremes of the interpretability spectrum:\n(1) linear models; (2) decision trees; and (3) neural networks. Our findings\noffer insights into both the local and global interpretability of these models.\nFor instance, under standard complexity assumptions such as P != NP, we prove\nthat selecting global sufficient subsets in linear models is computationally\nharder than selecting local subsets. Interestingly, with neural networks and\ndecision trees, the opposite is true: it is harder to carry out this task\nlocally than globally. We believe that our findings demonstrate how examining\nexplainability through a computational complexity lens can help us develop a\nmore rigorous grasp of the inherent interpretability of ML models.",
          "arxiv_id": "2406.02981v2"
        }
      ],
      "8": [
        {
          "title": "A single-loop SPIDER-type stochastic subgradient method for expectation-constrained nonconvex nonsmooth optimization",
          "year": "2025-01",
          "abstract": "Many real-world problems, such as those with fairness constraints, involve\ncomplex expectation constraints and large datasets, necessitating the design of\nefficient stochastic methods to solve them. Most existing research focuses on\ncases with no {constraint} or easy-to-project constraints or deterministic\nconstraints. In this paper, we consider nonconvex nonsmooth stochastic\noptimization problems with expectation constraints, for which we build a novel\nexact penalty model. We first show the relationship between the penalty model\nand the original problem. Then on solving the penalty problem, we present a\nsingle-loop SPIDER-type stochastic subgradient method, which utilizes the\nsubgradients of both the objective and constraint functions, as well as the\nconstraint function value at each iteration. Under certain regularity\nconditions (weaker than Slater-type constraint qualification or strong\nfeasibility assumed in existing works), we establish an iteration complexity\nresult of $O(\\epsilon^{-4})$ to reach a near-$\\epsilon$ stationary point of the\npenalized problem in expectation, matching the lower bound for such tasks.\nBuilding on the exact penalization, an $(\\epsilon,\\epsilon)$-KKT point of the\noriginal problem is obtained. For a few scenarios, our complexity of either the\n{objective} sample subgradient or the constraint sample function values can be\nlower than the state-of-the-art results by a factor of $\\epsilon^{-2}$.\nMoreover, on solving two fairness-constrained problems and a multi-class\nNeyman-Pearson classification problem, our method is significantly (up to 466\ntimes) faster than the state-of-the-art algorithms, including switching\nsubgradient method and inexact proximal point methods.",
          "arxiv_id": "2501.19214v2"
        },
        {
          "title": "Explicit Second-Order Min-Max Optimization Methods with Optimal Convergence Guarantee",
          "year": "2022-10",
          "abstract": "We propose and analyze several inexact regularized Newton-type methods for\nfinding a global saddle point of \\emph{convex-concave} unconstrained min-max\noptimization problems. Compared to first-order methods, our understanding of\nsecond-order methods for min-max optimization is relatively limited, as\nobtaining global rates of convergence with second-order information is much\nmore involved. In this paper, we examine how second-order information can be\nused to speed up extra-gradient methods, even under inexactness. Specifically,\nwe show that the proposed methods generate iterates that remain within a\nbounded set and that the averaged iterates converge to an $\\epsilon$-saddle\npoint within $O(\\epsilon^{-2/3})$ iterations in terms of a restricted gap\nfunction. This matched the theoretically established lower bound in this\ncontext. We also provide a simple routine for solving the subproblem at each\niteration, requiring a single Schur decomposition and $O(\\log\\log(1/\\epsilon))$\ncalls to a linear system solver in a quasi-upper-triangular system. Thus, our\nmethod improves the existing line-search-based second-order min-max\noptimization methods by shaving off an $O(\\log\\log(1/\\epsilon))$ factor in the\nrequired number of Schur decompositions. Finally, we present numerical\nexperiments on synthetic and real data that demonstrate the efficiency of the\nproposed methods.",
          "arxiv_id": "2210.12860v4"
        },
        {
          "title": "Complexity-optimal and parameter-free first-order methods for finding stationary points of composite optimization problems",
          "year": "2022-05",
          "abstract": "This paper develops and analyzes an accelerated proximal descent method for\nfinding stationary points of nonconvex composite optimization problems. The\nobjective function is of the form $f+h$ where $h$ is a proper closed convex\nfunction, $f$ is a differentiable function on the domain of $h$, and $\\nabla f$\nis Lipschitz continuous on the domain of $h$. The main advantage of this method\nis that it is \"parameter-free\" in the sense that it does not require knowledge\nof the Lipschitz constant of $\\nabla f$ or of any global topological properties\nof $f$. It is shown that the proposed method can obtain an\n$\\varepsilon$-approximate stationary point with iteration complexity bounds\nthat are optimal, up to logarithmic terms over $\\varepsilon$, in both the\nconvex and nonconvex settings. Some discussion is also given about how the\nproposed method can be leveraged in other existing optimization frameworks,\nsuch as min-max smoothing and penalty frameworks for constrained programming,\nto create more specialized parameter-free methods. Finally, numerical\nexperiments are presented to support the practical viability of the method.",
          "arxiv_id": "2205.13055v5"
        }
      ],
      "9": [
        {
          "title": "Spectral Planting and the Hardness of Refuting Cuts, Colorability, and Communities in Random Graphs",
          "year": "2020-08",
          "abstract": "We study the problem of efficiently refuting the k-colorability of a graph,\nor equivalently certifying a lower bound on its chromatic number. We give\nformal evidence of average-case computational hardness for this problem in\nsparse random regular graphs, showing optimality of a simple spectral\ncertificate. This evidence takes the form of a computationally-quiet planting:\nwe construct a distribution of d-regular graphs that has significantly smaller\nchromatic number than a typical regular graph drawn uniformly at random, while\nproviding evidence that these two distributions are indistinguishable by a\nlarge class of algorithms. We generalize our results to the more general\nproblem of certifying an upper bound on the maximum k-cut.\n  This quiet planting is achieved by minimizing the effect of the planted\nstructure (e.g. colorings or cuts) on the graph spectrum. Specifically, the\nplanted structure corresponds exactly to eigenvectors of the adjacency matrix.\nThis avoids the pushout effect of random matrix theory, and delays the point at\nwhich the planting becomes visible in the spectrum or local statistics. To\nillustrate this further, we give similar results for a Gaussian analogue of\nthis problem: a quiet version of the spiked model, where we plant an eigenspace\nrather than adding a generic low-rank perturbation.\n  Our evidence for computational hardness of distinguishing two distributions\nis based on three different heuristics: stability of belief propagation, the\nlocal statistics hierarchy, and the low-degree likelihood ratio. Of independent\ninterest, our results include general-purpose bounds on the low-degree\nlikelihood ratio for multi-spiked matrix models, and an improved low-degree\nanalysis of the stochastic block model.",
          "arxiv_id": "2008.12237v1"
        },
        {
          "title": "Computational Barriers to Estimation from Low-Degree Polynomials",
          "year": "2020-08",
          "abstract": "One fundamental goal of high-dimensional statistics is to detect or recover\nplanted structure (such as a low-rank matrix) hidden in noisy data. A growing\nbody of work studies low-degree polynomials as a restricted model of\ncomputation for such problems: it has been demonstrated in various settings\nthat low-degree polynomials of the data can match the statistical performance\nof the best known polynomial-time algorithms. Prior work has studied the power\nof low-degree polynomials for the task of detecting the presence of hidden\nstructures. In this work, we extend these methods to address problems of\nestimation and recovery (instead of detection). For a large class of \"signal\nplus noise\" problems, we give a user-friendly lower bound for the best possible\nmean squared error achievable by any degree-D polynomial. To our knowledge,\nthese are the first results to establish low-degree hardness of recovery\nproblems for which the associated detection problem is easy. As applications,\nwe give a tight characterization of the low-degree minimum mean squared error\nfor the planted submatrix and planted dense subgraph problems, resolving (in\nthe low-degree framework) open problems about the computational complexity of\nrecovery in both cases.",
          "arxiv_id": "2008.02269v2"
        },
        {
          "title": "Sum-of-Squares Lower Bounds for Sparse Independent Set",
          "year": "2021-11",
          "abstract": "The Sum-of-Squares (SoS) hierarchy of semidefinite programs is a powerful\nalgorithmic paradigm which captures state-of-the-art algorithmic guarantees for\na wide array of problems. In the average case setting, SoS lower bounds provide\nstrong evidence of algorithmic hardness or information-computation gaps. Prior\nto this work, SoS lower bounds have been obtained for problems in the \"dense\"\ninput regime, where the input is a collection of independent Rademacher or\nGaussian random variables, while the sparse regime has remained out of reach.\nWe make the first progress in this direction by obtaining strong SoS lower\nbounds for the problem of Independent Set on sparse random graphs. We prove\nthat with high probability over an Erdos-Renyi random graph $G\\sim\nG_{n,\\frac{d}{n}}$ with average degree $d>\\log^2 n$, degree-$D_{SoS}$ SoS fails\nto refute the existence of an independent set of size $k =\n\\Omega\\left(\\frac{n}{\\sqrt{d}(\\log n)(D_{SoS})^{c_0}} \\right)$ in $G$ (where\n$c_0$ is an absolute constant), whereas the true size of the largest\nindependent set in $G$ is $O\\left(\\frac{n\\log d}{d}\\right)$.\n  Our proof involves several significant extensions of the techniques used for\nproving SoS lower bounds in the dense setting. Previous lower bounds are based\non the pseudo-calibration heuristic of Barak et al [FOCS 2016] which produces a\ncandidate SoS solution using a planted distribution indistinguishable from the\ninput distribution via low-degree tests. In the sparse case the natural planted\ndistribution does admit low-degree distinguishers, and we show how to adapt the\npseudo-calibration heuristic to overcome this.\n  Another notorious technical challenge for the sparse regime is the quest for\nmatrix norm bounds. In this paper, we obtain new norm bounds for graph matrices\nin the sparse setting.",
          "arxiv_id": "2111.09250v1"
        }
      ],
      "10": [
        {
          "title": "Improved List Size for Folded Reed-Solomon Codes",
          "year": "2024-10",
          "abstract": "Folded Reed-Solomon (FRS) codes are variants of Reed-Solomon codes, known for\ntheir optimal list decoding radius. We show explicit FRS codes with rate $R$\nthat can be list decoded up to radius $1-R-\\epsilon$ with lists of size\n$\\mathcal{O}(1/ \\epsilon^2)$. This improves the best known list size among\nexplicit list decoding capacity achieving codes.\n  We also show a more general result that for any $k\\geq 1$, there are explicit\nFRS codes with rate $R$ and distance $1-R$ that can be list decoded arbitrarily\nclose to radius $\\frac{k}{k+1}(1-R)$ with lists of size $(k-1)^2+1$.\n  Our results are based on a new and simple combinatorial viewpoint of the\nintersections between Hamming balls and affine subspaces that recovers\npreviously known parameters. We then use folded Wronskian determinants to carry\nout an inductive proof that yields sharper bounds.",
          "arxiv_id": "2410.09031v1"
        },
        {
          "title": "Explicit Codes approaching Generalized Singleton Bound using Expanders",
          "year": "2025-02",
          "abstract": "We construct a new family of explicit codes that are list decodable to\ncapacity and achieve an optimal list size of $O(\\frac{1}{\\epsilon})$. In\ncontrast to existing explicit constructions of codes achieving list decoding\ncapacity, our arguments do not rely on algebraic structure but utilize simple\ncombinatorial properties of expander graphs.\n  Our construction is based on a celebrated distance amplification procedure\ndue to Alon, Edmonds, and Luby [FOCS'95], which transforms any high-rate code\ninto one with near-optimal rate-distance tradeoff. We generalize it to show\nthat the same procedure can be used to transform any high-rate code into one\nthat achieves list decoding capacity. Our proof can be interpreted as a\n\"local-to-global\" phenomenon for (a slight strengthening of) the generalized\nSingleton bound. Using this construction, for every $R, \\epsilon \\in (0,1)$ and\n$k \\in \\mathbb{N}^+$, we obtain an \\emph{explicit} family of codes $\\mathcal{C}\n\\subseteq \\Sigma^n$, with rate $R$ such that,\n  - They achieve the $\\epsilon$-relaxed generalized Singleton bound: for any $g\n\\in \\Sigma^n$ and any list $\\mathcal{H}$ of at most $k$ codewords, we have, \\[\n\\underset{h \\in \\mathcal{H}}{\\mathbb{E}} [\\Delta(g,h)] ~\\geq~\n\\frac{|\\mathcal{H}|-1}{|\\mathcal{H}|} \\cdot (1 - R - \\epsilon). \\]\n  - The alphabet size is a constant depending only on $\\epsilon$ and $k$.\n  - They can be list decoded up to radius $\\frac{k-1}{k}(1-R-\\epsilon)$, in\ntime $n^{O_{k,\\epsilon}(1)}$.\n  As a corollary of our result, we also obtain the first explicit construction\nof LDPC codes achieving list decoding capacity, and in fact arbitrarily close\nto the generalized Singleton bound.",
          "arxiv_id": "2502.07308v1"
        },
        {
          "title": "New Codes on High Dimensional Expanders",
          "year": "2023-08",
          "abstract": "We describe a new parameterized family of symmetric error-correcting codes\nwith low-density parity-check matrices (LDPC).\n  Our codes can be described in two seemingly different ways. First, in\nrelation to Reed-Muller codes: our codes are functions on a subset of\n$\\mathbb{F}^n$ whose restrictions to a prescribed set of affine lines has low\ndegree. Alternatively, they are Tanner codes on high dimensional expanders,\nwhere the coordinates of the codeword correspond to triangles of a\n$2$-dimensional expander, such that around every edge the local view forms a\nReed-Solomon codeword.\n  For some range of parameters our codes are provably locally testable, and\ntheir dimension is some fixed power of the block length. For another range of\nparameters our codes have distance and dimension that are both linear in the\nblock length, but we do not know if they are locally testable. The codes also\nhave the multiplication property: the coordinate-wise product of two codewords\nis a codeword in a related code.\n  The definition of the codes relies on the construction of a specific family\nof simplicial complexes which is a slight variant on the coset complexes of\nKaufman and Oppenheim. We show a novel way to embed the triangles of these\ncomplexes into $\\mathbb{F}^n$, with the property that links of edges embed as\naffine lines in $\\mathbb{F}^n$.\n  We rely on this embedding to lower bound the rate of these codes in a way\nthat avoids constraint-counting and thereby achieves non-trivial rate even when\nthe local codes themselves have arbitrarily small rate, and in particular below\n$1/2$.",
          "arxiv_id": "2308.15563v1"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T21:21:00Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}