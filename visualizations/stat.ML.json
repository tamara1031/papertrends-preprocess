{
  "topics": {
    "data": {
      "0": {
        "name": "0_policy_RL_reinforcement_reinforcement learning",
        "keywords": [
          [
            "policy",
            0.024358900509059436
          ],
          [
            "RL",
            0.02114532020703495
          ],
          [
            "reinforcement",
            0.017904353723624235
          ],
          [
            "reinforcement learning",
            0.017784855408577268
          ],
          [
            "learning",
            0.017242809292024385
          ],
          [
            "Reinforcement",
            0.01323955504073784
          ],
          [
            "agent",
            0.01259251294586908
          ],
          [
            "Learning",
            0.012236282687171574
          ],
          [
            "algorithm",
            0.010900645653641749
          ],
          [
            "state",
            0.010320920316969106
          ]
        ],
        "count": 2552
      },
      "1": {
        "name": "1_series_time series_time_forecasting",
        "keywords": [
          [
            "series",
            0.021691282176971947
          ],
          [
            "time series",
            0.020222019707574854
          ],
          [
            "time",
            0.01939094985444106
          ],
          [
            "forecasting",
            0.016113710423024934
          ],
          [
            "detection",
            0.01570407219036924
          ],
          [
            "data",
            0.013433343699731261
          ],
          [
            "anomaly",
            0.010655939854894264
          ],
          [
            "model",
            0.010418141846457056
          ],
          [
            "models",
            0.010197675122209591
          ],
          [
            "methods",
            0.008094800418083253
          ]
        ],
        "count": 1941
      },
      "2": {
        "name": "2_causal_treatment_Causal_effect",
        "keywords": [
          [
            "causal",
            0.042985846361673324
          ],
          [
            "treatment",
            0.02373098991338436
          ],
          [
            "Causal",
            0.015778989603323225
          ],
          [
            "effect",
            0.013827722856457223
          ],
          [
            "effects",
            0.013084638727852314
          ],
          [
            "data",
            0.012252039334208577
          ],
          [
            "variables",
            0.011873383246056727
          ],
          [
            "observational",
            0.010481298656264605
          ],
          [
            "estimation",
            0.009057420604008326
          ],
          [
            "inference",
            0.008554901698425254
          ]
        ],
        "count": 1829
      },
      "3": {
        "name": "3_graph_Graph_graphs_node",
        "keywords": [
          [
            "graph",
            0.04405180451611935
          ],
          [
            "Graph",
            0.021224510923968883
          ],
          [
            "graphs",
            0.020514486598610705
          ],
          [
            "node",
            0.018755838114036406
          ],
          [
            "GNNs",
            0.015169927950852851
          ],
          [
            "nodes",
            0.013215848159625638
          ],
          [
            "networks",
            0.013013591018980418
          ],
          [
            "network",
            0.011768017328647868
          ],
          [
            "GNN",
            0.009932839333576707
          ],
          [
            "structure",
            0.009069064144144693
          ]
        ],
        "count": 1744
      },
      "4": {
        "name": "4_generative_diffusion_models_distribution",
        "keywords": [
          [
            "generative",
            0.022514114256528682
          ],
          [
            "diffusion",
            0.018825396780587082
          ],
          [
            "models",
            0.01654196201567269
          ],
          [
            "distribution",
            0.011878166398668904
          ],
          [
            "GANs",
            0.011768595881304867
          ],
          [
            "generative models",
            0.01148526135030818
          ],
          [
            "latent",
            0.011428169314297644
          ],
          [
            "data",
            0.010979215283004556
          ],
          [
            "score",
            0.010584962860219907
          ],
          [
            "image",
            0.010559872487569757
          ]
        ],
        "count": 1419
      },
      "5": {
        "name": "5_regret_bandit_bandits_arm",
        "keywords": [
          [
            "regret",
            0.03424662158191112
          ],
          [
            "bandit",
            0.029441126005553264
          ],
          [
            "bandits",
            0.021854536913633708
          ],
          [
            "arm",
            0.02155648027696524
          ],
          [
            "algorithm",
            0.01810100842772261
          ],
          [
            "Bandits",
            0.017009046680189203
          ],
          [
            "reward",
            0.01592440723517247
          ],
          [
            "problem",
            0.015483995090945071
          ],
          [
            "armed",
            0.014752063899565139
          ],
          [
            "algorithms",
            0.013508976761546944
          ]
        ],
        "count": 1244
      },
      "6": {
        "name": "6_matrix_rank_tensor_sparse",
        "keywords": [
          [
            "matrix",
            0.019405988108304795
          ],
          [
            "rank",
            0.017207568818636592
          ],
          [
            "tensor",
            0.016910497255016494
          ],
          [
            "sparse",
            0.011159127302098275
          ],
          [
            "algorithm",
            0.010620373918019631
          ],
          [
            "low",
            0.010539000480710435
          ],
          [
            "low rank",
            0.010459206019852252
          ],
          [
            "problem",
            0.009825044692845352
          ],
          [
            "data",
            0.008529015982967087
          ],
          [
            "recovery",
            0.008492458286048901
          ]
        ],
        "count": 1207
      },
      "7": {
        "name": "7_language_LLMs_models_attention",
        "keywords": [
          [
            "language",
            0.021530193021360937
          ],
          [
            "LLMs",
            0.01578926141764943
          ],
          [
            "models",
            0.014143593394798936
          ],
          [
            "attention",
            0.013665884196597722
          ],
          [
            "model",
            0.012413370758648947
          ],
          [
            "language models",
            0.011450139932502695
          ],
          [
            "LLM",
            0.01052680807571883
          ],
          [
            "tasks",
            0.010306916387659867
          ],
          [
            "Language",
            0.01000057590720608
          ],
          [
            "text",
            0.00939261526157359
          ]
        ],
        "count": 1184
      },
      "8": {
        "name": "8_networks_neural_neural networks_network",
        "keywords": [
          [
            "networks",
            0.030320198802674083
          ],
          [
            "neural",
            0.026259387318445983
          ],
          [
            "neural networks",
            0.02079200207561902
          ],
          [
            "network",
            0.01884306486750194
          ],
          [
            "ReLU",
            0.017762250620874734
          ],
          [
            "layer",
            0.014862558267819683
          ],
          [
            "training",
            0.014741599916811066
          ],
          [
            "deep",
            0.013964642588452973
          ],
          [
            "width",
            0.012549604202409935
          ],
          [
            "activation",
            0.012547721271719757
          ]
        ],
        "count": 962
      },
      "9": {
        "name": "9_equations_systems_differential_data",
        "keywords": [
          [
            "equations",
            0.016401743013229355
          ],
          [
            "systems",
            0.012916351561558221
          ],
          [
            "differential",
            0.011841133480381896
          ],
          [
            "data",
            0.011507305070834096
          ],
          [
            "PDEs",
            0.011457086601639365
          ],
          [
            "model",
            0.011049006904367864
          ],
          [
            "neural",
            0.010927318042140565
          ],
          [
            "differential equations",
            0.010793197212917355
          ],
          [
            "physical",
            0.010653633954346318
          ],
          [
            "physics",
            0.010395866612795585
          ]
        ],
        "count": 864
      },
      "10": {
        "name": "10_adversarial_attacks_robustness_training",
        "keywords": [
          [
            "adversarial",
            0.052217318856376566
          ],
          [
            "attacks",
            0.031162941382551892
          ],
          [
            "robustness",
            0.027076212607527777
          ],
          [
            "training",
            0.017904829205612253
          ],
          [
            "attack",
            0.017682009200378684
          ],
          [
            "Adversarial",
            0.01671449591237527
          ],
          [
            "robust",
            0.01640419014283954
          ],
          [
            "perturbations",
            0.015102820066798523
          ],
          [
            "adversarial attacks",
            0.014257661417025898
          ],
          [
            "adversarial training",
            0.013761249921467844
          ]
        ],
        "count": 837
      },
      "11": {
        "name": "11_federated_Federated_communication_learning",
        "keywords": [
          [
            "federated",
            0.031147803336567897
          ],
          [
            "Federated",
            0.030300338899343155
          ],
          [
            "communication",
            0.025754489542661407
          ],
          [
            "learning",
            0.017730426009925398
          ],
          [
            "privacy",
            0.01649337201938625
          ],
          [
            "local",
            0.01568771812313092
          ],
          [
            "data",
            0.014650390086834999
          ],
          [
            "server",
            0.012585742875752981
          ],
          [
            "Learning",
            0.01258168607367867
          ],
          [
            "convergence",
            0.01163449639903907
          ]
        ],
        "count": 756
      },
      "12": {
        "name": "12_SGD_gradient_convergence_stochastic",
        "keywords": [
          [
            "SGD",
            0.024780793970144377
          ],
          [
            "gradient",
            0.023389125960598337
          ],
          [
            "convergence",
            0.022854660539100534
          ],
          [
            "stochastic",
            0.02265978134341024
          ],
          [
            "convex",
            0.020431677818333254
          ],
          [
            "optimization",
            0.01819577905157996
          ],
          [
            "rate",
            0.013680436954756967
          ],
          [
            "Stochastic",
            0.012970930587932533
          ],
          [
            "problems",
            0.012634925848728346
          ],
          [
            "descent",
            0.011407882126779664
          ]
        ],
        "count": 709
      },
      "13": {
        "name": "13_label_labels_supervised_learning",
        "keywords": [
          [
            "label",
            0.024685775559776114
          ],
          [
            "labels",
            0.02289692907718884
          ],
          [
            "supervised",
            0.018230973419699745
          ],
          [
            "learning",
            0.01567666274567337
          ],
          [
            "data",
            0.014218795383018006
          ],
          [
            "training",
            0.01282535160627838
          ],
          [
            "classification",
            0.011901581533132782
          ],
          [
            "supervised learning",
            0.010769882253172706
          ],
          [
            "noise",
            0.010200382204479344
          ],
          [
            "class",
            0.010134318940456818
          ]
        ],
        "count": 654
      },
      "14": {
        "name": "14_Langevin_Monte_Carlo_sampling",
        "keywords": [
          [
            "Langevin",
            0.024396795645230084
          ],
          [
            "Monte",
            0.020249029103171005
          ],
          [
            "Carlo",
            0.02024446067650243
          ],
          [
            "sampling",
            0.0179859027700109
          ],
          [
            "variational",
            0.014998786776418397
          ],
          [
            "gradient",
            0.014835084144671594
          ],
          [
            "MCMC",
            0.013858092337811288
          ],
          [
            "inference",
            0.012726367112703524
          ],
          [
            "distribution",
            0.012259148515269684
          ],
          [
            "posterior",
            0.010919847974676718
          ]
        ],
        "count": 604
      },
      "15": {
        "name": "15_survival_imputation_missing_data",
        "keywords": [
          [
            "survival",
            0.02130126205787869
          ],
          [
            "imputation",
            0.018721338427753056
          ],
          [
            "missing",
            0.018665739272353363
          ],
          [
            "data",
            0.01796436215005621
          ],
          [
            "model",
            0.013065703499089457
          ],
          [
            "models",
            0.012686572316944198
          ],
          [
            "clinical",
            0.012255579474981685
          ],
          [
            "patients",
            0.01122943481302185
          ],
          [
            "patient",
            0.01062244048170845
          ],
          [
            "time",
            0.010540737929785588
          ]
        ],
        "count": 602
      },
      "16": {
        "name": "16_privacy_private_DP_differential privacy",
        "keywords": [
          [
            "privacy",
            0.06146503017031707
          ],
          [
            "private",
            0.04038548561688154
          ],
          [
            "DP",
            0.03794542331514211
          ],
          [
            "differential privacy",
            0.02307618286993054
          ],
          [
            "Private",
            0.019175168747858306
          ],
          [
            "differential",
            0.018770189871367856
          ],
          [
            "Differentially",
            0.014104723007718218
          ],
          [
            "Privacy",
            0.013590256411028633
          ],
          [
            "data",
            0.011790721879208915
          ],
          [
            "algorithm",
            0.009860621012996637
          ]
        ],
        "count": 503
      },
      "17": {
        "name": "17_domain_target_source_adaptation",
        "keywords": [
          [
            "domain",
            0.045078167337040946
          ],
          [
            "target",
            0.028906687941963667
          ],
          [
            "source",
            0.026835432557062996
          ],
          [
            "adaptation",
            0.021967054078132757
          ],
          [
            "domain adaptation",
            0.01861478240070514
          ],
          [
            "domains",
            0.01790022352590876
          ],
          [
            "transfer",
            0.017416484826632557
          ],
          [
            "Domain",
            0.016984709299962448
          ],
          [
            "target domain",
            0.014982071096514237
          ],
          [
            "distribution",
            0.014933229646535647
          ]
        ],
        "count": 482
      },
      "18": {
        "name": "18_fairness_fair_Fairness_group",
        "keywords": [
          [
            "fairness",
            0.07562717327850779
          ],
          [
            "fair",
            0.03242382827964057
          ],
          [
            "Fairness",
            0.018116576700894116
          ],
          [
            "group",
            0.015280371391836873
          ],
          [
            "sensitive",
            0.01438650077247503
          ],
          [
            "Fair",
            0.01425423428349462
          ],
          [
            "groups",
            0.013795078800555248
          ],
          [
            "attributes",
            0.012393919380050774
          ],
          [
            "bias",
            0.011196692564922978
          ],
          [
            "data",
            0.010331060565996074
          ]
        ],
        "count": 481
      },
      "19": {
        "name": "19_explanations_explanation_models_model",
        "keywords": [
          [
            "explanations",
            0.025771053916325722
          ],
          [
            "explanation",
            0.01688389361708046
          ],
          [
            "models",
            0.015015460362869739
          ],
          [
            "model",
            0.01450218716474146
          ],
          [
            "AI",
            0.013027916863888557
          ],
          [
            "methods",
            0.012298269874175056
          ],
          [
            "data",
            0.011078833657368226
          ],
          [
            "learning",
            0.010406727483239458
          ],
          [
            "machine",
            0.010212653662714063
          ],
          [
            "machine learning",
            0.009771721357934469
          ]
        ],
        "count": 480
      },
      "20": {
        "name": "20_Gaussian_GP_GPs_processes",
        "keywords": [
          [
            "Gaussian",
            0.04549250182008711
          ],
          [
            "GP",
            0.028266622191447762
          ],
          [
            "GPs",
            0.020989237924046292
          ],
          [
            "processes",
            0.019570479438789874
          ],
          [
            "Gaussian process",
            0.019337805976488386
          ],
          [
            "Processes",
            0.018402218354835586
          ],
          [
            "Gaussian processes",
            0.018121816685385816
          ],
          [
            "process",
            0.01745164200036343
          ],
          [
            "kernel",
            0.01567769227188332
          ],
          [
            "Process",
            0.013418555043022256
          ]
        ],
        "count": 475
      },
      "21": {
        "name": "21_Wasserstein_distance_transport_OT",
        "keywords": [
          [
            "Wasserstein",
            0.03122125438047295
          ],
          [
            "distance",
            0.02229515885649435
          ],
          [
            "transport",
            0.02084183383303704
          ],
          [
            "OT",
            0.01915182766139975
          ],
          [
            "manifold",
            0.016098276754709696
          ],
          [
            "optimal transport",
            0.016064865522129684
          ],
          [
            "dimensional",
            0.013228325629276236
          ],
          [
            "data",
            0.013216280624758448
          ],
          [
            "optimal",
            0.012516652486732947
          ],
          [
            "measures",
            0.01178269226027886
          ]
        ],
        "count": 457
      },
      "22": {
        "name": "22_meta_tasks_task_learning",
        "keywords": [
          [
            "meta",
            0.033565209954018016
          ],
          [
            "tasks",
            0.0297493575943233
          ],
          [
            "task",
            0.0295736559367924
          ],
          [
            "learning",
            0.02887195544351274
          ],
          [
            "meta learning",
            0.024817782850067985
          ],
          [
            "shot",
            0.02085303955446939
          ],
          [
            "forgetting",
            0.01662297499840302
          ],
          [
            "continual",
            0.015972537903434527
          ],
          [
            "Learning",
            0.014747364022713103
          ],
          [
            "continual learning",
            0.014361960264094541
          ]
        ],
        "count": 443
      },
      "23": {
        "name": "23_calibration_prediction_conformal_Conformal",
        "keywords": [
          [
            "calibration",
            0.04360745201172293
          ],
          [
            "prediction",
            0.038237019266408045
          ],
          [
            "conformal",
            0.037826024926645274
          ],
          [
            "Conformal",
            0.026419828692787764
          ],
          [
            "coverage",
            0.026020954678737586
          ],
          [
            "conformal prediction",
            0.02405744900046949
          ],
          [
            "uncertainty",
            0.016550245201110113
          ],
          [
            "sets",
            0.016464882315452876
          ],
          [
            "prediction sets",
            0.013973922375138358
          ],
          [
            "Prediction",
            0.013926621964869164
          ]
        ],
        "count": 441
      },
      "24": {
        "name": "24_BO_Bayesian_optimization_Bayesian optimization",
        "keywords": [
          [
            "BO",
            0.037510674952639295
          ],
          [
            "Bayesian",
            0.036186839196568996
          ],
          [
            "optimization",
            0.03526430916399813
          ],
          [
            "Bayesian optimization",
            0.025680282583187906
          ],
          [
            "acquisition",
            0.021977320444229415
          ],
          [
            "Optimization",
            0.020317886160610327
          ],
          [
            "function",
            0.020254579051715526
          ],
          [
            "objective",
            0.01659424175092955
          ],
          [
            "functions",
            0.016158067377854114
          ],
          [
            "box",
            0.01551426695727685
          ]
        ],
        "count": 413
      },
      "25": {
        "name": "25_quantum_equivariant_Quantum_classical",
        "keywords": [
          [
            "quantum",
            0.08854347994564088
          ],
          [
            "equivariant",
            0.02582446300741344
          ],
          [
            "Quantum",
            0.025432476120705375
          ],
          [
            "classical",
            0.01764608716763692
          ],
          [
            "group",
            0.016507273885678762
          ],
          [
            "learning",
            0.015632154342181077
          ],
          [
            "machine",
            0.014591769673595647
          ],
          [
            "machine learning",
            0.014205754596997995
          ],
          [
            "networks",
            0.013706624880084792
          ],
          [
            "neural",
            0.012359215673978534
          ]
        ],
        "count": 405
      },
      "26": {
        "name": "26_uncertainty_Bayesian_neural_posterior",
        "keywords": [
          [
            "uncertainty",
            0.03473406521974606
          ],
          [
            "Bayesian",
            0.032462048810628426
          ],
          [
            "neural",
            0.020533359650120136
          ],
          [
            "posterior",
            0.018125647021561588
          ],
          [
            "networks",
            0.016591921550073697
          ],
          [
            "neural networks",
            0.016122544514101802
          ],
          [
            "deep",
            0.015062737645794497
          ],
          [
            "BNNs",
            0.014413700076700472
          ],
          [
            "Bayesian neural",
            0.013885689166532426
          ],
          [
            "inference",
            0.013017878782765887
          ]
        ],
        "count": 374
      },
      "27": {
        "name": "27_pruning_quantization_network_NAS",
        "keywords": [
          [
            "pruning",
            0.03397595217498337
          ],
          [
            "quantization",
            0.01926333437257337
          ],
          [
            "network",
            0.018499730211783656
          ],
          [
            "NAS",
            0.017346154857535764
          ],
          [
            "neural",
            0.016854148098047073
          ],
          [
            "networks",
            0.016621819675987182
          ],
          [
            "search",
            0.015199909261795104
          ],
          [
            "accuracy",
            0.01489495558340961
          ],
          [
            "training",
            0.014396732052473046
          ],
          [
            "compression",
            0.014089598645165527
          ]
        ],
        "count": 373
      },
      "28": {
        "name": "28_segmentation_EEG_images_ECG",
        "keywords": [
          [
            "segmentation",
            0.01919086484481916
          ],
          [
            "EEG",
            0.01812473960752374
          ],
          [
            "images",
            0.013954800353277614
          ],
          [
            "ECG",
            0.012955489114017362
          ],
          [
            "classification",
            0.012588098623151111
          ],
          [
            "image",
            0.012428778833120397
          ],
          [
            "medical",
            0.010686232891086557
          ],
          [
            "deep",
            0.010527145111198283
          ],
          [
            "model",
            0.010508711050527501
          ],
          [
            "data",
            0.010130726203047776
          ]
        ],
        "count": 371
      },
      "29": {
        "name": "29_trees_tree_boosting_decision",
        "keywords": [
          [
            "trees",
            0.027429359531019097
          ],
          [
            "tree",
            0.026353053103147273
          ],
          [
            "boosting",
            0.02025609244585963
          ],
          [
            "decision",
            0.014241577408958376
          ],
          [
            "importance",
            0.01382318432406937
          ],
          [
            "forest",
            0.013527883654683168
          ],
          [
            "feature",
            0.01341128047275121
          ],
          [
            "regression",
            0.013119222913743874
          ],
          [
            "forests",
            0.01272264608155887
          ],
          [
            "random",
            0.012608009869981129
          ]
        ],
        "count": 342
      },
      "30": {
        "name": "30_PAC_bounds_learning_bound",
        "keywords": [
          [
            "PAC",
            0.03393183328581177
          ],
          [
            "bounds",
            0.026493268081786685
          ],
          [
            "learning",
            0.02104430420211642
          ],
          [
            "bound",
            0.016618788439925297
          ],
          [
            "class",
            0.013169934220269177
          ],
          [
            "Bayes",
            0.013011492318252785
          ],
          [
            "generalization",
            0.012209155329038757
          ],
          [
            "complexity",
            0.01211634745387341
          ],
          [
            "dimension",
            0.011952561147733276
          ],
          [
            "setting",
            0.011863358500348144
          ]
        ],
        "count": 326
      },
      "31": {
        "name": "31_clustering_means_clusters_cluster",
        "keywords": [
          [
            "clustering",
            0.0696424198427845
          ],
          [
            "means",
            0.028925270154021066
          ],
          [
            "clusters",
            0.02845324295417102
          ],
          [
            "cluster",
            0.027678391496791226
          ],
          [
            "Clustering",
            0.023503754880835982
          ],
          [
            "data",
            0.01814191257181631
          ],
          [
            "algorithm",
            0.015883832858656185
          ],
          [
            "method",
            0.011638385471606545
          ],
          [
            "algorithms",
            0.011247021055850785
          ],
          [
            "number",
            0.009556712699508285
          ]
        ],
        "count": 298
      },
      "32": {
        "name": "32_user_recommendation_users_recommender",
        "keywords": [
          [
            "user",
            0.0368999273666017
          ],
          [
            "recommendation",
            0.030981767340072068
          ],
          [
            "users",
            0.025150298605472958
          ],
          [
            "recommender",
            0.02375918259901871
          ],
          [
            "items",
            0.021588218460909164
          ],
          [
            "item",
            0.019783400385582534
          ],
          [
            "systems",
            0.01583703611534543
          ],
          [
            "recommender systems",
            0.015419642157785832
          ],
          [
            "recommendations",
            0.014253501705714102
          ],
          [
            "Recommender",
            0.012853723344575768
          ]
        ],
        "count": 262
      },
      "33": {
        "name": "33_speech_audio_speaker_ASR",
        "keywords": [
          [
            "speech",
            0.04387557359790594
          ],
          [
            "audio",
            0.03211873664190317
          ],
          [
            "speaker",
            0.028400965699546192
          ],
          [
            "ASR",
            0.016227094301445012
          ],
          [
            "model",
            0.015807948238564483
          ],
          [
            "recognition",
            0.014627964126326323
          ],
          [
            "acoustic",
            0.012329713645888283
          ],
          [
            "training",
            0.011381236423266928
          ],
          [
            "Speech",
            0.01135189711490365
          ],
          [
            "models",
            0.010901225357255524
          ]
        ],
        "count": 261
      },
      "34": {
        "name": "34_traffic_road_travel_prediction",
        "keywords": [
          [
            "traffic",
            0.044692280595944295
          ],
          [
            "road",
            0.017208580786081056
          ],
          [
            "travel",
            0.016095221612716454
          ],
          [
            "prediction",
            0.01527224610395222
          ],
          [
            "time",
            0.013764179947062976
          ],
          [
            "temporal",
            0.013209268075594164
          ],
          [
            "data",
            0.012785222354548936
          ],
          [
            "model",
            0.012565307662557012
          ],
          [
            "transportation",
            0.012198484977462778
          ],
          [
            "trajectory",
            0.012122322187126564
          ]
        ],
        "count": 215
      }
    },
    "correlations": [
      [
        1.0,
        -0.7323304703026707,
        -0.7407118955802692,
        -0.7508375138152568,
        -0.7314294600107474,
        -0.6398094587616812,
        -0.7478326954696046,
        -0.7252544760978433,
        -0.7258101476788172,
        -0.7506585618996002,
        -0.7244754070233502,
        -0.6661268162809308,
        -0.7166728524334365,
        -0.6751831093829038,
        -0.7430770182760513,
        -0.7377587849845761,
        -0.7555346388190249,
        -0.7142618311580182,
        -0.7586408555954768,
        -0.7204528888265838,
        -0.7455397262302195,
        -0.7529205346968391,
        -0.6147649999300755,
        -0.7462526997627571,
        -0.7150252097897473,
        -0.7383457359494809,
        -0.7138312660163326,
        -0.732911524638016,
        -0.7511183638948491,
        -0.7584179112114866,
        -0.6277085666776571,
        -0.754579573647284,
        -0.735219998462466,
        -0.7605962454637165,
        -0.7478604427805806
      ],
      [
        -0.7323304703026707,
        1.0,
        -0.712215825750635,
        -0.7225141972691097,
        -0.6931874172728534,
        -0.7180853418878199,
        -0.7296759991801658,
        -0.7089592271367675,
        -0.6815183883985316,
        -0.7151971379631705,
        -0.7147845013843268,
        -0.7351500598986405,
        -0.7225379654847759,
        -0.7076565703176461,
        -0.7328161327163345,
        -0.3076249329701441,
        -0.7423054982738005,
        -0.6998427901675117,
        -0.7515460783166007,
        -0.6457604532247812,
        -0.7018743530502631,
        -0.7337753247702714,
        -0.6956427927369844,
        -0.7136747941790076,
        -0.7204503959647762,
        -0.7088184990018624,
        -0.6824578336911932,
        -0.705771616287399,
        -0.7129303239892629,
        -0.72574901799124,
        -0.7306903965200766,
        -0.7039409435110413,
        -0.7292322193626442,
        -0.7507266489896236,
        -0.7335546176136484
      ],
      [
        -0.7407118955802692,
        -0.712215825750635,
        1.0,
        -0.7101070960456266,
        -0.7417073059393993,
        -0.7393658626302816,
        -0.7595412094596068,
        -0.7429770225769918,
        -0.7457639456631948,
        -0.7596024943821518,
        -0.751220762583763,
        -0.7591932406229723,
        -0.7525852007500095,
        -0.7497190234730952,
        -0.7564055089669409,
        -0.7089235570923108,
        -0.7576213105582341,
        -0.7184360020776446,
        -0.7416386126238812,
        -0.7206573126294471,
        -0.7424668252223658,
        -0.7540643909059737,
        -0.7382392128237758,
        -0.7398835107143923,
        -0.7428226508803666,
        -0.7412503909443868,
        -0.7336999596717956,
        -0.7504505365282091,
        -0.7584631224859455,
        -0.7419035558757807,
        -0.7457649788548519,
        -0.7541533449228018,
        -0.7454865743246148,
        -0.7621745292725915,
        -0.76123424559617
      ],
      [
        -0.7508375138152568,
        -0.7225141972691097,
        -0.7101070960456266,
        1.0,
        -0.7320838595203378,
        -0.7380406902727052,
        -0.7387471455712074,
        -0.7336611310348194,
        -0.6752729680062725,
        -0.7551555574248667,
        -0.7372308942176002,
        -0.7480967094479987,
        -0.7448638051111454,
        -0.710309398321702,
        -0.7542348436768285,
        -0.7330507681190981,
        -0.7568628627408559,
        -0.7345477077774311,
        -0.7553812973622214,
        -0.7133465152014697,
        -0.741005630615081,
        -0.7350167466449924,
        -0.7103824762564332,
        -0.742103902625976,
        -0.7440108391916505,
        -0.7273227627580434,
        -0.7133771110374916,
        -0.7132884343184775,
        -0.7488048273310144,
        -0.7414130165291046,
        -0.742111211678502,
        -0.7004952236654801,
        -0.7340034232602379,
        -0.7620005971760276,
        -0.7497037333679004
      ],
      [
        -0.7314294600107474,
        -0.6931874172728534,
        -0.7417073059393993,
        -0.7320838595203378,
        1.0,
        -0.743357427126826,
        -0.751214935217284,
        -0.49355852452457805,
        -0.7032521994483814,
        -0.7200557393083844,
        -0.6912428390082295,
        -0.7494462520889853,
        -0.7305211284172499,
        -0.7264999916054672,
        -0.7020894007290222,
        -0.7050667883215285,
        -0.7459471349101369,
        -0.7002170957863872,
        -0.7534036502368305,
        -0.5164876534441651,
        -0.7090890331600757,
        -0.7031835003393689,
        -0.7078492680213293,
        -0.7371140171968389,
        -0.7249138235580274,
        -0.7212753184542613,
        -0.6908907160542441,
        -0.7208737603353637,
        -0.7417422257315754,
        -0.7431992726661014,
        -0.734695161047046,
        -0.7454424447435831,
        -0.7430088533674829,
        -0.7390556490630855,
        -0.7593856837548052
      ],
      [
        -0.6398094587616812,
        -0.7180853418878199,
        -0.7393658626302816,
        -0.7380406902727052,
        -0.743357427126826,
        1.0,
        -0.7229630007753582,
        -0.7415157649825508,
        -0.7371184151968716,
        -0.7554407806778771,
        -0.7052331016289626,
        -0.7217547363284491,
        -0.6900380654943465,
        -0.7276543765792344,
        -0.737555071909383,
        -0.7320909910120046,
        -0.7404457844122219,
        -0.7348774675693015,
        -0.7498628268265347,
        -0.7276401200170888,
        -0.7233319603487254,
        -0.7451458117507509,
        -0.7180238027964021,
        -0.7450236360416391,
        -0.6999471856775608,
        -0.7443429301457059,
        -0.7256324649549095,
        -0.7441908294614887,
        -0.7575950761009804,
        -0.7488043880518753,
        -0.6492816094755665,
        -0.7314862690259238,
        -0.6974110197937582,
        -0.7638032090742591,
        -0.7557925522157714
      ],
      [
        -0.7478326954696046,
        -0.7296759991801658,
        -0.7595412094596068,
        -0.7387471455712074,
        -0.751214935217284,
        -0.7229630007753582,
        1.0,
        -0.7430369687834506,
        -0.7240115862756643,
        -0.7533429389625692,
        -0.7506228949679534,
        -0.7538836224451264,
        -0.7098510804369387,
        -0.7454297594576063,
        -0.7489820863867913,
        -0.7211366455145132,
        -0.7574453464618836,
        -0.7498413365486691,
        -0.7627128128875821,
        -0.7309233260306531,
        -0.7294161826573486,
        -0.7463170743355273,
        -0.7414797500185513,
        -0.7564347063546553,
        -0.7324047280895423,
        -0.7371108529872474,
        -0.7348782387973869,
        -0.740370897035353,
        -0.7543181334637005,
        -0.7573394931170405,
        -0.729192887300215,
        -0.7318998291734342,
        -0.740998095898493,
        -0.7634491600092308,
        -0.7576221727534265
      ],
      [
        -0.7252544760978433,
        -0.7089592271367675,
        -0.7429770225769918,
        -0.7336611310348194,
        -0.49355852452457805,
        -0.7415157649825508,
        -0.7430369687834506,
        1.0,
        -0.7107022088169651,
        -0.7500716822364756,
        -0.715820066413682,
        -0.7463443847796365,
        -0.7353162535549786,
        -0.7280601280507011,
        -0.7430799676136762,
        -0.7130977957853292,
        -0.749428344772544,
        -0.7156456146386557,
        -0.7505834527707262,
        -0.5095789107148634,
        -0.7354337168081457,
        -0.7497827572503952,
        -0.6940552071015331,
        -0.7270752146578396,
        -0.7295095878305113,
        -0.7260216106380848,
        -0.7010869251425393,
        -0.7205320305840303,
        -0.7366267886291942,
        -0.7468004443357761,
        -0.7379289159934459,
        -0.746906380679821,
        -0.7252561835440057,
        -0.7346102882602639,
        -0.7572261871712032
      ],
      [
        -0.7258101476788172,
        -0.6815183883985316,
        -0.7457639456631948,
        -0.6752729680062725,
        -0.7032521994483814,
        -0.7371184151968716,
        -0.7240115862756643,
        -0.7107022088169651,
        1.0,
        -0.699435155358048,
        -0.656745395722411,
        -0.7296281715239196,
        -0.618063832468787,
        -0.6970950453962637,
        -0.7209132026925407,
        -0.7009754264916834,
        -0.7511484824531158,
        -0.700703743150839,
        -0.7581346160823885,
        -0.6631202322801556,
        -0.70482396817301,
        -0.731440418408822,
        -0.676378265436405,
        -0.7157235688097254,
        -0.70487271926878,
        -0.5720196988543078,
        -0.19013514039540585,
        -0.011458974015982962,
        -0.6945598912375677,
        -0.7359039440562408,
        -0.7032303064528151,
        -0.7372854104345696,
        -0.7388168555131317,
        -0.746376571145989,
        -0.7515170924128958
      ],
      [
        -0.7506585618996002,
        -0.7151971379631705,
        -0.7596024943821518,
        -0.7551555574248667,
        -0.7200557393083844,
        -0.7554407806778771,
        -0.7533429389625692,
        -0.7500716822364756,
        -0.699435155358048,
        1.0,
        -0.7463681804699984,
        -0.757862791841842,
        -0.7284466883326873,
        -0.7551715688780907,
        -0.7343949193198742,
        -0.7360824987600003,
        -0.7632542457541163,
        -0.7379845412329927,
        -0.7644454485176272,
        -0.7257547208623917,
        -0.7113103751079219,
        -0.7447505346221635,
        -0.7453133350834368,
        -0.7530127509752624,
        -0.7437448951166998,
        -0.7358680777032336,
        -0.7004576384649934,
        -0.7231269751399635,
        -0.7433773699939422,
        -0.7631447730115389,
        -0.7479320479260915,
        -0.7585682057559809,
        -0.7581263555295958,
        -0.7616893048438711,
        -0.7612108392534978
      ],
      [
        -0.7244754070233502,
        -0.7147845013843268,
        -0.751220762583763,
        -0.7372308942176002,
        -0.6912428390082295,
        -0.7052331016289626,
        -0.7506228949679534,
        -0.715820066413682,
        -0.656745395722411,
        -0.7463681804699984,
        1.0,
        -0.7287001548296419,
        -0.7191056989497939,
        -0.7066440437204674,
        -0.7464806151448957,
        -0.7232029732060988,
        -0.7334933732428757,
        -0.7027570732139665,
        -0.7454677470887003,
        -0.6872739592333603,
        -0.739466846066793,
        -0.736094798851159,
        -0.7138405685719156,
        -0.7388600280534324,
        -0.7179687014158826,
        -0.7195101921234031,
        -0.6799985814848111,
        -0.6850620192399917,
        -0.73430230540085,
        -0.75089379266679,
        -0.7275326921296386,
        -0.744704532095191,
        -0.7462373715846725,
        -0.7437855617994584,
        -0.7589726833546685
      ],
      [
        -0.6661268162809308,
        -0.7351500598986405,
        -0.7591932406229723,
        -0.7480967094479987,
        -0.7494462520889853,
        -0.7217547363284491,
        -0.7538836224451264,
        -0.7463443847796365,
        -0.7296281715239196,
        -0.757862791841842,
        -0.7287001548296419,
        1.0,
        -0.6891956582762633,
        -0.6595181341751237,
        -0.755179955227339,
        -0.7229868376698769,
        -0.5992946089888874,
        -0.7442517035578438,
        -0.7443007632010238,
        -0.7231474323325022,
        -0.7475548515702524,
        -0.7533798704506693,
        -0.6360574393383159,
        -0.7556862608787422,
        -0.7431357895630781,
        -0.7318065996949539,
        -0.7315279177391716,
        -0.736108497513905,
        -0.7471786165202161,
        -0.7601853154283646,
        -0.6430032456629537,
        -0.7475942217595675,
        -0.7302335319357105,
        -0.75712129934131,
        -0.754333511543312
      ],
      [
        -0.7166728524334365,
        -0.7225379654847759,
        -0.7525852007500095,
        -0.7448638051111454,
        -0.7305211284172499,
        -0.6900380654943465,
        -0.7098510804369387,
        -0.7353162535549786,
        -0.618063832468787,
        -0.7284466883326873,
        -0.7191056989497939,
        -0.6891956582762633,
        1.0,
        -0.729942401019674,
        -0.6945789020665132,
        -0.7305216987739009,
        -0.7294270003876859,
        -0.7255569872875915,
        -0.7573460585783419,
        -0.7138229282659546,
        -0.7230041846205546,
        -0.7101665097807527,
        -0.719998592297322,
        -0.7490355725941413,
        -0.5999049382858395,
        -0.7173560395154417,
        -0.6899264661602402,
        -0.6970858551243169,
        -0.7391997651433284,
        -0.7462344746229392,
        -0.6982725484161465,
        -0.7454093138941799,
        -0.7520122967784868,
        -0.7602138466188516,
        -0.7615966957411722
      ],
      [
        -0.6751831093829038,
        -0.7076565703176461,
        -0.7497190234730952,
        -0.710309398321702,
        -0.7264999916054672,
        -0.7276543765792344,
        -0.7454297594576063,
        -0.7280601280507011,
        -0.6970950453962637,
        -0.7551715688780907,
        -0.7066440437204674,
        -0.6595181341751237,
        -0.729942401019674,
        1.0,
        -0.7519356151341948,
        -0.6975410551725194,
        -0.7496794330327787,
        -0.7000834574728153,
        -0.7532542626120393,
        -0.694562464671564,
        -0.7396373107294978,
        -0.7429495837552791,
        -0.6064438490946515,
        -0.7305915647356174,
        -0.7320813131561573,
        -0.7206649583981175,
        -0.7076896178109533,
        -0.7214830422273113,
        -0.727928826220456,
        -0.7427758248522952,
        -0.6399293252649622,
        -0.7326675704524312,
        -0.7431440506374485,
        -0.7497168390466757,
        -0.7600998071839358
      ],
      [
        -0.7430770182760513,
        -0.7328161327163345,
        -0.7564055089669409,
        -0.7542348436768285,
        -0.7020894007290222,
        -0.737555071909383,
        -0.7489820863867913,
        -0.7430799676136762,
        -0.7209132026925407,
        -0.7343949193198742,
        -0.7464806151448957,
        -0.755179955227339,
        -0.6945789020665132,
        -0.7519356151341948,
        1.0,
        -0.7385734891878978,
        -0.7553053540585573,
        -0.7210832144817771,
        -0.7621830597267003,
        -0.7266979455306045,
        -0.7078601572080417,
        -0.7172581128427996,
        -0.7440923195810287,
        -0.7477266279553441,
        -0.7233930577719443,
        -0.7356253322679391,
        -0.6464260845956884,
        -0.7339810097171784,
        -0.7551821651141137,
        -0.7526891385327102,
        -0.7394606894699323,
        -0.7487254419193619,
        -0.7549349233181779,
        -0.7612662511240067,
        -0.7615939715307776
      ],
      [
        -0.7377587849845761,
        -0.3076249329701441,
        -0.7089235570923108,
        -0.7330507681190981,
        -0.7050667883215285,
        -0.7320909910120046,
        -0.7211366455145132,
        -0.7130977957853292,
        -0.7009754264916834,
        -0.7360824987600003,
        -0.7232029732060988,
        -0.7229868376698769,
        -0.7305216987739009,
        -0.6975410551725194,
        -0.7385734891878978,
        1.0,
        -0.7302517330777814,
        -0.7045064795897539,
        -0.7403720887925396,
        -0.6518025766745774,
        -0.7139475447629213,
        -0.7360363758081143,
        -0.7072302501250651,
        -0.7135290942948018,
        -0.7243454551004742,
        -0.7149335361089297,
        -0.692951763509817,
        -0.7207949139919445,
        -0.7223127680562758,
        -0.7154658333125932,
        -0.7315511730708261,
        -0.7099208015248979,
        -0.7242099856351973,
        -0.7500370438669239,
        -0.7449161628290841
      ],
      [
        -0.7555346388190249,
        -0.7423054982738005,
        -0.7576213105582341,
        -0.7568628627408559,
        -0.7459471349101369,
        -0.7404457844122219,
        -0.7574453464618836,
        -0.749428344772544,
        -0.7511484824531158,
        -0.7632542457541163,
        -0.7334933732428757,
        -0.5992946089888874,
        -0.7294270003876859,
        -0.7496794330327787,
        -0.7553053540585573,
        -0.7302517330777814,
        1.0,
        -0.7476298048463931,
        -0.7412196049245261,
        -0.7399620967770102,
        -0.7522632618003128,
        -0.7525099242006055,
        -0.7474421515848366,
        -0.7566500111682732,
        -0.7525970706535576,
        -0.7478013102291998,
        -0.750124645229926,
        -0.7516462263486703,
        -0.759299460561165,
        -0.7577505331879801,
        -0.730539777740575,
        -0.7550845600974756,
        -0.7237454815851778,
        -0.7564880490757102,
        -0.762837979751636
      ],
      [
        -0.7142618311580182,
        -0.6998427901675117,
        -0.7184360020776446,
        -0.7345477077774311,
        -0.7002170957863872,
        -0.7348774675693015,
        -0.7498413365486691,
        -0.7156456146386557,
        -0.700703743150839,
        -0.7379845412329927,
        -0.7027570732139665,
        -0.7442517035578438,
        -0.7255569872875915,
        -0.7000834574728153,
        -0.7210832144817771,
        -0.7045064795897539,
        -0.7476298048463931,
        1.0,
        -0.75013637442985,
        -0.6945029909856466,
        -0.7268073857762689,
        -0.705729212372005,
        -0.686698459824809,
        -0.727299197919739,
        -0.7211277091663986,
        -0.7233945599534981,
        -0.7011204335637315,
        -0.7185774484132584,
        -0.7394697741836689,
        -0.7469695241371115,
        -0.7259980060264843,
        -0.7446718525909459,
        -0.7367255802920942,
        -0.7426781384829386,
        -0.7580167449456487
      ],
      [
        -0.7586408555954768,
        -0.7515460783166007,
        -0.7416386126238812,
        -0.7553812973622214,
        -0.7534036502368305,
        -0.7498628268265347,
        -0.7627128128875821,
        -0.7505834527707262,
        -0.7581346160823885,
        -0.7644454485176272,
        -0.7454677470887003,
        -0.7443007632010238,
        -0.7573460585783419,
        -0.7532542626120393,
        -0.7621830597267003,
        -0.7403720887925396,
        -0.7412196049245261,
        -0.75013637442985,
        1.0,
        -0.7328361813941187,
        -0.7634775401223618,
        -0.7521514052686751,
        -0.7515527101212793,
        -0.7446175294455423,
        -0.7523845435701674,
        -0.7446827177160364,
        -0.753808157409622,
        -0.7579288322102167,
        -0.7608791500630666,
        -0.7556744776409171,
        -0.7542788663117984,
        -0.7478150119357385,
        -0.7402867091105166,
        -0.7619449741956288,
        -0.7639194528073907
      ],
      [
        -0.7204528888265838,
        -0.6457604532247812,
        -0.7206573126294471,
        -0.7133465152014697,
        -0.5164876534441651,
        -0.7276401200170888,
        -0.7309233260306531,
        -0.5095789107148634,
        -0.6631202322801556,
        -0.7257547208623917,
        -0.6872739592333603,
        -0.7231474323325022,
        -0.7138229282659546,
        -0.694562464671564,
        -0.7266979455306045,
        -0.6518025766745774,
        -0.7399620967770102,
        -0.6945029909856466,
        -0.7328361813941187,
        1.0,
        -0.6965040118809231,
        -0.7294352227466302,
        -0.6724602264982151,
        -0.7008797574613006,
        -0.6938486092962561,
        -0.38052299677014634,
        -0.6475793970433439,
        -0.6938473142847394,
        -0.7219382334972102,
        -0.7057139060178942,
        -0.7181693075483173,
        -0.7269168017268544,
        -0.7074175155054491,
        -0.7388617140614187,
        -0.7485698511183194
      ],
      [
        -0.7455397262302195,
        -0.7018743530502631,
        -0.7424668252223658,
        -0.741005630615081,
        -0.7090890331600757,
        -0.7233319603487254,
        -0.7294161826573486,
        -0.7354337168081457,
        -0.70482396817301,
        -0.7113103751079219,
        -0.739466846066793,
        -0.7475548515702524,
        -0.7230041846205546,
        -0.7396373107294978,
        -0.7078601572080417,
        -0.7139475447629213,
        -0.7522632618003128,
        -0.7268073857762689,
        -0.7634775401223618,
        -0.6965040118809231,
        1.0,
        -0.7261980213582515,
        -0.7268454133645084,
        -0.7365851644371624,
        -0.6638881772855061,
        -0.7320987288218275,
        -0.6581323425401795,
        -0.7252724862320343,
        -0.7500706159181374,
        -0.7506008590170344,
        -0.7241157780715586,
        -0.7364533676595233,
        -0.753295209461537,
        -0.7579823961098809,
        -0.7563240491656963
      ],
      [
        -0.7529205346968391,
        -0.7337753247702714,
        -0.7540643909059737,
        -0.7350167466449924,
        -0.7031835003393689,
        -0.7451458117507509,
        -0.7463170743355273,
        -0.7497827572503952,
        -0.731440418408822,
        -0.7447505346221635,
        -0.736094798851159,
        -0.7533798704506693,
        -0.7101665097807527,
        -0.7429495837552791,
        -0.7172581128427996,
        -0.7360363758081143,
        -0.7525099242006055,
        -0.705729212372005,
        -0.7521514052686751,
        -0.7294352227466302,
        -0.7261980213582515,
        1.0,
        -0.7440725283144537,
        -0.7527959770350774,
        -0.7342632231283678,
        -0.734346104153442,
        -0.727564819148172,
        -0.7401921000616289,
        -0.7560295112278654,
        -0.7547308452500179,
        -0.735887059493457,
        -0.7342014898330519,
        -0.7605212330905877,
        -0.7630194015724017,
        -0.761787859961651
      ],
      [
        -0.6147649999300755,
        -0.6956427927369844,
        -0.7382392128237758,
        -0.7103824762564332,
        -0.7078492680213293,
        -0.7180238027964021,
        -0.7414797500185513,
        -0.6940552071015331,
        -0.676378265436405,
        -0.7453133350834368,
        -0.7138405685719156,
        -0.6360574393383159,
        -0.719998592297322,
        -0.6064438490946515,
        -0.7440923195810287,
        -0.7072302501250651,
        -0.7474421515848366,
        -0.686698459824809,
        -0.7515527101212793,
        -0.6724602264982151,
        -0.7268454133645084,
        -0.7440725283144537,
        1.0,
        -0.7301727706879872,
        -0.7155774536875328,
        -0.7057396776498238,
        -0.6850139107361236,
        -0.6994987371610065,
        -0.7189738541760955,
        -0.7463291652650459,
        -0.6205388760982795,
        -0.7371723512898487,
        -0.7348569733357222,
        -0.7394040653529226,
        -0.7550941373519511
      ],
      [
        -0.7462526997627571,
        -0.7136747941790076,
        -0.7398835107143923,
        -0.742103902625976,
        -0.7371140171968389,
        -0.7450236360416391,
        -0.7564347063546553,
        -0.7270752146578396,
        -0.7157235688097254,
        -0.7530127509752624,
        -0.7388600280534324,
        -0.7556862608787422,
        -0.7490355725941413,
        -0.7305915647356174,
        -0.7477266279553441,
        -0.7135290942948018,
        -0.7566500111682732,
        -0.727299197919739,
        -0.7446175294455423,
        -0.7008797574613006,
        -0.7365851644371624,
        -0.7527959770350774,
        -0.7301727706879872,
        1.0,
        -0.7312036106505756,
        -0.7295871697639428,
        -0.6641939090515584,
        -0.7318295221327592,
        -0.7511078966264844,
        -0.734406460086893,
        -0.7448654188631276,
        -0.7529943022481724,
        -0.7418856004956313,
        -0.757699764047768,
        -0.7427961801650107
      ],
      [
        -0.7150252097897473,
        -0.7204503959647762,
        -0.7428226508803666,
        -0.7440108391916505,
        -0.7249138235580274,
        -0.6999471856775608,
        -0.7324047280895423,
        -0.7295095878305113,
        -0.70487271926878,
        -0.7437448951166998,
        -0.7179687014158826,
        -0.7431357895630781,
        -0.5999049382858395,
        -0.7320813131561573,
        -0.7233930577719443,
        -0.7243454551004742,
        -0.7525970706535576,
        -0.7211277091663986,
        -0.7523845435701674,
        -0.6938486092962561,
        -0.6638881772855061,
        -0.7342632231283678,
        -0.7155774536875328,
        -0.7312036106505756,
        1.0,
        -0.7209612932810459,
        -0.6483965132325371,
        -0.714931963370387,
        -0.7456439821935716,
        -0.738704861555379,
        -0.7237710027315174,
        -0.7419366505049404,
        -0.7346449443156926,
        -0.7570111005309992,
        -0.7583424945814805
      ],
      [
        -0.7383457359494809,
        -0.7088184990018624,
        -0.7412503909443868,
        -0.7273227627580434,
        -0.7212753184542613,
        -0.7443429301457059,
        -0.7371108529872474,
        -0.7260216106380848,
        -0.5720196988543078,
        -0.7358680777032336,
        -0.7195101921234031,
        -0.7318065996949539,
        -0.7173560395154417,
        -0.7206649583981175,
        -0.7356253322679391,
        -0.7149335361089297,
        -0.7478013102291998,
        -0.7233945599534981,
        -0.7446827177160364,
        -0.38052299677014634,
        -0.7320987288218275,
        -0.734346104153442,
        -0.7057396776498238,
        -0.7295871697639428,
        -0.7209612932810459,
        1.0,
        -0.6059807510770531,
        -0.5946118808638292,
        -0.7407220368114722,
        -0.7328323328789532,
        -0.7249922281379695,
        -0.7463049422957821,
        -0.7435656460638514,
        -0.7574104350726447,
        -0.7585860607756221
      ],
      [
        -0.7138312660163326,
        -0.6824578336911932,
        -0.7336999596717956,
        -0.7133771110374916,
        -0.6908907160542441,
        -0.7256324649549095,
        -0.7348782387973869,
        -0.7010869251425393,
        -0.19013514039540585,
        -0.7004576384649934,
        -0.6799985814848111,
        -0.7315279177391716,
        -0.6899264661602402,
        -0.7076896178109533,
        -0.6464260845956884,
        -0.692951763509817,
        -0.750124645229926,
        -0.7011204335637315,
        -0.753808157409622,
        -0.6475793970433439,
        -0.6581323425401795,
        -0.727564819148172,
        -0.6850139107361236,
        -0.6641939090515584,
        -0.6483965132325371,
        -0.6059807510770531,
        1.0,
        -0.2867181700683532,
        -0.5909328661599593,
        -0.7312934545009242,
        -0.7187431295948274,
        -0.7413561819641152,
        -0.7348626684718164,
        -0.7525521968921043,
        -0.7500583976975239
      ],
      [
        -0.732911524638016,
        -0.705771616287399,
        -0.7504505365282091,
        -0.7132884343184775,
        -0.7208737603353637,
        -0.7441908294614887,
        -0.740370897035353,
        -0.7205320305840303,
        -0.011458974015982962,
        -0.7231269751399635,
        -0.6850620192399917,
        -0.736108497513905,
        -0.6970858551243169,
        -0.7214830422273113,
        -0.7339810097171784,
        -0.7207949139919445,
        -0.7516462263486703,
        -0.7185774484132584,
        -0.7579288322102167,
        -0.6938473142847394,
        -0.7252724862320343,
        -0.7401921000616289,
        -0.6994987371610065,
        -0.7318295221327592,
        -0.714931963370387,
        -0.5946118808638292,
        -0.2867181700683532,
        1.0,
        -0.7153713274954558,
        -0.7400327472043731,
        -0.7241530052288258,
        -0.743744229214557,
        -0.7442611523569331,
        -0.7461018208789472,
        -0.7504416863792792
      ],
      [
        -0.7511183638948491,
        -0.7129303239892629,
        -0.7584631224859455,
        -0.7488048273310144,
        -0.7417422257315754,
        -0.7575950761009804,
        -0.7543181334637005,
        -0.7366267886291942,
        -0.6945598912375677,
        -0.7433773699939422,
        -0.73430230540085,
        -0.7471786165202161,
        -0.7391997651433284,
        -0.727928826220456,
        -0.7551821651141137,
        -0.7223127680562758,
        -0.759299460561165,
        -0.7394697741836689,
        -0.7608791500630666,
        -0.7219382334972102,
        -0.7500706159181374,
        -0.7560295112278654,
        -0.7189738541760955,
        -0.7511078966264844,
        -0.7456439821935716,
        -0.7407220368114722,
        -0.5909328661599593,
        -0.7153713274954558,
        1.0,
        -0.7494169380281874,
        -0.7460903804489698,
        -0.7506756182189394,
        -0.7532847520953526,
        -0.7404763316415001,
        -0.7545528129024248
      ],
      [
        -0.7584179112114866,
        -0.72574901799124,
        -0.7419035558757807,
        -0.7414130165291046,
        -0.7431992726661014,
        -0.7488043880518753,
        -0.7573394931170405,
        -0.7468004443357761,
        -0.7359039440562408,
        -0.7631447730115389,
        -0.75089379266679,
        -0.7601853154283646,
        -0.7462344746229392,
        -0.7427758248522952,
        -0.7526891385327102,
        -0.7154658333125932,
        -0.7577505331879801,
        -0.7469695241371115,
        -0.7556744776409171,
        -0.7057139060178942,
        -0.7506008590170344,
        -0.7547308452500179,
        -0.7463291652650459,
        -0.734406460086893,
        -0.738704861555379,
        -0.7328323328789532,
        -0.7312934545009242,
        -0.7400327472043731,
        -0.7494169380281874,
        1.0,
        -0.7484308639198436,
        -0.7406787924838771,
        -0.7530280350527307,
        -0.7631616921876223,
        -0.7612678890673632
      ],
      [
        -0.6277085666776571,
        -0.7306903965200766,
        -0.7457649788548519,
        -0.742111211678502,
        -0.734695161047046,
        -0.6492816094755665,
        -0.729192887300215,
        -0.7379289159934459,
        -0.7032303064528151,
        -0.7479320479260915,
        -0.7275326921296386,
        -0.6430032456629537,
        -0.6982725484161465,
        -0.6399293252649622,
        -0.7394606894699323,
        -0.7315511730708261,
        -0.730539777740575,
        -0.7259980060264843,
        -0.7542788663117984,
        -0.7181693075483173,
        -0.7241157780715586,
        -0.735887059493457,
        -0.6205388760982795,
        -0.7448654188631276,
        -0.7237710027315174,
        -0.7249922281379695,
        -0.7187431295948274,
        -0.7241530052288258,
        -0.7460903804489698,
        -0.7484308639198436,
        1.0,
        -0.7489833196365945,
        -0.744669113647805,
        -0.7600760889744533,
        -0.763422568599156
      ],
      [
        -0.754579573647284,
        -0.7039409435110413,
        -0.7541533449228018,
        -0.7004952236654801,
        -0.7454424447435831,
        -0.7314862690259238,
        -0.7318998291734342,
        -0.746906380679821,
        -0.7372854104345696,
        -0.7585682057559809,
        -0.744704532095191,
        -0.7475942217595675,
        -0.7454093138941799,
        -0.7326675704524312,
        -0.7487254419193619,
        -0.7099208015248979,
        -0.7550845600974756,
        -0.7446718525909459,
        -0.7478150119357385,
        -0.7269168017268544,
        -0.7364533676595233,
        -0.7342014898330519,
        -0.7371723512898487,
        -0.7529943022481724,
        -0.7419366505049404,
        -0.7463049422957821,
        -0.7413561819641152,
        -0.743744229214557,
        -0.7506756182189394,
        -0.7406787924838771,
        -0.7489833196365945,
        1.0,
        -0.7473117810912333,
        -0.7555022158245062,
        -0.7562313468495581
      ],
      [
        -0.735219998462466,
        -0.7292322193626442,
        -0.7454865743246148,
        -0.7340034232602379,
        -0.7430088533674829,
        -0.6974110197937582,
        -0.740998095898493,
        -0.7252561835440057,
        -0.7388168555131317,
        -0.7581263555295958,
        -0.7462373715846725,
        -0.7302335319357105,
        -0.7520122967784868,
        -0.7431440506374485,
        -0.7549349233181779,
        -0.7242099856351973,
        -0.7237454815851778,
        -0.7367255802920942,
        -0.7402867091105166,
        -0.7074175155054491,
        -0.753295209461537,
        -0.7605212330905877,
        -0.7348569733357222,
        -0.7418856004956313,
        -0.7346449443156926,
        -0.7435656460638514,
        -0.7348626684718164,
        -0.7442611523569331,
        -0.7532847520953526,
        -0.7530280350527307,
        -0.744669113647805,
        -0.7473117810912333,
        1.0,
        -0.7530099355737547,
        -0.7436116019003999
      ],
      [
        -0.7605962454637165,
        -0.7507266489896236,
        -0.7621745292725915,
        -0.7620005971760276,
        -0.7390556490630855,
        -0.7638032090742591,
        -0.7634491600092308,
        -0.7346102882602639,
        -0.746376571145989,
        -0.7616893048438711,
        -0.7437855617994584,
        -0.75712129934131,
        -0.7602138466188516,
        -0.7497168390466757,
        -0.7612662511240067,
        -0.7500370438669239,
        -0.7564880490757102,
        -0.7426781384829386,
        -0.7619449741956288,
        -0.7388617140614187,
        -0.7579823961098809,
        -0.7630194015724017,
        -0.7394040653529226,
        -0.757699764047768,
        -0.7570111005309992,
        -0.7574104350726447,
        -0.7525521968921043,
        -0.7461018208789472,
        -0.7404763316415001,
        -0.7631616921876223,
        -0.7600760889744533,
        -0.7555022158245062,
        -0.7530099355737547,
        1.0,
        -0.7631165991918174
      ],
      [
        -0.7478604427805806,
        -0.7335546176136484,
        -0.76123424559617,
        -0.7497037333679004,
        -0.7593856837548052,
        -0.7557925522157714,
        -0.7576221727534265,
        -0.7572261871712032,
        -0.7515170924128958,
        -0.7612108392534978,
        -0.7589726833546685,
        -0.754333511543312,
        -0.7615966957411722,
        -0.7600998071839358,
        -0.7615939715307776,
        -0.7449161628290841,
        -0.762837979751636,
        -0.7580167449456487,
        -0.7639194528073907,
        -0.7485698511183194,
        -0.7563240491656963,
        -0.761787859961651,
        -0.7550941373519511,
        -0.7427961801650107,
        -0.7583424945814805,
        -0.7585860607756221,
        -0.7500583976975239,
        -0.7504416863792792,
        -0.7545528129024248,
        -0.7612678890673632,
        -0.763422568599156,
        -0.7562313468495581,
        -0.7436116019003999,
        -0.7631165991918174,
        1.0
      ]
    ],
    "series": {
      "2020-01": [
        117,
        49,
        18,
        40,
        21,
        30,
        13,
        17,
        93,
        5,
        40,
        17,
        17,
        24,
        15,
        16,
        14,
        27,
        13,
        28,
        30,
        11,
        38,
        13,
        18,
        13,
        17,
        12,
        5,
        17,
        5,
        35,
        20,
        19,
        8
      ],
      "2020-02": [
        199,
        67,
        21,
        71,
        34,
        68,
        24,
        15,
        185,
        9,
        107,
        36,
        68,
        42,
        27,
        29,
        21,
        55,
        27,
        29,
        37,
        28,
        73,
        25,
        34,
        17,
        35,
        19,
        13,
        22,
        12,
        49,
        32,
        36,
        7
      ],
      "2020-03": [
        131,
        63,
        14,
        58,
        31,
        36,
        12,
        16,
        142,
        7,
        62,
        16,
        25,
        27,
        15,
        31,
        31,
        43,
        6,
        24,
        29,
        8,
        49,
        21,
        25,
        8,
        27,
        20,
        12,
        13,
        7,
        38,
        23,
        10,
        19
      ],
      "2020-04": [
        131,
        70,
        17,
        31,
        27,
        27,
        11,
        26,
        111,
        7,
        62,
        18,
        17,
        27,
        12,
        37,
        15,
        34,
        4,
        27,
        24,
        8,
        65,
        16,
        19,
        16,
        20,
        20,
        18,
        12,
        11,
        38,
        18,
        19,
        9
      ],
      "2020-05": [
        127,
        50,
        14,
        41,
        19,
        26,
        12,
        6,
        97,
        11,
        54,
        14,
        29,
        24,
        17,
        36,
        21,
        37,
        19,
        29,
        29,
        6,
        38,
        17,
        22,
        18,
        21,
        17,
        7,
        8,
        8,
        39,
        18,
        18,
        14
      ],
      "2020-06": [
        246,
        78,
        33,
        136,
        47,
        84,
        24,
        16,
        265,
        11,
        116,
        33,
        103,
        55,
        38,
        60,
        42,
        82,
        44,
        44,
        55,
        38,
        114,
        36,
        56,
        27,
        48,
        31,
        7,
        30,
        34,
        79,
        45,
        20,
        17
      ],
      "2020-07": [
        168,
        50,
        24,
        74,
        33,
        50,
        13,
        11,
        142,
        4,
        80,
        31,
        45,
        43,
        21,
        41,
        33,
        49,
        17,
        34,
        25,
        12,
        79,
        28,
        30,
        22,
        49,
        22,
        5,
        19,
        10,
        43,
        42,
        16,
        14
      ],
      "2020-08": [
        109,
        46,
        21,
        38,
        15,
        18,
        20,
        9,
        63,
        4,
        39,
        25,
        34,
        26,
        12,
        33,
        23,
        19,
        6,
        24,
        20,
        13,
        34,
        11,
        16,
        9,
        24,
        13,
        6,
        13,
        5,
        38,
        39,
        20,
        7
      ],
      "2020-09": [
        106,
        49,
        18,
        65,
        16,
        35,
        11,
        13,
        95,
        5,
        52,
        19,
        29,
        23,
        10,
        32,
        26,
        27,
        13,
        26,
        20,
        6,
        52,
        10,
        20,
        12,
        17,
        16,
        3,
        11,
        6,
        46,
        31,
        13,
        13
      ],
      "2020-10": [
        91,
        44,
        29,
        44,
        22,
        48,
        17,
        5,
        91,
        7,
        50,
        8,
        37,
        31,
        26,
        28,
        16,
        24,
        16,
        20,
        37,
        16,
        32,
        16,
        22,
        13,
        34,
        4,
        1,
        13,
        14,
        25,
        12,
        9,
        1
      ],
      "2020-11": [
        67,
        17,
        15,
        18,
        8,
        27,
        7,
        3,
        46,
        9,
        12,
        6,
        21,
        16,
        9,
        21,
        11,
        13,
        9,
        16,
        21,
        8,
        16,
        6,
        10,
        8,
        17,
        2,
        2,
        7,
        6,
        21,
        5,
        2,
        1
      ],
      "2020-12": [
        40,
        26,
        15,
        13,
        7,
        21,
        14,
        4,
        46,
        9,
        17,
        10,
        24,
        8,
        14,
        15,
        6,
        23,
        11,
        12,
        14,
        11,
        12,
        9,
        13,
        8,
        12,
        2,
        0,
        10,
        12,
        21,
        9,
        4,
        2
      ],
      "2021-01": [
        22,
        29,
        10,
        14,
        7,
        12,
        9,
        2,
        50,
        2,
        16,
        5,
        8,
        7,
        8,
        10,
        5,
        10,
        5,
        6,
        16,
        7,
        12,
        5,
        11,
        9,
        13,
        1,
        0,
        5,
        4,
        11,
        3,
        1,
        4
      ],
      "2021-02": [
        62,
        24,
        20,
        21,
        11,
        35,
        8,
        3,
        50,
        4,
        29,
        6,
        40,
        15,
        11,
        15,
        18,
        17,
        9,
        16,
        23,
        21,
        23,
        12,
        18,
        11,
        24,
        5,
        0,
        9,
        7,
        20,
        6,
        3,
        2
      ],
      "2021-03": [
        53,
        14,
        25,
        18,
        12,
        18,
        7,
        3,
        46,
        4,
        14,
        8,
        15,
        11,
        12,
        15,
        11,
        10,
        8,
        5,
        17,
        12,
        12,
        12,
        12,
        12,
        10,
        1,
        0,
        8,
        6,
        17,
        6,
        4,
        0
      ],
      "2021-04": [
        37,
        21,
        15,
        10,
        6,
        11,
        7,
        3,
        28,
        3,
        17,
        5,
        8,
        8,
        10,
        11,
        7,
        10,
        7,
        7,
        21,
        6,
        8,
        8,
        13,
        4,
        15,
        5,
        2,
        7,
        7,
        11,
        7,
        1,
        4
      ],
      "2021-05": [
        42,
        12,
        12,
        18,
        12,
        21,
        12,
        2,
        43,
        6,
        16,
        3,
        25,
        12,
        11,
        7,
        3,
        10,
        8,
        5,
        23,
        10,
        13,
        9,
        13,
        4,
        24,
        4,
        1,
        8,
        4,
        18,
        9,
        2,
        2
      ],
      "2021-06": [
        75,
        25,
        24,
        32,
        19,
        38,
        17,
        3,
        70,
        4,
        18,
        16,
        31,
        23,
        11,
        23,
        18,
        31,
        13,
        13,
        37,
        14,
        25,
        10,
        26,
        19,
        35,
        3,
        1,
        7,
        12,
        27,
        8,
        1,
        0
      ],
      "2021-07": [
        36,
        22,
        20,
        19,
        11,
        22,
        6,
        3,
        44,
        3,
        6,
        2,
        15,
        11,
        13,
        11,
        7,
        15,
        2,
        11,
        20,
        10,
        8,
        10,
        10,
        6,
        24,
        4,
        1,
        4,
        7,
        12,
        5,
        2,
        0
      ],
      "2021-08": [
        19,
        8,
        13,
        4,
        6,
        11,
        6,
        0,
        12,
        2,
        7,
        4,
        12,
        4,
        11,
        7,
        4,
        8,
        2,
        1,
        11,
        6,
        3,
        1,
        5,
        10,
        11,
        2,
        0,
        11,
        3,
        9,
        1,
        1,
        3
      ],
      "2021-09": [
        38,
        24,
        17,
        11,
        7,
        16,
        10,
        2,
        33,
        3,
        6,
        6,
        14,
        10,
        6,
        8,
        4,
        6,
        11,
        4,
        13,
        11,
        10,
        3,
        9,
        4,
        13,
        0,
        0,
        10,
        3,
        13,
        8,
        0,
        2
      ],
      "2021-10": [
        64,
        28,
        21,
        28,
        15,
        35,
        7,
        5,
        40,
        7,
        17,
        10,
        19,
        17,
        15,
        23,
        18,
        12,
        12,
        6,
        26,
        18,
        20,
        11,
        16,
        10,
        23,
        7,
        2,
        10,
        10,
        18,
        14,
        2,
        1
      ],
      "2021-11": [
        34,
        26,
        29,
        23,
        8,
        25,
        3,
        1,
        36,
        1,
        17,
        4,
        19,
        8,
        15,
        9,
        13,
        16,
        6,
        13,
        20,
        7,
        8,
        8,
        18,
        9,
        11,
        5,
        1,
        10,
        7,
        13,
        5,
        2,
        0
      ],
      "2021-12": [
        40,
        15,
        18,
        17,
        8,
        18,
        3,
        2,
        32,
        5,
        13,
        4,
        14,
        8,
        6,
        6,
        3,
        14,
        5,
        15,
        23,
        10,
        10,
        4,
        10,
        6,
        13,
        3,
        2,
        7,
        3,
        13,
        2,
        1,
        1
      ],
      "2022-01": [
        40,
        15,
        14,
        10,
        7,
        14,
        7,
        4,
        30,
        1,
        13,
        8,
        14,
        10,
        9,
        17,
        3,
        7,
        7,
        1,
        9,
        10,
        10,
        11,
        7,
        3,
        9,
        2,
        1,
        5,
        5,
        13,
        5,
        2,
        0
      ],
      "2022-02": [
        59,
        24,
        24,
        18,
        14,
        38,
        9,
        3,
        47,
        8,
        10,
        11,
        28,
        17,
        15,
        9,
        16,
        15,
        15,
        9,
        18,
        9,
        11,
        7,
        16,
        6,
        25,
        1,
        1,
        4,
        11,
        21,
        8,
        4,
        2
      ],
      "2022-03": [
        50,
        19,
        27,
        18,
        10,
        14,
        10,
        4,
        32,
        9,
        17,
        1,
        18,
        6,
        11,
        16,
        8,
        10,
        6,
        9,
        23,
        14,
        15,
        6,
        15,
        9,
        22,
        1,
        0,
        5,
        10,
        24,
        5,
        4,
        1
      ],
      "2022-04": [
        25,
        21,
        13,
        13,
        6,
        9,
        6,
        0,
        22,
        6,
        11,
        7,
        13,
        7,
        8,
        10,
        6,
        14,
        0,
        8,
        21,
        6,
        6,
        5,
        6,
        8,
        10,
        0,
        0,
        3,
        3,
        10,
        3,
        3,
        1
      ],
      "2022-05": [
        43,
        22,
        24,
        11,
        11,
        37,
        6,
        1,
        56,
        3,
        12,
        5,
        21,
        12,
        13,
        14,
        12,
        10,
        12,
        10,
        29,
        12,
        13,
        15,
        14,
        9,
        22,
        1,
        0,
        9,
        13,
        15,
        7,
        1,
        1
      ],
      "2022-06": [
        73,
        13,
        32,
        22,
        24,
        45,
        9,
        3,
        60,
        4,
        19,
        14,
        32,
        10,
        10,
        19,
        17,
        21,
        9,
        13,
        26,
        12,
        16,
        15,
        21,
        9,
        23,
        3,
        0,
        9,
        15,
        16,
        6,
        2,
        1
      ],
      "2022-07": [
        22,
        12,
        14,
        13,
        7,
        14,
        6,
        4,
        30,
        1,
        9,
        5,
        16,
        7,
        6,
        11,
        13,
        12,
        9,
        8,
        9,
        7,
        8,
        10,
        7,
        7,
        16,
        2,
        0,
        13,
        6,
        8,
        7,
        2,
        2
      ],
      "2022-08": [
        39,
        13,
        14,
        14,
        11,
        8,
        5,
        0,
        37,
        2,
        12,
        2,
        11,
        5,
        7,
        15,
        5,
        13,
        3,
        9,
        15,
        1,
        6,
        4,
        9,
        7,
        7,
        0,
        3,
        3,
        1,
        15,
        4,
        1,
        3
      ],
      "2022-09": [
        32,
        19,
        20,
        17,
        14,
        14,
        9,
        3,
        38,
        5,
        3,
        6,
        12,
        8,
        15,
        14,
        10,
        9,
        6,
        12,
        20,
        10,
        5,
        9,
        8,
        6,
        10,
        1,
        1,
        2,
        5,
        16,
        3,
        2,
        0
      ],
      "2022-10": [
        54,
        28,
        20,
        24,
        9,
        32,
        6,
        8,
        44,
        6,
        15,
        14,
        23,
        16,
        12,
        15,
        18,
        20,
        11,
        7,
        17,
        14,
        13,
        24,
        16,
        14,
        24,
        5,
        1,
        7,
        17,
        18,
        7,
        5,
        3
      ],
      "2022-11": [
        44,
        8,
        24,
        20,
        11,
        19,
        12,
        2,
        38,
        3,
        10,
        6,
        22,
        6,
        7,
        14,
        7,
        13,
        13,
        7,
        22,
        5,
        12,
        8,
        10,
        5,
        22,
        1,
        2,
        7,
        13,
        17,
        10,
        2,
        1
      ],
      "2022-12": [
        34,
        12,
        15,
        8,
        11,
        15,
        8,
        4,
        29,
        4,
        5,
        4,
        11,
        2,
        6,
        4,
        8,
        12,
        5,
        8,
        12,
        4,
        12,
        3,
        6,
        9,
        12,
        3,
        1,
        3,
        5,
        10,
        4,
        3,
        1
      ],
      "2023-01": [
        47,
        18,
        23,
        14,
        12,
        18,
        5,
        0,
        24,
        2,
        7,
        6,
        17,
        7,
        7,
        6,
        13,
        13,
        7,
        8,
        11,
        12,
        8,
        11,
        14,
        8,
        15,
        1,
        0,
        4,
        2,
        6,
        6,
        2,
        2
      ],
      "2023-02": [
        50,
        14,
        20,
        16,
        19,
        30,
        14,
        5,
        46,
        8,
        19,
        8,
        41,
        13,
        11,
        13,
        17,
        19,
        13,
        10,
        28,
        13,
        13,
        17,
        16,
        9,
        17,
        3,
        1,
        8,
        11,
        13,
        9,
        1,
        1
      ],
      "2023-03": [
        38,
        16,
        13,
        9,
        17,
        30,
        13,
        4,
        44,
        3,
        10,
        3,
        21,
        10,
        10,
        12,
        8,
        13,
        3,
        12,
        11,
        12,
        12,
        13,
        11,
        10,
        15,
        2,
        1,
        8,
        12,
        16,
        5,
        2,
        1
      ],
      "2023-04": [
        35,
        15,
        17,
        4,
        12,
        12,
        4,
        4,
        33,
        4,
        12,
        5,
        11,
        8,
        11,
        6,
        8,
        11,
        3,
        8,
        15,
        12,
        4,
        2,
        11,
        6,
        12,
        1,
        1,
        3,
        9,
        13,
        4,
        0,
        3
      ],
      "2023-05": [
        61,
        25,
        25,
        22,
        29,
        23,
        16,
        10,
        60,
        6,
        12,
        10,
        30,
        10,
        24,
        12,
        9,
        25,
        10,
        10,
        20,
        7,
        14,
        13,
        19,
        12,
        23,
        3,
        1,
        9,
        8,
        20,
        3,
        1,
        2
      ],
      "2023-06": [
        62,
        22,
        43,
        26,
        18,
        30,
        11,
        11,
        52,
        7,
        26,
        6,
        34,
        11,
        8,
        13,
        18,
        19,
        9,
        13,
        20,
        12,
        13,
        16,
        15,
        8,
        17,
        3,
        0,
        9,
        14,
        12,
        6,
        5,
        3
      ],
      "2023-07": [
        44,
        19,
        17,
        13,
        12,
        22,
        4,
        6,
        29,
        4,
        9,
        4,
        13,
        5,
        13,
        9,
        7,
        19,
        3,
        7,
        14,
        15,
        8,
        15,
        7,
        9,
        16,
        1,
        2,
        8,
        9,
        12,
        4,
        0,
        6
      ],
      "2023-08": [
        22,
        19,
        16,
        7,
        4,
        13,
        6,
        6,
        23,
        2,
        4,
        4,
        14,
        4,
        7,
        13,
        4,
        6,
        2,
        13,
        13,
        9,
        10,
        3,
        6,
        4,
        10,
        1,
        0,
        2,
        5,
        14,
        8,
        3,
        3
      ],
      "2023-09": [
        43,
        15,
        11,
        13,
        13,
        14,
        5,
        9,
        38,
        1,
        9,
        5,
        10,
        4,
        9,
        7,
        9,
        11,
        5,
        5,
        17,
        7,
        6,
        7,
        13,
        6,
        25,
        1,
        1,
        5,
        7,
        8,
        8,
        8,
        0
      ],
      "2023-10": [
        70,
        14,
        38,
        20,
        33,
        32,
        14,
        16,
        48,
        8,
        21,
        4,
        25,
        8,
        20,
        7,
        22,
        22,
        16,
        14,
        28,
        26,
        20,
        19,
        21,
        14,
        27,
        1,
        6,
        9,
        8,
        18,
        3,
        2,
        2
      ],
      "2023-11": [
        46,
        22,
        15,
        17,
        21,
        17,
        7,
        6,
        30,
        5,
        1,
        3,
        21,
        5,
        8,
        14,
        7,
        12,
        6,
        6,
        22,
        13,
        12,
        10,
        12,
        7,
        17,
        0,
        1,
        5,
        9,
        16,
        2,
        2,
        3
      ],
      "2023-12": [
        44,
        24,
        17,
        8,
        13,
        28,
        5,
        11,
        32,
        5,
        9,
        8,
        13,
        8,
        7,
        9,
        11,
        20,
        6,
        17,
        15,
        6,
        13,
        14,
        14,
        6,
        17,
        1,
        1,
        6,
        11,
        8,
        9,
        1,
        1
      ],
      "2024-01": [
        36,
        10,
        26,
        8,
        8,
        12,
        6,
        1,
        25,
        3,
        11,
        3,
        16,
        4,
        11,
        7,
        9,
        10,
        7,
        10,
        9,
        14,
        2,
        8,
        13,
        9,
        7,
        1,
        0,
        4,
        3,
        11,
        2,
        1,
        1
      ],
      "2024-02": [
        79,
        21,
        36,
        24,
        27,
        34,
        13,
        19,
        44,
        12,
        11,
        6,
        39,
        13,
        20,
        15,
        13,
        21,
        10,
        13,
        29,
        12,
        16,
        24,
        26,
        9,
        20,
        5,
        0,
        12,
        15,
        24,
        7,
        1,
        2
      ],
      "2024-03": [
        47,
        16,
        25,
        8,
        12,
        21,
        7,
        6,
        39,
        2,
        11,
        4,
        17,
        10,
        12,
        12,
        13,
        22,
        11,
        4,
        16,
        12,
        11,
        20,
        10,
        3,
        18,
        2,
        1,
        6,
        11,
        10,
        5,
        1,
        0
      ],
      "2024-04": [
        33,
        18,
        15,
        11,
        15,
        14,
        3,
        7,
        30,
        2,
        10,
        4,
        7,
        4,
        8,
        10,
        8,
        14,
        7,
        13,
        11,
        7,
        4,
        13,
        8,
        5,
        13,
        1,
        1,
        4,
        7,
        5,
        4,
        1,
        0
      ],
      "2024-05": [
        55,
        17,
        35,
        16,
        27,
        40,
        10,
        22,
        54,
        5,
        14,
        8,
        31,
        8,
        22,
        14,
        15,
        21,
        13,
        14,
        15,
        18,
        15,
        33,
        15,
        13,
        33,
        3,
        1,
        6,
        9,
        14,
        8,
        0,
        0
      ],
      "2024-06": [
        52,
        29,
        31,
        21,
        29,
        22,
        13,
        32,
        57,
        8,
        16,
        4,
        23,
        8,
        21,
        17,
        18,
        17,
        6,
        8,
        16,
        14,
        14,
        22,
        14,
        11,
        20,
        3,
        0,
        14,
        11,
        15,
        6,
        2,
        1
      ],
      "2024-07": [
        47,
        20,
        20,
        11,
        14,
        13,
        8,
        12,
        23,
        7,
        9,
        7,
        11,
        5,
        15,
        10,
        9,
        14,
        7,
        4,
        16,
        10,
        11,
        12,
        3,
        6,
        16,
        0,
        0,
        6,
        7,
        19,
        6,
        3,
        3
      ],
      "2024-08": [
        41,
        23,
        19,
        12,
        8,
        9,
        4,
        11,
        34,
        8,
        6,
        3,
        8,
        4,
        8,
        10,
        12,
        12,
        7,
        12,
        18,
        6,
        5,
        9,
        6,
        9,
        15,
        0,
        1,
        5,
        1,
        12,
        7,
        1,
        1
      ],
      "2024-09": [
        38,
        21,
        24,
        12,
        21,
        13,
        4,
        16,
        35,
        5,
        12,
        3,
        23,
        5,
        12,
        8,
        6,
        13,
        5,
        9,
        23,
        8,
        8,
        15,
        8,
        12,
        17,
        0,
        0,
        8,
        10,
        10,
        4,
        1,
        3
      ],
      "2024-10": [
        87,
        43,
        39,
        32,
        49,
        38,
        15,
        32,
        74,
        7,
        32,
        14,
        36,
        10,
        27,
        16,
        12,
        32,
        10,
        11,
        31,
        11,
        21,
        14,
        24,
        4,
        25,
        2,
        3,
        17,
        14,
        16,
        7,
        1,
        2
      ],
      "2024-11": [
        50,
        28,
        40,
        18,
        19,
        14,
        7,
        16,
        39,
        5,
        7,
        8,
        11,
        13,
        13,
        10,
        9,
        18,
        10,
        7,
        24,
        9,
        10,
        24,
        14,
        9,
        10,
        3,
        2,
        7,
        7,
        14,
        7,
        1,
        2
      ],
      "2024-12": [
        33,
        30,
        28,
        24,
        18,
        19,
        9,
        23,
        31,
        10,
        11,
        6,
        20,
        9,
        22,
        9,
        9,
        11,
        9,
        11,
        17,
        11,
        7,
        13,
        14,
        6,
        12,
        2,
        0,
        10,
        8,
        18,
        6,
        0,
        1
      ],
      "2025-01": [
        54,
        24,
        23,
        13,
        20,
        24,
        4,
        14,
        26,
        9,
        8,
        9,
        17,
        4,
        9,
        9,
        9,
        10,
        5,
        11,
        11,
        9,
        11,
        22,
        17,
        10,
        14,
        0,
        0,
        4,
        8,
        15,
        3,
        2,
        1
      ],
      "2025-02": [
        64,
        32,
        34,
        22,
        40,
        32,
        11,
        36,
        46,
        2,
        19,
        3,
        27,
        9,
        17,
        21,
        19,
        28,
        10,
        8,
        34,
        12,
        15,
        36,
        22,
        10,
        20,
        3,
        1,
        7,
        16,
        19,
        4,
        3,
        0
      ],
      "2025-03": [
        47,
        15,
        26,
        17,
        26,
        23,
        7,
        16,
        35,
        9,
        8,
        8,
        12,
        6,
        13,
        12,
        13,
        17,
        11,
        8,
        21,
        8,
        9,
        22,
        18,
        5,
        24,
        0,
        0,
        5,
        8,
        17,
        2,
        0,
        0
      ],
      "2025-04": [
        48,
        16,
        18,
        9,
        12,
        7,
        9,
        13,
        24,
        4,
        9,
        6,
        11,
        7,
        11,
        16,
        11,
        15,
        9,
        10,
        9,
        8,
        12,
        14,
        12,
        9,
        14,
        2,
        1,
        2,
        10,
        18,
        6,
        0,
        0
      ],
      "2025-05": [
        80,
        30,
        45,
        23,
        39,
        31,
        13,
        44,
        55,
        10,
        11,
        11,
        37,
        14,
        22,
        14,
        13,
        22,
        11,
        21,
        36,
        25,
        15,
        35,
        12,
        8,
        34,
        5,
        2,
        6,
        11,
        22,
        5,
        0,
        4
      ],
      "2025-06": [
        62,
        25,
        35,
        17,
        31,
        18,
        7,
        36,
        45,
        6,
        21,
        8,
        14,
        7,
        18,
        16,
        16,
        20,
        9,
        12,
        27,
        18,
        11,
        26,
        17,
        13,
        25,
        1,
        2,
        10,
        10,
        18,
        4,
        1,
        3
      ],
      "2025-07": [
        50,
        27,
        30,
        9,
        18,
        10,
        10,
        14,
        23,
        6,
        7,
        4,
        14,
        6,
        10,
        18,
        14,
        20,
        9,
        6,
        18,
        14,
        6,
        29,
        10,
        11,
        16,
        1,
        1,
        5,
        10,
        18,
        4,
        0,
        2
      ],
      "2025-08": [
        36,
        11,
        17,
        7,
        18,
        17,
        3,
        15,
        20,
        4,
        8,
        5,
        7,
        3,
        16,
        10,
        11,
        11,
        9,
        5,
        18,
        6,
        6,
        17,
        16,
        7,
        11,
        0,
        1,
        2,
        5,
        12,
        4,
        2,
        2
      ],
      "2025-09": [
        19,
        5,
        9,
        7,
        9,
        5,
        5,
        5,
        12,
        5,
        4,
        1,
        8,
        3,
        6,
        4,
        5,
        6,
        8,
        3,
        7,
        8,
        5,
        8,
        3,
        3,
        17,
        0,
        0,
        2,
        4,
        3,
        1,
        0,
        0
      ]
    },
    "papers": {
      "0": [
        {
          "title": "MOPO: Model-based Offline Policy Optimization",
          "year": "2020-05",
          "abstract": "Offline reinforcement learning (RL) refers to the problem of learning\npolicies entirely from a large batch of previously collected data. This problem\nsetting offers the promise of utilizing such datasets to acquire policies\nwithout any costly or dangerous active exploration. However, it is also\nchallenging, due to the distributional shift between the offline training data\nand those states visited by the learned policy. Despite significant recent\nprogress, the most successful prior methods are model-free and constrain the\npolicy to the support of data, precluding generalization to unseen states. In\nthis paper, we first observe that an existing model-based RL algorithm already\nproduces significant gains in the offline setting compared to model-free\napproaches. However, standard model-based RL methods, designed for the online\nsetting, do not provide an explicit mechanism to avoid the offline setting's\ndistributional shift issue. Instead, we propose to modify the existing\nmodel-based RL methods by applying them with rewards artificially penalized by\nthe uncertainty of the dynamics. We theoretically show that the algorithm\nmaximizes a lower bound of the policy's return under the true MDP. We also\ncharacterize the trade-off between the gain and risk of leaving the support of\nthe batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO),\noutperforms standard model-based RL algorithms and prior state-of-the-art\nmodel-free offline RL algorithms on existing offline RL benchmarks and two\nchallenging continuous control tasks that require generalizing from data\ncollected for a different task. The code is available at\nhttps://github.com/tianheyu927/mopo.",
          "arxiv_id": "2005.13239v6"
        },
        {
          "title": "The Value-Improvement Path: Towards Better Representations for Reinforcement Learning",
          "year": "2020-06",
          "abstract": "In value-based reinforcement learning (RL), unlike in supervised learning,\nthe agent faces not a single, stationary, approximation problem, but a sequence\nof value prediction problems. Each time the policy improves, the nature of the\nproblem changes, shifting both the distribution of states and their values. In\nthis paper we take a novel perspective, arguing that the value prediction\nproblems faced by an RL agent should not be addressed in isolation, but rather\nas a single, holistic, prediction problem. An RL algorithm generates a sequence\nof policies that, at least approximately, improve towards the optimal policy.\nWe explicitly characterize the associated sequence of value functions and call\nit the value-improvement path. Our main idea is to approximate the\nvalue-improvement path holistically, rather than to solely track the value\nfunction of the current policy. Specifically, we discuss the impact that this\nholistic view of RL has on representation learning. We demonstrate that a\nrepresentation that spans the past value-improvement path will also provide an\naccurate value approximation for future policy improvements. We use this\ninsight to better understand existing approaches to auxiliary tasks and to\npropose new ones. To test our hypothesis empirically, we augmented a standard\ndeep RL agent with an auxiliary task of learning the value-improvement path. In\na study of Atari 2600 games, the augmented agent achieved approximately double\nthe mean and median performance of the baseline agent.",
          "arxiv_id": "2006.02243v2"
        },
        {
          "title": "Reinforcement Learning",
          "year": "2020-05",
          "abstract": "Reinforcement learning (RL) is a general framework for adaptive control,\nwhich has proven to be efficient in many domains, e.g., board games, video\ngames or autonomous vehicles. In such problems, an agent faces a sequential\ndecision-making problem where, at every time step, it observes its state,\nperforms an action, receives a reward and moves to a new state. An RL agent\nlearns by trial and error a good policy (or controller) based on observations\nand numeric reward feedback on the previously performed action. In this\nchapter, we present the basic framework of RL and recall the two main families\nof approaches that have been developed to learn a good policy. The first one,\nwhich is value-based, consists in estimating the value of an optimal policy,\nvalue from which a policy can be recovered, while the other, called policy\nsearch, directly works in a policy space. Actor-critic methods can be seen as a\npolicy search technique where the policy value that is learned guides the\npolicy improvement. Besides, we give an overview of some extensions of the\nstandard RL framework, notably when risk-averse behavior needs to be taken into\naccount or when rewards are not available or not known.",
          "arxiv_id": "2005.14419v2"
        }
      ],
      "1": [
        {
          "title": "A machine learning approach for forecasting hierarchical time series",
          "year": "2020-05",
          "abstract": "In this paper, we propose a machine learning approach for forecasting\nhierarchical time series. When dealing with hierarchical time series, apart\nfrom generating accurate forecasts, one needs to select a suitable method for\nproducing reconciled forecasts. Forecast reconciliation is the process of\nadjusting forecasts to make them coherent across the hierarchy. In literature,\ncoherence is often enforced by using a post-processing technique on the base\nforecasts produced by suitable time series forecasting methods. On the\ncontrary, our idea is to use a deep neural network to directly produce accurate\nand reconciled forecasts. We exploit the ability of a deep neural network to\nextract information capturing the structure of the hierarchy. We impose the\nreconciliation at training time by minimizing a customized loss function. In\nmany practical applications, besides time series data, hierarchical time series\ninclude explanatory variables that are beneficial for increasing the\nforecasting accuracy. Exploiting this further information, our approach links\nthe relationship between time series features extracted at any level of the\nhierarchy and the explanatory variables into an end-to-end neural network\nproviding accurate and reconciled point forecasts. The effectiveness of the\napproach is validated on three real-world datasets, where our method\noutperforms state-of-the-art competitors in hierarchical forecasting.",
          "arxiv_id": "2006.00630v2"
        },
        {
          "title": "Dive into Time-Series Anomaly Detection: A Decade Review",
          "year": "2024-12",
          "abstract": "Recent advances in data collection technology, accompanied by the ever-rising\nvolume and velocity of streaming data, underscore the vital need for time\nseries analytics. In this regard, time-series anomaly detection has been an\nimportant activity, entailing various applications in fields such as cyber\nsecurity, financial markets, law enforcement, and health care. While\ntraditional literature on anomaly detection is centered on statistical\nmeasures, the increasing number of machine learning algorithms in recent years\ncall for a structured, general characterization of the research methods for\ntime-series anomaly detection. This survey groups and summarizes anomaly\ndetection existing solutions under a process-centric taxonomy in the time\nseries context. In addition to giving an original categorization of anomaly\ndetection methods, we also perform a meta-analysis of the literature and\noutline general trends in time-series anomaly detection research.",
          "arxiv_id": "2412.20512v1"
        },
        {
          "title": "Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case",
          "year": "2020-01",
          "abstract": "In this paper, we present a new approach to time series forecasting. Time\nseries data are prevalent in many scientific and engineering disciplines. Time\nseries forecasting is a crucial task in modeling time series data, and is an\nimportant area of machine learning. In this work we developed a novel method\nthat employs Transformer-based machine learning models to forecast time series\ndata. This approach works by leveraging self-attention mechanisms to learn\ncomplex patterns and dynamics from time series data. Moreover, it is a generic\nframework and can be applied to univariate and multivariate time series data,\nas well as time series embeddings. Using influenza-like illness (ILI)\nforecasting as a case study, we show that the forecasting results produced by\nour approach are favorably comparable to the state-of-the-art.",
          "arxiv_id": "2001.08317v1"
        }
      ],
      "2": [
        {
          "title": "Causal Mediation Analysis with Hidden Confounders",
          "year": "2021-02",
          "abstract": "An important problem in causal inference is to break down the total effect of\na treatment on an outcome into different causal pathways and to quantify the\ncausal effect in each pathway. For instance, in causal fairness, the total\neffect of being a male employee (i.e., treatment) constitutes its direct effect\non annual income (i.e., outcome) and the indirect effect via the employee's\noccupation (i.e., mediator). Causal mediation analysis (CMA) is a formal\nstatistical framework commonly used to reveal such underlying causal\nmechanisms. One major challenge of CMA in observational studies is handling\nconfounders, variables that cause spurious causal relationships among\ntreatment, mediator, and outcome. Conventional methods assume sequential\nignorability that implies all confounders can be measured, which is often\nunverifiable in practice. This work aims to circumvent the stringent sequential\nignorability assumptions and consider hidden confounders. Drawing upon proxy\nstrategies and recent advances in deep learning, we propose to simultaneously\nuncover the latent variables that characterize hidden confounders and estimate\nthe causal effects. Empirical evaluations using both synthetic and\nsemi-synthetic datasets validate the effectiveness of the proposed method. We\nfurther show the potentials of our approach for causal fairness analysis.",
          "arxiv_id": "2102.11724v3"
        },
        {
          "title": "Nonparametric Identifiability of Causal Representations from Unknown Interventions",
          "year": "2023-06",
          "abstract": "We study causal representation learning, the task of inferring latent causal\nvariables and their causal relations from high-dimensional mixtures of the\nvariables. Prior work relies on weak supervision, in the form of counterfactual\npre- and post-intervention views or temporal structure; places restrictive\nassumptions, such as linearity, on the mixing function or latent causal model;\nor requires partial knowledge of the generative process, such as the causal\ngraph or intervention targets. We instead consider the general setting in which\nboth the causal model and the mixing function are nonparametric. The learning\nsignal takes the form of multiple datasets, or environments, arising from\nunknown interventions in the underlying causal model. Our goal is to identify\nboth the ground truth latents and their causal graph up to a set of ambiguities\nwhich we show to be irresolvable from interventional data. We study the\nfundamental setting of two causal variables and prove that the observational\ndistribution and one perfect intervention per node suffice for identifiability,\nsubject to a genericity condition. This condition rules out spurious solutions\nthat involve fine-tuning of the intervened and observational distributions,\nmirroring similar conditions for nonlinear cause-effect inference. For an\narbitrary number of variables, we show that at least one pair of distinct\nperfect interventional domains per node guarantees identifiability. Further, we\ndemonstrate that the strengths of causal influences among the latent variables\nare preserved by all equivalent solutions, rendering the inferred\nrepresentation appropriate for drawing causal conclusions from new data. Our\nstudy provides the first identifiability results for the general nonparametric\nsetting with unknown interventions, and elucidates what is possible and\nimpossible for causal representation learning without more direct supervision.",
          "arxiv_id": "2306.00542v2"
        },
        {
          "title": "Active Bayesian Causal Inference",
          "year": "2022-06",
          "abstract": "Causal discovery and causal reasoning are classically treated as separate and\nconsecutive tasks: one first infers the causal graph, and then uses it to\nestimate causal effects of interventions. However, such a two-stage approach is\nuneconomical, especially in terms of actively collected interventional data,\nsince the causal query of interest may not require a fully-specified causal\nmodel. From a Bayesian perspective, it is also unnatural, since a causal query\n(e.g., the causal graph or some causal effect) can be viewed as a latent\nquantity subject to posterior inference -- other unobserved quantities that are\nnot of direct interest (e.g., the full causal model) ought to be marginalized\nout in this process and contribute to our epistemic uncertainty. In this work,\nwe propose Active Bayesian Causal Inference (ABCI), a fully-Bayesian active\nlearning framework for integrated causal discovery and reasoning, which jointly\ninfers a posterior over causal models and queries of interest. In our approach\nto ABCI, we focus on the class of causally-sufficient, nonlinear additive noise\nmodels, which we model using Gaussian processes. We sequentially design\nexperiments that are maximally informative about our target causal query,\ncollect the corresponding interventional data, and update our beliefs to choose\nthe next experiment. Through simulations, we demonstrate that our approach is\nmore data-efficient than several baselines that only focus on learning the full\ncausal graph. This allows us to accurately learn downstream causal queries from\nfewer samples while providing well-calibrated uncertainty estimates for the\nquantities of interest.",
          "arxiv_id": "2206.02063v2"
        }
      ],
      "3": [
        {
          "title": "Temporal Graph Rewiring with Expander Graphs",
          "year": "2024-06",
          "abstract": "Evolving relations in real-world networks are often modelled by temporal\ngraphs. Temporal Graph Neural Networks (TGNNs) emerged to model evolutionary\nbehaviour of such graphs by leveraging the message passing primitive at the\ncore of Graph Neural Networks (GNNs). It is well-known that GNNs are vulnerable\nto several issues directly related to the input graph topology, such as\nunder-reaching and over-squashing - we argue that these issues can often get\nexacerbated in temporal graphs, particularly as the result of stale nodes and\nedges. While graph rewiring techniques have seen frequent usage in GNNs to make\nthe graph topology more favourable for message passing, they have not seen any\nmainstream usage on TGNNs. In this work, we propose Temporal Graph Rewiring\n(TGR), the first approach for graph rewiring on temporal graphs, to the best of\nour knowledge. TGR constructs message passing highways between temporally\ndistant nodes in a continuous-time dynamic graph by utilizing expander graph\npropagation, a prominent framework used for graph rewiring on static graphs\nwhich makes minimal assumptions on the underlying graph structure. On the\nchallenging TGB benchmark, TGR achieves state-of-the-art results on\ntgbl-review, tgbl-coin, tgbl-comment and tgbl-flight datasets at the time of\nwriting. For tgbl-review, TGR has 50.5% improvement in MRR over the base TGN\nmodel and 22.2% improvement over the base TNCN model. The significant\nimprovement over base models demonstrates clear benefits of temporal graph\nrewiring.",
          "arxiv_id": "2406.02362v3"
        },
        {
          "title": "Multilevel Graph Matching Networks for Deep Graph Similarity Learning",
          "year": "2020-07",
          "abstract": "While the celebrated graph neural networks yield effective representations\nfor individual nodes of a graph, there has been relatively less success in\nextending to the task of graph similarity learning. Recent work on graph\nsimilarity learning has considered either global-level graph-graph interactions\nor low-level node-node interactions, however ignoring the rich cross-level\ninteractions (e.g., between each node of one graph and the other whole graph).\nIn this paper, we propose a multi-level graph matching network (MGMN) framework\nfor computing the graph similarity between any pair of graph-structured objects\nin an end-to-end fashion. In particular, the proposed MGMN consists of a\nnode-graph matching network for effectively learning cross-level interactions\nbetween each node of one graph and the other whole graph, and a siamese graph\nneural network to learn global-level interactions between two input graphs.\nFurthermore, to compensate for the lack of standard benchmark datasets, we have\ncreated and collected a set of datasets for both the graph-graph classification\nand graph-graph regression tasks with different sizes in order to evaluate the\neffectiveness and robustness of our models. Comprehensive experiments\ndemonstrate that MGMN consistently outperforms state-of-the-art baseline models\non both the graph-graph classification and graph-graph regression tasks.\nCompared with previous work, MGMN also exhibits stronger robustness as the\nsizes of the two input graphs increase.",
          "arxiv_id": "2007.04395v4"
        },
        {
          "title": "Revisiting the Necessity of Graph Learning and Common Graph Benchmarks",
          "year": "2024-12",
          "abstract": "Graph machine learning has enjoyed a meteoric rise in popularity since the\nintroduction of deep learning in graph contexts. This is no surprise due to the\nubiquity of graph data in large scale industrial settings. Tacitly assumed in\nall graph learning tasks is the separation of the graph structure and node\nfeatures: node features strictly encode individual data while the graph\nstructure consists only of pairwise interactions. The driving belief is that\nnode features are (by themselves) insufficient for these tasks, so benchmark\nperformance accurately reflects improvements in graph learning. In our paper,\nwe challenge this orthodoxy by showing that, surprisingly, node features are\noftentimes more-than-sufficient for many common graph benchmarks, breaking this\ncritical assumption. When comparing against a well-tuned feature-only MLP\nbaseline on seven of the most commonly used graph learning datasets, one gains\nlittle benefit from using graph structure on five datasets. We posit that these\ndatasets do not benefit considerably from graph learning because the features\nthemselves already contain enough graph information to obviate or substantially\nreduce the need for the graph. To illustrate this point, we perform a feature\nstudy on these datasets and show how the features are responsible for closing\nthe gap between MLP and graph-method performance. Further, in service of\nintroducing better empirical measures of progress for graph neural networks, we\npresent a challenging parametric family of principled synthetic datasets that\nnecessitate graph information for nontrivial performance. Lastly, we section\nout a subset of real-world datasets that are not trivially solved by an MLP and\nhence serve as reasonable benchmarks for graph neural networks.",
          "arxiv_id": "2412.06173v1"
        }
      ],
      "4": [
        {
          "title": "A Malliavin calculus approach to score functions in diffusion generative models",
          "year": "2025-07",
          "abstract": "Score-based diffusion generative models have recently emerged as a powerful\ntool for modelling complex data distributions. These models aim at learning the\nscore function, which defines a map from a known probability distribution to\nthe target data distribution via deterministic or stochastic differential\nequations (SDEs). The score function is typically estimated from data using a\nvariety of approximation techniques, such as denoising or sliced score\nmatching, Hyv\\\"arien's method, or Schr\\\"odinger bridges. In this paper, we\nderive an exact, closed-form, expression for the score function for a broad\nclass of nonlinear diffusion generative models. Our approach combines modern\nstochastic analysis tools such as Malliavin derivatives and their adjoint\noperators (Skorokhod integrals or Malliavin Divergence) with a new Bismut-type\nformula. The resulting expression for the score function can be written\nentirely in terms of the first and second variation processes, with all\nMalliavin derivatives systematically eliminated, thereby enhancing its\npractical applicability. The theoretical framework presented in this work\noffers a principled foundation for advancing score estimation methods in\ngenerative modelling, enabling the design of new sampling algorithms for\ncomplex probability distributions. Our results can be extended to broader\nclasses of stochastic differential equations, opening new directions for the\ndevelopment of score-based diffusion generative models.",
          "arxiv_id": "2507.05550v3"
        },
        {
          "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion",
          "year": "2021-12",
          "abstract": "Score-based generative models (SGMs) have demonstrated remarkable synthesis\nquality. SGMs rely on a diffusion process that gradually perturbs the data\ntowards a tractable distribution, while the generative model learns to denoise.\nThe complexity of this denoising task is, apart from the data distribution\nitself, uniquely determined by the diffusion process. We argue that current\nSGMs employ overly simplistic diffusions, leading to unnecessarily complex\ndenoising processes, which limit generative modeling performance. Based on\nconnections to statistical mechanics, we propose a novel critically-damped\nLangevin diffusion (CLD) and show that CLD-based SGMs achieve superior\nperformance. CLD can be interpreted as running a joint diffusion in an extended\nspace, where the auxiliary variables can be considered \"velocities\" that are\ncoupled to the data variables as in Hamiltonian dynamics. We derive a novel\nscore matching objective for CLD and show that the model only needs to learn\nthe score function of the conditional distribution of the velocity given data,\nan easier task than learning scores of the data directly. We also derive a new\nsampling scheme for efficient synthesis from CLD-based diffusion models. We\nfind that CLD outperforms previous SGMs in synthesis quality for similar\nnetwork architectures and sampling compute budgets. We show that our novel\nsampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our\nframework provides new insights into score-based denoising diffusion models and\ncan be readily used for high-resolution image synthesis. Project page and code:\nhttps://nv-tlabs.github.io/CLD-SGM.",
          "arxiv_id": "2112.07068v4"
        },
        {
          "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
          "year": "2020-11",
          "abstract": "Creating noise from data is easy; creating data from noise is generative\nmodeling. We present a stochastic differential equation (SDE) that smoothly\ntransforms a complex data distribution to a known prior distribution by slowly\ninjecting noise, and a corresponding reverse-time SDE that transforms the prior\ndistribution back into the data distribution by slowly removing the noise.\nCrucially, the reverse-time SDE depends only on the time-dependent gradient\nfield (\\aka, score) of the perturbed data distribution. By leveraging advances\nin score-based generative modeling, we can accurately estimate these scores\nwith neural networks, and use numerical SDE solvers to generate samples. We\nshow that this framework encapsulates previous approaches in score-based\ngenerative modeling and diffusion probabilistic modeling, allowing for new\nsampling procedures and new modeling capabilities. In particular, we introduce\na predictor-corrector framework to correct errors in the evolution of the\ndiscretized reverse-time SDE. We also derive an equivalent neural ODE that\nsamples from the same distribution as the SDE, but additionally enables exact\nlikelihood computation, and improved sampling efficiency. In addition, we\nprovide a new way to solve inverse problems with score-based models, as\ndemonstrated with experiments on class-conditional generation, image\ninpainting, and colorization. Combined with multiple architectural\nimprovements, we achieve record-breaking performance for unconditional image\ngeneration on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a\ncompetitive likelihood of 2.99 bits/dim, and demonstrate high fidelity\ngeneration of 1024 x 1024 images for the first time from a score-based\ngenerative model.",
          "arxiv_id": "2011.13456v2"
        }
      ],
      "5": [
        {
          "title": "Tight Regret Bounds for Single-pass Streaming Multi-armed Bandits",
          "year": "2023-06",
          "abstract": "Regret minimization in streaming multi-armed bandits (MABs) has been studied\nextensively in recent years. In the single-pass setting with $K$ arms and $T$\ntrials, a regret lower bound of $\\Omega(T^{2/3})$ has been proved for any\nalgorithm with $o(K)$ memory (Maiti et al. [NeurIPS'21]; Agarwal at al.\n[COLT'22]). On the other hand, however, the previous best regret upper bound is\nstill $O(K^{1/3} T^{2/3}\\log^{1/3}(T))$, which is achieved by the streaming\nimplementation of the simple uniform exploration. The $O(K^{1/3}\\log^{1/3}(T))$\ngap leaves the open question of the tight regret bound in the single-pass MABs\nwith sublinear arm memory.\n  In this paper, we answer this open problem and complete the picture of regret\nminimization in single-pass streaming MABs. We first improve the regret lower\nbound to $\\Omega(K^{1/3}T^{2/3})$ for algorithms with $o(K)$ memory, which\nmatches the uniform exploration regret up to a logarithm factor in $T$. We then\nshow that the $\\log^{1/3}(T)$ factor is not necessary, and we can achieve\n$O(K^{1/3}T^{2/3})$ regret by finding an $\\varepsilon$-best arm and committing\nto it in the rest of the trials. For regret minimization with high constant\nprobability, we can apply the single-memory $\\varepsilon$-best arm algorithms\nin Jin et al. [ICML'21] to obtain the optimal bound. Furthermore, for the\nexpected regret minimization, we design an algorithm with a single-arm memory\nthat achieves $O(K^{1/3} T^{2/3}\\log(K))$ regret, and an algorithm with\n$O(\\log^{*}(n))$-memory with the optimal $O(K^{1/3} T^{2/3})$ regret following\nthe $\\varepsilon$-best arm algorithm in Assadi and Wang [STOC'20].\n  We further tested the empirical performances of our algorithms. The\nsimulation results show that the proposed algorithms consistently outperform\nthe benchmark uniform exploration algorithm by a large margin, and on occasion,\nreduce the regret by up to 70%.",
          "arxiv_id": "2306.02208v1"
        },
        {
          "title": "Best Arm Identification in Restless Markov Multi-Armed Bandits",
          "year": "2022-03",
          "abstract": "We study the problem of identifying the best arm in a multi-armed bandit\nenvironment when each arm is a time-homogeneous and ergodic discrete-time\nMarkov process on a common, finite state space. The state evolution on each arm\nis governed by the arm's transition probability matrix (TPM). A decision entity\nthat knows the set of arm TPMs but not the exact mapping of the TPMs to the\narms, wishes to find the index of the best arm as quickly as possible, subject\nto an upper bound on the error probability. The decision entity selects one arm\nat a time sequentially, and all the unselected arms continue to undergo state\nevolution ({\\em restless} arms). For this problem, we derive the first-known\nproblem instance-dependent asymptotic lower bound on the growth rate of the\nexpected time required to find the index of the best arm, where the asymptotics\nis as the error probability vanishes. Further, we propose a sequential policy\nthat, for an input parameter $R$, forcibly selects an arm that has not been\nselected for $R$ consecutive time instants. We show that this policy achieves\nan upper bound that depends on $R$ and is monotonically non-increasing as\n$R\\to\\infty$. The question of whether, in general, the limiting value of the\nupper bound as $R\\to\\infty$ matches with the lower bound, remains open. We\nidentify a special case in which the upper and the lower bounds match. Prior\nworks on best arm identification have dealt with (a) independent and\nidentically distributed observations from the arms, and (b) rested Markov arms,\nwhereas our work deals with the more difficult setting of restless Markov arms.",
          "arxiv_id": "2203.15236v1"
        },
        {
          "title": "Indexability of Finite State Restless Multi-Armed Bandit and Rollout Policy",
          "year": "2023-04",
          "abstract": "We consider finite state restless multi-armed bandit problem. The decision\nmaker can act on M bandits out of N bandits in each time step. The play of arm\n(active arm) yields state dependent rewards based on action and when the arm is\nnot played, it also provides rewards based on the state and action. The\nobjective of the decision maker is to maximize the infinite horizon discounted\nreward. The classical approach to restless bandits is Whittle index policy. In\nsuch policy, the M arms with highest indices are played at each time step.\nHere, one decouples the restless bandits problem by analyzing relaxed\nconstrained restless bandits problem. Then by Lagrangian relaxation problem,\none decouples restless bandits problem into N single-armed restless bandit\nproblems. We analyze the single-armed restless bandit. In order to study the\nWhittle index policy, we show structural results on the single armed bandit\nmodel. We define indexability and show indexability in special cases. We\npropose an alternative approach to verify the indexable criteria for a single\narmed bandit model using value iteration algorithm. We demonstrate the\nperformance of our algorithm with different examples. We provide insight on\ncondition of indexability of restless bandits using different structural\nassumptions on transition probability and reward matrices. We also study online\nrollout policy and discuss the computation complexity of algorithm and compare\nthat with complexity of index computation. Numerical examples illustrate that\nindex policy and rollout policy performs better than myopic policy.",
          "arxiv_id": "2305.00410v1"
        }
      ],
      "6": [
        {
          "title": "Leave-One-Out Analysis for Nonconvex Robust Matrix Completion with General Thresholding Functions",
          "year": "2024-07",
          "abstract": "We study the problem of robust matrix completion (RMC), where the partially\nobserved entries of an underlying low-rank matrix is corrupted by sparse noise.\nExisting analysis of the non-convex methods for this problem either requires\nthe explicit but empirically redundant regularization in the algorithm or\nrequires sample splitting in the analysis. In this paper, we consider a simple\nyet efficient nonconvex method which alternates between a projected gradient\nstep for the low-rank part and a thresholding step for the sparse noise part.\nInspired by leave-one out analysis for low rank matrix completion, it is\nestablished that the method can achieve linear convergence for a general class\nof thresholding functions, including for example soft-thresholding and SCAD. To\nthe best of our knowledge, this is the first leave-one-out analysis on a\nnonconvex method for RMC. Additionally, when applying our result to low rank\nmatrix completion, it improves the sampling complexity of existing result for\nthe singular value projection method.",
          "arxiv_id": "2407.19446v2"
        },
        {
          "title": "Inference for Low-rank Tensors -- No Need to Debias",
          "year": "2020-12",
          "abstract": "In this paper, we consider the statistical inference for several low-rank\ntensor models. Specifically, in the Tucker low-rank tensor PCA or regression\nmodel, provided with any estimates achieving some attainable error rate, we\ndevelop the data-driven confidence regions for the singular subspace of the\nparameter tensor based on the asymptotic distribution of an updated estimate by\ntwo-iteration alternating minimization. The asymptotic distributions are\nestablished under some essential conditions on the signal-to-noise ratio (in\nPCA model) or sample size (in regression model). If the parameter tensor is\nfurther orthogonally decomposable, we develop the methods and non-asymptotic\ntheory for inference on each individual singular vector. For the rank-one\ntensor PCA model, we establish the asymptotic distribution for general linear\nforms of principal components and confidence interval for each entry of the\nparameter tensor. Finally, numerical simulations are presented to corroborate\nour theoretical discoveries.\n  In all these models, we observe that different from many matrix/vector\nsettings in existing work, debiasing is not required to establish the\nasymptotic distribution of estimates or to make statistical inference on\nlow-rank tensors. In fact, due to the widely observed\nstatistical-computational-gap for low-rank tensor estimation, one usually\nrequires stronger conditions than the statistical (or information-theoretic)\nlimit to ensure the computationally feasible estimation is achievable.\nSurprisingly, such conditions ``incidentally\" render a feasible low-rank tensor\ninference without debiasing.",
          "arxiv_id": "2012.14844v2"
        },
        {
          "title": "Guaranteed Nonconvex Low-Rank Tensor Estimation via Scaled Gradient Descent",
          "year": "2025-01",
          "abstract": "Tensors, which give a faithful and effective representation to deliver the\nintrinsic structure of multi-dimensional data, play a crucial role in an\nincreasing number of signal processing and machine learning problems. However,\ntensor data are often accompanied by arbitrary signal corruptions, including\nmissing entries and sparse noise. A fundamental challenge is to reliably\nextract the meaningful information from corrupted tensor data in a\nstatistically and computationally efficient manner. This paper develops a\nscaled gradient descent (ScaledGD) algorithm to directly estimate the tensor\nfactors with tailored spectral initializations under the tensor-tensor product\n(t-product) and tensor singular value decomposition (t-SVD) framework. In\ntheory, ScaledGD achieves linear convergence at a constant rate that is\nindependent of the condition number of the ground truth low-rank tensor for two\ncanonical problems -- tensor robust principal component analysis and tensor\ncompletion -- as long as the level of corruptions is not too large and the\nsample size is sufficiently large, while maintaining the low per-iteration cost\nof gradient descent. To the best of our knowledge, ScaledGD is the first\nalgorithm that provably has such properties for low-rank tensor estimation with\nthe t-SVD decomposition. Finally, numerical examples are provided to\ndemonstrate the efficacy of ScaledGD in accelerating the convergence rate of\nill-conditioned low-rank tensor estimation in these two applications.",
          "arxiv_id": "2501.01696v1"
        }
      ],
      "7": [
        {
          "title": "From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When",
          "year": "2024-05",
          "abstract": "Large language models (LLMs) like transformers demonstrate impressive\nin-context learning (ICL) capabilities, allowing them to make predictions for\nnew tasks based on prompt exemplars without parameter updates. While existing\nICL theories often assume structured training data resembling ICL tasks (e.g.,\nx-y pairs for linear regression), LLMs are typically trained unsupervised on\nunstructured text, such as web content, which lacks clear parallels to tasks\nlike word analogy. To address this gap, we examine what enables ICL in models\ntrained on unstructured data, focusing on critical sequence model requirements\nand training data structure. We find that many ICL capabilities can emerge\nsimply from co-occurrence of semantically related word pairs in unstructured\ndata; word analogy completion, for example, can provably arise purely through\nco-occurrence modeling, using classical language models like continuous bag of\nwords (CBOW), without needing positional information or attention mechanisms.\nHowever, positional information becomes crucial for logic reasoning tasks\nrequiring generalization to unseen tokens. Finally, we identify two cases where\nICL fails: one in logic reasoning tasks that require generalizing to new,\nunseen patterns, and another in analogy completion where relevant word pairs\nappear only in fixed training positions. These findings suggest that LLMs' ICL\nabilities depend heavily on the structural elements within their training data.",
          "arxiv_id": "2406.00131v2"
        },
        {
          "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
          "year": "2023-05",
          "abstract": "Large language models (LLMs) specializing in natural language generation\n(NLG) have recently started exhibiting promising capabilities across a variety\nof domains. However, gauging the trustworthiness of responses generated by LLMs\nremains an open challenge, with limited research on uncertainty quantification\n(UQ) for NLG. Furthermore, existing literature typically assumes white-box\naccess to language models, which is becoming unrealistic either due to the\nclosed-source nature of the latest LLMs or computational constraints. In this\nwork, we investigate UQ in NLG for *black-box* LLMs. We first differentiate\n*uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the\npotential predictions for a fixed input, and the latter refers to the\nconfidence on a particular prediction/generation. We then propose and compare\nseveral confidence/uncertainty measures, applying them to *selective NLG* where\nunreliable results could either be ignored or yielded for further assessment.\nExperiments were carried out with several popular LLMs on question-answering\ndatasets (for evaluation purposes). Results reveal that a simple measure for\nthe semantic dispersion can be a reliable predictor of the quality of LLM\nresponses, providing valuable insights for practitioners on uncertainty\nmanagement when adopting LLMs. The code to replicate our experiments is\navailable at https://github.com/zlin7/UQ-NLG.",
          "arxiv_id": "2305.19187v3"
        },
        {
          "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models",
          "year": "2025-04",
          "abstract": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment.",
          "arxiv_id": "2504.17004v2"
        }
      ],
      "8": [
        {
          "title": "Precise gradient descent training dynamics for finite-width multi-layer neural networks",
          "year": "2025-05",
          "abstract": "In this paper, we provide the first precise distributional characterization\nof gradient descent iterates for general multi-layer neural networks under the\ncanonical single-index regression model, in the `finite-width proportional\nregime' where the sample size and feature dimension grow proportionally while\nthe network width and depth remain bounded. Our non-asymptotic state evolution\ntheory captures Gaussian fluctuations in first-layer weights and concentration\nin deeper-layer weights, and remains valid for non-Gaussian features.\n  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF)\ntheories and tensor program (TP) in several key aspects. First, our theory\noperates in the finite-width regime whereas these existing theories are\nfundamentally infinite-width. Second, our theory allows weights to evolve from\nindividual initializations beyond the lazy training regime, whereas NTK and MF\nare either frozen at or only weakly sensitive to initialization, and TP relies\non special initialization schemes. Third, our theory characterizes both\ntraining and generalization errors for general multi-layer neural networks\nbeyond the uniform convergence regime, whereas existing theories study\ngeneralization almost exclusively in two-layer settings.\n  As a statistical application, we show that vanilla gradient descent can be\naugmented to yield consistent estimates of the generalization error at each\niteration, which can be used to guide early stopping and hyperparameter tuning.\nAs a further theoretical implication, we show that despite model\nmisspecification, the model learned by gradient descent retains the structure\nof a single-index function with an effective signal determined by a linear\ncombination of the true signal and the initialization.",
          "arxiv_id": "2505.04898v1"
        },
        {
          "title": "Approximation Power of Deep Neural Networks: an explanatory mathematical survey",
          "year": "2022-07",
          "abstract": "This survey provides an in-depth and explanatory review of the approximation\nproperties of deep neural networks, with a focus on feed-forward and residual\narchitectures. The primary objective is to examine how effectively neural\nnetworks approximate target functions and to identify conditions under which\nthey outperform traditional approximation methods. Key topics include the\nnonlinear, compositional structure of deep networks and the formalization of\nneural network tasks as optimization problems in regression and classification\nsettings. The survey also addresses the training process, emphasizing the role\nof stochastic gradient descent and backpropagation in solving these\noptimization problems, and highlights practical considerations such as\nactivation functions, overfitting, and regularization techniques. Additionally,\nthe survey explores the density of neural networks in the space of continuous\nfunctions, comparing the approximation capabilities of deep ReLU networks with\nthose of other approximation methods. It discusses recent theoretical\nadvancements in understanding the expressiveness and limitations of these\nnetworks. A detailed error-complexity analysis is also presented, focusing on\nerror rates and computational complexity for neural networks with ReLU and\nFourier-type activation functions in the context of bounded target functions\nwith minimal regularity assumptions. Alongside recent known results, the survey\nintroduces new findings, offering a valuable resource for understanding the\ntheoretical foundations of neural network approximation. Concluding remarks and\nfurther reading suggestions are provided.",
          "arxiv_id": "2207.09511v2"
        },
        {
          "title": "Theoretical Analysis of the Advantage of Deepening Neural Networks",
          "year": "2020-09",
          "abstract": "We propose two new criteria to understand the advantage of deepening neural\nnetworks. It is important to know the expressivity of functions computable by\ndeep neural networks in order to understand the advantage of deepening neural\nnetworks. Unless deep neural networks have enough expressivity, they cannot\nhave good performance even though learning is successful. In this situation,\nthe proposed criteria contribute to understanding the advantage of deepening\nneural networks since they can evaluate the expressivity independently from the\nefficiency of learning. The first criterion shows the approximation accuracy of\ndeep neural networks to the target function. This criterion has the background\nthat the goal of deep learning is approximating the target function by deep\nneural networks. The second criterion shows the property of linear regions of\nfunctions computable by deep neural networks. This criterion has the background\nthat deep neural networks whose activation functions are piecewise linear are\nalso piecewise linear. Furthermore, by the two criteria, we show that to\nincrease layers is more effective than to increase units at each layer on\nimproving the expressivity of deep neural networks.",
          "arxiv_id": "2009.11479v1"
        }
      ],
      "9": [
        {
          "title": "GrADE: A graph based data-driven solver for time-dependent nonlinear partial differential equations",
          "year": "2021-08",
          "abstract": "The physical world is governed by the laws of physics, often represented in\nform of nonlinear partial differential equations (PDEs). Unfortunately,\nsolution of PDEs is non-trivial and often involves significant computational\ntime. With recent developments in the field of artificial intelligence and\nmachine learning, the solution of PDEs using neural network has emerged as a\ndomain with huge potential. However, most of the developments in this field are\nbased on either fully connected neural networks (FNN) or convolutional neural\nnetworks (CNN). While FNN is computationally inefficient as the number of\nnetwork parameters can be potentially huge, CNN necessitates regular grid and\nsimpler domain. In this work, we propose a novel framework referred to as the\nGraph Attention Differential Equation (GrADE) for solving time dependent\nnonlinear PDEs. The proposed approach couples FNN, graph neural network, and\nrecently developed Neural ODE framework. The primary idea is to use graph\nneural network for modeling the spatial domain, and Neural ODE for modeling the\ntemporal domain. The attention mechanism identifies important inputs/features\nand assign more weightage to the same; this enhances the performance of the\nproposed framework. Neural ODE, on the other hand, results in constant memory\ncost and allows trading of numerical precision for speed. We also propose depth\nrefinement as an effective technique for training the proposed architecture in\nlesser time with better accuracy. The effectiveness of the proposed framework\nis illustrated using 1D and 2D Burgers' equations. Results obtained illustrate\nthe capability of the proposed framework in modeling PDE and its scalability to\nlarger domains without the need for retraining.",
          "arxiv_id": "2108.10639v1"
        },
        {
          "title": "Physics-informed learning of governing equations from scarce data",
          "year": "2020-05",
          "abstract": "Harnessing data to discover the underlying governing laws or equations that\ndescribe the behavior of complex physical systems can significantly advance our\nmodeling, simulation and understanding of such systems in various science and\nengineering disciplines. This work introduces a novel physics-informed deep\nlearning framework to discover governing partial differential equations (PDEs)\nfrom scarce and noisy data for nonlinear spatiotemporal systems. In particular,\nthis approach seamlessly integrates the strengths of deep neural networks for\nrich representation learning, physics embedding, automatic differentiation and\nsparse regression to (1) approximate the solution of system variables, (2)\ncompute essential derivatives, as well as (3) identify the key derivative terms\nand parameters that form the structure and explicit expression of the PDEs. The\nefficacy and robustness of this method are demonstrated, both numerically and\nexperimentally, on discovering a variety of PDE systems with different levels\nof data scarcity and noise accounting for different initial/boundary\nconditions. The resulting computational framework shows the potential for\nclosed-form model discovery in practical applications where large and accurate\ndatasets are intractable to capture.",
          "arxiv_id": "2005.03448v3"
        },
        {
          "title": "On Neural Differential Equations",
          "year": "2022-02",
          "abstract": "The conjoining of dynamical systems and deep learning has become a topic of\ngreat interest. In particular, neural differential equations (NDEs) demonstrate\nthat neural networks and differential equation are two sides of the same coin.\nTraditional parameterised differential equations are a special case. Many\npopular neural network architectures, such as residual networks and recurrent\nnetworks, are discretisations.\n  NDEs are suitable for tackling generative problems, dynamical systems, and\ntime series (particularly in physics, finance, ...) and are thus of interest to\nboth modern machine learning and traditional mathematical modelling. NDEs offer\nhigh-capacity function approximation, strong priors on model space, the ability\nto handle irregular data, memory efficiency, and a wealth of available theory\non both sides.\n  This doctoral thesis provides an in-depth survey of the field.\n  Topics include: neural ordinary differential equations (e.g. for hybrid\nneural/mechanistic modelling of physical systems); neural controlled\ndifferential equations (e.g. for learning functions of irregular time series);\nand neural stochastic differential equations (e.g. to produce generative models\ncapable of representing complex stochastic dynamics, or sampling from complex\nhigh-dimensional distributions).\n  Further topics include: numerical methods for NDEs (e.g. reversible\ndifferential equations solvers, backpropagation through differential equations,\nBrownian reconstruction); symbolic regression for dynamical systems (e.g. via\nregularised evolution); and deep implicit models (e.g. deep equilibrium models,\ndifferentiable optimisation).\n  We anticipate this thesis will be of interest to anyone interested in the\nmarriage of deep learning with dynamical systems, and hope it will provide a\nuseful reference for the current state of the art.",
          "arxiv_id": "2202.02435v1"
        }
      ],
      "10": [
        {
          "title": "Adversarial Feature Desensitization",
          "year": "2020-06",
          "abstract": "Neural networks are known to be vulnerable to adversarial attacks -- slight\nbut carefully constructed perturbations of the inputs which can drastically\nimpair the network's performance. Many defense methods have been proposed for\nimproving robustness of deep networks by training them on adversarially\nperturbed inputs. However, these models often remain vulnerable to new types of\nattacks not seen during training, and even to slightly stronger versions of\npreviously seen attacks. In this work, we propose a novel approach to\nadversarial robustness, which builds upon the insights from the domain\nadaptation field. Our method, called Adversarial Feature Desensitization (AFD),\naims at learning features that are invariant towards adversarial perturbations\nof the inputs. This is achieved through a game where we learn features that are\nboth predictive and robust (insensitive to adversarial attacks), i.e. cannot be\nused to discriminate between natural and adversarial data. Empirical results on\nseveral benchmarks demonstrate the effectiveness of the proposed approach\nagainst a wide range of attack types and attack strengths. Our code is\navailable at https://github.com/BashivanLab/afd.",
          "arxiv_id": "2006.04621v3"
        },
        {
          "title": "Semantics-Preserving Adversarial Training",
          "year": "2020-09",
          "abstract": "Adversarial training is a defense technique that improves adversarial\nrobustness of a deep neural network (DNN) by including adversarial examples in\nthe training data. In this paper, we identify an overlooked problem of\nadversarial training in that these adversarial examples often have different\nsemantics than the original data, introducing unintended biases into the model.\nWe hypothesize that such non-semantics-preserving (and resultingly ambiguous)\nadversarial data harm the robustness of the target models. To mitigate such\nunintended semantic changes of adversarial examples, we propose\nsemantics-preserving adversarial training (SPAT) which encourages perturbation\non the pixels that are shared among all classes when generating adversarial\nexamples in the training stage. Experiment results show that SPAT improves\nadversarial robustness and achieves state-of-the-art results in CIFAR-10 and\nCIFAR-100.",
          "arxiv_id": "2009.10978v1"
        },
        {
          "title": "Improving adversarial robustness of deep neural networks by using semantic information",
          "year": "2020-08",
          "abstract": "The vulnerability of deep neural networks (DNNs) to adversarial attack, which\nis an attack that can mislead state-of-the-art classifiers into making an\nincorrect classification with high confidence by deliberately perturbing the\noriginal inputs, raises concerns about the robustness of DNNs to such attacks.\nAdversarial training, which is the main heuristic method for improving\nadversarial robustness and the first line of defense against adversarial\nattacks, requires many sample-by-sample calculations to increase training size\nand is usually insufficiently strong for an entire network. This paper provides\na new perspective on the issue of adversarial robustness, one that shifts the\nfocus from the network as a whole to the critical part of the region close to\nthe decision boundary corresponding to a given class. From this perspective, we\npropose a method to generate a single but image-agnostic adversarial\nperturbation that carries the semantic information implying the directions to\nthe fragile parts on the decision boundary and causes inputs to be\nmisclassified as a specified target. We call the adversarial training based on\nsuch perturbations \"region adversarial training\" (RAT), which resembles\nclassical adversarial training but is distinguished in that it reinforces the\nsemantic information missing in the relevant regions. Experimental results on\nthe MNIST and CIFAR-10 datasets show that this approach greatly improves\nadversarial robustness even using a very small dataset from the training data;\nmoreover, it can defend against FGSM adversarial attacks that have a completely\ndifferent pattern from the model seen during retraining.",
          "arxiv_id": "2008.07838v2"
        }
      ],
      "11": [
        {
          "title": "Shuffled Model of Federated Learning: Privacy, Communication and Accuracy Trade-offs",
          "year": "2020-08",
          "abstract": "We consider a distributed empirical risk minimization (ERM) optimization\nproblem with communication efficiency and privacy requirements, motivated by\nthe federated learning (FL) framework. Unique challenges to the traditional ERM\nproblem in the context of FL include (i) need to provide privacy guarantees on\nclients' data, (ii) compress the communication between clients and the server,\nsince clients might have low-bandwidth links, (iii) work with a dynamic client\npopulation at each round of communication between the server and the clients,\nas a small fraction of clients are sampled at each round. To address these\nchallenges we develop (optimal) communication-efficient schemes for private\nmean estimation for several $\\ell_p$ spaces, enabling efficient gradient\naggregation for each iteration of the optimization solution of the ERM. We also\nprovide lower and upper bounds for mean estimation with privacy and\ncommunication constraints for arbitrary $\\ell_p$ spaces. To get the overall\ncommunication, privacy, and optimization performance operation point, we\ncombine this with privacy amplification opportunities inherent to this setup.\nOur solution takes advantage of the inherent privacy amplification provided by\nclient sampling and data sampling at each client (through Stochastic Gradient\nDescent) as well as the recently developed privacy framework using\nanonymization, which effectively presents to the server responses that are\nrandomly shuffled with respect to the clients. Putting these together, we\ndemonstrate that one can get the same privacy, optimization-performance\noperating point developed in recent methods that use full-precision\ncommunication, but at a much lower communication cost, i.e., effectively\ngetting communication efficiency for \"free\".",
          "arxiv_id": "2008.07180v2"
        },
        {
          "title": "A Framework for Evaluating Gradient Leakage Attacks in Federated Learning",
          "year": "2020-04",
          "abstract": "Federated learning (FL) is an emerging distributed machine learning framework\nfor collaborative model training with a network of clients (edge devices). FL\noffers default client privacy by allowing clients to keep their sensitive data\non local devices and to only share local training parameter updates with the\nfederated server. However, recent studies have shown that even sharing local\nparameter updates from a client to the federated server may be susceptible to\ngradient leakage attacks and intrude the client privacy regarding its training\ndata. In this paper, we present a principled framework for evaluating and\ncomparing different forms of client privacy leakage attacks. We first provide\nformal and experimental analysis to show how adversaries can reconstruct the\nprivate local training data by simply analyzing the shared parameter update\nfrom local training (e.g., local gradient or weight update vector). We then\nanalyze how different hyperparameter configurations in federated learning and\ndifferent settings of the attack algorithm may impact on both attack\neffectiveness and attack cost. Our framework also measures, evaluates, and\nanalyzes the effectiveness of client privacy leakage attacks under different\ngradient compression ratios when using communication efficient FL protocols.\nOur experiments also include some preliminary mitigation strategies to\nhighlight the importance of providing a systematic attack evaluation framework\ntowards an in-depth understanding of the various forms of client privacy\nleakage threats in federated learning and developing theoretical foundations\nfor attack mitigation.",
          "arxiv_id": "2004.10397v2"
        },
        {
          "title": "Ternary Compression for Communication-Efficient Federated Learning",
          "year": "2020-03",
          "abstract": "Learning over massive data stored in different locations is essential in many\nreal-world applications. However, sharing data is full of challenges due to the\nincreasing demands of privacy and security with the growing use of smart mobile\ndevices and IoT devices. Federated learning provides a potential solution to\nprivacy-preserving and secure machine learning, by means of jointly training a\nglobal model without uploading data distributed on multiple devices to a\ncentral server. However, most existing work on federated learning adopts\nmachine learning models with full-precision weights, and almost all these\nmodels contain a large number of redundant parameters that do not need to be\ntransmitted to the server, consuming an excessive amount of communication\ncosts. To address this issue, we propose a federated trained ternary\nquantization (FTTQ) algorithm, which optimizes the quantized networks on the\nclients through a self-learning quantization factor. Theoretical proofs of the\nconvergence of quantization factors, unbiasedness of FTTQ, as well as a reduced\nweight divergence are given. On the basis of FTTQ, we propose a ternary\nfederated averaging protocol (T-FedAvg) to reduce the upstream and downstream\ncommunication of federated learning systems. Empirical experiments are\nconducted to train widely used deep learning models on publicly available\ndatasets, and our results demonstrate that the proposed T-FedAvg is effective\nin reducing communication costs and can even achieve slightly better\nperformance on non-IID data in contrast to the canonical federated learning\nalgorithms.",
          "arxiv_id": "2003.03564v2"
        }
      ],
      "12": [
        {
          "title": "Provable Complexity Improvement of AdaGrad over SGD: Upper and Lower Bounds in Stochastic Non-Convex Optimization",
          "year": "2024-06",
          "abstract": "Adaptive gradient methods, such as AdaGrad, are among the most successful\noptimization algorithms for neural network training. While these methods are\nknown to achieve better dimensional dependence than stochastic gradient descent\n(SGD) for stochastic convex optimization under favorable geometry, the\ntheoretical justification for their success in stochastic non-convex\noptimization remains elusive. In fact, under standard assumptions of Lipschitz\ngradients and bounded noise variance, it is known that SGD is worst-case\noptimal in terms of finding a near-stationary point with respect to the\n$l_2$-norm, making further improvements impossible. Motivated by this\nlimitation, we introduce refined assumptions on the smoothness structure of the\nobjective and the gradient noise variance, which better suit the\ncoordinate-wise nature of adaptive gradient methods. Moreover, we adopt the\n$l_1$-norm of the gradient as the stationarity measure, as opposed to the\nstandard $l_2$-norm, to align with the coordinate-wise analysis and obtain\ntighter convergence guarantees for AdaGrad. Under these new assumptions and the\n$l_1$-norm stationarity measure, we establish an upper bound on the convergence\nrate of AdaGrad and a corresponding lower bound for SGD. In particular, we\nidentify non-convex settings in which the iteration complexity of AdaGrad is\nfavorable over SGD and show that, for certain configurations of problem\nparameters, it outperforms SGD by a factor of $d$, where $d$ is the problem\ndimension. To the best of our knowledge, this is the first result to\ndemonstrate a provable gain of adaptive gradient methods over SGD in a\nnon-convex setting. We also present supporting lower bounds, including one\nspecific to AdaGrad and one applicable to general deterministic first-order\nmethods, showing that our upper bound for AdaGrad is tight and unimprovable up\nto a logarithmic factor under certain conditions.",
          "arxiv_id": "2406.04592v3"
        },
        {
          "title": "Tackling benign nonconvexity with smoothing and stochastic gradients",
          "year": "2022-02",
          "abstract": "Non-convex optimization problems are ubiquitous in machine learning,\nespecially in Deep Learning. While such complex problems can often be\nsuccessfully optimized in practice by using stochastic gradient descent (SGD),\ntheoretical analysis cannot adequately explain this success. In particular, the\nstandard analyses do not show global convergence of SGD on non-convex\nfunctions, and instead show convergence to stationary points (which can also be\nlocal minima or saddle points). We identify a broad class of nonconvex\nfunctions for which we can show that perturbed SGD (gradient descent perturbed\nby stochastic noise -- covering SGD as a special case) converges to a global\nminimum (or a neighborhood thereof), in contrast to gradient descent without\nnoise that can get stuck in local minima far from a global solution. For\nexample, on non-convex functions that are relatively close to a convex-like\n(strongly convex or PL) function we show that SGD can converge linearly to a\nglobal optimum.",
          "arxiv_id": "2202.09052v1"
        },
        {
          "title": "Almost sure convergence rates for Stochastic Gradient Descent and Stochastic Heavy Ball",
          "year": "2020-06",
          "abstract": "We study stochastic gradient descent (SGD) and the stochastic heavy ball\nmethod (SHB, otherwise known as the momentum method) for the general stochastic\napproximation problem.\n  For SGD, in the convex and smooth setting, we provide the first \\emph{almost\nsure} asymptotic convergence \\emph{rates} for a weighted average of the\niterates . More precisely, we show that the convergence rate of the function\nvalues is arbitrarily close to $o(1/\\sqrt{k})$, and is exactly $o(1/k)$ in the\nso-called overparametrized case. We show that these results still hold when\nusing stochastic line search and stochastic Polyak stepsizes, thereby giving\nthe first proof of convergence of these methods in the non-overparametrized\nregime.\n  Using a substantially different analysis, we show that these rates hold for\nSHB as well, but at the last iterate. This distinction is important because it\nis the last iterate of SGD and SHB which is used in practice. We also show that\nthe last iterate of SHB converges to a minimizer \\emph{almost surely}.\nAdditionally, we prove that the function values of the deterministic HB\nconverge at a $o(1/k)$ rate, which is faster than the previously known\n$O(1/k)$.\n  Finally, in the nonconvex setting, we prove similar rates on the lowest\ngradient norm along the trajectory of SGD.",
          "arxiv_id": "2006.07867v2"
        }
      ],
      "13": [
        {
          "title": "Identifying noisy labels with a transductive semi-supervised leave-one-out filter",
          "year": "2020-09",
          "abstract": "Obtaining data with meaningful labels is often costly and error-prone. In\nthis situation, semi-supervised learning (SSL) approaches are interesting, as\nthey leverage assumptions about the unlabeled data to make up for the limited\namount of labels. However, in real-world situations, we cannot assume that the\nlabeling process is infallible, and the accuracy of many SSL classifiers\ndecreases significantly in the presence of label noise. In this work, we\nintroduce the LGC_LVOF, a leave-one-out filtering approach based on the Local\nand Global Consistency (LGC) algorithm. Our method aims to detect and remove\nwrong labels, and thus can be used as a preprocessing step to any SSL\nclassifier. Given the propagation matrix, detecting noisy labels takes O(cl)\nper step, with c the number of classes and l the number of labels. Moreover,\none does not need to compute the whole propagation matrix, but only an $l$ by\n$l$ submatrix corresponding to interactions between labeled instances. As a\nresult, our approach is best suited to datasets with a large amount of\nunlabeled data but not many labels. Results are provided for a number of\ndatasets, including MNIST and ISOLET. LGCLVOF appears to be equally or more\nprecise than the adapted gradient-based filter. We show that the best-case\naccuracy of the embedding of LGCLVOF into LGC yields performance comparable to\nthe best-case of $\\ell_1$-based classifiers designed to be robust to label\nnoise. We provide a heuristic to choose the number of removed instances.",
          "arxiv_id": "2009.11811v1"
        },
        {
          "title": "Harmless label noise and informative soft-labels in supervised classification",
          "year": "2021-04",
          "abstract": "Manual labelling of training examples is common practice in supervised\nlearning. When the labelling task is of non-trivial difficulty, the supplied\nlabels may not be equal to the ground-truth labels, and label noise is\nintroduced into the training dataset. If the manual annotation is carried out\nby multiple experts, the same training example can be given different class\nassignments by different experts, which is indicative of label noise. In the\nframework of model-based classification, a simple, but key observation is that\nwhen the manual labels are sampled using the posterior probabilities of class\nmembership, the noisy labels are as valuable as the ground-truth labels in\nterms of statistical information. A relaxation of this process is a random\neffects model for imperfect labelling by a group that uses approximate\nposterior probabilities of class membership. The relative efficiency of\nlogistic regression using the noisy labels compared to logistic regression\nusing the ground-truth labels can then be derived. The main finding is that\nlogistic regression can be robust to label noise when label noise and\nclassification difficulty are positively correlated. In particular, when\nclassification difficulty is the only source of label errors, multiple sets of\nnoisy labels can supply more information for the estimation of a classification\nrule compared to the single set of ground-truth labels.",
          "arxiv_id": "2104.02872v1"
        },
        {
          "title": "Evaluating Multi-label Classifiers with Noisy Labels",
          "year": "2021-02",
          "abstract": "Multi-label classification (MLC) is a generalization of standard\nclassification where multiple labels may be assigned to a given sample. In the\nreal world, it is more common to deal with noisy datasets than clean datasets,\ngiven how modern datasets are labeled by a large group of annotators on\ncrowdsourcing platforms, but little attention has been given to evaluating\nmulti-label classifiers with noisy labels. Exploiting label correlations now\nbecomes a standard component of a multi-label classifier to achieve competitive\nperformance. However, this component makes the classifier more prone to poor\ngeneralization - it overfits labels as well as label dependencies. We identify\nthree common real-world label noise scenarios and show how previous approaches\nper-form poorly with noisy labels. To address this issue, we present a\nContext-Based Multi-LabelClassifier (CbMLC) that effectively handles noisy\nlabels when learning label dependencies, without requiring additional\nsupervision. We compare CbMLC against other domain-specific state-of-the-art\nmodels on a variety of datasets, under both the clean and the noisy settings.\nWe show CbMLC yields substantial improvements over the previous methods in most\ncases.",
          "arxiv_id": "2102.08427v1"
        }
      ],
      "14": [
        {
          "title": "High-Order Langevin Monte Carlo Algorithms",
          "year": "2025-08",
          "abstract": "Langevin algorithms are popular Markov chain Monte Carlo (MCMC) methods for\nlarge-scale sampling problems that often arise in data science. We propose\nMonte Carlo algorithms based on the discretizations of $P$-th order Langevin\ndynamics for any $P\\geq 3$. Our design of $P$-th order Langevin Monte Carlo\n(LMC) algorithms is by combining splitting and accurate integration methods. We\nobtain Wasserstein convergence guarantees for sampling from distributions with\nlog-concave and smooth densities. Specifically, the mixing time of the $P$-th\norder LMC algorithm scales as\n$O\\left(d^{\\frac{1}{R}}/\\epsilon^{\\frac{1}{2R}}\\right)$ for $R=4\\cdot 1_{\\{\nP=3\\}}+ (2P-1)\\cdot 1_{\\{ P\\geq 4\\}}$, which has a better dependence on the\ndimension $d$ and the accuracy level $\\epsilon$ as $P$ grows. Numerical\nexperiments illustrate the efficiency of our proposed algorithms.",
          "arxiv_id": "2508.17545v1"
        },
        {
          "title": "Subspace Langevin Monte Carlo",
          "year": "2024-12",
          "abstract": "Sampling from high-dimensional distributions has wide applications in data\nscience and machine learning but poses significant computational challenges. We\nintroduce Subspace Langevin Monte Carlo (SLMC), a novel and efficient sampling\nmethod that generalizes random-coordinate Langevin Monte Carlo and\npreconditioned Langevin Monte Carlo by projecting the Langevin update onto\nsubsampled eigenblocks of a time-varying preconditioner at each iteration. The\nadvantage of SLMC is its superior adaptability and computational efficiency\ncompared to traditional Langevin Monte Carlo and preconditioned Langevin Monte\nCarlo. Using coupling arguments, we establish error guarantees for SLMC and\ndemonstrate its practical effectiveness through a few experiments on sampling\nfrom ill-conditioned distributions.",
          "arxiv_id": "2412.13928v2"
        },
        {
          "title": "Sampling Algorithms, from Survey Sampling to Monte Carlo Methods: Tutorial and Literature Review",
          "year": "2020-11",
          "abstract": "This paper is a tutorial and literature review on sampling algorithms. We\nhave two main types of sampling in statistics. The first type is survey\nsampling which draws samples from a set or population. The second type is\nsampling from probability distribution where we have a probability density or\nmass function. In this paper, we cover both types of sampling. First, we review\nsome required background on mean squared error, variance, bias, maximum\nlikelihood estimation, Bernoulli, Binomial, and Hypergeometric distributions,\nthe Horvitz-Thompson estimator, and the Markov property. Then, we explain the\ntheory of simple random sampling, bootstrapping, stratified sampling, and\ncluster sampling. We also briefly introduce multistage sampling, network\nsampling, and snowball sampling. Afterwards, we switch to sampling from\ndistribution. We explain sampling from cumulative distribution function, Monte\nCarlo approximation, simple Monte Carlo methods, and Markov Chain Monte Carlo\n(MCMC) methods. For simple Monte Carlo methods, whose iterations are\nindependent, we cover importance sampling and rejection sampling. For MCMC\nmethods, we cover Metropolis algorithm, Metropolis-Hastings algorithm, Gibbs\nsampling, and slice sampling. Then, we explain the random walk behaviour of\nMonte Carlo methods and more efficient Monte Carlo methods, including\nHamiltonian (or hybrid) Monte Carlo, Adler's overrelaxation, and ordered\noverrelaxation. Finally, we summarize the characteristics, pros, and cons of\nsampling methods compared to each other. This paper can be useful for different\nfields of statistics, machine learning, reinforcement learning, and\ncomputational physics.",
          "arxiv_id": "2011.00901v1"
        }
      ],
      "15": [
        {
          "title": "Interpretable Survival Analysis for Heart Failure Risk Prediction",
          "year": "2023-10",
          "abstract": "Survival analysis, or time-to-event analysis, is an important and widespread\nproblem in healthcare research. Medical research has traditionally relied on\nCox models for survival analysis, due to their simplicity and interpretability.\nCox models assume a log-linear hazard function as well as proportional hazards\nover time, and can perform poorly when these assumptions fail. Newer survival\nmodels based on machine learning avoid these assumptions and offer improved\naccuracy, yet sometimes at the expense of model interpretability, which is\nvital for clinical use. We propose a novel survival analysis pipeline that is\nboth interpretable and competitive with state-of-the-art survival models.\nSpecifically, we use an improved version of survival stacking to transform a\nsurvival analysis problem to a classification problem, ControlBurn to perform\nfeature selection, and Explainable Boosting Machines to generate interpretable\npredictions. To evaluate our pipeline, we predict risk of heart failure using a\nlarge-scale EHR database. Our pipeline achieves state-of-the-art performance\nand provides interesting and novel insights about risk factors for heart\nfailure.",
          "arxiv_id": "2310.15472v1"
        },
        {
          "title": "SAVAE: Leveraging the variational Bayes autoencoder for survival analysis",
          "year": "2023-12",
          "abstract": "As in many fields of medical research, survival analysis has witnessed a\ngrowing interest in the application of deep learning techniques to model\ncomplex, high-dimensional, heterogeneous, incomplete, and censored medical\ndata. Current methods often make assumptions about the relations between data\nthat may not be valid in practice. In response, we introduce SAVAE (Survival\nAnalysis Variational Autoencoder), a novel approach based on Variational\nAutoencoders. SAVAE contributes significantly to the field by introducing a\ntailored ELBO formulation for survival analysis, supporting various parametric\ndistributions for covariates and survival time (as long as the log-likelihood\nis differentiable). It offers a general method that consistently performs well\non various metrics, demonstrating robustness and stability through different\nexperiments. Our proposal effectively estimates time-to-event, accounting for\ncensoring, covariate interactions, and time-varying risk associations. We\nvalidate our model in diverse datasets, including genomic, clinical, and\ndemographic data, with varying levels of censoring. This approach demonstrates\ncompetitive performance compared to state-of-the-art techniques, as assessed by\nthe Concordance Index and the Integrated Brier Score. SAVAE also offers an\ninterpretable model that parametrically models covariates and time. Moreover,\nits generative architecture facilitates further applications such as\nclustering, data imputation, and the generation of synthetic patient data\nthrough latent space inference from survival data.",
          "arxiv_id": "2312.14651v1"
        },
        {
          "title": "DeepIFSAC: Deep Imputation of Missing Values Using Feature and Sample Attention within Contrastive Framework",
          "year": "2025-01",
          "abstract": "Missing values of varying patterns and rates in real-world tabular data pose\na significant challenge in developing reliable data-driven models. The most\ncommonly used statistical and machine learning methods for missing value\nimputation may be ineffective when the missing rate is high and not random.\nThis paper explores row and column attention in tabular data as between-feature\nand between-sample attention in a novel framework to reconstruct missing\nvalues. The proposed method uses CutMix data augmentation within a contrastive\nlearning framework to improve the uncertainty of missing value estimation. The\nperformance and generalizability of trained imputation models are evaluated in\nset-aside test data folds with missing values. The proposed framework is\ncompared with 11 state-of-the-art statistical, machine learning, and deep\nimputation methods using 12 diverse tabular data sets. The average performance\nrank of our proposed method demonstrates its superiority over the\nstate-of-the-art methods for missing rates between 10% and 90% and three\nmissing value types, especially when the missing values are not random. The\nquality of the imputed data using our proposed method is compared in a\ndownstream patient classification task using real-world electronic health\nrecords. This paper highlights the heterogeneity of tabular data sets to\nrecommend imputation methods based on missing value types and data\ncharacteristics.",
          "arxiv_id": "2501.10910v3"
        }
      ],
      "16": [
        {
          "title": "DPpack: An R Package for Differentially Private Statistical Analysis and Machine Learning",
          "year": "2023-09",
          "abstract": "Differential privacy (DP) is the state-of-the-art framework for guaranteeing\nprivacy for individuals when releasing aggregated statistics or building\nstatistical/machine learning models from data. We develop the open-source R\npackage DPpack that provides a large toolkit of differentially private\nanalysis. The current version of DPpack implements three popular mechanisms for\nensuring DP: Laplace, Gaussian, and exponential. Beyond that, DPpack provides a\nlarge toolkit of easily accessible privacy-preserving descriptive statistics\nfunctions. These include mean, variance, covariance, and quantiles, as well as\nhistograms and contingency tables. Finally, DPpack provides user-friendly\nimplementation of privacy-preserving versions of logistic regression, SVM, and\nlinear regression, as well as differentially private hyperparameter tuning for\neach of these models. This extensive collection of implemented differentially\nprivate statistics and models permits hassle-free utilization of differential\nprivacy principles in commonly performed statistical analysis. We plan to\ncontinue developing DPpack and make it more comprehensive by including more\ndifferentially private machine learning techniques, statistical modeling and\ninference in the future.",
          "arxiv_id": "2309.10965v1"
        },
        {
          "title": "Practical Privacy Filters and Odometers with Rényi Differential Privacy and Applications to Differentially Private Deep Learning",
          "year": "2021-03",
          "abstract": "Differential Privacy (DP) is the leading approach to privacy preserving deep\nlearning. As such, there are multiple efforts to provide drop-in integration of\nDP into popular frameworks. These efforts, which add noise to each gradient\ncomputation to make it DP, rely on composition theorems to bound the total\nprivacy loss incurred over this sequence of DP computations.\n  However, existing composition theorems present a tension between efficiency\nand flexibility. Most theorems require all computations in the sequence to have\na predefined DP parameter, called the privacy budget. This prevents the design\nof training algorithms that adapt the privacy budget on the fly, or that\nterminate early to reduce the total privacy loss. Alternatively, the few\nexisting composition results for adaptive privacy budgets provide complex\nbounds on the privacy loss, with constants too large to be practical.\n  In this paper, we study DP composition under adaptive privacy budgets through\nthe lens of R\\'enyi Differential Privacy, proving a simpler composition theorem\nwith smaller constants, making it practical enough to use in algorithm design.\nWe demonstrate two applications of this theorem for DP deep learning: adapting\nthe noise or batch size online to improve a model's accuracy within a fixed\ntotal privacy loss, and stopping early when fine-tuning a model to reduce total\nprivacy loss.",
          "arxiv_id": "2103.01379v2"
        },
        {
          "title": "A Statistical Viewpoint on Differential Privacy: Hypothesis Testing, Representation and Blackwell's Theorem",
          "year": "2024-09",
          "abstract": "Differential privacy is widely considered the formal privacy for\nprivacy-preserving data analysis due to its robust and rigorous guarantees,\nwith increasingly broad adoption in public services, academia, and industry.\nDespite originating in the cryptographic context, in this review paper we argue\nthat, fundamentally, differential privacy can be considered a \\textit{pure}\nstatistical concept. By leveraging David Blackwell's informativeness theorem,\nour focus is to demonstrate based on prior work that all definitions of\ndifferential privacy can be formally motivated from a hypothesis testing\nperspective, thereby showing that hypothesis testing is not merely convenient\nbut also the right language for reasoning about differential privacy. This\ninsight leads to the definition of $f$-differential privacy, which extends\nother differential privacy definitions through a representation theorem. We\nreview techniques that render $f$-differential privacy a unified framework for\nanalyzing privacy bounds in data analysis and machine learning. Applications of\nthis differential privacy definition to private deep learning, private convex\noptimization, shuffled mechanisms, and U.S.\\ Census data are discussed to\nhighlight the benefits of analyzing privacy bounds under this framework\ncompared to existing alternatives.",
          "arxiv_id": "2409.09558v2"
        }
      ],
      "17": [
        {
          "title": "Ensemble Multi-Source Domain Adaptation with Pseudolabels",
          "year": "2020-09",
          "abstract": "Given multiple source datasets with labels, how can we train a target model\nwith no labeled data? Multi-source domain adaptation (MSDA) aims to train a\nmodel using multiple source datasets different from a target dataset in the\nabsence of target data labels. MSDA is a crucial problem applicable to many\npractical cases where labels for the target data are unavailable due to privacy\nissues. Existing MSDA frameworks are limited since they align data without\nconsidering conditional distributions p(x|y) of each domain. They also miss a\nlot of target label information by not considering the target label at all and\nrelying on only one feature extractor. In this paper, we propose Ensemble\nMulti-source Domain Adaptation with Pseudolabels (EnMDAP), a novel method for\nmulti-source domain adaptation. EnMDAP exploits label-wise moment matching to\nalign conditional distributions p(x|y), using pseudolabels for the unavailable\ntarget labels, and introduces ensemble learning theme by using multiple feature\nextractors for accurate domain adaptation. Extensive experiments show that\nEnMDAP provides the state-of-the-art performance for multi-source domain\nadaptation tasks in both of image domains and text domains.",
          "arxiv_id": "2009.14248v1"
        },
        {
          "title": "Contradistinguisher: A Vapnik's Imperative to Unsupervised Domain Adaptation",
          "year": "2020-05",
          "abstract": "A complex combination of simultaneous supervised-unsupervised learning is\nbelieved to be the key to humans performing tasks seamlessly across multiple\ndomains or tasks. This phenomenon of cross-domain learning has been very well\nstudied in domain adaptation literature. Recent domain adaptation works rely on\nan indirect way of first aligning the source and target domain distributions\nand then train a classifier on the labeled source domain to classify the target\ndomain. However, this approach has the main drawback that obtaining a\nnear-perfect alignment of the domains in itself might be difficult/impossible\n(e.g., language domains). To address this, we follow Vapnik's imperative of\nstatistical learning that states any desired problem should be solved in the\nmost direct way rather than solving a more general intermediate task and\npropose a direct approach to domain adaptation that does not require domain\nalignment. We propose a model referred Contradistinguisher that learns\ncontrastive features and whose objective is to jointly learn to\ncontradistinguish the unlabeled target domain in an unsupervised way and\nclassify in a supervised way on the source domain. We achieve the\nstate-of-the-art on Office-31 and VisDA-2017 datasets in both single-source and\nmulti-source settings. We also notice that the contradistinguish loss improves\nthe model performance by increasing the shape bias.",
          "arxiv_id": "2005.14007v3"
        },
        {
          "title": "Unsupervised Domain Adaptation with Progressive Domain Augmentation",
          "year": "2020-04",
          "abstract": "Domain adaptation aims to exploit a label-rich source domain for learning\nclassifiers in a different label-scarce target domain. It is particularly\nchallenging when there are significant divergences between the two domains. In\nthe paper, we propose a novel unsupervised domain adaptation method based on\nprogressive domain augmentation. The proposed method generates virtual\nintermediate domains via domain interpolation, progressively augments the\nsource domain and bridges the source-target domain divergence by conducting\nmultiple subspace alignment on the Grassmann manifold. We conduct experiments\non multiple domain adaptation tasks and the results shows the proposed method\nachieves the state-of-the-art performance.",
          "arxiv_id": "2004.01735v2"
        }
      ],
      "18": [
        {
          "title": "Within-group fairness: A guidance for more sound between-group fairness",
          "year": "2023-01",
          "abstract": "As they have a vital effect on social decision-making, AI algorithms not only\nshould be accurate and but also should not pose unfairness against certain\nsensitive groups (e.g., non-white, women). Various specially designed AI\nalgorithms to ensure trained AI models to be fair between sensitive groups have\nbeen developed. In this paper, we raise a new issue that between-group fair AI\nmodels could treat individuals in a same sensitive group unfairly. We introduce\na new concept of fairness so-called within-group fairness which requires that\nAI models should be fair for those in a same sensitive group as well as those\nin different sensitive groups. We materialize the concept of within-group\nfairness by proposing corresponding mathematical definitions and developing\nlearning algorithms to control within-group fairness and between-group fairness\nsimultaneously. Numerical studies show that the proposed learning algorithms\nimprove within-group fairness without sacrificing accuracy as well as\nbetween-group fairness.",
          "arxiv_id": "2301.08375v1"
        },
        {
          "title": "Algorithmic Decision Making with Conditional Fairness",
          "year": "2020-06",
          "abstract": "Nowadays fairness issues have raised great concerns in decision-making\nsystems. Various fairness notions have been proposed to measure the degree to\nwhich an algorithm is unfair. In practice, there frequently exist a certain set\nof variables we term as fair variables, which are pre-decision covariates such\nas users' choices. The effects of fair variables are irrelevant in assessing\nthe fairness of the decision support algorithm. We thus define conditional\nfairness as a more sound fairness metric by conditioning on the fairness\nvariables. Given different prior knowledge of fair variables, we demonstrate\nthat traditional fairness notations, such as demographic parity and equalized\nodds, are special cases of our conditional fairness notations. Moreover, we\npropose a Derivable Conditional Fairness Regularizer (DCFR), which can be\nintegrated into any decision-making model, to track the trade-off between\nprecision and fairness of algorithmic decision making. Specifically, an\nadversarial representation based conditional independence loss is proposed in\nour DCFR to measure the degree of unfairness. With extensive experiments on\nthree real-world datasets, we demonstrate the advantages of our conditional\nfairness notation and DCFR.",
          "arxiv_id": "2006.10483v5"
        },
        {
          "title": "On the Identification of Fair Auditors to Evaluate Recommender Systems based on a Novel Non-Comparative Fairness Notion",
          "year": "2020-09",
          "abstract": "Decision-support systems are information systems that offer support to\npeople's decisions in various applications such as judiciary, real-estate and\nbanking sectors. Lately, these support systems have been found to be\ndiscriminatory in the context of many practical deployments. In an attempt to\nevaluate and mitigate these biases, algorithmic fairness literature has been\nnurtured using notions of comparative justice, which relies primarily on\ncomparing two/more individuals or groups within the society that is supported\nby such systems. However, such a fairness notion is not very useful in the\nidentification of fair auditors who are hired to evaluate latent biases within\ndecision-support systems. As a solution, we introduce a paradigm shift in\nalgorithmic fairness via proposing a new fairness notion based on the principle\nof non-comparative justice. Assuming that the auditor makes fairness\nevaluations based on some (potentially unknown) desired properties of the\ndecision-support system, the proposed fairness notion compares the system's\noutcome with that of the auditor's desired outcome. We show that the proposed\nfairness notion also provides guarantees in terms of comparative fairness\nnotions by proving that any system can be deemed fair from the perspective of\ncomparative fairness (e.g. individual fairness and statistical parity) if it is\nnon-comparatively fair with respect to an auditor who has been deemed fair with\nrespect to the same fairness notions. We also show that the converse holds true\nin the context of individual fairness. A brief discussion is also presented\nregarding how our fairness notion can be used to identify fair and reliable\nauditors, and how we can use them to quantify biases in decision-support\nsystems.",
          "arxiv_id": "2009.04383v1"
        }
      ],
      "19": [
        {
          "title": "Are Visual Explanations Useful? A Case Study in Model-in-the-Loop Prediction",
          "year": "2020-07",
          "abstract": "We present a randomized controlled trial for a model-in-the-loop regression\ntask, with the goal of measuring the extent to which (1) good explanations of\nmodel predictions increase human accuracy, and (2) faulty explanations decrease\nhuman trust in the model. We study explanations based on visual saliency in an\nimage-based age prediction task for which humans and learned models are\nindividually capable but not highly proficient and frequently disagree. Our\nexperimental design separates model quality from explanation quality, and makes\nit possible to compare treatments involving a variety of explanations of\nvarying levels of quality. We find that presenting model predictions improves\nhuman accuracy. However, visual explanations of various kinds fail to\nsignificantly alter human accuracy or trust in the model - regardless of\nwhether explanations characterize an accurate model, an inaccurate one, or are\ngenerated randomly and independently of the input image. These findings suggest\nthe need for greater evaluation of explanations in downstream decision making\ntasks, better design-based tools for presenting explanations to users, and\nbetter approaches for generating explanations.",
          "arxiv_id": "2007.12248v1"
        },
        {
          "title": "Aligning Explanations with Human Communication",
          "year": "2025-05",
          "abstract": "Machine learning explainability aims to make the decision-making process of\nblack-box models more transparent by finding the most important input features\nfor a given prediction task. Recent works have proposed composing explanations\nfrom semantic concepts (e.g., colors, patterns, shapes) that are inherently\ninterpretable to the user of a model. However, these methods generally ignore\nthe communicative context of explanation-the ability of the user to understand\nthe prediction of the model from the explanation. For example, while a medical\ndoctor might understand an explanation in terms of clinical markers, a patient\nmay need a more accessible explanation to make sense of the same diagnosis. In\nthis paper, we address this gap with listener-adaptive explanations. We propose\nan iterative procedure grounded in principles of pragmatic reasoning and the\nrational speech act to generate explanations that maximize communicative\nutility. Our procedure only needs access to pairwise preferences between\ncandidate explanations, relevant in real-world scenarios where a listener model\nmay not be available. We evaluate our method in image classification tasks,\ndemonstrating improved alignment between explanations and listener preferences\nacross three datasets. Furthermore, we perform a user study that demonstrates\nour explanations increase communicative utility.",
          "arxiv_id": "2505.15626v1"
        },
        {
          "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models",
          "year": "2020-08",
          "abstract": "With machine learning models being increasingly applied to various\ndecision-making scenarios, people have spent growing efforts to make machine\nlearning models more transparent and explainable. Among various explanation\ntechniques, counterfactual explanations have the advantages of being\nhuman-friendly and actionable -- a counterfactual explanation tells the user\nhow to gain the desired prediction with minimal changes to the input. Besides,\ncounterfactual explanations can also serve as efficient probes to the models'\ndecisions. In this work, we exploit the potential of counterfactual\nexplanations to understand and explore the behavior of machine learning models.\nWe design DECE, an interactive visualization system that helps understand and\nexplore a model's decisions on individual instances and data subsets,\nsupporting users ranging from decision-subjects to model developers. DECE\nsupports exploratory analysis of model decisions by combining the strengths of\ncounterfactual explanations at instance- and subgroup-levels. We also introduce\na set of interactions that enable users to customize the generation of\ncounterfactual explanations to find more actionable ones that can suit their\nneeds. Through three use cases and an expert interview, we demonstrate the\neffectiveness of DECE in supporting decision exploration tasks and instance\nexplanations.",
          "arxiv_id": "2008.08353v1"
        }
      ],
      "20": [
        {
          "title": "Matérn Gaussian Processes on Graphs",
          "year": "2020-10",
          "abstract": "Gaussian processes are a versatile framework for learning unknown functions\nin a manner that permits one to utilize prior information about their\nproperties. Although many different Gaussian process models are readily\navailable when the input space is Euclidean, the choice is much more limited\nfor Gaussian processes whose input space is an undirected graph. In this work,\nwe leverage the stochastic partial differential equation characterization of\nMat\\'ern Gaussian processes - a widely-used model class in the Euclidean\nsetting - to study their analog for undirected graphs. We show that the\nresulting Gaussian processes inherit various attractive properties of their\nEuclidean and Riemannian analogs and provide techniques that allow them to be\ntrained using standard methods, such as inducing points. This enables graph\nMat\\'ern Gaussian processes to be employed in mini-batch and non-conjugate\nsettings, thereby making them more accessible to practitioners and easier to\ndeploy within larger learning frameworks.",
          "arxiv_id": "2010.15538v3"
        },
        {
          "title": "Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces",
          "year": "2022-02",
          "abstract": "Bayesian learning using Gaussian processes provides a foundational framework\nfor making decisions in a manner that balances what is known with what could be\nlearned by gathering data. In this dissertation, we develop techniques for\nbroadening the applicability of Gaussian processes. This is done in two ways.\nFirstly, we develop pathwise conditioning techniques for Gaussian processes,\nwhich allow one to express posterior random functions as prior random functions\nplus a dependent update term. We introduce a wide class of efficient\napproximations built from this viewpoint, which can be randomly sampled once in\nadvance, and evaluated at arbitrary locations without any subsequent\nstochasticity. This key property improves efficiency and makes it simpler to\ndeploy Gaussian process models in decision-making settings. Secondly, we\ndevelop a collection of Gaussian process models over non-Euclidean spaces,\nincluding Riemannian manifolds and graphs. We derive fully constructive\nexpressions for the covariance kernels of scalar-valued Gaussian processes on\nRiemannian manifolds and graphs. Building on these ideas, we describe a\nformalism for defining vector-valued Gaussian processes on Riemannian\nmanifolds. The introduced techniques allow all of these models to be trained\nusing standard computational methods. In total, these contributions make\nGaussian processes easier to work with and allow them to be used within a wider\nclass of domains in an effective and principled manner. This, in turn, makes it\npossible to potentially apply Gaussian processes to novel decision-making\nsettings.",
          "arxiv_id": "2202.10613v3"
        },
        {
          "title": "Amortized Variational Inference for Deep Gaussian Processes",
          "year": "2024-09",
          "abstract": "Gaussian processes (GPs) are Bayesian nonparametric models for function\napproximation with principled predictive uncertainty estimates. Deep Gaussian\nprocesses (DGPs) are multilayer generalizations of GPs that can represent\ncomplex marginal densities as well as complex mappings. As exact inference is\neither computationally prohibitive or analytically intractable in GPs and\nextensions thereof, some existing methods resort to variational inference (VI)\ntechniques for tractable approximations. However, the expressivity of\nconventional approximate GP models critically relies on independent inducing\nvariables that might not be informative enough for some problems. In this work\nwe introduce amortized variational inference for DGPs, which learns an\ninference function that maps each observation to variational parameters. The\nresulting method enjoys a more expressive prior conditioned on fewer input\ndependent inducing variables and a flexible amortized marginal posterior that\nis able to model more complicated functions. We show with theoretical reasoning\nand experimental results that our method performs similarly or better than\nprevious approaches at less computational cost.",
          "arxiv_id": "2409.12301v1"
        }
      ],
      "21": [
        {
          "title": "Theoretical Guarantees for Bridging Metric Measure Embedding and Optimal Transport",
          "year": "2020-02",
          "abstract": "We propose a novel approach for comparing distributions whose supports do not\nnecessarily lie on the same metric space. Unlike Gromov-Wasserstein (GW)\ndistance which compares pairwise distances of elements from each distribution,\nwe consider a method allowing to embed the metric measure spaces in a common\nEuclidean space and compute an optimal transport (OT) on the embedded\ndistributions. This leads to what we call a sub-embedding robust Wasserstein\n(SERW) distance. Under some conditions, SERW is a distance that considers an OT\ndistance of the (low-distorted) embedded distributions using a common metric.\nIn addition to this novel proposal that generalizes several recent OT works,\nour contributions stand on several theoretical analyses: (i) we characterize\nthe embedding spaces to define SERW distance for distribution alignment; (ii)\nwe prove that SERW mimics almost the same properties of GW distance, and we\ngive a cost relation between GW and SERW. The paper also provides some\nnumerical illustrations of how SERW behaves on matching problems.",
          "arxiv_id": "2002.08314v5"
        },
        {
          "title": "Entropic regularization of Wasserstein distance between infinite-dimensional Gaussian measures and Gaussian processes",
          "year": "2020-11",
          "abstract": "This work studies the entropic regularization formulation of the\n2-Wasserstein distance on an infinite-dimensional Hilbert space, in particular\nfor the Gaussian setting. We first present the Minimum Mutual Information\nproperty, namely the joint measures of two Gaussian measures on Hilbert space\nwith the smallest mutual information are joint Gaussian measures. This is the\ninfinite-dimensional generalization of the Maximum Entropy property of Gaussian\ndensities on Euclidean space. We then give closed form formulas for the optimal\nentropic transport plan, entropic 2-Wasserstein distance, and Sinkhorn\ndivergence between two Gaussian measures on a Hilbert space, along with the\nfixed point equations for the barycenter of a set of Gaussian measures. Our\nformulations fully exploit the regularization aspect of the entropic\nformulation and are valid both in singular and nonsingular settings. In the\ninfinite-dimensional setting, both the entropic 2-Wasserstein distance and\nSinkhorn divergence are Fr\\'echet differentiable, in contrast to the exact\n2-Wasserstein distance, which is not differentiable. Our Sinkhorn barycenter\nequation is new and always has a unique solution. In contrast, the\nfinite-dimensional barycenter equation for the entropic 2-Wasserstein distance\nfails to generalize to the Hilbert space setting. In the setting of reproducing\nkernel Hilbert spaces (RKHS), our distance formulas are given explicitly in\nterms of the corresponding kernel Gram matrices, providing an interpolation\nbetween the kernel Maximum Mean Discrepancy (MMD) and the kernel 2-Wasserstein\ndistance.",
          "arxiv_id": "2011.07489v3"
        },
        {
          "title": "Heterogeneous Wasserstein Discrepancy for Incomparable Distributions",
          "year": "2021-06",
          "abstract": "Optimal Transport (OT) metrics allow for defining discrepancies between two\nprobability measures. Wasserstein distance is for longer the celebrated\nOT-distance frequently-used in the literature, which seeks probability\ndistributions to be supported on the $\\textit{same}$ metric space. Because of\nits high computational complexity, several approximate Wasserstein distances\nhave been proposed based on entropy regularization or on slicing, and\none-dimensional Wassserstein computation. In this paper, we propose a novel\nextension of Wasserstein distance to compare two incomparable distributions,\nthat hinges on the idea of $\\textit{distributional slicing}$, embeddings, and\non computing the closed-form Wassertein distance between the sliced\ndistributions. We provide a theoretical analysis of this new divergence, called\n$\\textit{heterogeneous Wasserstein discrepancy (HWD)}$, and we show that it\npreserves several interesting properties including rotation-invariance. We show\nthat the embeddings involved in HWD can be efficiently learned. Finally, we\nprovide a large set of experiments illustrating the behavior of HWD as a\ndivergence in the context of generative modeling and in query framework.",
          "arxiv_id": "2106.02542v2"
        }
      ],
      "22": [
        {
          "title": "Yet Meta Learning Can Adapt Fast, It Can Also Break Easily",
          "year": "2020-09",
          "abstract": "Meta learning algorithms have been widely applied in many tasks for efficient\nlearning, such as few-shot image classification and fast reinforcement\nlearning. During meta training, the meta learner develops a common learning\nstrategy, or experience, from a variety of learning tasks. Therefore, during\nmeta test, the meta learner can use the learned strategy to quickly adapt to\nnew tasks even with a few training samples. However, there is still a dark side\nabout meta learning in terms of reliability and robustness. In particular, is\nmeta learning vulnerable to adversarial attacks? In other words, would a\nwell-trained meta learner utilize its learned experience to build wrong or\nlikely useless knowledge, if an adversary unnoticeably manipulates the given\ntraining set? Without the understanding of this problem, it is extremely risky\nto apply meta learning in safety-critical applications. Thus, in this paper, we\nperform the initial study about adversarial attacks on meta learning under the\nfew-shot classification problem. In particular, we formally define key elements\nof adversarial attacks unique to meta learning and propose the first attacking\nalgorithm against meta learning under various settings. We evaluate the\neffectiveness of the proposed attacking strategy as well as the robustness of\nseveral representative meta learning algorithms. Experimental results\ndemonstrate that the proposed attacking strategy can easily break the meta\nlearner and meta learning is vulnerable to adversarial attacks. The\nimplementation of the proposed framework will be released upon the acceptance\nof this paper.",
          "arxiv_id": "2009.01672v1"
        },
        {
          "title": "Task-similarity Aware Meta-learning through Nonparametric Kernel Regression",
          "year": "2020-06",
          "abstract": "This paper investigates the use of nonparametric kernel-regression to obtain\na tasksimilarity aware meta-learning algorithm. Our hypothesis is that the use\nof tasksimilarity helps meta-learning when the available tasks are limited and\nmay contain outlier/ dissimilar tasks. While existing meta-learning approaches\nimplicitly assume the tasks as being similar, it is generally unclear how this\ntask-similarity could be quantified and used in the learning. As a result, most\npopular metalearning approaches do not actively use the\nsimilarity/dissimilarity between the tasks, but rely on availability of huge\nnumber of tasks for their working. Our contribution is a novel framework for\nmeta-learning that explicitly uses task-similarity in the form of kernels and\nan associated meta-learning algorithm. We model the task-specific parameters to\nbelong to a reproducing kernel Hilbert space where the kernel function captures\nthe similarity across tasks. The proposed algorithm iteratively learns a\nmeta-parameter which is used to assign a task-specific descriptor for every\ntask. The task descriptors are then used to quantify the task-similarity\nthrough the kernel function. We show how our approach conceptually generalizes\nthe popular meta-learning approaches of model-agnostic meta-learning (MAML) and\nMeta-stochastic gradient descent (Meta-SGD) approaches. Numerical experiments\nwith regression tasks show that our algorithm outperforms these approaches when\nthe number of tasks is limited, even in the presence of outlier or dissimilar\ntasks. This supports our hypothesis that task-similarity helps improve the\nmetalearning performance in task-limited and adverse settings.",
          "arxiv_id": "2006.07212v2"
        },
        {
          "title": "A Comprehensive Overview and Survey of Recent Advances in Meta-Learning",
          "year": "2020-04",
          "abstract": "This article reviews meta-learning also known as learning-to-learn which\nseeks rapid and accurate model adaptation to unseen tasks with applications in\nhighly automated AI, few-shot learning, natural language processing and\nrobotics. Unlike deep learning, meta-learning can be applied to few-shot\nhigh-dimensional datasets and considers further improving model generalization\nto unseen tasks. Deep learning is focused upon in-sample prediction and\nmeta-learning concerns model adaptation for out-of-sample prediction.\nMeta-learning can continually perform self-improvement to achieve highly\nautonomous AI. Meta-learning may serve as an additional generalization block\ncomplementary for original deep learning model. Meta-learning seeks adaptation\nof machine learning models to unseen tasks which are vastly different from\ntrained tasks. Meta-learning with coevolution between agent and environment\nprovides solutions for complex tasks unsolvable by training from scratch.\nMeta-learning methodology covers a wide range of great minds and thoughts. We\nbriefly introduce meta-learning methodologies in the following categories:\nblack-box meta-learning, metric-based meta-learning, layered meta-learning and\nBayesian meta-learning framework. Recent applications concentrate upon the\nintegration of meta-learning with other machine learning framework to provide\nfeasible integrated problem solutions. We briefly present recent meta-learning\nadvances and discuss potential future research directions.",
          "arxiv_id": "2004.11149v7"
        }
      ],
      "23": [
        {
          "title": "Kandinsky Conformal Prediction: Beyond Class- and Covariate-Conditional Coverage",
          "year": "2025-02",
          "abstract": "Conformal prediction is a powerful distribution-free framework for\nconstructing prediction sets with coverage guarantees. Classical methods, such\nas split conformal prediction, provide marginal coverage, ensuring that the\nprediction set contains the label of a random test point with a target\nprobability. However, these guarantees may not hold uniformly across different\nsubpopulations, leading to disparities in coverage. Prior work has explored\ncoverage guarantees conditioned on events related to the covariates and label\nof the test point. We present Kandinsky conformal prediction, a framework that\nsignificantly expands the scope of conditional coverage guarantees. In contrast\nto Mondrian conformal prediction, which restricts its coverage guarantees to\ndisjoint groups -- reminiscent of the rigid, structured grids of Piet\nMondrian's art -- our framework flexibly handles overlapping and fractional\ngroup memberships defined jointly on covariates and labels, reflecting the\nlayered, intersecting forms in Wassily Kandinsky's compositions. Our algorithm\nunifies and extends existing methods, encompassing covariate-based group\nconditional, class conditional, and Mondrian conformal prediction as special\ncases, while achieving a minimax-optimal high-probability conditional coverage\nbound. Finally, we demonstrate the practicality of our approach through\nempirical evaluation on real-world datasets.",
          "arxiv_id": "2502.17264v2"
        },
        {
          "title": "E-Values Expand the Scope of Conformal Prediction",
          "year": "2025-03",
          "abstract": "Conformal prediction is a powerful framework for distribution-free\nuncertainty quantification. The standard approach to conformal prediction\nrelies on comparing the ranks of prediction scores: under exchangeability, the\nrank of a future test point cannot be too extreme relative to a calibration\nset. This rank-based method can be reformulated in terms of p-values. In this\npaper, we explore an alternative approach based on e-values, known as conformal\ne-prediction. E-values offer key advantages that cannot be achieved with\np-values, enabling new theoretical and practical capabilities. In particular,\nwe present three applications that leverage the unique strengths of e-values:\nbatch anytime-valid conformal prediction, fixed-size conformal sets with\ndata-dependent coverage, and conformal prediction under ambiguous ground truth.\nOverall, these examples demonstrate that e-value-based constructions provide a\nflexible expansion of the toolbox of conformal prediction.",
          "arxiv_id": "2503.13050v3"
        },
        {
          "title": "Conformal e-prediction",
          "year": "2020-01",
          "abstract": "This paper discusses a counterpart of conformal prediction for e-values,\nconformal e-prediction. Conformal e-prediction is conceptually simpler and had\nbeen developed in the 1990s as a precursor of conformal prediction. When\nconformal prediction emerged as result of replacing e-values by p-values, it\nseemed to have important advantages over conformal e-prediction without obvious\ndisadvantages. This paper re-examines relations between conformal prediction\nand conformal e-prediction systematically from a modern perspective. Conformal\ne-prediction has advantages of its own, such as the ease of designing\nconditional conformal e-predictors and the guaranteed validity of\ncross-conformal e-predictors (whereas for cross-conformal predictors validity\nis only an empirical fact and can be broken with excessive randomization). Even\nwhere conformal prediction has clear advantages, conformal e-prediction can\noften emulate those advantages, more or less successfully.",
          "arxiv_id": "2001.05989v5"
        }
      ],
      "24": [
        {
          "title": "Thinking inside the box: A tutorial on grey-box Bayesian optimization",
          "year": "2022-01",
          "abstract": "Bayesian optimization (BO) is a framework for global optimization of\nexpensive-to-evaluate objective functions. Classical BO methods assume that the\nobjective function is a black box. However, internal information about\nobjective function computation is often available. For example, when optimizing\na manufacturing line's throughput with simulation, we observe the number of\nparts waiting at each workstation, in addition to the overall throughput.\nRecent BO methods leverage such internal information to dramatically improve\nperformance. We call these \"grey-box\" BO methods because they treat objective\ncomputation as partially observable and even modifiable, blending the black-box\napproach with so-called \"white-box\" first-principles knowledge of objective\nfunction computation. This tutorial describes these methods, focusing on BO of\ncomposite objective functions, where one can observe and selectively evaluate\nindividual constituents that feed into the overall objective; and\nmulti-fidelity BO, where one can evaluate cheaper approximations of the\nobjective function by varying parameters of the evaluation oracle.",
          "arxiv_id": "2201.00272v1"
        },
        {
          "title": "MBORE: Multi-objective Bayesian Optimisation by Density-Ratio Estimation",
          "year": "2022-03",
          "abstract": "Optimisation problems often have multiple conflicting objectives that can be\ncomputationally and/or financially expensive. Mono-surrogate Bayesian\noptimisation (BO) is a popular model-based approach for optimising such\nblack-box functions. It combines objective values via scalarisation and builds\na Gaussian process (GP) surrogate of the scalarised values. The location which\nmaximises a cheap-to-query acquisition function is chosen as the next location\nto expensively evaluate. While BO is an effective strategy, the use of GPs is\nlimiting. Their performance decreases as the problem input dimensionality\nincreases, and their computational complexity scales cubically with the amount\nof data. To address these limitations, we extend previous work on BO by\ndensity-ratio estimation (BORE) to the multi-objective setting. BORE links the\ncomputation of the probability of improvement acquisition function to that of\nprobabilistic classification. This enables the use of state-of-the-art\nclassifiers in a BO-like framework. In this work we present MBORE:\nmulti-objective Bayesian optimisation by density-ratio estimation, and compare\nit to BO across a range of synthetic and real-world benchmarks. We find that\nMBORE performs as well as or better than BO on a wide variety of problems, and\nthat it outperforms BO on high-dimensional and real-world problems.",
          "arxiv_id": "2203.16912v1"
        },
        {
          "title": "Simulation Based Bayesian Optimization",
          "year": "2024-01",
          "abstract": "Bayesian Optimization (BO) is a powerful method for optimizing black-box\nfunctions by combining prior knowledge with ongoing function evaluations. BO\nconstructs a probabilistic surrogate model of the objective function given the\ncovariates, which is in turn used to inform the selection of future evaluation\npoints through an acquisition function. For smooth continuous search spaces,\nGaussian Processes (GPs) are commonly used as the surrogate model as they offer\nanalytical access to posterior predictive distributions, thus facilitating the\ncomputation and optimization of acquisition functions. However, in complex\nscenarios involving optimization over categorical or mixed covariate spaces,\nGPs may not be ideal. This paper introduces Simulation Based Bayesian\nOptimization (SBBO) as a novel approach to optimizing acquisition functions\nthat only requires sampling-based access to posterior predictive distributions.\nSBBO allows the use of surrogate probabilistic models tailored for\ncombinatorial spaces with discrete variables. Any Bayesian model in which\nposterior inference is carried out through Markov chain Monte Carlo can be\nselected as the surrogate model in SBBO. We demonstrate empirically the\neffectiveness of SBBO using various choices of surrogate models in applications\ninvolving combinatorial optimization.",
          "arxiv_id": "2401.10811v3"
        }
      ],
      "25": [
        {
          "title": "Quantum tangent kernel",
          "year": "2021-11",
          "abstract": "Quantum kernel method is one of the key approaches to quantum machine\nlearning, which has the advantages that it does not require optimization and\nhas theoretical simplicity. By virtue of these properties, several experimental\ndemonstrations and discussions of the potential advantages have been developed\nso far. However, as is the case in classical machine learning, not all quantum\nmachine learning models could be regarded as kernel methods. In this work, we\nexplore a quantum machine learning model with a deep parameterized quantum\ncircuit and aim to go beyond the conventional quantum kernel method. In this\ncase, the representation power and performance are expected to be enhanced,\nwhile the training process might be a bottleneck because of the barren plateaus\nissue. However, we find that parameters of a deep enough quantum circuit do not\nmove much from its initial values during training, allowing first-order\nexpansion with respect to the parameters. This behavior is similar to the\nneural tangent kernel in the classical literatures, and such a deep variational\nquantum machine learning can be described by another emergent kernel, quantum\ntangent kernel. Numerical simulations show that the proposed quantum tangent\nkernel outperforms the conventional quantum kernel method for an\nansatz-generated dataset. This work provides a new direction beyond the\nconventional quantum kernel method and explores potential power of quantum\nmachine learning with deep parameterized quantum circuits.",
          "arxiv_id": "2111.02951v2"
        },
        {
          "title": "Iterative Quantum Feature Maps",
          "year": "2025-06",
          "abstract": "Quantum machine learning models that leverage quantum circuits as quantum\nfeature maps (QFMs) are recognized for their enhanced expressive power in\nlearning tasks. Such models have demonstrated rigorous end-to-end quantum\nspeedups for specific families of classification problems. However, deploying\ndeep QFMs on real quantum hardware remains challenging due to circuit noise and\nhardware constraints. Additionally, variational quantum algorithms often suffer\nfrom computational bottlenecks, particularly in accurate gradient estimation,\nwhich significantly increases quantum resource demands during training. We\npropose Iterative Quantum Feature Maps (IQFMs), a hybrid quantum-classical\nframework that constructs a deep architecture by iteratively connecting shallow\nQFMs with classically computed augmentation weights. By incorporating\ncontrastive learning and a layer-wise training mechanism, IQFMs effectively\nreduces quantum runtime and mitigates noise-induced degradation. In tasks\ninvolving noisy quantum data, numerical experiments show that IQFMs outperforms\nquantum convolutional neural networks, without requiring the optimization of\nvariational quantum parameters. Even for a typical classical image\nclassification benchmark, a carefully designed IQFMs achieves performance\ncomparable to that of classical neural networks. This framework presents a\npromising path to address current limitations and harness the full potential of\nquantum-enhanced machine learning.",
          "arxiv_id": "2506.19461v1"
        },
        {
          "title": "Supervised quantum machine learning models are kernel methods",
          "year": "2021-01",
          "abstract": "With near-term quantum devices available and the race for fault-tolerant\nquantum computers in full swing, researchers became interested in the question\nof what happens if we replace a supervised machine learning model with a\nquantum circuit. While such \"quantum models\" are sometimes called \"quantum\nneural networks\", it has been repeatedly noted that their mathematical\nstructure is actually much more closely related to kernel methods: they analyse\ndata in high-dimensional Hilbert spaces to which we only have access through\ninner products revealed by measurements. This technical manuscript summarises\nand extends the idea of systematically rephrasing supervised quantum models as\na kernel method. With this, a lot of near-term and fault-tolerant quantum\nmodels can be replaced by a general support vector machine whose kernel\ncomputes distances between data-encoding quantum states. Kernel-based training\nis then guaranteed to find better or equally good quantum models than\nvariational circuit training. Overall, the kernel perspective of quantum\nmachine learning tells us that the way that data is encoded into quantum states\nis the main ingredient that can potentially set quantum models apart from\nclassical machine learning models.",
          "arxiv_id": "2101.11020v2"
        }
      ],
      "26": [
        {
          "title": "A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness",
          "year": "2022-05",
          "abstract": "Accurate uncertainty quantification is a major challenge in deep learning, as\nneural networks can make overconfident errors and assign high confidence\npredictions to out-of-distribution (OOD) inputs. The most popular approaches to\nestimate predictive uncertainty in deep learning are methods that combine\npredictions from multiple neural networks, such as Bayesian neural networks\n(BNNs) and deep ensembles. However their practicality in real-time,\nindustrial-scale applications are limited due to the high memory and\ncomputational cost. Furthermore, ensembles and BNNs do not necessarily fix all\nthe issues with the underlying member networks. In this work, we study\nprincipled approaches to improve uncertainty property of a single network,\nbased on a single, deterministic representation. By formalizing the uncertainty\nquantification as a minimax learning problem, we first identify distance\nawareness, i.e., the model's ability to quantify the distance of a testing\nexample from the training data, as a necessary condition for a DNN to achieve\nhigh-quality (i.e., minimax optimal) uncertainty estimation. We then propose\nSpectral-normalized Neural Gaussian Process (SNGP), a simple method that\nimproves the distance-awareness ability of modern DNNs with two simple changes:\n(1) applying spectral normalization to hidden weights to enforce bi-Lipschitz\nsmoothness in representations and (2) replacing the last output layer with a\nGaussian process layer. On a suite of vision and language understanding\nbenchmarks, SNGP outperforms other single-model approaches in prediction,\ncalibration and out-of-domain detection. Furthermore, SNGP provides\ncomplementary benefits to popular techniques such as deep ensembles and data\naugmentation, making it a simple and scalable building block for probabilistic\ndeep learning. Code is open-sourced at\nhttps://github.com/google/uncertainty-baselines",
          "arxiv_id": "2205.00403v2"
        },
        {
          "title": "Hybrid Bayesian Neural Networks with Functional Probabilistic Layers",
          "year": "2021-07",
          "abstract": "Bayesian neural networks provide a direct and natural way to extend standard\ndeep neural networks to support probabilistic deep learning through the use of\nprobabilistic layers that, traditionally, encode weight (and bias) uncertainty.\nIn particular, hybrid Bayesian neural networks utilize standard deterministic\nlayers together with few probabilistic layers judicially positioned in the\nnetworks for uncertainty estimation. A major aspect and benefit of Bayesian\ninference is that priors, in principle, provide the means to encode prior\nknowledge for use in inference and prediction. However, it is difficult to\nspecify priors on weights since the weights have no intuitive interpretation.\nFurther, the relationships of priors on weights to the functions computed by\nnetworks are difficult to characterize. In contrast, functions are intuitive to\ninterpret and are direct since they map inputs to outputs. Therefore, it is\nnatural to specify priors on functions to encode prior knowledge, and to use\nthem in inference and prediction based on functions. To support this, we\npropose hybrid Bayesian neural networks with functional probabilistic layers\nthat encode function (and activation) uncertainty. We discuss their foundations\nin functional Bayesian inference, functional variational inference, sparse\nGaussian processes, and sparse variational Gaussian processes. We further\nperform few proof-of-concept experiments using GPflus, a new library that\nprovides Gaussian process layers and supports their use with deterministic\nKeras layers to form hybrid neural network and Gaussian process models.",
          "arxiv_id": "2107.07014v1"
        },
        {
          "title": "Understanding the Trade-offs in Accuracy and Uncertainty Quantification: Architecture and Inference Choices in Bayesian Neural Networks",
          "year": "2025-03",
          "abstract": "As modern neural networks get more complex, specifying a model with high\npredictive performance and sound uncertainty quantification becomes a more\nchallenging task. Despite some promising theoretical results on the true\nposterior predictive distribution of Bayesian neural networks, the properties\nof even the most commonly used posterior approximations are often questioned.\nComputational burdens and intractable posteriors expose miscalibrated Bayesian\nneural networks to poor accuracy and unreliable uncertainty estimates.\nApproximate Bayesian inference aims to replace unknown and intractable\nposterior distributions with some simpler but feasible distributions. The\ndimensions of modern deep models, coupled with the lack of identifiability,\nmake Markov chain Monte Carlo (MCMC) tremendously expensive and unable to fully\nexplore the multimodal posterior. On the other hand, variational inference\nbenefits from improved computational complexity but lacks the asymptotical\nguarantees of sampling-based inference and tends to concentrate around a single\nmode. The performance of both approaches heavily depends on architectural\nchoices; this paper aims to shed some light on this by considering the\ncomputational costs, accuracy and uncertainty quantification in different\nscenarios including large width and out-of-sample data. To improve posterior\nexploration, different model averaging and ensembling techniques are studied,\nalong with their benefits on predictive performance. In our experiments,\nvariational inference overall provided better uncertainty quantification than\nMCMC; further, stacking and ensembles of variational approximations provided\ncomparable accuracy to MCMC at a much-reduced cost.",
          "arxiv_id": "2503.11808v2"
        }
      ],
      "27": [
        {
          "title": "Differentiable Joint Pruning and Quantization for Hardware Efficiency",
          "year": "2020-07",
          "abstract": "We present a differentiable joint pruning and quantization (DJPQ) scheme. We\nframe neural network compression as a joint gradient-based optimization\nproblem, trading off between model pruning and quantization automatically for\nhardware efficiency. DJPQ incorporates variational information bottleneck based\nstructured pruning and mixed-bit precision quantization into a single\ndifferentiable loss function. In contrast to previous works which consider\npruning and quantization separately, our method enables users to find the\noptimal trade-off between both in a single training procedure. To utilize the\nmethod for more efficient hardware inference, we extend DJPQ to integrate\nstructured pruning with power-of-two bit-restricted quantization. We show that\nDJPQ significantly reduces the number of Bit-Operations (BOPs) for several\nnetworks while maintaining the top-1 accuracy of original floating-point models\n(e.g., 53x BOPs reduction in ResNet18 on ImageNet, 43x in MobileNetV2).\nCompared to the conventional two-stage approach, which optimizes pruning and\nquantization independently, our scheme outperforms in terms of both accuracy\nand BOPs. Even when considering bit-restricted quantization, DJPQ achieves\nlarger compression ratios and better accuracy than the two-stage approach.",
          "arxiv_id": "2007.10463v2"
        },
        {
          "title": "Towards Hardware-Specific Automatic Compression of Neural Networks",
          "year": "2022-12",
          "abstract": "Compressing neural network architectures is important to allow the deployment\nof models to embedded or mobile devices, and pruning and quantization are the\nmajor approaches to compress neural networks nowadays. Both methods benefit\nwhen compression parameters are selected specifically for each layer. Finding\ngood combinations of compression parameters, so-called compression policies, is\nhard as the problem spans an exponentially large search space. Effective\ncompression policies consider the influence of the specific hardware\narchitecture on the used compression methods. We propose an algorithmic\nframework called Galen to search such policies using reinforcement learning\nutilizing pruning and quantization, thus providing automatic compression for\nneural networks. Contrary to other approaches we use inference latency measured\non the target hardware device as an optimization goal. With that, the framework\nsupports the compression of models specific to a given hardware target. We\nvalidate our approach using three different reinforcement learning agents for\npruning, quantization and joint pruning and quantization. Besides proving the\nfunctionality of our approach we were able to compress a ResNet18 for CIFAR-10,\non an embedded ARM processor, to 20% of the original inference latency without\nsignificant loss of accuracy. Moreover, we can demonstrate that a joint search\nand compression using pruning and quantization is superior to an individual\nsearch for policies using a single compression method.",
          "arxiv_id": "2212.07818v1"
        },
        {
          "title": "Pruning Filters while Training for Efficiently Optimizing Deep Learning Networks",
          "year": "2020-03",
          "abstract": "Modern deep networks have millions to billions of parameters, which leads to\nhigh memory and energy requirements during training as well as during inference\non resource-constrained edge devices. Consequently, pruning techniques have\nbeen proposed that remove less significant weights in deep networks, thereby\nreducing their memory and computational requirements. Pruning is usually\nperformed after training the original network, and is followed by further\nretraining to compensate for the accuracy loss incurred during pruning. The\nprune-and-retrain procedure is repeated iteratively until an optimum tradeoff\nbetween accuracy and efficiency is reached. However, such iterative retraining\nadds to the overall training complexity of the network. In this work, we\npropose a dynamic pruning-while-training procedure, wherein we prune filters of\nthe convolutional layers of a deep network during training itself, thereby\nprecluding the need for separate retraining. We evaluate our dynamic\npruning-while-training approach with three different pre-existing pruning\nstrategies, viz. mean activation-based pruning, random pruning, and L1\nnormalization-based pruning. Our results for VGG-16 trained on CIFAR10 shows\nthat L1 normalization provides the best performance among all the techniques\nexplored in this work with less than 1% drop in accuracy after pruning 80% of\nthe filters compared to the original network. We further evaluated the L1\nnormalization based pruning mechanism on CIFAR100. Results indicate that\npruning while training yields a compressed network with almost no accuracy loss\nafter pruning 50% of the filters compared to the original network and ~5% loss\nfor high pruning rates (>80%). The proposed pruning methodology yields 41%\nreduction in the number of computations and memory accesses during training for\nCIFAR10, CIFAR100 and ImageNet compared to training with retraining for 10\nepochs .",
          "arxiv_id": "2003.02800v1"
        }
      ],
      "28": [
        {
          "title": "Uncovering the structure of clinical EEG signals with self-supervised learning",
          "year": "2020-07",
          "abstract": "Objective. Supervised learning paradigms are often limited by the amount of\nlabeled data that is available. This phenomenon is particularly problematic in\nclinically-relevant data, such as electroencephalography (EEG), where labeling\ncan be costly in terms of specialized expertise and human processing time.\nConsequently, deep learning architectures designed to learn on EEG data have\nyielded relatively shallow models and performances at best similar to those of\ntraditional feature-based approaches. However, in most situations, unlabeled\ndata is available in abundance. By extracting information from this unlabeled\ndata, it might be possible to reach competitive performance with deep neural\nnetworks despite limited access to labels. Approach. We investigated\nself-supervised learning (SSL), a promising technique for discovering structure\nin unlabeled data, to learn representations of EEG signals. Specifically, we\nexplored two tasks based on temporal context prediction as well as contrastive\npredictive coding on two clinically-relevant problems: EEG-based sleep staging\nand pathology detection. We conducted experiments on two large public datasets\nwith thousands of recordings and performed baseline comparisons with purely\nsupervised and hand-engineered approaches. Main results. Linear classifiers\ntrained on SSL-learned features consistently outperformed purely supervised\ndeep neural networks in low-labeled data regimes while reaching competitive\nperformance when all labels were available. Additionally, the embeddings\nlearned with each method revealed clear latent structures related to\nphysiological and clinical phenomena, such as age effects. Significance. We\ndemonstrate the benefit of self-supervised learning approaches on EEG data. Our\nresults suggest that SSL may pave the way to a wider use of deep learning\nmodels on EEG data.",
          "arxiv_id": "2007.16104v1"
        },
        {
          "title": "Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network",
          "year": "2020-03",
          "abstract": "Chest X-ray is the first imaging technique that plays an important role in\nthe diagnosis of COVID-19 disease. Due to the high availability of large-scale\nannotated image datasets, great success has been achieved using convolutional\nneural networks (CNNs) for image recognition and classification. However, due\nto the limited availability of annotated medical images, the classification of\nmedical images remains the biggest challenge in medical diagnosis. Thanks to\ntransfer learning, an effective mechanism that can provide a promising solution\nby transferring knowledge from generic object recognition tasks to\ndomain-specific tasks. In this paper, we validate and adapt our previously\ndeveloped CNN, called Decompose, Transfer, and Compose (DeTraC), for the\nclassification of COVID-19 chest X-ray images. DeTraC can deal with any\nirregularities in the image dataset by investigating its class boundaries using\na class decomposition mechanism. The experimental results showed the capability\nof DeTraC in the detection of COVID-19 cases from a comprehensive image dataset\ncollected from several hospitals around the world. High accuracy of 95.12%\n(with a sensitivity of 97.91%, a specificity of 91.87%, and a precision of\n93.36%) was achieved by DeTraC in the detection of COVID-19 X-ray images from\nnormal, and severe acute respiratory syndrome cases.",
          "arxiv_id": "2003.13815v3"
        },
        {
          "title": "A review: Deep learning for medical image segmentation using multi-modality fusion",
          "year": "2020-04",
          "abstract": "Multi-modality is widely used in medical imaging, because it can provide\nmultiinformation about a target (tumor, organ or tissue). Segmentation using\nmultimodality consists of fusing multi-information to improve the segmentation.\nRecently, deep learning-based approaches have presented the state-of-the-art\nperformance in image classification, segmentation, object detection and\ntracking tasks. Due to their self-learning and generalization ability over\nlarge amounts of data, deep learning recently has also gained great interest in\nmulti-modal medical image segmentation. In this paper, we give an overview of\ndeep learning-based approaches for multi-modal medical image segmentation task.\nFirstly, we introduce the general principle of deep learning and multi-modal\nmedical image segmentation. Secondly, we present different deep learning\nnetwork architectures, then analyze their fusion strategies and compare their\nresults. The earlier fusion is commonly used, since it's simple and it focuses\non the subsequent segmentation network architecture. However, the later fusion\ngives more attention on fusion strategy to learn the complex relationship\nbetween different modalities. In general, compared to the earlier fusion, the\nlater fusion can give more accurate result if the fusion method is effective\nenough. We also discuss some common problems in medical image segmentation.\nFinally, we summarize and provide some perspectives on the future research.",
          "arxiv_id": "2004.10664v2"
        }
      ],
      "29": [
        {
          "title": "Heterogeneous Random Forest",
          "year": "2024-10",
          "abstract": "Random forest (RF) stands out as a highly favored machine learning approach\nfor classification problems. The effectiveness of RF hinges on two key factors:\nthe accuracy of individual trees and the diversity among them. In this study,\nwe introduce a novel approach called heterogeneous RF (HRF), designed to\nenhance tree diversity in a meaningful way. This diversification is achieved by\ndeliberately introducing heterogeneity during the tree construction.\nSpecifically, features used for splitting near the root node of previous trees\nare assigned lower weights when constructing the feature sub-space of the\nsubsequent trees. As a result, dominant features in the prior trees are less\nlikely to be employed in the next iteration, leading to a more diverse set of\nsplitting features at the nodes. Through simulation studies, it was confirmed\nthat the HRF method effectively mitigates the selection bias of trees within\nthe ensemble, increases the diversity of the ensemble, and demonstrates\nsuperior performance on datasets with fewer noise features. To assess the\ncomparative performance of HRF against other widely adopted ensemble methods,\nwe conducted tests on 52 datasets, comprising both real-world and synthetic\ndata. HRF consistently outperformed other ensemble methods in terms of accuracy\nacross the majority of datasets.",
          "arxiv_id": "2410.19022v1"
        },
        {
          "title": "Rule Covering for Interpretation and Boosting",
          "year": "2020-07",
          "abstract": "We propose two algorithms for interpretation and boosting of tree-based\nensemble methods. Both algorithms make use of mathematical programming models\nthat are constructed with a set of rules extracted from an ensemble of decision\ntrees. The objective is to obtain the minimum total impurity with the least\nnumber of rules that cover all the samples. The first algorithm uses the\ncollection of decision trees obtained from a trained random forest model. Our\nnumerical results show that the proposed rule covering approach selects only a\nfew rules that could be used for interpreting the random forest model.\nMoreover, the resulting set of rules closely matches the accuracy level of the\nrandom forest model. Inspired by the column generation algorithm in linear\nprogramming, our second algorithm uses a rule generation scheme for boosting\ndecision trees. We use the dual optimal solutions of the linear programming\nmodels as sample weights to obtain only those rules that would improve the\naccuracy. With a computational study, we observe that our second algorithm\nperforms competitively with the other well-known boosting methods. Our\nimplementations also demonstrate that both algorithms can be trivially coupled\nwith the existing random forest and decision tree packages.",
          "arxiv_id": "2007.06379v2"
        },
        {
          "title": "Boosting-Based Sequential Meta-Tree Ensemble Construction for Improved Decision Trees",
          "year": "2024-02",
          "abstract": "A decision tree is one of the most popular approaches in machine learning\nfields. However, it suffers from the problem of overfitting caused by overly\ndeepened trees. Then, a meta-tree is recently proposed. It solves the problem\nof overfitting caused by overly deepened trees. Moreover, the meta-tree\nguarantees statistical optimality based on Bayes decision theory. Therefore,\nthe meta-tree is expected to perform better than the decision tree. In contrast\nto a single decision tree, it is known that ensembles of decision trees, which\nare typically constructed boosting algorithms, are more effective in improving\npredictive performance. Thus, it is expected that ensembles of meta-trees are\nmore effective in improving predictive performance than a single meta-tree, and\nthere are no previous studies that construct multiple meta-trees in boosting.\nTherefore, in this study, we propose a method to construct multiple meta-trees\nusing a boosting approach. Through experiments with synthetic and benchmark\ndatasets, we conduct a performance comparison between the proposed methods and\nthe conventional methods using ensembles of decision trees. Furthermore, while\nensembles of decision trees can cause overfitting as well as a single decision\ntree, experiments confirmed that ensembles of meta-trees can prevent\noverfitting due to the tree depth.",
          "arxiv_id": "2402.06386v1"
        }
      ],
      "30": [
        {
          "title": "PAC-Bayes Analysis Beyond the Usual Bounds",
          "year": "2020-06",
          "abstract": "We focus on a stochastic learning model where the learner observes a finite\nset of training examples and the output of the learning process is a\ndata-dependent distribution over a space of hypotheses. The learned\ndata-dependent distribution is then used to make randomized predictions, and\nthe high-level theme addressed here is guaranteeing the quality of predictions\non examples that were not seen during training, i.e. generalization. In this\nsetting the unknown quantity of interest is the expected risk of the\ndata-dependent randomized predictor, for which upper bounds can be derived via\na PAC-Bayes analysis, leading to PAC-Bayes bounds.\n  Specifically, we present a basic PAC-Bayes inequality for stochastic kernels,\nfrom which one may derive extensions of various known PAC-Bayes bounds as well\nas novel bounds. We clarify the role of the requirements of fixed 'data-free'\npriors, bounded losses, and i.i.d. data. We highlight that those requirements\nwere used to upper-bound an exponential moment term, while the basic PAC-Bayes\ntheorem remains valid without those restrictions. We present three bounds that\nillustrate the use of data-dependent priors, including one for the unbounded\nsquare loss.",
          "arxiv_id": "2006.13057v3"
        },
        {
          "title": "A Limitation of the PAC-Bayes Framework",
          "year": "2020-06",
          "abstract": "PAC-Bayes is a useful framework for deriving generalization bounds which was\nintroduced by McAllester ('98). This framework has the flexibility of deriving\ndistribution- and algorithm-dependent bounds, which are often tighter than\nVC-related uniform convergence bounds. In this manuscript we present a\nlimitation for the PAC-Bayes framework. We demonstrate an easy learning task\nthat is not amenable to a PAC-Bayes analysis.\n  Specifically, we consider the task of linear classification in 1D; it is\nwell-known that this task is learnable using just $O(\\log(1/\\delta)/\\epsilon)$\nexamples. On the other hand, we show that this fact can not be proved using a\nPAC-Bayes analysis: for any algorithm that learns 1-dimensional linear\nclassifiers there exists a (realizable) distribution for which the PAC-Bayes\nbound is arbitrarily large.",
          "arxiv_id": "2006.13508v3"
        },
        {
          "title": "PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate bounds that handle general VC classes",
          "year": "2021-06",
          "abstract": "We give a novel, unified derivation of conditional PAC-Bayesian and mutual\ninformation (MI) generalization bounds. We derive conditional MI bounds as an\ninstance, with special choice of prior, of conditional MAC-Bayesian (Mean\nApproximately Correct) bounds, itself derived from conditional PAC-Bayesian\nbounds, where `conditional' means that one can use priors conditioned on a\njoint training and ghost sample. This allows us to get nontrivial PAC-Bayes and\nMI-style bounds for general VC classes, something recently shown to be\nimpossible with standard PAC-Bayesian/MI bounds. Second, it allows us to get\nfaster rates of order $O \\left(({\\text{KL}}/n)^{\\gamma}\\right)$ for $\\gamma >\n1/2$ if a Bernstein condition holds and for exp-concave losses (with\n$\\gamma=1$), which is impossible with both standard PAC-Bayes generalization\nand MI bounds. Our work extends the recent work by Steinke and Zakynthinou\n[2020] who handle MI with VC but neither PAC-Bayes nor fast rates, the recent\nwork of Hellstr\\\"om and Durisi [2020] who extend the latter to the PAC-Bayes\nsetting via a unifying exponential inequality, and Mhammedi et al. [2019] who\ninitiated fast rate PAC-Bayes generalization error bounds but handle neither MI\nnor general VC classes.",
          "arxiv_id": "2106.09683v1"
        }
      ],
      "31": [
        {
          "title": "Non-Exhaustive, Overlapping Co-Clustering: An Extended Analysis",
          "year": "2020-04",
          "abstract": "The goal of co-clustering is to simultaneously identify a clustering of rows\nas well as columns of a two dimensional data matrix. A number of co-clustering\ntechniques have been proposed including information-theoretic co-clustering and\nthe minimum sum-squared residue co-clustering method. However, most existing\nco-clustering algorithms are designed to find pairwise disjoint and exhaustive\nco-clusters while many real-world datasets contain not only a large overlap\nbetween co-clusters but also outliers which should not belong to any\nco-cluster. In this paper, we formulate the problem of Non-Exhaustive,\nOverlapping Co-Clustering where both of the row and column clusters are allowed\nto overlap with each other and outliers for each dimension of the data matrix\nare not assigned to any cluster. To solve this problem, we propose intuitive\nobjective functions, and develop an an efficient iterative algorithm which we\ncall the NEO-CC algorithm. We theoretically show that the NEO-CC algorithm\nmonotonically decreases the proposed objective functions. Experimental results\nshow that the NEO-CC algorithm is able to effectively capture the underlying\nco-clustering structure of real-world data, and thus outperforms\nstate-of-the-art clustering and co-clustering methods. This manuscript includes\nan extended analysis of [21].",
          "arxiv_id": "2004.11530v1"
        },
        {
          "title": "Convex Clustering through MM: An Efficient Algorithm to Perform Hierarchical Clustering",
          "year": "2022-11",
          "abstract": "Convex clustering is a modern method with both hierarchical and $k$-means\nclustering characteristics. Although convex clustering can capture complex\nclustering structures hidden in data, the existing convex clustering algorithms\nare not scalable to large data sets with sample sizes greater than several\nthousands. Moreover, it is known that convex clustering sometimes fails to\nproduce a complete hierarchical clustering structure. This issue arises if\nclusters split up or the minimum number of possible clusters is larger than the\ndesired number of clusters. In this paper, we propose convex clustering through\nmajorization-minimization (CCMM) -- an iterative algorithm that uses cluster\nfusions and a highly efficient updating scheme derived using diagonal\nmajorization. Additionally, we explore different strategies to ensure that the\nhierarchical clustering structure terminates in a single cluster. With a\ncurrent desktop computer, CCMM efficiently solves convex clustering problems\nfeaturing over one million objects in seven-dimensional space, achieving a\nsolution time of 51 seconds on average.",
          "arxiv_id": "2211.01877v2"
        },
        {
          "title": "K-expectiles clustering",
          "year": "2021-03",
          "abstract": "$K$-means clustering is one of the most widely-used partitioning algorithm in\ncluster analysis due to its simplicity and computational efficiency. However,\n$K$-means does not provide an appropriate clustering result when applying to\ndata with non-spherically shaped clusters. We propose a novel partitioning\nclustering algorithm based on expectiles. The cluster centers are defined as\nmultivariate expectiles and clusters are searched via a greedy algorithm by\nminimizing the within cluster '$\\tau$ -variance'. We suggest two schemes: fixed\n$\\tau$ clustering, and adaptive $\\tau$ clustering. Validated by simulation\nresults, this method beats both $K$-means and spectral clustering on data with\nasymmetric shaped clusters, or clusters with a complicated structure, including\nasymmetric normal, beta, skewed $t$ and $F$ distributed clusters. Applications\nof adaptive $\\tau$ clustering on crypto-currency (CC) market data are provided.\nOne finds that the expectiles clusters of CC markets show the phenomena of an\ninstitutional investors dominated market. The second application is on image\nsegmentation. compared to other center based clustering methods, the adaptive\n$\\tau$ cluster centers of pixel data can better capture and describe the\nfeatures of an image. The fixed $\\tau$ clustering brings more flexibility on\nsegmentation with a decent accuracy.",
          "arxiv_id": "2103.09329v1"
        }
      ],
      "32": [
        {
          "title": "DiffNet++: A Neural Influence and Interest Diffusion Network for Social Recommendation",
          "year": "2020-01",
          "abstract": "Social recommendation has emerged to leverage social connections among users\nfor predicting users' unknown preferences, which could alleviate the data\nsparsity issue in collaborative filtering based recommendation. Early\napproaches relied on utilizing each user's first-order social neighbors'\ninterests for better user modeling and failed to model the social influence\ndiffusion process from the global social network structure. Recently, we\npropose a preliminary work of a neural influence diffusion network (i.e.,\nDiffNet) for social recommendation (Diffnet), which models the recursive social\ndiffusion process to capture the higher-order relationships for each user.\nHowever, we argue that, as users play a central role in both user-user social\nnetwork and user-item interest network, only modeling the influence diffusion\nprocess in the social network would neglect the users' latent collaborative\ninterests in the user-item interest network. In this paper, we propose\nDiffNet++, an improved algorithm of DiffNet that models the neural influence\ndiffusion and interest diffusion in a unified framework. By reformulating the\nsocial recommendation as a heterogeneous graph with social network and interest\nnetwork as input, DiffNet++ advances DiffNet by injecting these two network\ninformation for user embedding learning at the same time. This is achieved by\niteratively aggregating each user's embedding from three aspects: the user's\nprevious embedding, the influence aggregation of social neighbors from the\nsocial network, and the interest aggregation of item neighbors from the\nuser-item interest network. Furthermore, we design a multi-level attention\nnetwork that learns how to attentively aggregate user embeddings from these\nthree aspects. Finally, extensive experimental results on two real-world\ndatasets clearly show the effectiveness of our proposed model.",
          "arxiv_id": "2002.00844v4"
        },
        {
          "title": "Controllable Multi-Interest Framework for Recommendation",
          "year": "2020-05",
          "abstract": "Recently, neural networks have been widely used in e-commerce recommender\nsystems, owing to the rapid development of deep learning. We formalize the\nrecommender system as a sequential recommendation problem, intending to predict\nthe next items that the user might be interacted with. Recent works usually\ngive an overall embedding from a user's behavior sequence. However, a unified\nuser embedding cannot reflect the user's multiple interests during a period. In\nthis paper, we propose a novel controllable multi-interest framework for the\nsequential recommendation, called ComiRec. Our multi-interest module captures\nmultiple interests from user behavior sequences, which can be exploited for\nretrieving candidate items from the large-scale item pool. These items are then\nfed into an aggregation module to obtain the overall recommendation. The\naggregation module leverages a controllable factor to balance the\nrecommendation accuracy and diversity. We conduct experiments for the\nsequential recommendation on two real-world datasets, Amazon and Taobao.\nExperimental results demonstrate that our framework achieves significant\nimprovements over state-of-the-art models. Our framework has also been\nsuccessfully deployed on the offline Alibaba distributed cloud platform.",
          "arxiv_id": "2005.09347v2"
        },
        {
          "title": "FINN.no Slates Dataset: A new Sequential Dataset Logging Interactions, allViewed Items and Click Responses/No-Click for Recommender Systems Research",
          "year": "2021-11",
          "abstract": "We present a novel recommender systems dataset that records the sequential\ninteractions between users and an online marketplace. The users are\nsequentially presented with both recommendations and search results in the form\nof ranked lists of items, called slates, from the marketplace. The dataset\nincludes the presented slates at each round, whether the user clicked on any of\nthese items and which item the user clicked on. Although the usage of exposure\ndata in recommender systems is growing, to our knowledge there is no open\nlarge-scale recommender systems dataset that includes the slates of items\npresented to the users at each interaction. As a result, most articles on\nrecommender systems do not utilize this exposure information. Instead, the\nproposed models only depend on the user's click responses, and assume that the\nuser is exposed to all the items in the item universe at each step, often\ncalled uniform candidate sampling. This is an incomplete assumption, as it\ntakes into account items the user might not have been exposed to. This way\nitems might be incorrectly considered as not of interest to the user. Taking\ninto account the actually shown slates allows the models to use a more natural\nlikelihood, based on the click probability given the exposure set of items, as\nis prevalent in the bandit and reinforcement learning literature.\n\\cite{Eide2021DynamicSampling} shows that likelihoods based on uniform\ncandidate sampling (and similar assumptions) are implicitly assuming that the\nplatform only shows the most relevant items to the user. This causes the\nrecommender system to implicitly reinforce feedback loops and to be biased\ntowards previously exposed items to the user.",
          "arxiv_id": "2111.03340v1"
        }
      ],
      "33": [
        {
          "title": "AudioPaLM: A Large Language Model That Can Speak and Listen",
          "year": "2023-06",
          "abstract": "We introduce AudioPaLM, a large language model for speech understanding and\ngeneration. AudioPaLM fuses text-based and speech-based language models, PaLM-2\n[Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified\nmultimodal architecture that can process and generate text and speech with\napplications including speech recognition and speech-to-speech translation.\nAudioPaLM inherits the capability to preserve paralinguistic information such\nas speaker identity and intonation from AudioLM and the linguistic knowledge\npresent only in text large language models such as PaLM-2. We demonstrate that\ninitializing AudioPaLM with the weights of a text-only large language model\nimproves speech processing, successfully leveraging the larger quantity of text\ntraining data used in pretraining to assist with the speech tasks. The\nresulting model significantly outperforms existing systems for speech\ntranslation tasks and has the ability to perform zero-shot speech-to-text\ntranslation for many languages for which input/target language combinations\nwere not seen in training. AudioPaLM also demonstrates features of audio\nlanguage models, such as transferring a voice across languages based on a short\nspoken prompt. We release examples of our method at\nhttps://google-research.github.io/seanet/audiopalm/examples",
          "arxiv_id": "2306.12925v1"
        },
        {
          "title": "AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition",
          "year": "2023-09",
          "abstract": "Audio-visual speech contains synchronized audio and visual information that\nprovides cross-modal supervision to learn representations for both automatic\nspeech recognition (ASR) and visual speech recognition (VSR). We introduce\ncontinuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a\nsemi-supervised method to train an audio-visual speech recognition (AVSR) model\non a combination of labeled and unlabeled videos with continuously regenerated\npseudo-labels. Our models are trained for speech recognition from audio-visual\ninputs and can perform speech recognition using both audio and visual\nmodalities, or only one modality. Our method uses the same audio-visual model\nfor both supervised training and pseudo-label generation, mitigating the need\nfor external speech recognition models to generate pseudo-labels. AV-CPL\nobtains significant improvements in VSR performance on the LRS3 dataset while\nmaintaining practical ASR and AVSR performance. Finally, using visual-only\nspeech data, our method is able to leverage unlabeled visual speech to improve\nVSR.",
          "arxiv_id": "2309.17395v1"
        },
        {
          "title": "Speech Enhancement using Self-Adaptation and Multi-Head Self-Attention",
          "year": "2020-02",
          "abstract": "This paper investigates a self-adaptation method for speech enhancement using\nauxiliary speaker-aware features; we extract a speaker representation used for\nadaptation directly from the test utterance. Conventional studies of deep\nneural network (DNN)--based speech enhancement mainly focus on building a\nspeaker independent model. Meanwhile, in speech applications including speech\nrecognition and synthesis, it is known that model adaptation to the target\nspeaker improves the accuracy. Our research question is whether a DNN for\nspeech enhancement can be adopted to unknown speakers without any auxiliary\nguidance signal in test-phase. To achieve this, we adopt multi-task learning of\nspeech enhancement and speaker identification, and use the output of the final\nhidden layer of speaker identification branch as an auxiliary feature. In\naddition, we use multi-head self-attention for capturing long-term dependencies\nin the speech and noise. Experimental results on a public dataset show that our\nstrategy achieves the state-of-the-art performance and also outperform\nconventional methods in terms of subjective quality.",
          "arxiv_id": "2002.05873v1"
        }
      ],
      "34": [
        {
          "title": "Advance Real-time Detection of Traffic Incidents in Highways using Vehicle Trajectory Data",
          "year": "2024-08",
          "abstract": "A significant number of traffic crashes are secondary crashes that occur\nbecause of an earlier incident on the road. Thus, early detection of traffic\nincidents is crucial for road users from safety perspectives with a potential\nto reduce the risk of secondary crashes. The wide availability of GPS devices\nnow-a-days gives an opportunity of tracking and recording vehicle trajectories.\nThe objective of this study is to use vehicle trajectory data for advance\nreal-time detection of traffic incidents on highways using machine\nlearning-based algorithms. The study uses three days of unevenly sequenced\nvehicle trajectory data and traffic incident data on I-10, one of the most\ncrash-prone highways in Louisiana. Vehicle trajectories are converted to\ntrajectories based on virtual detector locations to maintain spatial uniformity\nas well as to generate historical traffic data for machine learning algorithms.\nTrips matched with traffic incidents on the way are separated and along with\nother trips with similar spatial attributes are used to build a database for\nmodeling. Multiple machine learning algorithms such as Logistic Regression,\nRandom Forest, Extreme Gradient Boost, and Artificial Neural Network models are\nused to detect a trajectory that is likely to face an incident in the\ndownstream road section. Results suggest that the Random Forest model achieves\nthe best performance for predicting an incident with reasonable recall value\nand discrimination capability.",
          "arxiv_id": "2408.16773v1"
        },
        {
          "title": "Space Meets Time: Local Spacetime Neural Network For Traffic Flow Forecasting",
          "year": "2021-09",
          "abstract": "Traffic flow forecasting is a crucial task in urban computing. The challenge\narises as traffic flows often exhibit intrinsic and latent spatio-temporal\ncorrelations that cannot be identified by extracting the spatial and temporal\npatterns of traffic data separately. We argue that such correlations are\nuniversal and play a pivotal role in traffic flow. We put forward {spacetime\ninterval learning} as a paradigm to explicitly capture these correlations\nthrough a unified analysis of both spatial and temporal features. Unlike the\nstate-of-the-art methods, which are restricted to a particular road network, we\nmodel the universal spatio-temporal correlations that are transferable from\ncities to cities. To this end, we propose a new spacetime interval learning\nframework that constructs a local-spacetime context of a traffic sensor\ncomprising the data from its neighbors within close time points. Based on this\nidea, we introduce local spacetime neural network (STNN), which employs novel\nspacetime convolution and attention mechanism to learn the universal\nspatio-temporal correlations. The proposed STNN captures local traffic\npatterns, which does not depend on a specific network structure. As a result, a\ntrained STNN model can be applied on any unseen traffic networks. We evaluate\nthe proposed STNN on two public real-world traffic datasets and a simulated\ndataset on dynamic networks. The experiment results show that STNN not only\nimproves prediction accuracy by 4% over state-of-the-art methods, but is also\neffective in handling the case when the traffic network undergoes dynamic\nchanges as well as the superior generalization capability.",
          "arxiv_id": "2109.05225v2"
        },
        {
          "title": "Newell's theory based feature transformations for spatio-temporal traffic prediction",
          "year": "2023-07",
          "abstract": "Deep learning (DL) models for spatio-temporal traffic flow forecasting employ\nconvolutional or graph-convolutional filters along with recurrent neural\nnetworks to capture spatial and temporal dependencies in traffic data. These\nmodels, such as CNN-LSTM, utilize traffic flows from neighboring detector\nstations to predict flows at a specific location of interest. However, these\nmodels are limited in their ability to capture the broader dynamics of the\ntraffic system, as they primarily learn features specific to the detector\nconfiguration and traffic characteristics at the target location. Hence, the\ntransferability of these models to different locations becomes challenging,\nparticularly when data is unavailable at the new location for model training.\nTo address this limitation, we propose a traffic flow physics-based feature\ntransformation for spatio-temporal DL models. This transformation incorporates\nNewell's uncongested and congested-state estimators of traffic flows at the\ntarget locations, enabling the models to learn broader dynamics of the system.\nOur methodology is empirically validated using traffic data from two different\nlocations. The results demonstrate that the proposed feature transformation\nimproves the models' performance in predicting traffic flows over different\nprediction horizons, as indicated by better goodness-of-fit statistics. An\nimportant advantage of our framework is its ability to be transferred to new\nlocations where data is unavailable. This is achieved by appropriately\naccounting for spatial dependencies based on station distances and various\ntraffic parameters. In contrast, regular DL models are not easily transferable\nas their inputs remain fixed. It should be noted that due to data limitations,\nwe were unable to perform spatial sensitivity analysis, which calls for further\nresearch using simulated data.",
          "arxiv_id": "2307.05949v2"
        }
      ]
    }
  },
  "metadata": {
    "lastUpdated": "2025-09-24T22:47:48Z",
    "dataVersion": "0.0.2",
    "period": {
      "start": "2020-01",
      "end": "2025-09"
    }
  }
}